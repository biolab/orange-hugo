<html><head><title>Orange Data Mining - Orange Fairness - Dataset Bias</title><meta property="og:title" content="Orange Fairness - Dataset Bias"><meta property="og:description" content="Orange now supports methods for detecting and mitigating bias in machine learning."><meta property="og:image" content="/blog_img/2023/2023-09-18-fairness-dataset-bias-thumb.png"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Orange Data Mining Toolbox"><meta name=author content="Bioinformatics Laboratory, University of Ljubljana"><meta name=google-site-verification content="DS7GH5M7ABi78pIC2rMcTBFx-UeBFObXOeLvR7Ow3EQ"><link rel="shortcut icon" href=/images/favicon.ico><link href='https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic' rel=stylesheet type=text/css><link href='https://fonts.googleapis.com/css?family=Covered+By+Your+Grace' rel=stylesheet type=text/css><link rel=stylesheet href=/plugins/bootstrap/css/bootstrap.min.css><link rel=stylesheet href=/plugins/font-awesome/css/font-awesome.css><link rel=stylesheet href=/plugins/lightgallery/css/lightgallery.css><link href=https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css rel=stylesheet><link rel=stylesheet href=/scss/main.58e8531e166454b3665e4c7132aa2f1762a5569c179efa4bbf56b266ce93bfac.css><script id=dsq-count-scr src=//orange-4.disqus.com/count.js async></script><noscript><style>.jsonly{display:none!important}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-J6PJZF75EX"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-J6PJZF75EX",{anonymize_ip:!1})}</script><script type=text/javascript src=/plugins/jquery-1.10.2.min.js></script>
<script type=text/javascript src=/plugins/jquery.form.js></script>
<script type=text/javascript src=/plugins/bootstrap/js/bootstrap.min.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lightgallery.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lg-thumbnail.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lg-video.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lg-zoom.js></script>
<script type=text/javascript src=/js/custom.js></script>
<script type=text/javascript src=/js/header_helpers.js></script>
<script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-T5X4T27")</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5X4T27" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><script type=text/javascript>$(document).ready(function(){$("a").off("click").click(function(e){ga("send","event","a",e.type,$(this).attr("id"),1)}),$("[class *= trackClicks_]").off("click").click(function(e){var n=$(this).attr("class").split(/\s+/),t="";$.each(n,function(e,n){n.indexOf("trackClicks_")===0&&(t=n)}),ga("send","event","a",e.type,t,1)})})</script></head><body><header id=top class="header navbar-fixed-top"><div class=container><h1 class="logo pull-left"><a id=header-orangelogo href=/><img id=logo-image class=logo-image src=/images/orange_logo_hq.png alt=Logo></a></h1><nav id=main-nav class="main-nav navbar-right" role=navigation><div class=navbar-header><button class=navbar-toggle type=button data-toggle=collapse data-target=#navbar-collapse>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="navbar-collapse collapse" id=navbar-collapse><ul class="nav navbar-nav"><li class=nav-item><a id=header-screenshots-link href=/screenshots/>Screenshots</a></li><li class=nav-item><a id=header-screenshots-link href=/workflows/>Workflows</a></li><li class=nav-item><a id=header-screenshots-link href=/download/>Download</a></li><li class=nav-item><a id=header-screenshots-link href=/blog/>Blog</a></li><li class=nav-item><a id=header-screenshots-link href=/docs/>Docs</a></li><li class=nav-item><a id=header-screenshots-link href=/training/>Workshops</a></li><li class=nav-search-wrapper><div><a class="btn nav-btn-search btn-warning nav-btn fa fa-search nav-btn-desktop" id=search-btn-js role=button></a><a class="btn nav-btn-search btn-warning nav-btn fa fa-search nav-btn-mobile" role=button href="/search/?q="></a><input type=text class="header-search-input jsonly search-widget-workflow nav-item" id=search-header placeholder=Search tabindex=1 onkeydown=check_key_header(event) hidden autofocus></div></li><li><div class="donate-button transform_header donate_div"><a class="btn btn-warning nav-btn" id=header-donate-link role=button target=_blank href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=A76TAX87ZVR3J">Donate</a></div></li></ul></div></nav></div></header><div id=main><div class="container-fluid clear-top"><div id=overflow-container class=overflow-container><div id=loop-container class="offset-header container"><div class="blog-font-size post type-post status-publish format-standard category-standard category-travel full-without-featured odd excerpt-1 no-image"><div class="entry-meta remove-padding-l-r"><span>Categories:</span>
<span class=category><a href=/blog/fairness>fairness</a></span>
<span class=category><span>|</span>
<a href=/blog/dataset-bias>dataset bias</a></span></div><div class='entry-header remove-padding-l-r'><h1 class=entry-title>Orange Fairness - Dataset Bias</h1><span class=author-blog>By: Žan Mervič,
Sep 18, 2023</span></div><div class="entry-container remove-padding-l-r"><div class="entry-content md"><article><div id=blog_images_><h2 id=fairness-in-machine-learning>Fairness in Machine Learning</h2><p>Artificial intelligence and machine learning are increasingly used in everyday decisions that deeply impact individuals. This includes areas like employment, court sentencing, and credit application approvals. It&rsquo;s crucial that these decision-making tools don&rsquo;t favor certain demographic groups over others.</p><p>Bias in machine learning can stem from multiple sources. It might arise from skewed training data or the model&rsquo;s design. This bias can result in unequal model outcomes: unprivileged groups might receive lower salaries despite having similar qualifications as privileged ones or face longer prison sentences for identical crimes. Such biases can significantly affect individuals&rsquo; lives.</p><p>For example, in the &ldquo;Adult&rdquo; dataset, a notable bias exists between male and female genders. When predicting if a person&rsquo;s salary is &ldquo;>50K&rdquo; or &ldquo;&lt;=50K&rdquo;, females tend to be classified as &ldquo;&lt;=50K&rdquo; more frequently than their male counterparts with similar attributes. This disparity showcases the dataset&rsquo;s bias, which can lead to biased and unfair predictions.</p><h3 id=but-why-is-it-a-problem-to-have-models-that-reflect-real-world-bias>But why is it a problem to have models that reflect real-world bias?</h3><p>In essence, machine learning models that merely mimic real-world biases perpetuate those biases, resulting in a vicious cycle. For instance, if we were to train a model on a dataset reflecting gender wage disparities and then use this model to automate salary decisions, we would reinforce the same disparity. This is not about denying the reality of the dataset but recognizing that our goal in AI and machine learning should be to achieve fairer, more equitable outcomes. If models are only used to replicate existing societal conditions, then we miss the opportunity for technology to be a force for positive change.</p><p>Furthermore, creating models that actively seek to reduce or eliminate bias is essential for maintaining trust in AI and machine learning systems. When people see these systems merely replicating past biases, they are less likely to trust and adopt them. In contrast, fairness-aware models can be trusted to make decisions that prioritize justice and equity over merely echoing the patterns in the data.</p><p>Recognizing these challenges, there has been a surge in efforts to mitigate this bias. The outcome has been the development of models that aim for balanced outcomes for all demographic groups or maintain consistent true positive rates across them. Mitigating bias might mean a slight dip in overall accuracy, but it is a necessary trade-off to ensure fairness in our models. Using fairness algorithms is ethical and highly beneficial to ensure people&rsquo;s trust in machine learning and AI.</p><h2 id=introducing-the-fairness-add-on-in-orange>Introducing the Fairness add-on in Orange</h2><p>Hopefully, the above has emphasized the importance of fairness in machine learning. With this in mind, let us discuss the new Orange fairness add-on. It aims to empower users with tools to detect and mitigate bias in machine-learning workflows. At the time of writing, the add-on includes seven widgets, four of which are used for bias detection and mitigation. The remaining three are used to support the other widgets and provide additional functionality.</p><p>In this blog and the following ones, we will familiarize ourselves with the add-on widgets and their use cases. We will also demonstrate how to use them in real-world scenarios and explain relevant concepts. Let us take a look at the first two widgets in more detail.</p><h2 id=exploring-the-widgets-dataset-bias-and-as-fairness>Exploring the Widgets: Dataset Bias and As Fairness</h2><p>This blog will introduce the first two fairness widgets: As Fairness Data and Dataset Bias. Both aim to address the different bias identification and correction stages in machine learning workflows.</p><h4 id=1-as-fairness-widget>1. As Fairness Widget:</h4><p>The As Fairness Widget does not do anything about fairness on its own. Instead, it allows users to designate fairness attributes in the dataset, which are essential for other fairness algorithms to operate.</p><p>When inputting a dataset into the widget, options for three attributes will appear:</p><ul><li><p>Favorable Class Value: Define which class value is viewed as favorable.</p></li><li><p>Protected Attribute: Select the dataset feature representing or containing potentially biased groups or those for which fair predictions are desired, such as race, gender, or age.</p></li><li><p>Privileged Protected Attribute Values: Specify values within the protected attribute deemed privileged.</p></li></ul><a href=/blog_img/2023/2023-09-18-fairness-dataset-bias-as-fairness-data.png class=blog-image_><img src=/blog_img/2023/2023-09-18-fairness-dataset-bias-as-fairness-data.png class=window-screenshot></a><h4 id=2-dataset-bias-widget>2. Dataset Bias Widget:</h4><p>Before training a model, it is crucial to understand if the data itself is skewed. The Dataset Bias widget aids in precisely that. After giving it a dataset as input, the widget will calculate two fairness metrics according to the dataset:</p><ul><li>Disparate Impact (DI): Measures the ratio of favorable outcomes for an unprivileged group to that of the privileged group. An ideal value of 1.0 means the ratio of favorable outcomes is the same for both groups.<ul><li>DI &lt; 1.0: The privileged group receives favorable outcomes at a higher rate.</li><li>DI > 1.0: The privileged group receives favorable outcomes at a lower rate.</li></ul></li><li>Statistical Parity Difference (SPD): This is very similar to disparate impact. Instead of the ratio, it measures the difference in favorable outcomes between the unprivileged and the privileged groups. An ideal value for this metric is 0.<ul><li>SPD &lt; 0: The privileged group has a higher rate of favorable outcomes.</li><li>SPD > 0: The privileged group has a lower rate of favorable outcomes.</li></ul></li></ul><a href=/blog_img/2023/2023-09-18-fairness-dataset-bias.png class=blog-image_><img src=/blog_img/2023/2023-09-18-fairness-dataset-bias.png class=window-screenshot></a><p>For anyone interested in learning more about these metrics, other fairness metrics and ai fairness in general, we recommend reading the article <a href=https://arxiv.org/pdf/1908.09635.pdf>A Survey on Bias and Fairness in Machine Learning</a>.</p><h2 id=orange-use-case>Orange use case</h2><p>Now that we understand the functions of the As Fairness and Dataset Bias widgets, let us examine their application in a real-world scenario.</p><p>For this example, we will utilize the <a href=https://archive.ics.uci.edu/ml/datasets/adult>Adult dataset</a>, which is among the most popular in the machine learning community. It consists of 48824 instances with 15 attributes describing demographic details from the 1994 census. These attributes include six numerical and seven categorical factors such as age, employment type, education, marital status, occupation, race, gender, capital gain, capital loss, weekly working hours, and native country. The primary task is to predict if an individual earns more than $50,000 annually. This dataset&rsquo;s most commonly protected attributes are gender, age, and race.</p><p>Using the As Fairness Data widget, we will define &ldquo;sex&rdquo; as the protected attribute and &ldquo;male&rdquo; as the privileged, protected attribute value. Subsequently, we will employ the Dataset Bias widget to compute the fairness metrics and a Box Plot for visualization.</p><a href=/blog_img/2023/2023-09-18-fairness-dataset-bias-use-case.png class=blog-image_><img src=/blog_img/2023/2023-09-18-fairness-dataset-bias-use-case.png class=window-screenshot></a><p>The results show that the dataset exhibits bias: the disparate impact is 0.358, and the statistical parity difference stands at -0.196. In an ideal scenario signifying no bias, these values would be 1 and 0, respectively. We can further visualize the dataset&rsquo;s bias using the Box Plot.</p><a href=/blog_img/2023/2023-09-18-fairness-dataset-bias-box-plot.png class=blog-image_><img src=/blog_img/2023/2023-09-18-fairness-dataset-bias-box-plot.png class=window-screenshot></a><p>The Box Plot illustrates the distribution of the target variable across both privileged and unprivileged groups. Hovering over the plot reveals that only 10.95% of the females in the dataset belong to the favorable class, in contrast to the 30.57% of males. By dividing the percentage of the favorable class for the unprivileged group by that of the privileged group, we arrive at a disparate impact value of 0.358—precisely what the Dataset Bias widget determined. We can do a similar calculation for the statistical parity difference.</p></div></article></div></div></div></div></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://orange-4.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div><script type=text/javascript>$(document).ready(function(){$("#blog_images_").lightGallery({thumbnail:!1,selector:"a:has(img)",width:"60%",mode:"lg-fade",counter:!1,download:!1,enableSwipe:!1,enableDrag:!1,speed:1,useLeft:!0,hideBarsDelay:1e3})})</script></div></div><footer class=footer><div class=container><div class=row><div class="col-md-4 col-sm-6 col-xs-12"><div class=row><div class="col-sm-4 sitemap-group"><ul class="links list-sitemap"><li><b><div class=sitemap-cat>Orange</div></b></li><li><a class=sitemap-sub href=/faq/>FAQ</a></li><li><a class=sitemap-sub href=/license/>License</a></li><li><a class=sitemap-sub href=/privacy/>Privacy</a></li><li><a class=sitemap-sub href=/citation/>Citation</a></li><li><a class=sitemap-sub href=/contact/>Contact</a></li></ul></div><div class="col-sm-4 sitemap-group"><ul class="links list-sitemap"><li><div class=sitemap-cat>Download</div></li><li><a class=sitemap-sub href=/download/#windows>Windows</a></li><li><a class=sitemap-sub href=/download/#macos>Mac OS</a></li></ul></div><div class="col-sm-4 sitemap-group"><ul class="links list-sitemap"><li><b><div class=sitemap-cat>Community</div></b></li><li><a class=sitemap-sub href=https://www.facebook.com/orangedatamining target=_blank>Facebook</a></li><li><a class=sitemap-sub href=https://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g target=_blank>YouTube</a></li><li><a class=sitemap-sub href=https://twitter.com/orangedataminer target=_blank>Twitter</a></li><li><a class=sitemap-sub href=https://datascience.stackexchange.com/questions/tagged/orange target=_blank>Stack Exchange</a></li><li><a class=sitemap-sub href=https://discord.gg/FWrfeXV target=_blank>Discord</a></li></ul></div></div></div><div class="col-md-3 col-sm-6 col-xs-12"><div class=row><div class="col-sm-5 col-md-7 col-lg-6 sitemap-group docs-devs"><ul class="links list-sitemap"><li><div class=sitemap-cat>Documentation</div></li><li><a class=sitemap-sub href=/getting-started/>Get started</a></li><li><a class=sitemap-sub href=/widget-catalog/>Widgets</a></li><li><a class=sitemap-sub href=http://docs.biolab.si/3/data-mining-library/>Scripting</a></li></ul></div><div class="col-sm-3 sitemap-group"><ul class="links list-sitemap"><li><b><div class=sitemap-cat>Developers</div></b></li><li><a class=sitemap-sub target=_blank href=https://github.com/biolab/orange3>GitHub</a></li><li><a class=sitemap-sub target=_blank href=http://docs.biolab.si/3/development/>Getting Started</a></li></ul></div></div></div><div class="col-md-5 col-sm-12 col-xs-12 sitemap-group"><div class=latest-blog-posts><ul class="links list-sitemap" style=padding-bottom:10px><li><b><a class=sitemap-cat href=/blog/>Latest blog posts</a></b></li><li><div class=row><div class=col-md-2><a class=sitemap-sub>19 Sep</a></div><div class=col-md-10><a class=sitemap-sub style=color:#f79211 href=/blog/2023/2023-09-19-fairness-reweighing-preprocessor/>Orange Fairness - Reweighing as a preprocessor</a></div></div></li><li><div class=row><div class=col-md-2><a class=sitemap-sub>19 Sep</a></div><div class=col-md-10><a class=sitemap-sub style=color:#f79211 href=/blog/2023/2023-09-19-fairness-reweighing-dataset/>Orange Fairness - Reweighing a Dataset</a></div></div></li><li><div class=row><div class=col-md-2><a class=sitemap-sub>18 Sep</a></div><div class=col-md-10><a class=sitemap-sub style=color:#f79211 href=/blog/2023/2023-09-18-fairness-dataset-bias/>Orange Fairness - Dataset Bias</a></div></div></li></ul></div></div></div><div class="row footer-pad"><div class=col-xs-12><small class="copyright pull-left">Copyright © University of Ljubljana</small></div></div></div></footer></body></html>