<html><head><title>Orange Data Mining - Orange Fairness - Adversarial Debiasing</title><meta property="og:title" content="Orange Fairness - Adversarial Debiasing"><meta property="og:description" content="Learn how to use the Adversarial Debiasing model in Orange for fairer machine learning."><meta property="og:image" content="/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-thumb.png"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Orange Data Mining Toolbox"><meta name=author content="Bioinformatics Laboratory, University of Ljubljana"><meta name=google-site-verification content="DS7GH5M7ABi78pIC2rMcTBFx-UeBFObXOeLvR7Ow3EQ"><link rel="shortcut icon" href=/images/favicon.ico><link href='https://fonts.googleapis.com/css?family=Lato:300,400,700,300italic,400italic,700italic' rel=stylesheet type=text/css><link href='https://fonts.googleapis.com/css?family=Covered+By+Your+Grace' rel=stylesheet type=text/css><link rel=stylesheet href=/plugins/bootstrap/css/bootstrap.min.css><link rel=stylesheet href=/plugins/font-awesome/css/font-awesome.css><link rel=stylesheet href=/plugins/lightgallery/css/lightgallery.css><link href=https://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css rel=stylesheet><link rel=stylesheet href=/scss/main.58e8531e166454b3665e4c7132aa2f1762a5569c179efa4bbf56b266ce93bfac.css><script id=dsq-count-scr src=//orange-4.disqus.com/count.js async></script><noscript><style>.jsonly{display:none!important}</style></noscript><script async src="https://www.googletagmanager.com/gtag/js?id=G-J6PJZF75EX"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-J6PJZF75EX",{anonymize_ip:!1})}</script><script type=text/javascript src=/plugins/jquery-1.10.2.min.js></script>
<script type=text/javascript src=/plugins/jquery.form.js></script>
<script type=text/javascript src=/plugins/bootstrap/js/bootstrap.min.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lightgallery.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lg-thumbnail.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lg-video.js></script>
<script type=text/javascript src=/plugins/lightgallery/js/lg-zoom.js></script>
<script type=text/javascript src=/js/custom.js></script>
<script type=text/javascript src=/js/header_helpers.js></script>
<script>(function(e,t,n,s,o){e[s]=e[s]||[],e[s].push({"gtm.start":(new Date).getTime(),event:"gtm.js"});var a=t.getElementsByTagName(n)[0],i=t.createElement(n),r=s!="dataLayer"?"&l="+s:"";i.async=!0,i.src="https://www.googletagmanager.com/gtm.js?id="+o+r,a.parentNode.insertBefore(i,a)})(window,document,"script","dataLayer","GTM-T5X4T27")</script><noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-T5X4T27" height=0 width=0 style=display:none;visibility:hidden></iframe></noscript><script type=text/javascript>$(document).ready(function(){$("a").off("click").click(function(e){ga("send","event","a",e.type,$(this).attr("id"),1)}),$("[class *= trackClicks_]").off("click").click(function(e){var n=$(this).attr("class").split(/\s+/),t="";$.each(n,function(e,n){n.indexOf("trackClicks_")===0&&(t=n)}),ga("send","event","a",e.type,t,1)})})</script></head><body><header id=top class="header navbar-fixed-top"><div class=container><h1 class="logo pull-left"><a id=header-orangelogo href=/><img id=logo-image class=logo-image src=/images/orange_logo_hq.png alt=Logo></a></h1><nav id=main-nav class="main-nav navbar-right" role=navigation><div class=navbar-header><button class=navbar-toggle type=button data-toggle=collapse data-target=#navbar-collapse>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button></div><div class="navbar-collapse collapse" id=navbar-collapse><ul class="nav navbar-nav"><li class=nav-item><a id=header-screenshots-link href=/screenshots/>Screenshots</a></li><li class=nav-item><a id=header-screenshots-link href=/workflows/>Workflows</a></li><li class=nav-item><a id=header-screenshots-link href=/download/>Download</a></li><li class=nav-item><a id=header-screenshots-link href=/blog/>Blog</a></li><li class=nav-item><a id=header-screenshots-link href=/docs/>Docs</a></li><li class=nav-item><a id=header-screenshots-link href=/training/>Workshops</a></li><li class=nav-search-wrapper><div><a class="btn nav-btn-search btn-warning nav-btn fa fa-search nav-btn-desktop" id=search-btn-js role=button></a><a class="btn nav-btn-search btn-warning nav-btn fa fa-search nav-btn-mobile" role=button href="/search/?q="></a><input type=text class="header-search-input jsonly search-widget-workflow nav-item" id=search-header placeholder=Search tabindex=1 onkeydown=check_key_header(event) hidden autofocus></div></li><li><div class="donate-button transform_header donate_div"><a class="btn btn-warning nav-btn" id=header-donate-link role=button target=_blank href="https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&amp;hosted_button_id=A76TAX87ZVR3J">Donate</a></div></li></ul></div></nav></div></header><div id=main><div class="container-fluid clear-top"><div id=overflow-container class=overflow-container><div id=loop-container class="offset-header container"><div class="blog-font-size post type-post status-publish format-standard category-standard category-travel full-without-featured odd excerpt-1 no-image"><div class="entry-meta remove-padding-l-r"><span>Categories:</span>
<span class=category><a href=/blog/fairness>fairness</a></span>
<span class=category><span>|</span>
<a href=/blog/adversarial-debiasing>adversarial debiasing</a></span></div><div class='entry-header remove-padding-l-r'><h1 class=entry-title>Orange Fairness - Adversarial Debiasing</h1><span class=author-blog>By: Žan Mervič,
Sep 19, 2023</span></div><div class="entry-container remove-padding-l-r"><div class="entry-content md"><article><div id=blog_images_><p>In the <a href=/blog/2023/2023-09-19-fairness-reweighing-preprocessor/>previous blog post</a>, we talked about how to use the Reweighing widget as a preprocessor for a model. This blog post will discuss the Adversarial Debiasing model, a bias-aware model. We will also show how to use it in Orange.</p><h3 id=adversarial-debiasing>Adversarial Debiasing:</h3><p><a href=https://arxiv.org/abs/1801.07593>Adversarial Debiasing</a> is an in-processing type of fairness mitigation algorithm. It is a technique that uses adversarial training to mitigate bias. It involves simultaneous training of a predictor and a discriminator. The goal of the predictor is to predict the target variable accurately. At the same time, the discriminator aims to predict the protected variable (such as gender or race) based on the predictor&rsquo;s predictions. The main goal is to maximize the predictor&rsquo;s ability to predict the target variable while reducing the discriminator&rsquo;s ability to predict the protected variable based on those predictions. Because of the Adversarial Debiasing implementation in the <a href=https://aif360.res.ibm.com/>AIF360</a> package we are using, the algorithm focuses on Disparate Impact and Statistical Parity Difference fairness metrics.</p><p>The widget&rsquo;s interface is shown in the image below.</p><a href=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing.png class=blog-image_><img src=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing.png class=window-screenshot></a><p>As seen from the image, there are some unique parameters for this widget:</p><ul><li>Neurons in hidden layers: The number of neurons in each of the hidden layers of the neural network.</li><li>Use Debiasing: Whether to use the debiasing or not. The widget will function as a regular neural network model if this option is not selected.</li><li>Adversary loss weight: The weight of the adversary loss in the total loss function. The adversary loss is the loss function of the discriminator. The higher the weight, the more the model will try to reduce the discriminator&rsquo;s ability to predict the protected variable at the expense of the predictor&rsquo;s ability to predict the target variable.</li></ul><h2 id=orange-use-case>Orange use case</h2><p>Now that we know how the Adversarial Debiasing widget works and how to use it, let us look at a real-world example for a classification task.</p><p>For this example, we will use the <a href=https://archive.ics.uci.edu/ml/datasets/adult>Adult dataset</a>, which we have used <a href=/blog/2023/2023-08-23-fairness-dataset-bias/>before</a>. The Adult dataset consists of 48824 instances with 15 attributes describing demographic details from the 1994 census. The primary task is to predict if an individual earns more than $50,000 annually. Unlike previously, we will not use the As Fairness widget to select fairness attributes; instead, we will keep the default ones, &ldquo;sex&rdquo; for the protected attribute and &ldquo;male&rdquo; for the privileged protected attribute value.</p><p>We will train two Adversarial Debiasing models, one with and one without debiasing, and compare them to Random Forests.</p><a href=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-use-case.png class=blog-image_><img src=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-use-case.png class=window-screenshot></a>
<a href=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-scores.png class=blog-image_><img src=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-scores.png class=window-screenshot></a><p>Test and Score shows that debiasing improved fairness metrics, particularly Disparate Impact, and Statistical Parity Difference. Without debiasing, the results are similar to that of the Random Forest model. Disparate Impact moved from 0.294 to 1.051, while Statistical Parity Difference went from -0.180 to 0.006, indicating a near-zero bias between these groups regarding favorable outcomes.</p><p>However, this does not mean all forms of bias were addressed. Equal Opportunity Difference and Average Odds Difference metrics worsened from -0.097 to 0.358 and -0.087 to 0.197, respectively. This suggests that even though the algorithm has been optimized for specific fairness criteria, other biases have emerged or become more pronounced.</p><p>Why is this happening?</p><ul><li><p>Trade-offs in fairness: Addressing one type of fairness sometimes comes at the expense of another. When we optimize for Disparate Impact or Statistical Parity Difference, it may affect how often positive outcomes are correctly predicted for both privileged and unprivileged groups (equal opportunity). Moreover, it could alter the balance between true positive rates and false positive rates for these groups (average odds difference).</p></li><li><p>Adversarial Debiasing implementation: The current algorithm implementation aims to make predictions independent of protected attributes, focusing on balancing outcomes. This approach inherently focuses on balancing outcomes (reflected in the Disparate Impact and Statistical Parity Difference metrics). However, the nuances of other fairness measures might not be as effectively addressed.</p></li></ul><p>When we use debiasing, there can also be a slight decrease in accuracy. This happens because the model is now not only trying to be accurate but also fair. This balance is a necessary trade-off we accept when we want to remove bias. However, it is worth noting that in this example, the accuracy remains comparable to the scenario without debiasing.</p><p>Next, let us look at the Box Plot widget. We will use it to show the Disparate Impact and Statistical Parity Difference, much like our previous blog approach.</p><a href=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-box-plot-bias.png class=blog-image_><img src=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-box-plot-bias.png class=window-screenshot></a>
<a href=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-box-plot-debias.png class=blog-image_><img src=/blog_img/2023/2023-09-19-fairness-adversarial-debiasing-box-plot-debias.png class=window-screenshot></a><p>From the first box plot we can see that when using the model without debiasing, males (the privileged group) tends to get classified with &ldquo;>50K&rdquo; (the favorable class) more often than females (the unprivileged grouo) do. This is reflected in the Disparate Impact and Statistical Parity Difference metrics which are 0.294 and -0.180, respectively, both below their optimal value, indicating bias towards the unprivileged group.</p><p>In the second box plot, we can see that when using the model with debiasing, males and females get classified with the favorable class at a very similar rate. This is also indicated by the Disparate Impact and Statistical Parity Difference metrics which are 1.051 and 0.006, respectively, both very close to their optimal value, indicating a negligible amount of bias towards the privileged group.</p></div></article></div></div></div></div></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://orange-4.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript></div><script type=text/javascript>$(document).ready(function(){$("#blog_images_").lightGallery({thumbnail:!1,selector:"a:has(img)",width:"60%",mode:"lg-fade",counter:!1,download:!1,enableSwipe:!1,enableDrag:!1,speed:1,useLeft:!0,hideBarsDelay:1e3})})</script></div></div><footer class=footer><div class=container><div class=row><div class="col-md-4 col-sm-6 col-xs-12"><div class=row><div class="col-sm-4 sitemap-group"><ul class="links list-sitemap"><li><b><div class=sitemap-cat>Orange</div></b></li><li><a class=sitemap-sub href=/faq/>FAQ</a></li><li><a class=sitemap-sub href=/license/>License</a></li><li><a class=sitemap-sub href=/privacy/>Privacy</a></li><li><a class=sitemap-sub href=/citation/>Citation</a></li><li><a class=sitemap-sub href=/contact/>Contact</a></li></ul></div><div class="col-sm-4 sitemap-group"><ul class="links list-sitemap"><li><div class=sitemap-cat>Download</div></li><li><a class=sitemap-sub href=/download/#windows>Windows</a></li><li><a class=sitemap-sub href=/download/#macos>Mac OS</a></li></ul></div><div class="col-sm-4 sitemap-group"><ul class="links list-sitemap"><li><b><div class=sitemap-cat>Community</div></b></li><li><a class=sitemap-sub href=https://www.facebook.com/orangedatamining target=_blank>Facebook</a></li><li><a class=sitemap-sub href=https://www.youtube.com/channel/UClKKWBe2SCAEyv7ZNGhIe4g target=_blank>YouTube</a></li><li><a class=sitemap-sub href=https://twitter.com/orangedataminer target=_blank>Twitter</a></li><li><a class=sitemap-sub href=https://datascience.stackexchange.com/questions/tagged/orange target=_blank>Stack Exchange</a></li><li><a class=sitemap-sub href=https://discord.gg/FWrfeXV target=_blank>Discord</a></li></ul></div></div></div><div class="col-md-3 col-sm-6 col-xs-12"><div class=row><div class="col-sm-5 col-md-7 col-lg-6 sitemap-group docs-devs"><ul class="links list-sitemap"><li><div class=sitemap-cat>Documentation</div></li><li><a class=sitemap-sub href=/getting-started/>Get started</a></li><li><a class=sitemap-sub href=/widget-catalog/>Widgets</a></li><li><a class=sitemap-sub href=http://docs.biolab.si/3/data-mining-library/>Scripting</a></li></ul></div><div class="col-sm-3 sitemap-group"><ul class="links list-sitemap"><li><b><div class=sitemap-cat>Developers</div></b></li><li><a class=sitemap-sub target=_blank href=https://github.com/biolab/orange3>GitHub</a></li><li><a class=sitemap-sub target=_blank href=http://docs.biolab.si/3/development/>Getting Started</a></li></ul></div></div></div><div class="col-md-5 col-sm-12 col-xs-12 sitemap-group"><div class=latest-blog-posts><ul class="links list-sitemap" style=padding-bottom:10px><li><b><a class=sitemap-cat href=/blog/>Latest blog posts</a></b></li><li><div class=row><div class=col-md-2><a class=sitemap-sub>19 Sep</a></div><div class=col-md-10><a class=sitemap-sub style=color:#f79211 href=/blog/2023/2023-09-19-fairness-hiding-protected-attribute/>Why Removing Features Isn't Enough</a></div></div></li><li><div class=row><div class=col-md-2><a class=sitemap-sub>19 Sep</a></div><div class=col-md-10><a class=sitemap-sub style=color:#f79211 href=/blog/2023/2023-09-19-fairness-reweighing-preprocessor/>Orange Fairness - Reweighing as a preprocessor</a></div></div></li><li><div class=row><div class=col-md-2><a class=sitemap-sub>19 Sep</a></div><div class=col-md-10><a class=sitemap-sub style=color:#f79211 href=/blog/2023/2023-09-19-fairness-reweighing-dataset/>Orange Fairness - Reweighing a Dataset</a></div></div></li></ul></div></div></div><div class="row footer-pad"><div class=col-xs-12><small class="copyright pull-left">Copyright © University of Ljubljana</small></div></div></div></footer></body></html>