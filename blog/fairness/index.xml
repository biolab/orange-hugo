<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>fairness on Orange</title><link>/blog/fairness/</link><description>Recent content in fairness on Orange</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 18 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="/blog/fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Orange Fairness - Dataset Bias</title><link>/blog/2023/2023-09-18-fairness-dataset-bias/</link><pubDate>Mon, 18 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-18-fairness-dataset-bias/</guid><description>Fairness in Machine Learning Artificial intelligence and machine learning are increasingly used in everyday decisions that deeply impact individuals. This includes areas like employment, court sentencing, and credit application approvals. It&amp;rsquo;s crucial that these decision-making tools don&amp;rsquo;t favor certain demographic groups over others.
Bias in machine learning can stem from multiple sources. It might arise from skewed training data or the model&amp;rsquo;s design. This bias can result in unequal model outcomes: unprivileged groups might receive lower salaries despite having similar qualifications as privileged ones or face longer prison sentences for identical crimes.</description></item></channel></rss>