<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>fairness on Orange</title><link>/blog/fairness/</link><description>Recent content in fairness on Orange</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 19 Sep 2023 00:00:00 +0000</lastBuildDate><atom:link href="/blog/fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Orange Fairness - Adversarial Debiasing</title><link>/blog/2023/2023-09-19-fairness-adversarial-debiasing/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-19-fairness-adversarial-debiasing/</guid><description>In the previous blog post, we talked about how to use the Reweighing widget as a preprocessor for a model. This blog post will discuss the Adversarial Debiasing model, a bias-aware model. We will also show how to use it in Orange.
Adversarial Debiasing: Adversarial Debiasing is an in-processing type of fairness mitigation algorithm. It is a technique that uses adversarial training to mitigate bias. It involves simultaneous training of a predictor and a discriminator.</description></item><item><title>Orange Fairness - Equal Odds Postprocessing</title><link>/blog/2023/2023-09-19-fairness-equal-odds-postprocessing/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-19-fairness-equal-odds-postprocessing/</guid><description>In the previous blog post, we discussed the Adversarial Debiasing model, a bias-aware model. This blog post will discuss the Equal Odds Postprocessing widget, a bias-aware post-processor, which can be used with any model to mitigate bias in its predictions.
Equal Odds Postprocessing: The Equal Odds Postprocessing widget is a post-processing type of fairness mitigation algorithm for supervised learning. It modifies the predictions of any given classifier to meet certain fairness criteria, specifically focusing on &amp;ldquo;Equalized Odds&amp;rdquo; or more relaxed criteria like Equal Opportunity.</description></item><item><title>Orange Fairness - Reweighing a Dataset</title><link>/blog/2023/2023-09-19-fairness-reweighing-dataset/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-19-fairness-reweighing-dataset/</guid><description>In the previous blog post, we introduced the Orange fairness addon along with the Dataset Bias and As Fairness widgets. We also demonstrated how to use them to detect bias in a dataset and visualized the results for a better understanding. In this blog, we will introduce the Reweighing widget, which we can use to mitigate bias in a dataset, resulting in fairer machine learning models learning from it.
Reweighing: The Reweighing widget offers a solution to mitigate bias in datasets by assigning weights to individual instances.</description></item><item><title>Orange Fairness - Reweighing as a preprocessor</title><link>/blog/2023/2023-09-19-fairness-reweighing-preprocessor/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-19-fairness-reweighing-preprocessor/</guid><description>In the previous blog post, we introduced the Orange fairness Reweighing widget and used it to reweigh a dataset. In this blog, we will explore another use case for the Reweighing widget: using it as a preprocessor for a specific model.
Fairness metrics: With the fairness addon and widgets that come with it, we also introduced four bias scoring metrics which we can use to evaluate the fairness of model predictions.</description></item><item><title>Why Removing Features Isn't Enough</title><link>/blog/2023/2023-09-19-fairness-hiding-protected-attribute/</link><pubDate>Tue, 19 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-19-fairness-hiding-protected-attribute/</guid><description>Previously, we introduced and explained different fairness algorithms that can be used to mitigate bias in a dataset or model predictions. Here, we will discuss a common misconception: removing the protected attribute from the dataset will remove bias. We show why this is not the case and why it is essential to use fairness algorithms.
Hiding Protected Attribute: Our setup is the following: we have two workflows, and both are using the adult data set.</description></item><item><title>Orange Fairness - Dataset Bias</title><link>/blog/2023/2023-09-18-fairness-dataset-bias/</link><pubDate>Mon, 18 Sep 2023 00:00:00 +0000</pubDate><guid>/blog/2023/2023-09-18-fairness-dataset-bias/</guid><description>Fairness in Machine Learning Artificial intelligence and machine learning are increasingly used in everyday decisions that deeply impact individuals. This includes areas like employment, court sentencing, and credit application approvals. It&amp;rsquo;s crucial that these decision-making tools don&amp;rsquo;t favor certain demographic groups over others.
Bias in machine learning can stem from multiple sources. It might arise from skewed training data or the model&amp;rsquo;s design. This bias can result in unequal model outcomes: unprivileged groups might receive lower salaries despite having similar qualifications as privileged ones or face longer prison sentences for identical crimes.</description></item></channel></rss>