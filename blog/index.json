
[
    
    {
    "uri": "/blog/2022/2022-02-11-edit-image-collection/",
	"title": "Editing the photographs collection with the help of machine learning",
	"categories": ["orange", "image analytics", "images", "machine learning"],
	"description": "",
	"content": "The core element of Orange\u0026rsquo;s image analysis is embedding images in the vector space. Last year, we upgraded the embedding server infrastructure, enabling around ten times faster image embedding.\nWe switched from an old cluster with a fixed number of workers for each embedding to a new infrastructure where workers turn on when needed. It means that when a user sends images to the embedder, it will turn on as many instances as required until processors' cores are available. Inception-v3, VGG16, and VGG19 embedders also perform computation on GPU, making them even faster.\nWe also made some modifications in the implementation on Orange\u0026rsquo;s part, making embedding more reliable.\nHow does embedding work? The Image Embedding widget gets the table with images' metadata such as an image name, location and size, but they are not ready for analysis. Images are sent to the server to be embedded in a format understandable to machine learning algorithms. The server pushes images through a pre-trained deep neural network and returns the number vectors to the widget.\n Deep neural networks are trained on various specific tasks. Inception v3, VGG16, VGG19 and SqueezeNet are trained to classify images into 1000 classes according to the object at the image. Painters are trained to predict paintings' authors, DeepLoc to predict yeast\u0026rsquo;s cells structures and OpenFace for face recognition. We disregard the classification layer of the network and consider the penultimate layer instead and use that for the image\u0026rsquo;s vector-based representation.\nEditing photo collection I have taken many photographs in the last few years, but I have not had the time to edit the collection. I want to sort them into subfolders based on the categories. Can machine learning algorithms and image embeddings help me with that?\nThis blog post will show two approaches for identifying categories and sorting images. The first one is unsupervised, and the second one is supervised.\nFirst, we load photos. We load a folder of images with a collection that we would like to sort in the subfolders (download dataset).\n Both approaches will include embedding images. We connect the Image Embedding widget to the Import Images widget to embed pictures and select Inception v3 embedder.\nOur server deletes all images immediately after embedding. If you would not like to share your photos with our server anyway, you can select the SqueezeNet embedder that embeds images at your computer.\n The Image Embedding widget added 2048 columns with numbers to each image in the data table. Those numbers describe properties that neural networks inferred from pictures. Numbers are not understandable to humans but make sense to machine learning algorithms.\nImage grid will place similar photographs closer on the grid The Image Grid uses t-sne embedding to project images on 2-dimensional plane based on their image embeddings. More similar images will be closer in the plot.\nWe passed embedded images from the Image Embedding to the Image Grid widget.\n Image Grid has placed similar images closer in the grid. For example, in the top right corner are images of food, in the top left corner are photos of cities, on the right side are pictures of mountains, and at the bottom left are images of sunsets. The Image Grid had more difficulties with snowy photos. Some are placed at the bottom (they seem to include some mountain image elements), and three central ones seem to have elements of cities, mountains and snow.\nSince images that belong to the same group are close together, it is easy to mark groups of images manually (colours around images). Marked groups are in the output table as class attributes. We can save them on the computer such that each group is a separate folder with the Save Images widget.\nRelated: Image Analytics: Clustering\nTraining the model to sort images The Image Grid is excellent to discover similar images. Still, it is not practical to sort photos into category groups. If we would like to do that more often, we would always need to assign images to groups manually. Can we have a pipeline that would allow us to make a model that we can reuse later when we take new photos s?\nRelated: Video on image classification\nWe can train a classification model that will mark images with categories.\nWe prepared a separate training dataset with pictures in 5 subfolders. Folders are named: city, food, mountains, sunsets and winter. Names of the folders define categories; once the model is trained, it will always classify images in those categories.\nWe will first test how accurate the model is.\n The Import Images widget loads images from the training dataset (download the training data). Since images are sorted in multiple folders, the widget assigns the folder name to images as a category, which we can see in the Data Table. We embed images with Inception v3 embedder in the Image Embedding widget and performs cross-validation in the Test and Score widget. The model we use is logistic regression. We are happy with the model with AUC 0.992 and classification accuracy of 0.943.\nNow we can train the model that will be used to label an unlabeled set of photos.\n We train the logistic regression model on the training images in the top branch on the labelled training dataset. Data that we want to label (sort in subfolders) are loaded and embedded in the bottom branch. We use the same images as in the first part of the blog. The Predictions widget assigns labels to the images based on the logistic regression model.\nWe use the Image Viewer to inspect assigned labels to images. Labels seem to be meaningful and correct. Now that we have photos labelled, we save them to the folder structure with the Save Images widget such that each folder is one category. Since the Prediction widget places predictions (column named Logistic Regression) in the meta part of the table, we need to first set the attribute as a target with the Select Columns widget \u0026mdash; the Save Images widget uses the target column as a subfolder name.\nWe also connected the Save Model widget to the Logistic Regression. This way, the model is saved for later and can be reused.\nWant to learn more? We showed two ways of analysing images with Orange\u0026rsquo;s Image Analytics module. The Image Grid is a great tool to identify similar images and see them in the grid. The second one needs an extra effort to prepare a training dataset, but this leads to an automatic pipeline to identify categories.\nIf you want to learn more about Image Analysis in Orange watch the tutorial wideos or read related blogs: Image Analytics: Clustering, Clustering of Monet and Manet, and Outliers in Traffic Signs\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2022/2022-02-11-grid-title.jpg",
	"shortExcerpt" : "How to use Orange's image embedding to identify similar images, classify them in categories and make an order in your photographs collection?",
	"longExcerpt" :  "The core element of Orange's image analysis is embedding images in the vector space, which just became a feaster with our infrastructure upgrades. We use this opportunity to show possible ways of analyzing images through observing similar images and classifying them." ,
	"author" : "Primož Godec",
	"summary" : "The core element of Orange\u0026rsquo;s image analysis is embedding images in the vector space. Last year, we upgraded the embedding server infrastructure, enabling around ten times faster image embedding.\nWe switched from an old cluster with a fixed number of workers for each embedding to a new infrastructure where workers turn on when needed. It means that when a user sends images to the embedder, it will turn on as many instances as required until processors' cores are available.",
	"date" : "Feb 11, 2022"
}

    
    , {
    "uri": "/blog/2022/2022-02-01-machine-learning-jargon/",
	"title": "Machine Learning Jargon",
	"categories": ["orange", "education", "machine learning"],
	"description": "",
	"content": "We all know each profession has its own jargon. Most of us remember from any hospital series something like: \u0026ldquo;\u0026ldquo;Pass me the scalpel, stat!\u0026rdquo; Meaning \u0026ldquo;give me the scalpel quickly\u0026rdquo;.\nData scientists are no different. We have our own terminology that requires some getting used to. Here\u0026rsquo;s a list of common terms that we use in machine learning. Some terms are specific to Orange and might not be used elsewhere. The post is intended to get you familiar with concepts used in Orange.\n Data   variable, also known as attribute or feature (sometimes even predictor). In statistics they would be called independent variables. This all refers to descriptions of data samples. If you have patients in rows, that variables are in columns and they describe these patients (i.e. with name, date of birth, cholesterol level, heart rate, and so on).\n  data sample, also known as data instance. These are the rows in your table and represent the objects or subjects you are researching. For example, patients, passengers on Titanic, species of iris, or a children\u0026rsquo;s fairy tales.\n  categorical or discrete variable. This refers to a variable with categories as values, for example eye color, nationality, ticket class (first, second, third), and so on. In Orange, these would be marked with a green C.\n  numeric or continuous variable. This refers to a variable with numbers as values, for example cholesterol level, heart rate, age, and so on. In Orange, these would be marked with a red N.\n  string or text variable. This refers to a variable with text as value, for example the address, the ID of a product, or the name of the person. In Orange, these would be marked with a black S. They would always be in meta variables.\n  datetime variable. This refers to a variable with time as value, for example date of birth or time of the event. In Orange, these would be marked with a blue T.\n     meta variable is a variable that adds additional information on a data sample, but it is not used in computation. Such variables are text variables, but they can also be of other types. For example, if we set the age of the person as a meta variable, this variable will not be considered in clustering or classification. In Orange, they will be marked with beige in a data table.\n  target variable, in statistics known as dependent variable. This is the variable you are trying to predict, for example the survival of Titanic passengers or the price of a house in Boston. In Orange, they will be marked with gray in a data table.\n  class variable is a target variable with categorical values. It is used for classification tasks.\n     domain is something a bit specific to Orange. It contains the information on all the variables in the data set, specifically the type of variables (numeric, categorical, etc.), the role of variables (attributes, metas, target), and the special properties of variables (assigned color, the order of categorical values, etc.).\n  sparse data is a data with many missing values. Such data is usually found in text mining, where among all the words in the corpus, a document would contain only a small amount.\n  Preprocessing   continuize is an operation that transforms categorical variables to numeric, usually by creating a column for each categorical value, i.e. gender=female and giving 1 if female else 0. Many algorithms require numeric data, making continuization extremely important. Orange usually continuizes by default.\n  discretize is an operation that transforms numeric variables to categorical, usually by creating value ranges or bins.\n  impute is an operation that handles missing values. Certain algorithms cannot work with missing data, so the user needs to handle them beforehand. If there are few missing values, such instances would be removed. If there are many missing values, one would usually use imputing a mean or most frequent value. Or one could use an algorithm that works with sparse data.\n  merge is an operation that adds additional columns to the data. For example, we have some patients, and the friendly hospital sends us their records. We can add the records to the patients by matching by the patient ID.\n  concatenate is an operation that add additional rows to the data. For example, we have some patients, and then another 10 patients are admitted to the hospital. We can add them to the table by matching by column names.\n   Modelling   training data is a sample of the data set used for training the model. Usually, this is the larger portion of the data, say 70% or 80% of all data samples.\n  test data is a sample of the data set used for testing the model and evaluating its performance. Usually, this is a smaller portion of the data, say 10%-20% of all data samples.\n  learner is an algorithm (procedure or recipe) for training the model. It is passed to the selected evaluation method, usually to Test and Score widget for cross-validation. It does not contain any data, just the procedure.\n  model is an a result of training the data with a procedure (a learner) and the training data. A model is essentially a set of all recognized patterns in the data relating to the target variable, which are assembled into a final mathemathical function. The model can be used for prediction.\n   To sum up We hope you found this informative. This information should help you navigate Orange easily and talk more precisely about your data mining and machine learning tasks.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2022/2022-02-01-word-cloud.png",
	"shortExcerpt" : "What do 'target variable', 'attribute', and 'learner' mean?",
	"longExcerpt" :  "Data scientists have a specific language. Learn about what certain terms mean and become more confident in your data science speak!" ,
	"author" : "Ajda Pretnar Žagar",
	"summary" : "We all know each profession has its own jargon. Most of us remember from any hospital series something like: \u0026ldquo;\u0026ldquo;Pass me the scalpel, stat!\u0026rdquo; Meaning \u0026ldquo;give me the scalpel quickly\u0026rdquo;.\nData scientists are no different. We have our own terminology that requires some getting used to. Here\u0026rsquo;s a list of common terms that we use in machine learning. Some terms are specific to Orange and might not be used elsewhere. The post is intended to get you familiar with concepts used in Orange.",
	"date" : "Feb 1, 2022"
}

    
    , {
    "uri": "/blog/2022/2022-01-14-universities/",
	"title": "Orange in Classroom, pt. 2",
	"categories": ["orange", "education", "teaching", "university"],
	"description": "",
	"content": "A year ago, we put out a survey asking the Orange community if they use Orange for teaching (or learning) and at which educational institution. We got around 300 replies from 200 universities in the first round.\nIn the past year, we were starting to establish a global community of educators that teach statistics, data mining, and machine learning (or perhaps something entirely different) and are using Orange for this purpose. Hence we continued gathering the data. We got 417 replies from 305 universities and educational institutions in 76 countries in the second round! That is 39% of the world!\nWe once again thank everyone for your invaluable support!\n  Afyon Kocatepe University, Turkey Ahmad Dahlan University, Indonesia Aix-Marseille University, France Amirkabir University of Technology, Iran Andrés Bello Catholic University, Venezuela Anglia Ruskin University, UK Anil Neerukonda Institute of Technology \u0026amp; Sciences, India Antenor Orrego Private University, Peru Aston University, UK Atılım University, Turkey Australian National University, Australia Autonomous University of Madrid, Spain BSE Institute Ltd., India Babasaheb Bhimrao Ambedkar University, India Babeș-Bolyai University, Romania Bandung Institute of Technology, Indonesia Bandırma Onyedi Eylül University, Turkey Batumi Shota Rustaveli State University, Georgia Baylor College of Medicine, USA Birkbeck, University of London, UK Birla Institute of Technology and Science, India Bocconi University, Italy Bombay Stock Exchange Institute, India Buckinghamshire New University, UK Budi Luhur University, Indonesia California Polytechnic State University, USA Çankırı Karatekin university, Turkey Capital University of Science \u0026amp; Technology, Pakistan Carleton University, Canada Catholic University of the Sacred Heart, Italy Center for Applied Mathemathics, Mexico Center for Integrated Health Programs, Nigeria Center of Higher Education of Brasilia, Brazil Central University of Venezuela, Venezuela Central Washington University, USA Centro Universitário das Faculdades Metropolitanas Unidas, Brazil Cestar College of Business, Health and Technology, Canada Charles Darwin University, Australia Charles University, Faculty of Science, Czechia Cheongju University, South Korea Chiang Mai University, Thailand Complutense University of Madrid, Spain Cork Institute of Technology, Ireland Daegu Software High School, South Korea Darmstadt University of Applied Sciences, Germany Delft University of Technology, Netherlands Des Moines Area Community College, USA Dian Nuswantoro University, Indonesia Duta Bangsa University, Indonesia École des Mines d\u0026rsquo;Albi, France Ecuador Technological University, Ecuador Eskişehir Technical University, Turkey European Academy of Neurology, Austria Faculdade de Ciências da Universidade do Porto, Portugal Faculty of Informatics Lemos de Castro, Brazil Faculty of Organizational Sciences, Serbia Faculty of Sciences of the University of Lisbon, Portugal Federal Centers of Technical Education, Brasil Federal Institute of Bahia, Brazil Federal Institute of Education, Science and Technology of Tocantins, Brazil Federal Institute of São Paulo, Brazil Federal University of Ceará, Brazil Federal University of Goiás, Brazil Federal University of Parana, Brazil Federal University of Pelotas, Brazil Federal University of Recôncavo da Bahia, Brazil Federal University of Rio Grande, Brazil Federal University of Rio Grande do Norte, Brazil Federal University of Rio Grande do Sul, Brazil Federal University of Santa Catarina, Brazil Federal University of Santa Maria, Brazil Federal University of Technology – Paraná, Brazil Florida State University, USA Francisco José de Caldas District University, Colombia Gazi University, Turkey Gaziantep University, Turkey Giresun University, Turkey Graz University of Technology, Austria Gunadarma University, Indonesia Guru Gobind Singh Indraprastha University, India Hacettepe University, Turkey Hamad Bin Khalifa University, Qatar Hankuk University of Foreign Studies, South Korea Harrisburg University of Science \u0026amp; Technology, USA Hiroshima Institute of Technology, Japan Holon Institute of Technology, Israel Hungnam National University, South Korea I.E.S. Juan Carlos I, Spain ICFAI Business School, India ICFAI Business School Hyderabad, India IES Santiago Apóstol, Spain IPB University, Indonesia ISLA Santarém, Portugal ITB STIKOM Bali, Indonesia ITC Infotech, India ITESO, Universidad Jesuita de Guadalajara, Mexico ITM Business School, India IULM University - Milan, Italy Indian Institute of Management, India Indian Institute of Management Sambalpur, India Indian Institute of Technology Madras, India Indian Institutes of Management, India Indian Statistical Institute, India Indiana University, USA Informatics \u0026amp; Business Institute Darmajaya, Indonesia Inonu Universitesi, Turkey Institut catholique d\u0026rsquo;arts et métiers, France Institute of Technical Education and Research, India Instituto Potosino de Investigación Científica y Tecnológica, Mexico Instituto Superior de Engenharia de Lisboa, Portugal Instituto Tecnológico Superior de Xalapa, Mexico International Islamic University Malaysia, Malaysia International Trademark Association, USA Istanbul Health and Technology University, Turkey JK Faculty, Brazil Jagiellonian University, Poland Jakarta State Polytechnic, Indonesia KAIST, South Korea Karlsruhe Institute of Technology, Germany Kendriya Vidyalaya Janakpuri, India Kielce University of Technology, Poland Kindai University, Japan King Mongkut\u0026rsquo;s Institute of Technology Ladkrabang, Thailand Laval University, Canada Linnaeus University, Sweden Liverpool John Moores University, UK Lodz University of Technology, Poland Lviv Polytechnic National University, Ukraine MGMU Institute of Biosciences \u0026amp; Technology, India Mahidol University, Thailand Manipal University Jaipur, India Mauricio de Nassau Faculty, Brazil Memorial University of Newfoundland, Canada Mercu Buana University, Indonesia Mexican Institute of Knowledge Management, Mexico Mexican Institute of Social Security, Mexico Mittelhessen University of Applied Sciences, Germany Monash University, Australia National Central University, Taiwan National Chung Hsing University, Taiwan National Conservatory of Arts and Crafts, France National Institute of Technology Kurukshetra, India National Institute of Technology Tiruchirappalli, India National Law School of India University, India National School of Computer Sciences, Tunisia National Service for Industrial Training, Brazil National Technological Institute of Mexico, Mexico National Technological University, Argentina National University, USA National University of General San Martín, Argentina National University of Singapore, Singapore Naval Postgraduate School, USA New Bulgarian University, Bulgaria Nigerian Defence Academy, Nigeria North American University, USA Northern Alberta Institute of Technology, Canada Odessa National Polytechnic University, Ukraine Ohio University, USA Open University of the Netherlands, The Netherlands Osnabrück University of Applied Sciences, Germany PES University, India Pablo de Olavide University, Spain Padjadjaran University, Indonesia Palacký University Olomouc, Czechia Pamukkale University, Turkey Panamerican University, Mexico Panteion University of Social and Political Sciences, Greece Peking University, China Pennsylvania State University, USA Pirogov Russian National Research Medical University, Russia Plovdiv University \u0026ldquo;Paisii Hilendarski\u0026rdquo;, Bulgaria Politecnica Salesiana University, Ecuador Polytechnic Institute of Coimbra, Portugal Polytechnic University of Yucatan, Mexico Pontifical Catholic University of Minas Gerais, Brazil Pontifical Catholic University of Peru, Peru Pontifical Catholic University of Rio de Janeiro, Brazil Pontifical Catholic University of Valparaiso, Chile Pontifical Xavierian University, Colombia Poznań University of Economics and Business, Poland Prague University of Economics and Business, Czech Republic Praxis Business School, India Professional Institute Santo Tomas, Chile Queen Mary\u0026rsquo;s College, UK RWTH Aachen University, Germany Regis University, USA Republic Polytechnic, Singapore Research Institute for Development, France Rheinische University of Applied Science, Germany Riga Technical University, Latvia Riken, Japan Riphah International University, Pakistan Rochester Institute of Technology, USA Royal University of Phnom Penh, Cambodia Sabanci University, Turkey Saint Petersburg Electrotechnical University, Russia Sakarya University, Turkey Salzburg University of Applied Sciences, Austria Santo Tomas, Spain School of Information Management and Computer IKMI, Indonesia School of Technology and Management of Oliveira do Hospital, Portugal Sepuluh Nopember Institute of Technology, Indonesia Shahid Beheshti University, Iran Shahroud University of Technology, Iran Siberian Institute of Management, Russia Silesian University of Technology, Poland Singapore Polytechnic, Singapore Soongsil University, South Korea South Ural State University, Russia Sree Ramu College Of Arts And Science, India St. Petersburg State University of Industrial Technologies and Design, Russia Stellenbosch University, South Africa Sungkyunkwan University, South Korea Syracuse University School of Information Studies, USA Södertörn University, Sweden Taif University, Saudi Arabia Tecnológico de Monterrey, Mexico Telkom Institute of Technology, Indonesia Telkom Institute of Technology Purwokerto, Indonesia Texas State University, USA The American University (Nicaragua), Nicaragua The Chinese University of Hong Kong, Hong Kong The Free University of Berlin, Germany The Hong Kong Polytechnic University, Hongkong The Institute of Bioengineering of Catalonia, Spain The United Nations University, The Netherlands The University of Santa Cruz do Sul, Brazil The University of Utah, USA The University of Virginia, USA Tohoku University of Community Service and Science, Japan Tokyo University of Science, Japan Treptow-Köpenick University of Appplied Sciences, Germany UNICAMP Universidade Estadual de Campinas, Brazil Universidad Autónoma Latinoamericana, Colombia Universidad Autónoma del Estado de Morelos, Mexico Universidad EAFIT, Colombia Universidad Internacional SEK, Ecuador Universidad Norbert Wiener, Peru Universidad Tecnológica de Pereira, Colombia Universidade Estácio de Sá, Brazil Universidade de Sao Paulo, Brazil Universidade do Sul de Santa Catarina, Brazil Universitas Gadjah Mada, Indonesia Universitas Katolik Widya Mandira, Indonesia Universitas Narotama, Indonesia Universitas Negeri Surabaya, Indonesia Universiti Teknologi MARA Shah Alam, Malaysia Universiti Utara Malaysia, Malaysia University Putra Malaysia, Malaysia University of Advanced Studies, Mexico University of Algarve, Portugal University of Applied Sciences Mittelhessen, Germany University of Azad Jammu and Kashmir, Pakistan University of Barcelona, Spain University of Belgrade, Serbia University of Bologna, Italy University of Brasília, Brazil University of Central Florida, USA University of Djelfa, Algeria University of Essex, UK University of Granada, Spain University of Guadalajara, Mexico University of Guelph, Canada University of Haifa, Israel University of Health Sciences, Turkey University of Houston, USA University of Indonesia, Indonesia University of Karachi, Pakistan University of La Laguna, Spain University of Ljubljana, Slovenia University of London, UK University of Malaya, Malaysia University of Maryland Global Campus, USA University of Mauritius, Mauritius University of Milano-Bicocca, Italy University of Monterrey, Mexico University of Montreal, Canada University of Nairobi, Kenya University of Nebraska Kearney, USA University of Oviedo, Spain University of Paris, France University of Portsmouth, UK University of South Dakota, USA University of Split, Croatia University of Stirling, UK University of Strathclyde, UK University of São Paolo, Brazil University of Technology and Applied Sciences, Oman University of Tehran, Iran University of Tennessee, USA University of Texas Austin, USA University of Toronto, Canada University of Tübingen, Germany University of the Basque Country, Basque Country University of the Peloponnese, Greece University of the Republic, Uruguay Università di Foggia, Italy Univesity of Chile, Chile Utah Valley University, USA Van Yüzüncü Yıl University, Turkey Vellore Institute of Technology, India Virtual University of the State of Sao Paulo, Brazil Vytautas Magnus University, Lithuania Weill Cornell Medicine, USA YARSI University, Indonesia Yogyakarta State University, Indonesia Zwickau University of Applied Sciences, Germany  ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2022/2022-01-14-universities-small.png",
	"shortExcerpt" : "Orange is being used in over 300 universities around the world.",
	"longExcerpt" :  "Orange is being used inside and outside the classroom, by professors and students in over 300 universities around the world." ,
	"author" : "Ajda Pretnar",
	"summary" : "A year ago, we put out a survey asking the Orange community if they use Orange for teaching (or learning) and at which educational institution. We got around 300 replies from 200 universities in the first round.\nIn the past year, we were starting to establish a global community of educators that teach statistics, data mining, and machine learning (or perhaps something entirely different) and are using Orange for this purpose.",
	"date" : "Jan 14, 2022"
}

    
    , {
    "uri": "/blog/2022/2022-01-10-orange2/",
	"title": "Compiling Orange 2 on modern Linux",
	"categories": ["python", "development", "linux"],
	"description": "",
	"content": "We abandoned Orange 2 in 2015 because we did not have enough resources to split between maintaining Orange 2 and building the new version from scratch. Orange was due for a rewrite for quite some time. The core of the pre-3 Orange was written mainly in C++. When Orange was conceived, extending Python with C was the only option to make it run fast enough. Orange was designed before NumPy and even before NumPy\u0026rsquo;s predecessors, Numarray and Numeric. The resulting code was hard to maintain. In the meantime, Python libraries such as NumPy and scikit-learn matured, so the rewrite seemed reasonable: Orange 3 was born.\nCurrently, we need Orange 2 as a reference implementation for interaction analysis, something that Orange 3 still lacks. Two features positively interact if observing the features in combination yields more information than the sum of information of observing both features separately. The Interaction Graph widget from Orange 2, which uses the interaction gain measure, is a great tool to discover such interacting features.\nAs a Linux user, I admire the backward compatibility we learned to expect on Windows: if an application ever did work there, it probably still does. On Windows, an old Orange 2 build from the download archive probably still works. In contrast, old Mac OS Orange packages are likely problematic because recent Mac OS versions stopped supporting 32-bit binaries.\nLinux/GNU users do not have the luxury of installing old binary Orange packages. They do not exist because building them presented a lot of work that would benefit a few users. Even if they had, they would be hard to run; while the Linux kernel is highly backward compatible, its userspace is not.\nTherefore, we need to dig into the code, starting with a checkout of Orange 2 code for the git repository:\ngit clone git@github.com:biolab/orange2.git cd orange2 The source code includes the INSTALL.txt, which suggests installing a bunch of Python libraries with the Linux distribution\u0026rsquo;s package manager and then compiling Orange. Now, the problems start. The instructions say they were tested with Ubuntu 14.04, which is not more than seven years old. Curiously, a quick git blame confirms that I was the author of the instructions, so Ubuntu 14.04 seems to be the latest distribution I have ever used to install Orange 2.\nHere the problems begin. The required libraries no longer exist as packages in Ubuntu 20.04 that I am using, and neither does Python 2. Fortunately, Anaconda comes to the rescue. We can use it to create a new Python 2 environment:\nconda create --name py2 -c defaults --override-channels python=2.7 conda activate py2 Then, we can install the needed libraries with conda. I found the list of required packages in the setup.py file.\nconda install -c defaults --override-channels numpy scipy setuptools conda install -c conda-forge --override-channels pyqt=4.11 I needed quite some time to figure out which combination of packages and sources is installable. We can now try to do a development installation of Orange:\npython setup.py develop That fails:\nsource/include/stat.hpp:33:27: error: 'double abs(double)' conflicts with a previous declaration Such problems are expected when we compile old code with a newer compiler or libraries. I tried installing older compiler versions, but my distribution only went back to GCC-7, which was already too new. So I had to fix the code. The following change fixed the error:\n--- a/source/include/stat.hpp +++ b/source/include/stat.hpp @@ -30,8 +30,8 @@ using namespace std; #endif #ifndef _MSC_VER_70 -inline double abs(double x) -{ return fabs(x); } +//inline double abs(double x) +//{ return fabs(x); } #endif There were other errors with return types of C++ functions called from Python. After about 10 changed lines and quite some waiting, Orange was finally installed! The compilation took 10 minutes on my newish laptop. So a lot of code had to be compiled. And someone had to write it! Amazingly, most of the C++ code of Orange 2 was written by a single developer, Janez Demšar.\nWell, now Orange is installed, but does it work? Let\u0026rsquo;s try to run it:\norange-canvas Nope, not quite:\n ... File \u0026quot;/home/marko/dev/orange2/Orange/OrangeCanvas/main.py\u0026quot;, line 23, in \u0026lt;module\u0026gt; from Orange.OrangeCanvas.application.canvasmain import CanvasMainWindow File \u0026quot;/home/marko/dev/orange2/Orange/OrangeCanvas/application/canvasmain.py\u0026quot;, line 28, in \u0026lt;module\u0026gt; from PyQt4.QtWebKit import QWebView ImportError: libQtWebKit.so.4: cannot open shared object file: No such file or directory It seems that the installed Qt package lacks WebKit. I could not find any prebuilt Qt4 with it, and I really did not want to go into compiling Qt and PyQt myself! Remembering that the web interface is mostly used in two places in Orange, help and reports, I tried making WebKit import and its usage optional: see changes to canvasmain.py.\nNow, orange-canvas finally opened. But where are the widgets? All widget categories are empty:\n To find out why, we need to run with debugging output:\norange-canvas -l4 --force-discovery We see new exceptions, namely the repeated problems when widgets try to import the reporting functionality:\n ... File \u0026quot;/home/marko/dev/orange2/Orange/OrangeWidgets/OWWidget.py\u0026quot;, line 319, in \u0026lt;module\u0026gt; import OWReport File \u0026quot;/home/marko/dev/orange2/Orange/OrangeWidgets/OWReport.py\u0026quot;, line 34, in \u0026lt;module\u0026gt; from PyQt4.QtWebKit import * ImportError: libQtWebKit.so.4: cannot open shared object file: No such file or directory By making reports optional with changes to OWReport.py, some widgets finally appear. Among them, we are lucky to find File and Interaction Graph widgets, precisely the ones we need. For the Interaction Graph to work properly, I also needed to install graphviz:\nsudo apt install graphviz The widgets that still fail to work are mainly visualizations. And for a good reason: Orange 2 visualizations were based on pyqwt5, a library that was already abandoned and hard to install even when Orange 2 was still being actively developed. I did not find any conda-installable packages of pyqwt5, and my first attempts at installing it from the source code failed.\nTherefore, I stop. I already have all the parts I need. Now, on to interactions analysis!\n This was fun. If anyone attempts a similar journey, I\u0026rsquo;d love to hear about it, especially if visualizations are working.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2022/2022-01-10-orange2-about.png",
	"shortExcerpt" : "How to compile an ancient version of Orange on Ubuntu 20.04 or how to spend a cold winter evening.",
	"longExcerpt" :  "How to compile an ancient version of Orange on Ubuntu 20.04 or how to spend a cold winter evening." ,
	"author" : "Marko Toplak",
	"summary" : "We abandoned Orange 2 in 2015 because we did not have enough resources to split between maintaining Orange 2 and building the new version from scratch. Orange was due for a rewrite for quite some time. The core of the pre-3 Orange was written mainly in C++. When Orange was conceived, extending Python with C was the only option to make it run fast enough. Orange was designed before NumPy and even before NumPy\u0026rsquo;s predecessors, Numarray and Numeric.",
	"date" : "Jan 10, 2022"
}

    
    , {
    "uri": "/blog/2021/2021-12-17-visualizations-101/",
	"title": "Visualizations 101",
	"categories": ["visualization", "data mining", "box plot", "scatter plot", "distributions"],
	"description": "",
	"content": "Orange has a wide array of visualizations, which enable exploring the data from different perspectives. But each visualization is unique - it is used for a specific purpose, which is closely related to how one interprets the plot. Let\u0026rsquo;s have a look at the most common visualizations in Orange, when to use them and how to read them. We will use the heart_disease.tab data set from the File widget in the next examples.\nScatter Plot Scatter plot is suitable for displaying a relationship between two numeric variables. It shows a 2-dimensional plot where points represent data instances (rows). The position of each point is defined by its value for x axis and y axis. The plot can also show relations to the third variable, either numeric or categorical, by setting the color, size, or shape of data points.\n In the above image we see a relationship between age and max HR for the 303 patients in the data set. The color of the point corresponds to the presence or absence of diameter narrowing. We see that the presence of heart disease is slightly more frequent at the age of 60, but the difference is not entirely clear and we should\u0026rsquo;t draw too many conclusions from this. What we can conclude is that the maximum heart rate falls with age - regression lines are sloping downwards. Interestingly, the values of max HR at younger ages are already noticeably lower for patients with diameter narrowing (red points and line). That said, the average maximum heart rate for healthy patients (no diameter narrowing) approaches that for sick patients (with diameter narrowing).\nHow to read the plot: age and max HR are negatively correlated. The higher the age, the lower the maximum heart rate.\nBox Plot Box plot is perfect for initial exploration of the data set. It shows basic statistics for each variable and it can handle both numeric and categorical variables.\n Box plot for numeric variables shows a whisker plot, which displays essential statistics. Above is an example with the age variable, which is by default split by the target variable diameter narrowing in Orange. The visualization shows two box plots, one for each target value (patients with and without diameter narrowing).\nThe yellow vertical line shows the median value for each box. The blue vertical line shows the mean with the standard deviation represented by the horizontal thin blue line, which shows how \u0026ldquo;spread out\u0026rdquo; the values in the sample are. The big blue box is the data between the 25th and 75th percentile, that is the central 50% of the data. The line on the left extends to the minimum value, while the line of the right extends to the maximum value.\n Categorical variables show the number (or proportion) of data instances for each value. These are no longer box plot, but stacked bar charts.\nA great option in Orange is setting the Order by relevance to subgroup checkbox, which orders the variables in the top left box by how much their values differ between groups defined by the selected Subgroup variable. It uses Chi-square test for categorical variables and compares the means of numeric variables using Student\u0026rsquo;s T for two and ANOVA for multiple groups. Values of the test statistic are displayed below the plot. Normally, higher values are preferred, but the most important is the p-value - if it is below 0.05, the difference in distributions is generally considered significant.\nHow to read the plot: thal is the variable that best separates between patients with and without diameter narrowing. There is a higher incidence of reversable defect in patients with diameter narrowing (64% have reversable defect) than in those without diameter narrowing (17% have reversable defect).\nDistributions Distributions show histograms for numeric variables, where values within a given range will be assigned a bin. For categorical data, they show bar plots, that is the frequency of each value. In the below example, there are two bins, one with values below 50.87 and one with values above 50.78.\n The widget can be used to display differences in distributions.\nHow to read the plot: In the group of patients that are younger than 50 years, there is a 50% lower presence of diameter narrowing (70 % of patients don\u0026rsquo;t have the condition).\nSieve Diagram Sieve diagram is a complex, yet extremely useful visualization. It shows the difference between the expected and observed variable frequency. In other words, if a combination of two values, i.e. being 60 years old and having a heart disease, is more frequent than expected, Sieve diagram will show it. Hence it is primarily intended for categorical variables - numeric variables will be discretized.\n In the scatter plot above, we observed a slightly higher incidence of diameter narrowing in patient around 60 years of age. Let us check this is Sieve diagram.\nOn y axis we see diameter narrowing, with the size of each box vertically corresponding to the proportion of instances with and without condition. On x axis we see age, with the size of each box horizontally corresponding to the size of each bin. As numeric variables are discretized with equal frequency discretization, the box will always be the same size.\nThe color of the box corresponds to how overrepresented (blue) or underrepresented (red) a certain combination is. For example, there are 73 patients (24 %) between 55.5 and 60.5 years old in the data set. There are also 139 patients (46%) with diameter narrowing.\nIf these two variables would be unrelated, we would expect around 11% of patients with both values (0.24 x 0.46 = 0.11), that is patients between 55 and 60 years old with a diameter narrowing.\nBut if we observe the data set, we can see there are actually 15% of such patients. That is more than we would expect, so a combination of the two values could be statistically significant.\nIn the left corner of the plot we once again meet the Chi-squared, which tells us how significant the combination actually is.\nHow to read the plot: Based on the data set, it is more likely the patients between 55 and 60 years of age will have diameter narrowing. The p-value is below 0.05.\nWord of caution Data mining is about finding patterns in the data. But sometimes, patterns are random and correlation does not necessarily mean causation. So be careful when interpreting the results. Always disclose the size of your data set and how it was gathered. Report the results with the awareness, that you are only observing a sample of the population. Remember that p-values are the chance that the null hypothesis is true. Even if the p-value is 0.05, the null hypothesis could still be true - you would actually expect this to happen 5 times out of a 100, so keep this in mind, especially when you test 100+ variables!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-12-17-viz.png",
	"shortExcerpt" : "How to read different visualizations?",
	"longExcerpt" :  "When to use certain visualization and how to read it?" ,
	"author" : "Ajda Pretnar",
	"summary" : "Orange has a wide array of visualizations, which enable exploring the data from different perspectives. But each visualization is unique - it is used for a specific purpose, which is closely related to how one interprets the plot. Let\u0026rsquo;s have a look at the most common visualizations in Orange, when to use them and how to read them. We will use the heart_disease.tab data set from the File widget in the next examples.",
	"date" : "Dec 17, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-10-26-explainable-ai/",
	"title": "Explainable AI Project Meeting",
	"categories": ["project", "explainable ai", "teaching"],
	"description": "",
	"content": "Recently, we have attended an xAIM project meeting in Hannover, Germany. xAIM is an EU project whose aim is to develop a Master\u0026rsquo;s degree tailored to the medical professionals or those wanting to work in the medical field. The focus of the MA is on explainable artificial intelligence, that is on the explanation of machine learning models, ethical aspects of AI, and the translation of models into the medical setting.\nIn two days we have finalized the syllabus of the MA, which will consist of three modules - artificial intelligence, healthcare management, and ethics. Orange will be heavily used in (at least) two courses, namely the Introduction to Data Science and Text Mining. The first course is the core subject of the MA, while the second course is an elective.\nAfter two days of hard work we had to relax a bit, so we visited the Christmas market in the center of Hannover. It smelled of cinnamon, Glühwein, and, of course, of oranges.\n ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-11-26-xaim.jpeg",
	"shortExcerpt" : "Orange will be used in two Explainable AI courses.",
	"longExcerpt" :  "Orange will be used in two courses of the new Explainable AI Master's degree." ,
	"author" : "Ajda Pretnar",
	"summary" : "Recently, we have attended an xAIM project meeting in Hannover, Germany. xAIM is an EU project whose aim is to develop a Master\u0026rsquo;s degree tailored to the medical professionals or those wanting to work in the medical field. The focus of the MA is on explainable artificial intelligence, that is on the explanation of machine learning models, ethical aspects of AI, and the translation of models into the medical setting.",
	"date" : "Nov 26, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-10-21-cluster-explanation/",
	"title": "Characterizing Clusters with a Box Plot",
	"categories": ["clustering", "explanation", "box plot"],
	"description": "",
	"content": "There are many ways to cluster the data in Orange. Hiearchical clustering, k-means, and DBSCAN are just few of the widgets we can use to find groups of data instances with similar values of attributes. Once we infer the clusters, we need to analyze them to determine their characterizing features. It is there that actually the fun begins.\n Out of many ways for cluster analysis, perhaps the simplest one is by using the Box Plot. Consider the following example on the employee attrition data set. I have used t-SNE to observe that that this data perhaps contains about five clusters. I have selected the data instances from the rightmost cluster, for which I would like to know which features are those that separate this cluster from everything else. To do so, I use the Box Plot. But note: by default, Orange will wire the connections from t-SNE to Box Plot so as to communicate only the selected data instances. Instead, we would like to send all the data to the Box Plot, but include the column called Selected as an selection indicator. Selected data instances will have this feature set to Yes, and all other to No. To rewire the connections between t-SNE to Box Plot appropriately, we need to double click the link and between these two widgets and set it so that entire data is send to the Box Plot.\n Fine, we are ready now. In the Box Plot, we subgroup the data by the feature Selected, and order the variables by the relevance to the subgroups. The most relevant feature is the position of the data points in the horizontal direction of the t-SNE: we selected the data points from the rightmost cluster. We ignore it, and focus on original data features. At the top is the job role. Very interesting, the cluster we selected contains people that work in sales! That\u0026rsquo;s a great explanation. Also, the Department, the next-ranked feature, tells us that the cluster almost exclusively includes the employees from the Sales Department. These employees were primarily educated marketing, as reported by next-ranked feature EducationField.\n Placing the t-SNE and the Box Plot widget side-by-side, we can now characterize other clusters. The small one on the top contains only managers. And there is a cluster of employees in human resource department, and researcher, and some other managers.\n We could also use the Box Plot to report on the differences between two selected clusters. In t-SNE, as in any point-based visualization in Orange, we mark different clusters by selecting points with a shift-modifier. We also need to rewire our workflow to, this time, send only the selected data instances to the box plot. And the subgrouping feature would now need to be the cluster indicator, that is, a feature called Group. But let us leave these details and examples to some other blog.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-10-21-cluster-explain.png",
	"shortExcerpt" : "Box Plot widget offers a simple means for explaining clusters.",
	"longExcerpt" :  "Box Plot widget offers a simple means for explaining clusters." ,
	"author" : "Blaž Zupan",
	"summary" : "There are many ways to cluster the data in Orange. Hiearchical clustering, k-means, and DBSCAN are just few of the widgets we can use to find groups of data instances with similar values of attributes. Once we infer the clusters, we need to analyze them to determine their characterizing features. It is there that actually the fun begins.\n Out of many ways for cluster analysis, perhaps the simplest one is by using the Box Plot.",
	"date" : "Oct 21, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-09-17-semantic-analysis/",
	"title": "Semantic Analysis of Documents",
	"categories": ["semantic analysis", "text mining", "corpus", "keywords"],
	"description": "",
	"content": "Our recent project with the Ministry of Public Administration comprises building a semantic analysis pipeline in Orange, enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don\u0026rsquo;t worry, here\u0026rsquo;s a quick demo on how to perform semantic analysis in Orange.\nFirst, we will use the pre-prepared corpus of proposals to the government, which you can download here. These are the initiatives which the citizens of Slovenia propose to the current government for consideration. The present corpus contains 1093 such proposals.\n  Each proposal contains a title, the content of the proposal, the author, the date when it was published, number of upvotes, and so on. But for a thousand proposals, it would take a long time to read all of them and see which policy areas they cover. Instead, we will use two new Orange widgets to determine the content (main keywords) of a subset of documents.\n As always, we will first preprocess the corpus to create tokens, the core units of our analysis. The preprocessing pipeline is sequential; first, we lowercase the text, then we split the text into words (this is what the regular expression \\w+. does), transform the words into lemmas with UDPipe, and finally remove stopwords from the list (such as \u0026ldquo;in\u0026rdquo;, \u0026ldquo;da\u0026rdquo;, \u0026ldquo;če\u0026rdquo;). Since we are working with a Slovenian language text, we have to select the corresponding models and stopword lists.\n  After preprocessing, we build a document-term matrix using the Bag of Words widget with the Count+IDF setting. Next, we pass the data to t-SNE to observe the document map. t-SNE takes the document-term matrix and finds a 2D projection, where similar documents lie close together.\n  Now we will select a small subset of the document, say in the lower right corner, where we have a nice cluster of points. The question is, what are these documents talking about?\nWe will use Extract Keywords widget to find the most significant keywords in the selection. There are several methods we can use, even the popuar YAKE!, but we will go with a simple TF-IDF method, which takes the words with the highest TF-IDF score. Note that the vectorizer uses the default sklearn\u0026rsquo;s TfidfVectorizer settings, that is the tf-idf transform with L2 norm, keeping the passed tokens as they are.\n  It looks like the top words characterizing the selected subcorpus are \u0026ldquo;študent\u0026rdquo; (student), \u0026ldquo;delati\u0026rdquo; (to work), and \u0026ldquo;delo\u0026rdquo; (the work). Apparently, the documents talk mostly about student work. Let us explore this a bit further. It would be nice to have a certain score attached to the documents, which would correspond to how much a document talks about student work. In other words, we would like to score the documents based on how many of the selected words they contain (and in what proportion).\nTo achieve this, we will use Score Documents. We will pass it the document-term matrix and the list of selected keywords from Extract Keywords. The widget again offers several different ways of scoring documents. A simple way to score them is to compute how often selected words appear in each document, which corresponds to the \u0026ldquo;Word Count\u0026rdquo; method.\n  Score Documents returns keyword scores for each document. Let us pass the scored documents to another t-SNE widget. If we set the color and the size of the points to \u0026ldquo;Word Count\u0026rdquo; variable, t-SNE plot will expose the documents with the highest scores. These documents talk the most about students and work. A great thing is that we can see documents with high scores that were not a part of our selection, which means the general bottom-right area contains documents relating to this topic.\n  Now try selecting a different subset yourself and see what the documents are about. You can use any corpus you want, even the ones that come with Orange.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-09-17-seman.png",
	"shortExcerpt" : "How to use Text add-on for semantic analysis of documents.",
	"longExcerpt" :  "How to use Text add-on to extract keywords from documents, score documents on keywords, and display semantic content in a map." ,
	"author" : "Ajda Pretnar",
	"summary" : "Our recent project with the Ministry of Public Administration comprises building a semantic analysis pipeline in Orange, enabling the users to quickly and efficiently explore the content of documents, compare a subset against the corpus, extract keywords, and semantically explore document maps. If this sounds too vague, don\u0026rsquo;t worry, here\u0026rsquo;s a quick demo on how to perform semantic analysis in Orange.\nFirst, we will use the pre-prepared corpus of proposals to the government, which you can download here.",
	"date" : "Sep 17, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-09-15-connlu-files/",
	"title": "New in Orange: Support for CONLL-U files",
	"categories": ["conllu", "text mining", "corpus", "lemma"],
	"description": "",
	"content": "CONLL-U files are ubiquitous in text mining and natural language processing. They can hold a great deal of linguistic data, specifically sentence boundaries, word lemmas, universal POS tags, language specific POS tag, morphological features, dependency relations, named entities, and so on. This is how a typical CONLL-U file looks like.\n# sent_id = ParlaMint-GB_2021-01-05-lords.seg2.2 # text = The Hybrid Sitting of the House will now begin. 1 The the DET DT Definite=Def|PronType=Art 2 det _ NER=O 2 Hybrid hybrid NOUN NN Number=Sing 9 nsubj _ NER=O 3 Sitting sit VERB VBG VerbForm=Ger 2 acl _ NER=O 4 of of ADP IN _ 6 case _ NER=O 5 the the DET DT Definite=Def|PronType=Art 6 det _ NER=O 6 House House PROPN NNP Number=Sing 3 obl _ NER=B-ORG 7 will will VERB MD VerbForm=Fin 9 aux _ NER=O 8 now now ADV RB _ 9 advmod _ NER=O 9 begin begin VERB VB VerbForm=Inf 0 root _ NER=O|SpaceAfter=No 10 . . PUNCT . _ 9 punct _ NER=O  Since the release of Text v. 1.5.0, Orange can import CONNL-U files and its textual information. Specifically, Orange will import each utterance as a separate text entity. If selected in import options, it will append lemmas as tokens, POS tags as POS tags, and named entities as a separate text column. It will also add meta information to the imported corpus, if present in the folder.\nHere\u0026rsquo;s an example. We are using ParlaMint-GB v2.1 data from CLARIN repository, which contains annotated parliament speeches. Using Import Documents widget, we will import all the sessions for the year 2021.\n In the widget, we can set which parts of the CONLL-U file will be imported. Let us go with lemmas and POS tags. With this, we don\u0026rsquo;t need preprocessing as usual, as lemmas will be automatically considered downstream (for example in bag of words or topic modeling).\nHowever, the new release of Text also enables us to filter on POS tags. Say we wish to keep only nouns and verbs. We can use Preprocess Text to keep only the specified tags. Remember to remove the default preprocessors as they will override the pre-set tokens.\n Looking at the Word Cloud, we can see that indeed only verbs and nouns were kept after preprocessing. And not only that! As we have selected to import lemmas, our words will already be normalized. Most of the preprocessing work is done for us! Now you can play with downstream analysis - for example, try to determine which words are significant for which MP using Word Enrichment!\n  ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-09-15-conllu.png",
	"shortExcerpt" : "Orange can now work with CONLL-U files!",
	"longExcerpt" :  "Orange can now work with CONLL-U files, including its lemmas, POS tags, and named entities." ,
	"author" : "Ajda Pretnar",
	"summary" : "CONLL-U files are ubiquitous in text mining and natural language processing. They can hold a great deal of linguistic data, specifically sentence boundaries, word lemmas, universal POS tags, language specific POS tag, morphological features, dependency relations, named entities, and so on. This is how a typical CONLL-U file looks like.\n# sent_id = ParlaMint-GB_2021-01-05-lords.seg2.2 # text = The Hybrid Sitting of the House will now begin. 1 The the DET DT Definite=Def|PronType=Art 2 det _ NER=O 2 Hybrid hybrid NOUN NN Number=Sing 9 nsubj _ NER=O 3 Sitting sit VERB VBG VerbForm=Ger 2 acl _ NER=O 4 of of ADP IN _ 6 case _ NER=O 5 the the DET DT Definite=Def|PronType=Art 6 det _ NER=O 6 House House PROPN NNP Number=Sing 3 obl _ NER=B-ORG 7 will will VERB MD VerbForm=Fin 9 aux _ NER=O 8 now now ADV RB _ 9 advmod _ NER=O 9 begin begin VERB VB VerbForm=Inf 0 root _ NER=O|SpaceAfter=No 10 .",
	"date" : "Sep 15, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-08-13-apply-domain/",
	"title": "Why You Should Use Apply Domain",
	"categories": ["domain", "PCA", "transformation", "apply domain"],
	"description": "",
	"content": "It can happen you\u0026rsquo;d see a widget in Orange and think: \u0026ldquo;What on Earth does this even do?\u0026rdquo; We admit, finding informative widget names is not always easy and Apply Domain had a least 5 different names so far. While it might not be clear what the widget does from its name, the actual functionality is one of the nicer ones Orange has to offer.\nSay you are transforming your data with PCA. There\u0026rsquo;s training data and test data (say you expect to get new data at some later point, so we are simulating the split here). For this example, we\u0026rsquo;ll be using Wine data from Datasets widget.\nTransforming the data with PCA is straightforward. Apply the PCA, select a number of components that cover a solid amount of variance and output the transformed data. We can observe the 2-dimensional PCA plot in Scatter Plot.\n Now, let us transform the test data with another PCA, using the same parameters. Use Concatenate to merge all the data points into a single table and plot the points in Scatter Plot. Make sure that Treat variables with the same name as the same variable is checked in Concatenate!\n  Scatter plot looks strange. The wine types are mixed up, especially in the lower part of the plot. This is because the test data and the train data were not transformed to the same PCA space.\nEnter Apply Domain. The widget uses template data to transform new data to the same domain (essentially the same data space). Connect Transformed Data to Apply Domain as Template Data. And Remaining Data from Data Sampler as Data.\nApply Domain outputs transformed data, which can be once again merged with Concatenate (keep the same settings as before). Now, observe the results in a scatter plot.\n  Well, look at that! The data is properly transformed and can be nicely discriminated by wine type in the PCA space!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-08-13-applydomain.png",
	"shortExcerpt" : "Apply Domain is a mystery widget with an amazing functionality.",
	"longExcerpt" :  "What does Apply Domain even do? Actually, it is an extremely useful widget for all your data transformation problems!" ,
	"author" : "Ajda Pretnar",
	"summary" : "It can happen you\u0026rsquo;d see a widget in Orange and think: \u0026ldquo;What on Earth does this even do?\u0026rdquo; We admit, finding informative widget names is not always easy and Apply Domain had a least 5 different names so far. While it might not be clear what the widget does from its name, the actual functionality is one of the nicer ones Orange has to offer.\nSay you are transforming your data with PCA.",
	"date" : "Aug 13, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-08-05-violin-plot/",
	"title": "Box Plot Alternative: Violin Plot",
	"categories": ["visualization", "violin plot", "box plot"],
	"description": "",
	"content": "There\u0026rsquo;s nothing more beautiful than seeing your data in plot. Preferably one that exposes interesting properties of the data or relationships between data instances. We recently wrote about embeddings and dimensionality reduction techniques, but now we are back to basics - whisker plots, means, medians, and distributions.\nRelated: PCA vs. MDS vs. t-SNE\nMost of you will be familiar with a good ol' box plot, also called whisker plot. The visualization shows essential data properties - mean, median, interquartile range, and variance. We can expose outliers and data irregularities with a quick glance at the box plot. A great thing about box plot is that is also allows us to split the data by a categorical variable, adding another layer of interpretation.\n But if box plot is so great, why do we need another visualization? Well, there are some properties of the data that box plot cannot reveal. Enter violin plot, a box plot lookalike with kernel density estimation (KDE).\nViolin plot has most of the properties of a box plot. The white dot in the center of the plot represents the median, the thick black stripe the interquartile range, and the thin black line the Tukey\u0026rsquo;s fence*.\n While violin plot doesn\u0026rsquo;t reveal certain aspects of the data, such as the mean and the exact values statistics, it does reveal something very interesting. Violin plot applies a kernel density estimation over data points, which show the likelihood of data points given a normal distribution.\nIn other words, where the violin is \u0026ldquo;fatter\u0026rdquo;, there are more data points in the neighborhood. And where it is \u0026ldquo;thinner\u0026rdquo;, there are less. Violin plots are thus great for exposing underlying distributions, especially if they are multimodal, which cannot be determined from the box plot.\n Consider a simple data set we have painted with Paint Data. It clearly has a bimodal distribution, with the first mean around 0.25 and the second one around 0.7.\n Box plot, however, only shows a single mean, which is around 0.47. It fails to expose the bimodal nature of the data, something that could be very relevant downstream.\n Violin plot to the rescue! The plot nicely shows two \u0026ldquo;bellies\u0026rdquo;, exposing a bimodal distribution of the data.\n One final thing. We have mentioned Tukey\u0026rsquo;s fences, which a simple approach for finding the outliers. Data points that lie outside of the thin black line are considered outliers. We can simply select all the instances that fall inside the line and output them. Thus we have removed the outliers. Keep in mind, though, that these are the outliers only for the specified attribute.\n  Happy plotting!\n* Tukey\u0026rsquo;s fences represent the first and the third quartile with added 1.5 x interquartile range, which defines the outliers, that is the data instances lying outside the defined range.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-08-05-violin.png",
	"shortExcerpt" : "Violin plot can tell you more than a box plot.",
	"longExcerpt" :  "Box plots with an upgrade - violin plots are your new favorite visualization!" ,
	"author" : "Ajda Pretnar",
	"summary" : "There\u0026rsquo;s nothing more beautiful than seeing your data in plot. Preferably one that exposes interesting properties of the data or relationships between data instances. We recently wrote about embeddings and dimensionality reduction techniques, but now we are back to basics - whisker plots, means, medians, and distributions.\nRelated: PCA vs. MDS vs. t-SNE\nMost of you will be familiar with a good ol' box plot, also called whisker plot. The visualization shows essential data properties - mean, median, interquartile range, and variance.",
	"date" : "Aug 5, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-06-17-pca-mds-tsne/",
	"title": "PCA vs. MDS vs. t-SNE",
	"categories": ["embeddding", "PCA", "dimensionality reduction", "workshop"],
	"description": "",
	"content": "I recently enjoy studying and showing differences between t-SNE and other data embedding and projection techniques. In particular, in some recent hands-on courses, we often introduce data visualization by principal component analysis, multidimensional scaling, and t-SNE. We would start with the zoo data set, where the data set is smaller, and the difference are less pronounced. We then traverse through employee attrition data set with exciting clusters exposed in t-SNE. To finish, and especially for the academic audience, we then show and compare the three different dimensionality reduction techniques on data from single-cell gene expression data. There, t-SNE discovers clusters of same-type cells, while PCA and MDA fail to expose interesting data structures.\n All the data sets I have mentioned above are available in Orange through the Dataset widget.\nWhile all the three methods aim to reduce the dimensionality of the data, their goal is different. The principal component analysis seeks to preserve the variance in the data. When we use it to construct a two-dimensional projection, it finds the projection plane were the most spread data. Multidimensional scaling aims to preserve the distances between pairs of data points, focusing on pairs of distant points in the original space. Differently, t-SNE focuses on maintaining neighborhood data points. Data points that are close in the original data space will be tight in the t-SNE embeddings.\nInterestingly, MDS and PCA visualizations bear many similarities, while t-SNE embeddings are pretty different. We use t-SNE to expose the clustering structure, MDS when global relations matter, and PCA as a preprocessing technique to reduce dimensionality and remove noise.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-06-17-tsne.png",
	"shortExcerpt" : "Oh, the joy and variety of data embedding and projection techniques!",
	"longExcerpt" :  "Oh, the joy and variety of data embedding and projection techniques!" ,
	"author" : "Blaž Zupan",
	"summary" : "I recently enjoy studying and showing differences between t-SNE and other data embedding and projection techniques. In particular, in some recent hands-on courses, we often introduce data visualization by principal component analysis, multidimensional scaling, and t-SNE. We would start with the zoo data set, where the data set is smaller, and the difference are less pronounced. We then traverse through employee attrition data set with exciting clusters exposed in t-SNE. To finish, and especially for the academic audience, we then show and compare the three different dimensionality reduction techniques on data from single-cell gene expression data.",
	"date" : "Jun 17, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-05-30-archaeology-workshop2/",
	"title": "Data Mining for Archaeologists, part II",
	"categories": ["archaeology", "workshop", "preprocess", "geolocation", "maps"],
	"description": "",
	"content": "This is the second part of a blog on archaeological data analysis in Orange. In the first part, we wrote about image analytics and how to predict amphora types. This blog will show a simpler analysis, where we will plot excavation sites onto maps and interpret the results.\nRelated: Data Mining for Archaeologists, part I\nThis time we will be working with the pottery data from the Antikythera Survey Project. You can download the data (pottery.csv) or simply copy-paste the below link in the File widget\u0026rsquo;s URL line.\nhttps://archaeologydataservice.ac.uk/catalogue/adsdata/arch-1115-2/dissemination/csv/pottery/pottery.csv\u0026amp;hs=true\n All is well, out data is here. Let us check it in the Box Plot, to see what are the distributions of our features.\n  Hmmm, it seems like the VesselPart feature contains a lot of bodies (B), bases (Ba), handles (H) and rims (R), while other shard parts are significantly less represented. Machine learning doesn\u0026rsquo;t handle discrete features with many values very well, so we will preprocess the data to make it easier to interpret and handle downstream.\n For data preprocessing, we will use Edit Domain. Let us select VesselPart. Feature values will be shown on the right. One option would be to manually merge values together, say Ba? and Ba. We do this by selecting the two values and clicking M in the lower right corner. Group selected values will group them together. Don\u0026rsquo;t forget to give them a name, say Ba_all.\n To avoid doing this for a large number of values, Edit Domain offers a nice way to merge infrequent values together. The final option is called Group all except N most frequent values, where we can set the N ourselves. We know from Box Plot there are 4 fairly frequent values, while the others are less frequent. So let us select 4.\n In the widget, you will see that all infrequent values are now merged and will appear under the label other. You can do the same for other features. Don\u0026rsquo;t forget to press Apply once you are done with preprocessing.\nCheck the results in a Box Plot. Much better!\n Now, let us plot the data on a map. You might have noticed the pottery shards have a location, where they were excavated (XSugg and YSugg). Unfortunately, these coordinates are encoded in an EPSG:32634 system, so we will have to transform them to the more common EPSG:4326 projection.\nFor this, we will need pyproj package, which you can install by going to Options - Add-ons. In the upper right corner, you will see Add more\u0026hellip; option. Select it. Now enter pyproj in the field. It will appear in the list of possible add-ons. Select it and run the installation.\n Once pyproj is installed, you can use Python Script widget to transform any projection to another one. Copy-paste the below script in the widget and run it.\nfrom pyproj import Transformer from Orange.data import ContinuousVariable new_lon = [] new_lat = [] transformer = Transformer.from_crs(\u0026quot;epsg:32634\u0026quot;, \u0026quot;epsg:4326\u0026quot;) for i in range(len(in_data)): lat, lon = transformer.transform(in_data[i][0], in_data[i][1]) new_lat.append(lat) new_lon.append(lon) out = in_data.add_column(ContinuousVariable(\u0026quot;lat\u0026quot;), new_lat) out_data = out.add_column(ContinuousVariable(\u0026quot;lon\u0026quot;), new_lon) To avoid installing pyproj, here is the transformed data set. Load it with the File widget and continue along.\nAt last, it is time to plot the data on a map. For this, you will need Geo add-on, which you can also install in Options - Add-ons.\nConnect Geo Map widget to Python Script (or directly to the File widget if you are using transformed data). Geo Map will automatically try to find latitude and longitude features. As with most visualizations in Orange, you can use other features to enhance the information gained from the plot. Let us use the VesselPart feature for coloring the points on the map.\n  Now for some homework: in which part of the island are located shards from the Hellenic period? (Hint: Use Select Rows and set the Hell value to be higher than 70, then plot the subset on the map.)\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-05-30-archaeology-small.png",
	"shortExcerpt" : "Mapping excavation sites in Orange.",
	"longExcerpt" :  "How to preprocess and map archaeological data sets." ,
	"author" : "Ajda Pretnar",
	"summary" : "This is the second part of a blog on archaeological data analysis in Orange. In the first part, we wrote about image analytics and how to predict amphora types. This blog will show a simpler analysis, where we will plot excavation sites onto maps and interpret the results.\nRelated: Data Mining for Archaeologists, part I\nThis time we will be working with the pottery data from the Antikythera Survey Project. You can download the data (pottery.",
	"date" : "May 30, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-04-23-archaeology-workshop/",
	"title": "Data Mining for Archaeologists, part I",
	"categories": ["archaeology", "workshop", "image analytics", "amphorae"],
	"description": "",
	"content": "Recently, we held a workshop for a group of archaeologists. While archaeologists are quite well-versed in quantitative analysis, data science was still quite new for most of the participants. Our aim was to introduce basic data science concepts through archaeological use cases. One such case that came to our mind was predicting a type of the artefact from the image.\nRelated: Data Mining for Anthropologists\nWe took three best-documented amphora types (types with the highest number of images) from the Archaeology Data Service portal. We also added some metadata describing each amphora subtype.\nThis is how our data looks like.\n Each row represents one amphora, with type, image URL, subtype, and metadata included. Let us observe the data in an Image Viewer from the Image Analytics add-on.\n  Images now have to be converted to numbers, so that predictive models will know how to infer patterns from them. The procedure of describing an image with a vector is called embedding and in Orange, it can be found in Image Embedding widget. We will use a simple, pre-trained Inception v3 model, but it is possible to train custom models specifically for archaeology.\nThe result of embedding is a long line of numbers.\n For the predictive model to consider only image vectors, we need to move metadata to \u0026hellip; well, meta attributes. We will do this with Select Columns.\n  Now, we can build our prediction model. Or a couple of them. We will use Logistic Regression, kNN, and SVM, as these are quite successful for working with images. We connect the data and the learners to Test and Score. Seems like all of our models are quite accurate, with logistic regression having the highest AUC score.\n Looking at the Confusion Matrix, logistic regression also best distinguishes between Dressels and Gauloises. It makes 13 mistakes, fairly equally confusing Dressels with Gauloises and vice versa. The other two classifiers more frequently confuse Gauloises for Dressels, so they are slightly biased in this sense.\n  It always makes sense to check the distribution of misclassifications to determine the quality of the model. If the model just predicts the most frequent class, it is useless. Having more data would surely make this model distinguish between amphora type better.\nWe can use the model for predicting the type of amphora for unlabelled images. Go to the internet and find some Dressels, Gauloises, and Keays. I have three images here. I put them in a single folder and I will load them with Import Images widget. We have to pass the data through another Image Embedding widget, because this data too needs numbers. Finally, we pass the data and one of the models (say, logistic regression) to Predictions. Don\u0026rsquo;t forget, logistic regression needs the data input to word with Predictions (you need to pass the model, not the learner).\n Seems like Dressel and Gauloise were successfully predicted, while Keay was mislabelled as a Gauloise. Not what we would have expected. Could archaeologists among you figure out, why this Keay amphora was mislabelled?\n In the second part of Data Mining for Archaeologists, we will have a look at geo-tagged data and how to plot them on a map.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-04-23-archaeology-small.png",
	"shortExcerpt" : "The many things archaeologists can do in Orange.",
	"longExcerpt" :  "A workshop about different kinds of analyses archaeologists can do in Orange." ,
	"author" : "Ajda Pretnar",
	"summary" : "Recently, we held a workshop for a group of archaeologists. While archaeologists are quite well-versed in quantitative analysis, data science was still quite new for most of the participants. Our aim was to introduce basic data science concepts through archaeological use cases. One such case that came to our mind was predicting a type of the artefact from the image.\nRelated: Data Mining for Anthropologists\nWe took three best-documented amphora types (types with the highest number of images) from the Archaeology Data Service portal.",
	"date" : "Apr 23, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-03-05-overfitting-course/",
	"title": "Hands-On Training About Overfitting",
	"categories": ["education", "teaching", "machine learning"],
	"description": "",
	"content": "PLOS Computation Biology has just published our paper on training about overfitting:\n Demšar J, Zupan B (2021) Hands-on training about overfitting. PLoS Comput Biol 17(3): e1008671.  Machine learning has recently propelled approaches for the analysis of data, but \u0026ldquo;for the uninitiated, the technology poses significant difficulties\u0026rdquo; (Deep learning for biology, Nature, Feb 22, 2018). One of the hard concepts for starters in machine learning is overfitting. Overfitting can lead to models that include patterns that do not generalize well and could be meaningless. It is thus vital to include teaching about overfitting in any data science course.\n For years, we have been developing Orange, a data science platform. Since we are also educators, we have designed Orange to support the teaching of concepts in machine learning. In the paper, we lay out a short course that uses Orange to teach about overfitting. The specific advantage of our proposed course is that it is entirely hands-on, can be carried out in few hours, does not require any prerequisites or much background knowledge, and is suitable for students of biomedicine or molecular biology that do not necessarily know how to code. The course layout we are proposing is practical; students learn by analyzing the data, making mistakes in the analysis that lead to overfitting, and correcting these by adjusting the workflows.\nIn the past several years, we have been giving and perfecting the lecture we are reporting in the paper. The lecture is carried out yearly at the University of Ljubljana, Slovenia, and at Baylor College of Medicine in Houston. The lecture was also included in over 50 short hands-on courses on machine learning we have been giving around the world. Our paper reports on the course structure, pedagogical principles we use in teaching, and a walk through the course that educators can use for teaching material and ideas within their lessons.\nOur other manuscripts, where we report on Orange as a tool for education in data science, include\n Stražar M, Žagar L, Kokošar J, Tanko V, Erjavec A, Poličar P, Starič A, Demšar J, Shaulsky G, Menon V, Lamire A, Parikh A, and Zupan B (2019) scOrange – A Tool for Hands-On Training of Concepts from Single Cell Data Analytics, Bioinformatics 35(14):i4-i12. Godec P, Pančur M, Ilenič N, Čopar A, Stražar M, Erjavec A, Pretnar A, Demšar J, Starič A, Toplak M, Žagar L, Hartman J, Wang H, Bellazzi R, Petrovič U, Garagna S, Zuccotti M, Park D, Shaulsky G, Zupan B (2019) Democratized image analytics by visual programming through integration of deep models and small-scale machine learning, Nature Communications 10(1):4551.  ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-01-11-orange-in-education-small.png",
	"shortExcerpt" : "We have designed a course on overfitting.",
	"longExcerpt" :  "PLOS Computation Biology has just published our paper on training about overfitting." ,
	"author" : "Blaž Zupan",
	"summary" : "PLOS Computation Biology has just published our paper on training about overfitting:\n Demšar J, Zupan B (2021) Hands-on training about overfitting. PLoS Comput Biol 17(3): e1008671.  Machine learning has recently propelled approaches for the analysis of data, but \u0026ldquo;for the uninitiated, the technology poses significant difficulties\u0026rdquo; (Deep learning for biology, Nature, Feb 22, 2018). One of the hard concepts for starters in machine learning is overfitting. Overfitting can lead to models that include patterns that do not generalize well and could be meaningless.",
	"date" : "Mar 5, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-02-10-explaining-models/",
	"title": "Explaining Predictive Models",
	"categories": ["model explanation", "explainable AI", "explain", "predictive modelling"],
	"description": "",
	"content": "It is easy to build powerful predictive models in Orange. But how does the model \u0026ldquo;look like\u0026rdquo;? Which attributes and which values of those attributes are important? And when making predictions, which attributes contributed to the decision? Orange\u0026rsquo;s new Explain add-on helps you answer all those questions.\nRelated: Explaining Models\nGo to Options \u0026ndash;\u0026gt; Add-ons and install Explain add-on. Restart Orange for the add-on to appear. It only contains two widgets, but boy are they great!\nLet us start with the attrition data set from the Datasets widget. We will go with Attrition - Train, which a data set on which employees resigned from the company and which stayed. The target variable is called Attrition, where No means that the employee stayed and Yes that the employee resigned. The other attributes describe the employee - her position, education, department, years since promotion, and so on.\n Next, we will build a simple logistic regression predictive model. If inspecting the model in Test and Score, we learn that the model has an AUC of 0.788 and 86 % CA. But what kind of a model is this? How does it makes its decisions?\nLet us add Explain Model to Logistic Regression and add another connection passing the data. The workflow should look like this:\n Now open Explain Model. The widget lists top ranked variables, which means they contribute the most to the selected target variable. As we are trying to understand why people leave the company, we have set the target variable to Yes.\nThe highest ranked variable is OverTime - this is the variable with the highest impact on the prediction. Having a value Yes in the categorical attribute OverTime (red dots on the right) means the employee is likely to quit. Also, having low job satisfaction contributes to attrition (blue values on the right). The visualization shows the values which have a high impact on the prediction of the selected class on the right and those which vote against the selected class on the left. The color of dot represents the value of the attribute (red for higher values and blue for lower).\n Let us look at, for example, YearsAtCompany. How would we interpret this? The variable has more red dots on the left, which means high variable values contribute against the target value (against attrition, i.e. employees will stay). Red dots refer to the value of the attribute. So if the employee is with the company for a long time (high value == red dot), it means it is more likely she will stay (dots are on the left, while our target value is Yes).\nGreat, now we understand the model and we are ready to make some predictions. Let us load Attrition - Predict with another Dataset widget. We have three new employees, who are described with all the previous variables, but they are lacking Attrition - we do not know, who is more likely to leave.\n Now pass the logistic regression model and the train data set to Explain Predictions. Then select John from the table and pass the selection to Explain Predictions. The widget requires three inputs: the model, training data, and the instance we are predicting (John).\n Once again, we are interested in target value Yes. Variables in red increase the probability of the target value (conversely, blue decrease it). The size of the arrow corresponds to the SHAP value - in other words, the larger the arrow the larger the variable\u0026rsquo;s contribution to the target value. The model also predicted that John will leave the job with 77 % probability.\n As before, the most important variable for John is overtime. Him working overtime contributes a lot to the final prediction. Also, his job satisfaction is low (1 out of 5), making him likely to quit.\nThe results correspond very much to those of the model, but it might not always be the case. Some people might leave because they are very dissatisfied without working overtime. This would show in Explain Predictions. See how the results change for the other two employees, Rachel and Veronica. Or make up your own employee with Excel and see what would the prediction be.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-02-10-explain-models-small.png",
	"shortExcerpt" : "New Orange add-on for explaining predictive models.",
	"longExcerpt" :  "New Orange Explain add-on for understanding predictions and predictive models." ,
	"author" : "Ajda Pretnar",
	"summary" : "It is easy to build powerful predictive models in Orange. But how does the model \u0026ldquo;look like\u0026rdquo;? Which attributes and which values of those attributes are important? And when making predictions, which attributes contributed to the decision? Orange\u0026rsquo;s new Explain add-on helps you answer all those questions.\nRelated: Explaining Models\nGo to Options \u0026ndash;\u0026gt; Add-ons and install Explain add-on. Restart Orange for the add-on to appear. It only contains two widgets, but boy are they great!",
	"date" : "Feb 10, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-01-27-word-distribution/",
	"title": "Observing Word Distribution",
	"categories": ["text mining", "word distribution", "bar plot", "word cloud"],
	"description": "",
	"content": "In text mining, one of key tasks is understanding and inspecting the corpus. It makes it easier to determine the preprocessing techniques and downstream analysis (the selection of document frequency weights, topic modelling technique, lemmatization and so on).\nEven though Orange sometimes doesn\u0026rsquo;t have a widget for a specific task, the said task can be achieved with a combination of widgets and their outputs. Let us look at an example of word distribution. There is no such widget in Orange, but word distributions are generally available in Word Cloud. Word Cloud shows a list of most frequent words and their frequencies on the left and a cloud visualization on the right.\n This is a great start, but Word Cloud only shows the 100 words and the visualization doesn\u0026rsquo;t directly correspond to the word frequency (words are scaled so that very frequent words don\u0026rsquo;t overwhelm less frequent ones). Yet Word Cloud has an output called Word Counts, which outputs a table with words and their frequencies in columns. Just what we would like to see!\n Can we see these frequencies as distributions? Yes, we can. A general widget showing numeric values (such as word counts) is Bar Plot. We pass the Word Counts output of Word Cloud to Bar Plot. We can also label each bar by setting Annotations to Word.\n If we zoom in, we can see that \u0026ldquo;the\u0026rdquo;, \u0026ldquo;and\u0026rdquo;, \u0026ldquo;of\u0026rdquo;, \u0026ldquo;to\u0026rdquo;, \u0026ldquo;a\u0026rdquo; are by far the most frequent words. This calls for some preprocessing, particularly stopword removal. We place Preprocess Text between Corpus and Word Cloud and use default preprocessing. Our bar plot has changed. Now, the most frequent word is \u0026ldquo;said\u0026rdquo;, so perhaps another round of stopword removal is necessary.\n  Orange widgets are intended to be as general as possible, which it easy to stack them into custom workflows. Don\u0026rsquo;t forget to explore all the outputs different widgets offer, for example the All Topics output from Topic Modelling, Concordances from Concordance, or Grouped Data from Pivot Table.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-01-27-word-distribution-small.png",
	"shortExcerpt" : "How to inspect word distribution in a corpus with in Orange.",
	"longExcerpt" :  "How to inspect word distribution in a corpus with a clever combination of widgets in Orange." ,
	"author" : "Ajda Pretnar",
	"summary" : "In text mining, one of key tasks is understanding and inspecting the corpus. It makes it easier to determine the preprocessing techniques and downstream analysis (the selection of document frequency weights, topic modelling technique, lemmatization and so on).\nEven though Orange sometimes doesn\u0026rsquo;t have a widget for a specific task, the said task can be achieved with a combination of widgets and their outputs. Let us look at an example of word distribution.",
	"date" : "Jan 27, 2021"
}

    
    , {
    "uri": "/blog/2021/2021-01-11-orange-in-classroom/",
	"title": "Orange in Classroom",
	"categories": ["Orange", "education", "teaching", "university"],
	"description": "",
	"content": "Orange in Classroom About three weeks ago, we put out a short survey asking professors, teaching assistants, and students to tell us how they use Orange in class. We have gotten four-hundred responses, and it turns out that over two hundred universities from around the world actively use Orange in the classroom.\nWe sincerely thank everyone for the answers!\n Here is a a list of universities, in alphabetical order, we have compiled from the survey.\n Ahmad Dahlan University, Indonesia Aix-Marseille University, France Amirkabir University of Technology, Iran Andrés Bello Catholic University, Venezuela Anil Neerukonda Institute of Technology \u0026amp; Sciences, India Antenor Orrego Private University, Peru BSE Institute Ltd., India Babeș-Bolyai University, Romania Bandung Institute of Technology, Indonesia Bandırma Onyedi Eylül University, Turkey Baylor College of Medicine, USA Birla Institute of Technology and Science, India Bocconi University, Italy Bombay Stock Exchange Institute, India Budi Luhur University, Indonesia California Polytechnic State University, USA Carleton University, Canada Center for Applied Mathemathics, Mexico Center of Higher Education of Brasilia, Brazil Central Washington University, USA Cestar College of Business, Health and Technology Charles University, Faculty of Science Chiang Mai University, Thailand Complutense University of Madrid, Spain DNACapitals, Singapore Daegu Software High School, South Korea Darmstadt University of Applied Sciences, Germany Data Science Dojo, USA Delft University of Technology, Netherlands Des Moines Area Community College, USA Dian Nuswantoro University, Indonesia Duta Bangsa University, Indonesia Ecuador Technological University, Ecuador Eskişehir Technical University, Turkey Estio Training, UK European Academy of Neurology, Austria Faculty of Sciences of the University of Lisbon, Portugal Federal Institute of Bahia, Brazil Federal Institute of Education, Science and Technology of Tocantins Federal Institute of São Paulo, Brazil Federal University of Goiás, Brazil Federal University of Pelotas, Brazil Federal University of Rio Grande, Brazil Federal University of Rio Grande do Norte, Brazil Federal University of Santa Maria, Brazil Florida State University, USA Francisco José de Caldas District University, Colombia Giresun University, Turkey Gunadarma University, Indonesia Guru Gobind Singh Indraprastha University, India Hacettepe University, Turkey Hankuk University of Foreign Studies, South Korea Harrisburg University of Science \u0026amp; Technology, USA Holon Institute of Technology, Israel I.E.S. Juan Carlos I, Spain ICFAI Business School, India ICFAI Business School Hyderabad, India IPB University, Indonesia ISLA Santarém, Portugal ITB STIKOM Bali, Indonesia ITC Infotech, India ITESO, Universidad Jesuita de Guadalajara ITM Business School, India IULM University - Milan, Italy Indian Institute of Management, India Indian Institute of Management Sambalpur, India Indian Statistical Institute, India Informatics \u0026amp; Business Institute Darmajaya, Indonesia Institut catholique d\u0026rsquo;arts et métiers, France Institute of Technical Education and Research, India Instituto Potosino de Investigación Científica y Tecnológica, Mexico Instituto Superior de Engenharia de Lisboa, Portugal Instituto Tecnológico Superior de Xalapa, Mexico International Trademark Association, USA JK Faculty, Brazil Jakarta State Polytechnic, Indonesia KAIST, South Korea Kielce University of Technology, Poland King Mongkut\u0026rsquo;s Institute of Technology Ladkrabang, Thailand Laval University, Canada Linnaeus University, Sweden Liverpool John Moores University, UK Lodz University of Technology, Poland Lviv Polytechnic National University, Ukraine MGMU Institute of Biosciences \u0026amp; Technology, India Mahidol University, Thailand Mauricio de Nassau Faculty, Brazil Memorial University of Newfoundland, Canada Mercu Buana University, Indonesia Mexican Institute of Knowledge Management, Mexico Mexican Institute of Social Security, Mexico Mittelhessen University of Applied Sciences, Germany National Central University, Taiwan National Chung Hsing University, Taiwan National Conservatory of Arts and Crafts, France National Institute of Technology Kurukshetra, India National School of Computer Sciences, Tunisia National Service for Industrial Training, Brazil National University, USA National University of General San Martín, Argentina New Bulgarian University, Bulgaria Nigerian Defence Academy, Nigeria North American University, USA Northern Alberta Institute of Technology, Canada Ohio University, USA Pablo de Olavide University, Spain Padjadjaran University, Indonesia Palacký University Olomouc, Czechia Panamerican University, Mexico Panteion University of Social and Political Sciences, Greece Peking University, China Pennsylvania State University, USA Pirogov Russian National Research Medical University, Russia Plovdiv University \u0026ldquo;Paisii Hilendarski\u0026rdquo;, Bolgaria Politecnica Salesiana University, Ecuador Polytechnic Institute of Coimbra, Portugal Polytechnic University of Yucatan, Mexico Pontifical Catholic University of Peru, Peru Pontifical Catholic University of Rio de Janeiro, Brazil Poznań University of Economics and Business, Poland Prague University of Economics and Business, Czech Republic Praxis Business School, India Professional Institute Santo Tomas, Chile RWTH Aachen University, Germany Republic Polytechnic, Singapore Research Institute for Development, France Rheinische University of Applied Science, Germany Riga Technical University, Latvia Riphah International University, Pakistan Rochester Institute of Technology, USA Sabanci University, Turkey Sabancı University, Turkey Sakarya University, Turkey Santo Tomas, Spain School of Information Management and Computer IKMI, Indonesia School of Technology and Management of Oliveira do Hospital, Portugal Sepuluh Nopember Institute of Technology, Indonesia Shahroud University of Technology, Iran Silesian University of Technology, Poland Singapore Polytechnic, Singapore Sol Solution, France Soongsil University, South Korea South Ural State University, Russia St. Petersburg State University of Industrial Technologies and Design, Russia Sungkyunkwan University, South Korea Syracuse University School of Information Studies, USA Södertörn University, Sweden Tecnológico de Monterrey, Mexico Telkom Institute of Technology, Indonesia Telkom Institute of Technology Purwokerto, Indonesia Texas State University, USA The American University (Nicaragua), Nicaragua The Free University of Berlin, Germany The Hong Kong Polytechnic University, Hongkong The Institute of Bioengineering of Catalonia, ? The United Nations University, The Netherlands The University of Santa Cruz do Sul, Brazil The University of Utah, USA Tohoku University of Community Service and Science, Japan Tokyo University of Science, Japan Treptow-Köpenick University of Appplied Sciences, Germany UNICAMP Universidade Estadual de Campinas, Brazil Universidad Autónoma Latinoamericana, Colombia Universidad EAFIT, Colombia Universidad Internacional SEK, Ecuador Universidad Norbert Wiener, Peru Universidad Tecnológica de Pereira, Colombia Universidade Estácio de Sá, Brazil Universidade do Sul de Santa Catarina, Brazil Universitas Gadjah Mada, Indonesia Universitas Narotama, Indonesia Universiti Utara Malaysia, Malaysia University Putra Malaysia, Malaysia University of Algarve, Portugal University of Applied Sciences Mittelhessen, Germany University of Belgrade, Serbia University of Brasília, Brazil University of Essex, UK University of Guadalajara, Mexico University of Guelph, Canada University of Health Sciences, Turkey University of Houston, USA University of Indonesia, Indonesia University of La Laguna, Spain University of Ljubljana, Slovenia University of Malaya, Malaysia University of Milano-Bicocca, Italy University of Monterrey, Mexico University of Montreal, Canada University of Nebraska Kearney, USA University of Oviedo, Spain University of Paris, France University of Split, Croatia University of Stirling, UK University of Technology and Applied Sciences, Oman University of Tübingen, Germany University of the Basque Country, Basque Country University of the Republic, Uruguay Università di Foggia, Italy Utah Valley University, USA Weill Cornell Medicine, USA Yogyakarta State University, Indonesia Zwickau University of Applied Sciences, Germany  ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2021/2021-01-11-orange-in-education-small.png",
	"shortExcerpt" : "Orange is used in over two hundred universities around the world.",
	"longExcerpt" :  "Orange is actively used in classrooms at over two hundred universities from around the world." ,
	"author" : "Ajda Pretnar",
	"summary" : "Orange in Classroom About three weeks ago, we put out a short survey asking professors, teaching assistants, and students to tell us how they use Orange in class. We have gotten four-hundred responses, and it turns out that over two hundred universities from around the world actively use Orange in the classroom.\nWe sincerely thank everyone for the answers!\n Here is a a list of universities, in alphabetical order, we have compiled from the survey.",
	"date" : "Jan 11, 2021"
}

    
    , {
    "uri": "/blog/2020/2020-12-21-year-in-code/",
	"title": "2020 - Year in Code",
	"categories": ["2020", "code", "overview", "Github"],
	"description": "",
	"content": "2020 - Year in Code 2020 is coming a to close. This year had its share of challenges, but we are among the lucky ones being able to work from home. Of course, some of us had to manage being a parent and a developer at the same time, but for the most part, we were successful. We\u0026rsquo;ve managed to write a couple of new widgets, solve issues, implement enhancement, wrote documentation, and tried to keep the Orange community alive and kicking.\nHere\u0026rsquo;s Orange\u0026rsquo;s year in code (and other stats).\nWe\u0026rsquo;ve made 1302 commits since Jan 1 2020, which amounts to more than 3 and a half per day! There were 20 contributors to the repository. The top contributor with 201 commits was @janezd, who wrote the first piece of Orange code back in 1996. A close second with 185 commits is @ales-erjavec, who is in charge of keeping Orange in top shape (has written and deleted the most lines of code too, +12,369 and -168,333).\n We\u0026rsquo;ve gone from version 3.24.0 to 3.27.1. There\u0026rsquo;s one new widget in core Orange (Bar Plot), 3 in Text mining (Statistics, Document Embedding, Corpus to Network), 1 in Network (Node Embedding), and one new add-on (Explain with 2 widgets).\nWe\u0026rsquo;ve had 651,479 views of YouTube videos and 6,700 new subscribers, which made us really happy! We produced 7 new videos, 3 about analyzing COVID-19 data and 4 about text mining. We\u0026rsquo;ve not been so good in terms of blog writing - we\u0026rsquo;ve only published 12 blogs. New Year\u0026rsquo;s resolution - write more blogs.\n Despite the pandemic, we\u0026rsquo;ve had three workshops, one summer school, and one online course. We absolutely hate holding lectures online, because we love to interact with our students and see in person how they use Orange. On the other hand, we\u0026rsquo;ve joined Discord, where users can ask and answer questions and we\u0026rsquo;ve trying Github Discussions as well. Our main goal for 2021 is to create an online community of Orange novices and experts, where the users will be able to exchange information, ideas, projects, data, code, and experiences (or just chat).\nWe wish you all a lovely holiday season and may 2021 treat us better. To a fruitful and fun new year!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-12-21-github-stats.png",
	"shortExcerpt" : "Statistics of Orange development in 2020.",
	"longExcerpt" :  "Statistical report on Orange software development and educational content for 2020." ,
	"author" : "Ajda Pretnar",
	"summary" : "2020 - Year in Code 2020 is coming a to close. This year had its share of challenges, but we are among the lucky ones being able to work from home. Of course, some of us had to manage being a parent and a developer at the same time, but for the most part, we were successful. We\u0026rsquo;ve managed to write a couple of new widgets, solve issues, implement enhancement, wrote documentation, and tried to keep the Orange community alive and kicking.",
	"date" : "Dec 21, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-10-15-document-embedders/",
	"title": "How to identify fake news with document embeddings",
	"categories": ["text mining", "corpus", "classification"],
	"description": "",
	"content": "Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText. In this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.\nWord embedding and document embedding Before we understand document embeddings, we need to understand the concept of word embeddings. Word embedding is a representation of a word in multidimensional spaces such that words with similar meanings have similar embedding. It means that each word is mapped to the vector of real numbers that represents the word. Embedding models are mostly based on neural networks.\nDocument embedding is computed in two steps. First, each word is embedded with the word embedding then word embeddings are aggregated. The most common type of aggregation is the average over each dimension.\nWhy and when should we use embedders? Compared to bag-of-words, which counts the number of appearances of each token in the document, embeddings have two main advantages:\n They do not have a dimensionality problem The result of bag-of-words is a table which has the number of features equal to the number of unique tokens in all documents in a corpus. Large corpora with long texts result in a large number of unique tokens. It results in huge tables which can exceed memory in the computer. Huge tables also increase the learning and evaluation time of machine learning models. Embedders have constant dimensionality of the vector, which is 300 for fastText embeddings that Orange uses. Most of the preprocessing is not required In case of bag-of-words approach, we solve the dimensionality problem with the text preprocessing where we remove tokens (e.g. words) that seem to be less important for the analysis. It can also cause the removal of some important tokens. When using embedders, we do not need to remove tokens, so we are not losing the accuracy this way. Also most of the basic preprocessing can be omitted (such as normalization) in case of fastText embeddings. They can be pretrained Word embedding models can be pretrained on large corpora with billions of tokens. That way, they capture the significant characteristics of the language and produce the embeddings of high quality. Pretrained models are then used to obtain embeddings of smaller datasets. Our Document Embedding widget uses pretrained fastText models and is suitable for corpora of any size.  The shortcoming of the embedders is that they are difficult to understand. For example, when we use a bag-of-words, we can easily observe which tokens are important for classification with Nomogram widget since tokens themselves are features. In the case of document embeddings, features are numbers which are not understandable to human by themselves.\nDocument Embedding widget Orange now offers document embedders through Document Embedding widget. We decided to use fastText pretrained embedders, which support 157 languages. Orange\u0026rsquo;s Document Embedding widget currently supports 31 most common languages.\n In the widget, the user sets the language of documents and the aggregation method \u0026ndash; it is how embeddings for each word in a document are aggregated into one document embedding.\nThe Fake News dataset For this tutorial, we use the sample of Fake News dataset. The dataset sample is available at Orange\u0026rsquo;s file repository. It is a zip archive containing two datasets: training set including 2725 text items and testing set with 275 items. Each item is an article which is labelled as a real or fake.\nFake news identification Here we present a fake news identification. First, we will load a training part of the dataset with the Corpus widget.\n After the dataset is loaded, we make sure that the text variable is selected in the Used text features field. It means that the text in this variable is used in the text analysis. When the dataset is loaded we connect the Corpus widget to the Document embedder widget which will compute text embeddings. Our workflow should look like this now:\n In the document embeddings widget, we check that language is set to English since texts in this dataset are English. We will use mean (average) aggregation in this experiment \u0026ndash; it is the most standard one. After minute documents are embedded \u0026ndash; embedding progress is shown with the bar around the widget.\nWhen embeddings are ready, we can train models. In this tutorial, we train two models \u0026ndash; Logistic regression and Random forest. We will use default settings for both learners.\n When our models are trained, we prepare the testing data. To load testing data, we use another Corpus widget and connect it to the Document embedder widget. Settings are the same as before. The only difference is that this time we load testing part of the dataset in the second Corpus widget. To make predictions and inspect the prediction results on the testing dataset, we use the prediction widget.\n In the bottom part of the widget, we inspect the accuracies. In the column with name CA (classification accuracy), we can see that both models perform with around 80 % accuracy. In the table above, we can find cases where models made mistakes. If we select rows, we can check them in the Corpus Viewer widget which is connected to the Predictions widget. We have also connected the confusion matrix widget to our workflow, which shows the proportions between the predicted and actual classes.\n We can see that Logistic regression is slightly more accurate in cases of real news while Random forest model is better for predicting fake news.\nIt is just one example which shows how to use document embeddings. You can also use them for other tasks such as clustering, regression or other types of analysis.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-10-15-document-embedding-widget.png",
	"shortExcerpt" : "New Document embedder widget and its use for classification",
	"longExcerpt" :  "Presenting document embeddings widget and how to identify fake news." ,
	"author" : "Primož Godec and Nikola Đukić",
	"summary" : "Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText. In this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.",
	"date" : "Oct 15, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-09-28-text-tutorials/",
	"title": "New Video Tutorials on Text Mining",
	"categories": ["text mining", "tutorial", "video", "twitter", "sentiment analysis", "embedding"],
	"description": "",
	"content": "In July, we were pleasantly surprised to be awarded a NumFocus Small Development Grant, which is intended to support small tasks in open source projects they sponsor. We decided to extend our text mining tutorials with four new videos, which cover the recent additions to the Text Mining add-on. Our YouTube channel already has a playlist for getting started with Orange and several specialized playlists for learning spectroscopy, single-cell analysis, text mining and image analytics with Orange.\nRelated: Getting Started Series Part 2\nWhile Twitter widget is not a new addition to the Text add-on, it has been missing a tutorial all this time. In the video, we describe how to use the widget and how to perform topic modelling on tweets.\n The second video describes sentiment analysis on tweets for monitoring topic or brand sentiment.\n The third video shows the alternative to computing a bag-of-words matrix. Document embeddings are a popular alternative, which can increase model\u0026rsquo;s accuracy.\n Finally, we show how to compute a network from Twitter mentions. This tutorials also shows how to mix-and-match Orange components from different add-ons.\n Don\u0026rsquo;t forget to subscribe to our channel for more videos! And give us a thumbs up if you enjoyed the tutorials. :)\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-09-28-text-tutorials.png",
	"shortExcerpt" : "New video tutorials on text mining available on our YouTube channel.",
	"longExcerpt" :  "New video tutorials on text mining available on our YouTube channel." ,
	"author" : "Ajda Pretnar",
	"summary" : "In July, we were pleasantly surprised to be awarded a NumFocus Small Development Grant, which is intended to support small tasks in open source projects they sponsor. We decided to extend our text mining tutorials with four new videos, which cover the recent additions to the Text Mining add-on. Our YouTube channel already has a playlist for getting started with Orange and several specialized playlists for learning spectroscopy, single-cell analysis, text mining and image analytics with Orange.",
	"date" : "Sep 28, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-07-27-story-arcs/",
	"title": "Detecting Story Arcs with Orange",
	"categories": ["text mining", "sentiment analysis", "corpus", "story arc", "heat map", "line chart"],
	"description": "",
	"content": "Reading is fun because it takes you on a journey. Mostly, it is a journey of emotions as you live and breathe with the protagonist and her adventures. Today, we will have a look at how to detect sentiment in a story, plot story arcs and analyze the content of the key segments in a corpus.\nRelated: Text Workshops in Ljubljana\nWe will be using a corpus of Anderson\u0026rsquo;s tales, which is available in the Corpus widget (data set anderson.tab). Load it in the widget. Next, we will select a single tale which we will analyze, say, Little Match Seller. Connect Corpus to Data Table and select the tale. We all know the story of a little girl selling matches on a New Year\u0026rsquo;s Eve and freezing to death. It is one of the saddest stories ever told. One could almost forget there are positive parts, such as the girl\u0026rsquo;s visions in the moments before her death, which show a glimmer of hope, the only consolation the girl had in her life. Let us verify this in Orange.\n   With our story selected, we have to split it into sentences. At the moment, our story is a single row in the data, but we wish to have each sentence in its own row. We will use Preprocess Text and select Sentence tokenization. I have removed the redundant preprocessors and kept the only one we need.\n Some Python magic will help us create a new corpus from the existing tokens (sentences). Copy and paste the script below into the Python Script widget. Do not forget to press Run once you have pasted the script into the widget.\nimport numpy as np from Orange.data import Domain, StringVariable from orangecontrib.text.corpus import Corpus tokens = in_data.tokens new_domain = Domain(attributes=[], metas=[StringVariable('Sentences'), StringVariable('Title')]) titles = [] content = [] for i, doc in enumerate(tokens): for t in doc: titles.append(in_data[i]['Title'].value) content.append(t) metas = np.column_stack((content, titles)) out_data = Corpus.from_numpy(domain=new_domain, X=np.empty((len(content), 0)), metas=metas) out_data.set_text_features([StringVariable('Sentences')])   Perfect, our data is now ready for the final step. Add another Preprocess Text and keep the default preprocessors. They will lowercase our sentences, split by words and remove English stopwords.\n Finally, add the Sentiment Analysis widget. By default, the widget uses the Vader algorithm, which works quite well for English. Please note that it won\u0026rsquo;t work for other languages, as all of Orange\u0026rsquo;s sentiment models are language specific. You can use Multilingual sentiment for non-English texts.\n  At last, it is time to analyze the data. We will use Timeseries add-on to plot sequential data. First, we will pass the data to the As Timeseries widget and set the Sequence implied by the instance order option. This tells Orange that our data is already ordered by time - in our case, by the order in which each sentence appears in the story.\n Connect Line Chart to As Timeseries. In the widget, select Compound variable, which shows the total sentiment of each sentence. The peaks represent the parts of the story with positive emotions and the drops the parts with the negative ones.\n  To explore the data in depth, connect Heat Map to Sentiment Analysis. Heat Map will show all 4 sentiment attributes, namely positive (pos), negative (neg), neutral (neu) and compound sentiment. But our data is all over the place. Let us order it. Select Clustering (opt. ordering) under the Clustering - Rows option. This will cluster the sentences by their similarity, specifically by how similar their emotion is.\n Great! Now this looks like something useful. The blue sections represent the negative sentences, while the yellow and white sections represent the positive ones. Let us select the cluster with negative sentences and observe it in a Corpus Viewer.\n  I have set Sentences as the single Display variable and selected all the sentences to show them in a list. Unsurprisingly, here is the last sentence of the story, in which a girl if found dead in the street on a New Year\u0026rsquo;s Day.\nTo finish, let us explore the positive sentences, too. Select the positive section in the Heat Map and observe it in a Corpus Viewer. Now rethink the story, reread her visions in the last moments of her life and how happy she was when she died. Couldn\u0026rsquo;t we say that \u0026hellip; the story has a happy ending?\n While the workflow is quite long, it is conceptually very simple. This is a quick and easy way to explore the story arcs and sentiment in a text. We imagine this to be a very useful tool for the teachers who wish to experiment a bit in their language classes and offer a fun and fruitful way of exploring literature.\n ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-07-27-story-arcs-small.png",
	"shortExcerpt" : "How to detect and analyze story arcs in a corpus.",
	"longExcerpt" :  "How to detect sentiment, plot story arcs and analyze the key segments in a corpus." ,
	"author" : "Ajda Pretnar",
	"summary" : "Reading is fun because it takes you on a journey. Mostly, it is a journey of emotions as you live and breathe with the protagonist and her adventures. Today, we will have a look at how to detect sentiment in a story, plot story arcs and analyze the content of the key segments in a corpus.\nRelated: Text Workshops in Ljubljana\nWe will be using a corpus of Anderson\u0026rsquo;s tales, which is available in the Corpus widget (data set anderson.",
	"date" : "Jul 27, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-06-19-edit-domain/",
	"title": "Managing Data with Edit Domain",
	"categories": ["edit", "domain", "data"],
	"description": "",
	"content": "Importing data into Orange is easy. File, import, and voila, your data is here. But what about if you want to rename a variable, change it\u0026rsquo;s type or edit labels? Edit Domain to the rescue!\nFirst of all, what is \u0026lsquo;domain\u0026rsquo;. Domain is like a metadata of your data - it describes column names, column types (categorical, numeric, string, datetime), and values for categorical variables. You will come across domain everywhere in Orange, because Orange\u0026rsquo;s table (Orange.data.Table for programmers) is nothing without it.\nEdit Domain helps you organize the domain of your data. Let us use Datasets widget and load HDI, a dataset of human development index for most countries in the world. We have 188 rows (countries) and 66 features (index variables).\n Now, let us check the domain in Edit Domain. First of all, our variable names are long, so we can make them shorter with Edit Domain. Select, say, Gross National Income (GNI) per capita and rename it to GNI. Simple! If you are unhappy with the change, simply press Reset Selected at the bottom of the widget.\n Scroll down and check the rest of the variables. We have two categorical variables in the data, child labour and maternity leave. Let us check Child labour (% ages 5-14) 2009-2015 and rename it first to Child labour. You can see the variable has many values, from 1 to 47. How about we merge the less frequent values into one?\n Press M on the right side of the widget and a new window will pop up. There are several ways to group less frequent variables, but let\u0026rsquo;s go with merging all but top 10 most frequent values. Let us also change the label of these values from other to infrequent.\n Finally, we will select Country from the list and set it as a categorical variable. Click on Type drop down and select Categorical. This will reinterpret the variable according to the selected type. Now you can also see the values of the reinterpreted variable. Double-click Antigua and Barb. and rename it to Antigua and Barbuda. You can use up and down arrows to change the order of the variable. Let us push Antigua and Barbuda to the top. The order will be evident in, say, visualizations and their legends. If you connect Line Plot to Edit Domain and set Group by to Country, Antigua and Barbuda will have a long name and will be placed at the top.\n  Edit Domain is a great widget to organize your data. See documentation for other great widgets, such as Create Class or Feature Statistics.\n ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-06-19-edit-domain-small.png",
	"shortExcerpt" : "Handle your data with Edit Domain.",
	"longExcerpt" :  "How to handle your data with Edit Domain - rename, change type, merge, sort..." ,
	"author" : "Ajda Pretnar",
	"summary" : "Importing data into Orange is easy. File, import, and voila, your data is here. But what about if you want to rename a variable, change it\u0026rsquo;s type or edit labels? Edit Domain to the rescue!\nFirst of all, what is \u0026lsquo;domain\u0026rsquo;. Domain is like a metadata of your data - it describes column names, column types (categorical, numeric, string, datetime), and values for categorical variables. You will come across domain everywhere in Orange, because Orange\u0026rsquo;s table (Orange.",
	"date" : "Jun 19, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-04-15-covid-19-part-3/",
	"title": "Data Mining COVID-19 Epidemics: Part 3",
	"categories": ["covid-19", "visualization", "addons", "trends", "time"],
	"description": "",
	"content": "So far, we\u0026rsquo;ve seen how to make basic visualizations related to the corona virus and how to look at the disease progression on the map. Be sure to check them out first, before delving into this one.\nWe are now heading towards somewhat more advanced visualizations that let us observe trends in the data. Just as a heads up: your results may be different, depending on the day you downloaded the data. We are working with confirmed cases up to April 13, on the previously mentioned data from the John Hopkins University.\nPrerequisite: Timeseries add-on The spread of the virus is influenced by many factors, time being one of them. Timeseries add-on specializes in manipulation of time-related data. If you followed our blogs, you should already have it installed. If you don\u0026rsquo;t, just take a peek in the second one.\nFormatting the data As Timeseries We\u0026rsquo;ll load our data as usual with the File widget. The data contains latitude and longitude columns, which are just getting in the way in our further analysis. We will use Select Columns to put them into meta columns and thus exclude them from any calculations. Then we\u0026rsquo;ll select some countries we are interested in with the help of the Data Table. You could also do it straight after inspecting the data in Geo Map.\n Now we just need to tell Orange, which variable contains time stamps. Our dates are represented as columns instead of as rows, so we\u0026rsquo;ll use Transpose widget to make each row represent a datum and then connect it to As Timeseries Widget. In Transpose, we set the variable whose values will be used to name the rows when they are turned into columns. We\u0026rsquo;ll name them by the column Country, as shown in the image, since the Province field is mostly empty. In As Timeseries, we set sequence as implied by instance order.\n Here is our chain of widgets. We should now be ready to start making some plots. Phew.\n Line Chart and log plot Make sure you\u0026rsquo;ve selected countries you are interested in the Data Table. I\u0026rsquo;ve chosen Italy, US, Iran, France and Chinese province Hubei. Then, connect As Timeseries widget and Line Chart and let\u0026rsquo;s get plotting. Once in Line Chart, we can select multiple countries by clicking Ctrl/Cmd and draw them on the same axis or we could compare multiple plots by clicking on Add plot.\n Here, we see how the virus started its path in China, where the number of cases quickly rose but has since then stayed level. Other countries were hit later on. The slopes of the curves tell us how fast the numbers are growing, but more on that later. One thing to notice though: due to the fast growth of the virus in some countries, some smaller slopes are almost invisible. We can remedy that by checking the Logarithmic axis box just like this:\n We can now see the first steps as well as the current situation.\nPython Script We can see from these plots, that different countries came into contact with the virus at different times. It seems it would be much easier to compare the curves if all of them started their climb at the same time. We can do that with just a few lines of code. Example snippet here aligns the curves with the moment where the country recorded 100 cases. You can set n to any other (positive) number if you will, though.\nimport numpy as np from copy import deepcopy n = 100 out_data = deepcopy(in_data) out_data.X[out_data.X \u0026lt; n] = 0 ar = np.argwhere(out_data.X) cols, shifts = np.unique(ar[:, 1], return_counts=True) out_data.X = np.array([np.roll(out_data.X[:, col], shift) for col, shift in zip(cols, shifts)]).T out_data.X[out_data.X==0]= np.nan Copy these lines into Python Script widget, and plug it into our initial snake of widgets like so: \nLook at the aligned curves now, they are much easier to compare. \nAbsolute growth with derivative Let\u0026rsquo;s take a look at how fast the confirmed cases are spreading. Connect As Timeseries with Difference widget, choose Differencing and select countries of interest. Differencing order of 1 means we\u0026rsquo;ll be looking at derivative of first order, which just means daily change in our case. We\u0026rsquo;ll leave shift as is, on 1.\n Again, we\u0026rsquo;ll view the transformed curves in Line Chart. Notice how the difference (and every other) transform adds a new column? That means we can compare our curves in different states, so be sure to always check what is being shown. For starters, try looking at, for example, China and its transformed version. See how easy it is to spot the daily spikes that are perhaps out of the ordinary and need to be checked? The most prominent spike here is probably due to the change in counting.\n If we again compare multiple countries, we notice the US\u0026rsquo;s speedy climb and Iran seemingly being more successful in curbing the growth.\n The difference could be only due to the difference in population and what is an overwhelmingly large number of patients for one country might be almost business as usual for another one.\nScale free growth with quotient Difference widget also has the option to output the quotients. It will allow us to observe the relative growth and compare countries directly.\n Let\u0026rsquo;s look at, for example, China (Hubei province) vs France.\n It seems China is doing worse in the beginning and better later on, but it is impossible to tell at which point the trends start to shift. There are just too many jumps, due to noise in testing and reporting. So for our final step, let\u0026rsquo;s smooth things out.\nMoving transform and smoothing Smoothing is usually a part of preprocessing, so insert the Moving Transform widget between As Timeseries and Difference, and add some moving average transforms, like this:\n  Selecting now smoothened China and the France data in Difference widget, yields the following plot in Line Chart:\n The trends now seem clearer. The countries have somewhat similar trends up to 15th day, when China\u0026rsquo;s growth falls decisively below the French.\nQuotient plot obviously benefited from smoothing, as did the difference plot. For completeness, we added another Difference widget after Moving Transform, and plotted now smoothed difference between the two countries. Notice how the turning point is now seemingly further down the line. Orange prides itself on interactivity, so just by sliding the mouse over the plot, we get the exact information regarding the days and counts. The turning point seems to be on the 25th day.\n This blog concludes our Corona virus series for now. Until our next blog, stay safe.\n  Orange is a multi-platform open-source machine learning and data visualization tool for beginners and experts alike. Download Orange, and load and explore your own data sets!\nIn addition to a variety of learning materials posted online in the form of blog posts, tutorial videos, we\u0026rsquo;ve created a Discord server. Join the community, tell us what you think!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-04-15-thumb-log-growth.png",
	"shortExcerpt" : "Inspection of Covid-19 time trends.",
	"longExcerpt" :  "Inspecting and comparing Covid-19 time trends, absolute and relative growth." ,
	"author" : "Andreja Kovačič",
	"summary" : "So far, we\u0026rsquo;ve seen how to make basic visualizations related to the corona virus and how to look at the disease progression on the map. Be sure to check them out first, before delving into this one.\nWe are now heading towards somewhat more advanced visualizations that let us observe trends in the data. Just as a heads up: your results may be different, depending on the day you downloaded the data.",
	"date" : "Apr 20, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-04-09-covid-19-part-2/",
	"title": "Data Mining COVID-19 Epidemics: Part 2",
	"categories": ["covid-19", "visualization", "addons", "geo", "time"],
	"description": "",
	"content": "Previously on Data Mining COVID-19 Epidemics: Part 1 we fired up a COVID-19 epidemics data set and looked at some basic visualizations. If you haven\u0026rsquo;t got your hands dirty with it yet, check that out first.\nIn this post, we\u0026rsquo;ll try putting the data on a map. We\u0026rsquo;ll also expand on the \u0026ldquo;data mining is interactive\u0026rdquo; mantra by creating some animations showing the epidemics spread throughout the world.\nPrerequisite: installing add-ons Orange already comes with a great selection of widgets that work with all sorts of data. But sometimes we\u0026rsquo;re faced with a particular data set, such as textual data, time-series, or geographical data, which requires widgets with specific functionality – add-ons to the rescue! Add-ons are packages of widgets specially designed to handle specific data types.\nWe\u0026rsquo;ll be working with the Geo and Timeseries add-ons. To install them, open Options in the menu bar and select Add-ons. A new window will open, showing a list of officially supported add-ons. Select Orange3-Geo and Orange3-TimeSeries and click OK. After installation, restart Orange. Check out the new sections in the widget toolbox; you should see some cool new widgets.\n Note: the two add-ons have been revamped very recently. If you already have them, use the Add-ons dialog to upgrade them to the latest version.\nFrom Scatter Plot to Geo Map You should already be familiar with the Scatter Plot widget. In the previous blog post, we used it as an impromptu map by putting latitude and longitude on the axes.\n A better solution is to use Geo Map from the Orange3-Geo add-on. It provides a variety of maps as backgrounds. With them, it\u0026rsquo;s much easier to associate points with locations. To start, first load the COVID-19 data set using the File widget, as shown in the previous post. Then, add and connect a Geo Map widget.\n Latitude and longitude are automatically selected. Below, you can define similar point properties, as in the scatter plot. The size of a point can intuitively represent the number of cases, so let\u0026rsquo;s select the latest date in the Size dropdown. If you\u0026rsquo;re feeling adventurous, try changing the map type to satellite or topological.\n Just like in Scatter Plot, you can scroll to zoom in and out, and click/drag to select data points. As opposed to using Scatter Plot, showing the points over a map of Europe makes it easier to select the countries we want to inspect.\n The chloro\u0026hellip; chorophl\u0026hellip; chloropet\u0026hellip; map with areas colored in The map is nice and informative, and it\u0026rsquo;s easy to distinguish larger epidemics from smaller ones. Still, those similarly-sized points on the map represent entire countries from big Russia to small Slovenia. We could, alternatively, represent countries by coloring their surface area. This is a job for Choropleth Map. Let\u0026rsquo;s add it and connect it.\n This widget is similar to Geo Map, but instead of displaying data as points, it aggregates them into regions (e.g., Country, State, Province), and displays those instead. Under Attribute, select the latest date and under Agg., select sum. The colored regions now show the sum of confirmed COVID-19 cases for each country.\nTry choosing different dates. Notice how China initially experiences rapid growth, followed by a period of stagnation, and how in the past week, the USA overtook all other countries' confirmed COVID-19 case count.\n A warning that 7 points are not in any region refers to some small islands and cruise ships included in the data.\nIf a picture paints a thousand words, an animation paints a million When working with time-series data, the coolest insights arise from observing trends – how data changes depending on time. Let\u0026rsquo;s animate our maps!\nWe\u0026rsquo;ll be using a special widget from Orange3-Timeseries: Time Slice, which lets you select data in a time window. It also features a playback function that slowly moves the time window, outputting corresponding data as it goes.\nTranspose: part I Time Slice requires time instances in the form of rows, while our data specifies them in columns. Let\u0026rsquo;s start again with the File widget that loads the data. Move Lat and Long to meta attributes by double-clicking feature in the Role column and selecting meta. This will tell Orange to exclude them from data manipulations.\n To flip rows and columns, we use the Transpose widget. Connect it to the File widget.\n Set From variable to Country/Region. Ignore the warning that indices were added to variables with the same name; it won\u0026rsquo;t hurt us.\n To better understand what just happened, connect a Data Table to Transpose and see the result: rows and columns were swapped. Columns are named after countries (because we chose Country/Region in the Transpose widget), while a new column, Feature name, contains the original names of the columns.\n Right now, Orange is treating values in Feature name as strings. We need to tell Orange that the feature contains timestamps. Let\u0026rsquo;s connect an Edit Domain widget.\n Scroll to the bottom of Variables and select Feature name. In Edit, change its type to Time. While you\u0026rsquo;re at it, change its name to something meaningful, like date. Finally, click Apply\n The data is now ready to be processed by Time Slice.\n  The top indicates the currently selected slice of data. You can change the interval by interacting with the histogram, or by adjusting the start and end dates directly below. Let\u0026rsquo;s select a day\u0026rsquo;s slice between the 25th and 26th January 2020.\nWith a click of the play button, the slice moves, and the data output updates accordingly. To verify that it works, connect a Data Table, and see it in action.\nTranspose: part II Make sure to stop the Time Slice playback before proceeding. We\u0026rsquo;re almost at the finish line; we just need to transform the data back into its original form. To do so, connect another Transpose widget.\n Under Future names select Generic, which will name the new column Feature 1.\n Let\u0026rsquo;s connect a Data Table for a final check of our data before putting it on a map.\n Looking good!\nThe final step entails connecting the Choropleth Map, selecting Feature 1 as Attribute, and Sum as the Agg.\n To observe the animation, open up both Choropleth and Time Slice at the same time.\nProtip: pressing Ctrl/CMD+Up reveals the workflow, Ctrl/CMD+Down reveals widget windows.\nAll that\u0026rsquo;s left is to hit play.\n Now that we\u0026rsquo;re familiar with Time Slice, let\u0026rsquo;s use it with Geo Map too. For bonus points, let\u0026rsquo;s also remove countries with zero confirmed cases. That way, the number of points on the map will increase as the virus spreads throughout the world. For this, we\u0026rsquo;ll use a Select Rows widget.\n Under Conditions, select Feature 1, select is greater than, and type in 0.\n Add and connect a Geo Map widget. Zoom out, so you see the whole world, and on the left side, check Freeze map. This will prevented the map from jumping around as new points are added.\n Press play once again – let\u0026rsquo;s see what we\u0026rsquo;ve built!\n Where from here? In the blog, we explained how to turn Orange into an interactive data exploration tool for COVID-19 data! With a few widgets we were able to determine the change in time for the confirmed cases and plot the results on a map.\nTime Slice isn\u0026rsquo;t the only widget in the Orange3-Timeseries add-on. Stay tuned for next week\u0026rsquo;s blog, where we\u0026rsquo;ll delve deeper into the patterns emerging from time-series data.\n Orange is a multi-platform open-source machine learning and data visualization tool for beginners and experts alike. Download Orange, and load and explore your own data sets!\nIn addition to a variety of learning materials posted online in the form of blog posts, tutorial videos, we\u0026rsquo;ve created a Discord server. Join the community, tell us what you think!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-04-09-choropleth.png",
	"shortExcerpt" : "Visualizing COVID-19 data using maps.",
	"longExcerpt" :  "Visualizing COVID-19 data using area and point maps and interactive timeseries." ,
	"author" : "Robert Cvitkovič",
	"summary" : "Previously on Data Mining COVID-19 Epidemics: Part 1 we fired up a COVID-19 epidemics data set and looked at some basic visualizations. If you haven\u0026rsquo;t got your hands dirty with it yet, check that out first.\nIn this post, we\u0026rsquo;ll try putting the data on a map. We\u0026rsquo;ll also expand on the \u0026ldquo;data mining is interactive\u0026rdquo; mantra by creating some animations showing the epidemics spread throughout the world.\nPrerequisite: installing add-ons Orange already comes with a great selection of widgets that work with all sorts of data.",
	"date" : "Apr 13, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-04-02-covid-19-basic/",
	"title": "Data Mining COVID-19 Epidemics: Part 1",
	"categories": ["covid-19", "feature construction", "line plot"],
	"description": "",
	"content": "These days we are all following the statistics of COVID-19, looking at how our own country is faring and how it\u0026rsquo;s comparing with other countries. Luckily, only a few have a statistically meaningful number of deaths (which solemnly reminds us of the difference between statistical and practical significance!), so we concentrate on the number of confirmed cases.\nYou\u0026rsquo;re reading the first and most basic blog post from a series in which we will investigate this data using Orange. Most people are capable of doing something in Excel(-like programs), and some can do everything in Python with pandas and jupyter. I\u0026rsquo;ll show you how many people can do many things in Orange.\nToday, we will see how to get this data into Orange, draw some basic curves, and relate it to other data sources. Don\u0026rsquo;t expect anything dramatic; this will be more about showing some creative ways of connecting a few widgets, and starting to explore the data about COVID-19 epidemics.\nGetting the data John Hopkins University collated some COVID-19 information in a machine-readable format and published it on Github. We will examine the table with confirmed cases by regions and countries.\nTo get the numbers behind the above table, click \u0026ldquo;Raw\u0026rdquo;. Or this link: https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv. Save the page and you\u0026rsquo;ll have a file to play with.\n Easier still, Orange\u0026rsquo;s File widget can load the data directly from the web, and handles a basic .csv file (for more fine-grained options, use CSV File Import). \nSo, for starters, add the File widget to the canvas and copy the above link to the URL field. You can then connect it to a Data Table widget and check that everything\u0026rsquo;s loaded OK. You should see a table with rows corresponding to regions and countries, and columns corresponding to dates, with two additional columns detailing region locations (latitude, longitude).\nNote that this is live data, so your results will differ from those we show here.\nPlot the usual plots  Connect the File to a Line Plot widget to see the graph. By default, Line Plot shows means and ranges, while we are interested in raw lines. Let\u0026rsquo;s click the checkboxes accordingly.\n Pretty boring. Anybody can do this in Excel. Orange allows us to play with these curves, though.\n The curve that grows rapidly but then flattens out \u0026ndash; is probably China, right? To check this, select it. It may be hard to click the curve, so select it by dragging a line across it.\n Then, connect the Line Plot widget to Data Table, and it will show the data for this curve. You\u0026rsquo;ll learn that this is, of course, the Hubei province – where it all started.\n What are the fast-rising curves? Select them by dragging a line across them, and check the Data Table widget: these are the US, Iran, Italy, Spain, Germany and France.\n We can do it the other way around: connect File to Data Table and Data Table to Line Plot, such that the Data Table sends its selected subset of data.\n In the Data Table, you can now find and click your favourite country. Line Plot will now highlight the curve for Slovenia (or whichever country you chose). You can also select multiple countries, like all the Chinese provinces. Or all European countries.\nWell, the latter is a bit difficult; there is no data about the continent. There\u0026rsquo;s a trick, though.\n Replace the Data Table widget with a Scatter Plot widget, and choose Long and Lat for x and y respectively. Zoom in on Europe and select the points by dragging across them. Line Plot will highlight the curves belonging to the countries chosen in the Scatter Plot. \nIn the case of Slovenia (and possibly your choice as well), we couldn\u0026rsquo;t really see its curve because it\u0026rsquo;s negligent in comparison with others. The only thing we\u0026rsquo;ve learned is that the 327-million-man US have more cases of COVID-19 than a 2-million-man Slovenia. The same goes for Chinese provinces: the only one that sticks out is Hubei. Also, the \u0026ldquo;map\u0026rdquo; we improvised in Scatter Plot was a poor-man\u0026rsquo;s map. Orange has proper geographical support (Orange3-Geo add-on), we\u0026rsquo;ll look at it in-depth in a follow-up post.\nThe message here regards the essential difference between plotting curves and real data mining: static curves are dead; data mining is interactive.\nConnecting data sets Let\u0026rsquo;s solve the problem of negligible curves for small countries. This is not just about populations: the biggest problem is that confirmed cases are the result of testing, and testing strategies vary by country. Icelanders and Koreans test like crazy, while the number of tests in the US was (initially) so small it was comparable to that of Slovenia (and with it, the number of confirmed cases). The number of tests could thus normalize the number of confirmed cases, but this wouldn\u0026rsquo;t do either; the testing isn\u0026rsquo;t equivalent – some countries test more at random, while others save tests for groups at greater risk. And regardless, exact data about the number of tests is not readily available.\nSo, let\u0026rsquo;s fallback to the number of cases per million inhabitants. Neither this nor the population of countries are present in the data provided by John Hopkins University. We do, however, have another data set at arm\u0026rsquo;s reach: the population of countries (for year 2015) appears in the Worldbank\u0026rsquo;s Human development index (HDI) data.\n To load it, use the Datasets widget, find and double-click the HDI data set. \nNow we\u0026rsquo;re working with two data sets.\n Let\u0026rsquo;s merge them using the Merge Data widget: connect both the File widget (with the John Hopkins data), and the Datasets widget (with the HDI data) to Merge Data. Make sure to match the \u0026ldquo;Country/Region\u0026rdquo; feature in COVID-19 data with the \u0026ldquo;Country\u0026rdquo; feature in HDI. It is also important to connect the widgets in this order, so File provides the main data and the Datasets widget augments it with additional annotation columns. If you do it incorrectly, double-click the connections to fix it.\nThe match is imperfect: some countries appear with different names, for instance, \u0026ldquo;Russia\u0026rdquo; from John Hopkins doesn\u0026rsquo;t match the \u0026ldquo;Russian Federation\u0026rdquo; from HDI, and the John Hopkin\u0026rsquo;s \u0026ldquo;US\u0026rdquo; and doesn\u0026rsquo;t match HDI\u0026rsquo;s \u0026ldquo;United States\u0026rdquo;.\nThe population of Liechtenstein in millions with one decimal is 0.0; similar for Palau.\n To remove countries with unknown or zero population, we continue with Select Rows, where we set the condition \u0026ldquo;Total Population (millions) 2015 is greater than 0\u0026rdquo;. \nNow for some constructive business.\n To get the number of cases on some particular day, we use the Feature Constructor widget, where we can input a formula to compute new data columns. In \u0026ldquo;New\u0026rdquo; we select Numeric, type cases per million as the name of the new column, Select Feature \u0026ldquo;3_29_20\u0026rdquo; (or whichever date we\u0026rsquo;d like), add /, and then select \u0026ldquo;Total_Population__millions__2015\u0026rdquo;. As a shortcut, you can just copy _3_29_20 / Total_Population__millions__2015 into the \u0026ldquo;Expression\u0026hellip;\u0026rdquo; line. (Note that an underscore precedes 3_29_20: this way Orange knows that this is not a number but a name of a column.)\n Things got crowded, so perhaps we can remove some columns.\n We add Select Columns, open it, and remove all columns we don\u0026rsquo;t need right now. We can just select all features (Ctrl-A or Cmd-A) and drag them to the left, and then drag \u0026ldquo;cases per million\u0026rdquo; back to features. \nFinally, add and connect a Data Table.\n Sort by the last column and, voilá, you\u0026rsquo;ve ranked countries the number of cases per a million inhabitants.\n Missing in action: China. The data for China is split into provinces, yet every row is divided by the country\u0026rsquo;s entire population. France seems to have a similar problem: it is given as a single country, but followed by external territories whose numbers are divided by the entire population. Also missing in action: Russia / Russian Federation, and US / United States, as we\u0026rsquo;ve previously explained.\nFixing these glitches will require some manual work.\nEditing the data To fix the non-unique country names, we could open one of the two files in Excel and fix discrepancies, but there\u0026rsquo;s no need for it: this is easier done in Orange, which will help us identify the missing countries. They are removed in the Select Rows widget. We hence connect a Data Table to Select Rows, double click the connection and do some editing: click the line between Matching Data and Data to remove it, and drag a line from Unmatched Data to Data.\n This way, we can see which countries were filtered out. They seem to be absent due to either not being present in HDI, or having too small of a population.\nLet\u0026rsquo;s insert an Edit Domain widget between the Datasets widget and Merge Data.\n We find and select column Country. Now change its type from Text to Categorical. Note, this is conceptually wrong: \u0026ldquo;Country\u0026rdquo; is a text variable, it contains a name and not a \u0026ldquo;category\u0026rdquo; of country, like the continent or the direction of writing. However, Edit Domain only lets us map its values if we change it to Categorical. And having it as categorical won\u0026rsquo;t hurt.\n After changing the variable type, the widget will show another box, Values, with a mapping of values. Let\u0026rsquo;s look at the countries in the new excluded rows Data Table widget. We see, for instance, \u0026ldquo;Bosnia and Herzegovina\u0026rdquo;. We need to match to how John Hopkins data refers to countries, so go back to Edit Domain, and find \u0026ldquo;Bosnia and Herz.\u0026rdquo;. Double click it and change it to \u0026ldquo;Bosnia and Herzegovina\u0026rdquo;.\nSimilarly, Czech Rep. should become Czechia, Macedonia is now officially North Macedonia, Russian Federation is Russia, United States is US and Viet Nam is Vietnam. John Hopkins has \u0026ldquo;Korea, South\u0026rdquo; and HDI includes just Korea. Based on its high human development index and GDP, we can safely assume that this is the northern one. (Kidding, map this Korea to Korea, South, of course. There\u0026rsquo;s no virus in the other Korea because Kim anticipated it before it even appeared and already invented a cure long ago.)\nNow that I\u0026rsquo;ve taught you how to fish, I give you a fish for today: after editing these and all the other countries whose matches I found, I saved it to a file you can download and use. Replace the Datasets widget with a File, and load it.\nAnalysis beyond population Comparing this data with the World Bank\u0026rsquo;s data is great because it allows us to relate the epidemiological data to data about particular countries.\n In Select Columns, we removed most of the features. Bring back all those that come from HDI. Now connect, say, a Scatter Plot and observe the relation between the number of cases per million and the number of physicians per ten thousand, which is the closest approximation to the capabilities of health systems.\n The conclusion here would be that you may want to find yourself in Qatar, Cuba or Greece right now. People are often surprised to see Cuba in such contexts, though Cuba has an excellent public health system and pretty good (though perhaps not entirely unbiased) free education. Seeing this graph, it shouldn\u0026rsquo;t come as a surprise that Cuban doctors are heading to Italy and other countries to help fight the epidemics.\nA seemingly related measure, Public health expenditure doesn\u0026rsquo;t work here. It is expressed as a percentage of GDP, so it is adjusted for physicians' salaries, but not necessarily for the prices of (imported) equipment. Furthermore, higher spending may reflect a more expensive but not necessarily a more efficient health system.\nAnother potentially interesting factor to follow will be the age of the population. HDI tells us the number of people older than 65 years per 100 people between 15 and 64.\n I should have used future tense; the population age will be interesting to observe with respect to mortality rate, but not enough countries have a sufficient number of deaths. Let\u0026rsquo;s hope it stays this way.\nWe will eventually be able to (or \u0026ndash; let\u0026rsquo;s hope we won\u0026rsquo;t) explore the relations between epidemics and other sociodemographic data. Generally, the problem with analyzing this data per country is currently that the disease has mostly spread (or been detected?) in developed countries, resulting in many correlations like the speed of spreading and percentage of urban populations, which may or may not reflect actual causation.\nBack to curves What about curves showing the number of confirmed cases per million? We computed the cases per million for a single day, not for all days. While Orange is a general-purpose data mining tool, some operations are too specific to be implemented in widgets. For such cases, we can program some processing in Python, using the Python Script widget.\n The start of the sequence is the same as before: we merge the two data sources and remove countries with unknown or zero population.\nIn Select Columns, we move all variables except daily data from features to metas.\nIn Python Script, we type the following program and run it:\nimport numpy as np import Orange normalized = in_data.X / in_data[:, \u0026#34;Total Population (millions) 2015\u0026#34;].metas out_data = Orange.data.Table.from_numpy( in_data.domain, normalized, in_data.Y, in_data.metas)  With in_data[:, \u0026quot;Total Population (millions) 2015\u0026quot;] we take the table corresponding to the column with the population. Since population is now a meta attribute, we take .metas. We divide in_data.X with this column. Finally, we construct a new table named out_data, with the same domain (that is, the same variables), the normalized data, and the original data for target variable(s) and metas.\nWe get a similar line plot as before, just with proper scale. Observe it. There are two countries with 3000-4000 confirmed cases per million?! Which? Check yourself!\n Would you like to see a plot with a logarithmic axis? And Line Plot doesn\u0026rsquo;t support it? (Yet?) The beauty of using the Python script widget is that we can transform the data any way we want. Change the line that computes the normalized data to\nnormalized = np.log10(in_data.X / in_data[:, \u0026#34;Total Population (millions) 2015\u0026#34;].metas) And we have a logarithmic axis (just with wrong labels).\nWhere from here? This data is related to countries. Hence it would be nice to put it on a map. It also deals with time, and core Orange is not well-equipped for it. In the two follow-up posts, we will explore the add-ons for geographical data and for time series.\n Orange is a multi-platform open-source machine learning and data visualization tool for beginners and experts alike. Download Orange, and load and explore your own data sets!\nIn addition to a variety of learning materials posted online in the form of blog posts, tutorial videos, we\u0026rsquo;ve created a Discord server. Join the community, tell us what you think!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-04-02-europe-scatter-line-plot.png",
	"shortExcerpt" : "Basic tricks for loading and analysing COVID-19 data.",
	"longExcerpt" :  "Basic tricks for loading and analysing COVID-19 data." ,
	"author" : "Janez Demšar",
	"summary" : "These days we are all following the statistics of COVID-19, looking at how our own country is faring and how it\u0026rsquo;s comparing with other countries. Luckily, only a few have a statistically meaningful number of deaths (which solemnly reminds us of the difference between statistical and practical significance!), so we concentrate on the number of confirmed cases.\nYou\u0026rsquo;re reading the first and most basic blog post from a series in which we will investigate this data using Orange.",
	"date" : "Apr 2, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-02-24-anaconda-navigator/",
	"title": "Installing with Anaconda Navigator",
	"categories": ["installation", "anaconda", "navigator"],
	"description": "",
	"content": "We are fortunate enough to be featured on the front page of Anaconda Navigator, a graphical user interface for conda package management. Orange has been a conda package for some time now, since this is the easiest way to provide pre-compiled packages for Windows. And since most of our user base uses Windows, this was the way to go.\nIf you are an avid Anaconda user and you wish to install Orange with Anaconda Navigator, there are some steps you need to take to ensure everything works correctly. First, install Orange in the home screen. Once Orange is installed, it will appear at the top.\n Next, go to the Environments pane. You likely see only base (root) environment. Environments in Python are special \u0026lsquo;containers\u0026rsquo; that isolate all your dependencies for different project. You can create a new environment called \u0026lsquo;Orange\u0026rsquo; to keep everything Orange-related separate from your base environment. Click Create to make a new environment and follow instructions.\n Here, we will use base, but the procedure is the same for any other environment. Select the environment you wish to set. In the upper right, select Channels. You should see defaults in your options.\n In the upper right, select Add\u0026hellip;, then type conda-forge. Conda-forge channel is where the most recent versions of Orange and its add-ons live. Click Update channels once you have added the conda-forge channel.\n That\u0026rsquo;s it. Your channel is set. Now you can update Orange to the latest version and use add-on that require pre-compiled packages, such as Text, Network, and so on.\n Make sure to regularly update Orange to get the latest bug fixes and features.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-02-24-navigator-small.png",
	"shortExcerpt" : "Essential information for installing Orange via Anaconda Navigator.",
	"longExcerpt" :  "Essential information for installing Orange via Anaconda Navigator." ,
	"author" : "Ajda Pretnar",
	"summary" : "We are fortunate enough to be featured on the front page of Anaconda Navigator, a graphical user interface for conda package management. Orange has been a conda package for some time now, since this is the easiest way to provide pre-compiled packages for Windows. And since most of our user base uses Windows, this was the way to go.\nIf you are an avid Anaconda user and you wish to install Orange with Anaconda Navigator, there are some steps you need to take to ensure everything works correctly.",
	"date" : "Feb 24, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-02-08-lecture-notes/",
	"title": "Orange Lecture Notes",
	"categories": ["education", "workshop"],
	"description": "",
	"content": "In the past, we, the developers of Orange at the University of Ljubljana, have carried out over fifty hands-on workshops. We carried out the workshops for students in secondary schools, universities, Ph.D. programs, employees of scientific institutes, companies, and government institutions. We carried out the courses around the world and lectured in places like Houston, Pavia, Hanover, Moscow, Verona, Montreal, Luxemburg, Kolkata, Liverpool, Bari, Ashburn, Sao Paolo, Trieste, Bled, Lisbon, Konstanz, Oslo, Belgrade, Ostrava, Melbourne, Ås, Bochum, and Ljubljana. The lectures comprised an introduction to machine learning and data visualization and sometimes focused on more specific topics, like text or image mining, mining of spectral data, or even data mining in molecular biology.\nAll of our workshops are hands-on. We start with the data and a problem and then dive into solving a problem with Orange. No PowerPoint slides, no dull and detached lectures. In the workshop, Orange allows us to go straight into data analysis. Orange is different from other workflow-based tools since we designed it for teaching. Our workshops are, therefore, a perfect testbed for Orange. Through their design and execution, we learn about possible improvements and functionalities that we then add to Orange and try out at a forthcoming workshop.\nWe plan our Orange workshops by crafting a workshop program that we assemble into lecture notes. These notes are a refreshment material for the students and help workshop organizers who follow the lectures in the notes more or less strictly, depending on the audience, their engagements, and questions they may have during the workshop.\nWe are aware that our lecture notes can be of assistance to other lecturers and workshop organizers. We are releasing them here for reading, browsing, or reuse. Please find them on our server, or download them from the following list of selected workshops:\n Data Mining, a sixteen-hour two-week Ph.D. course at Baylor College of Medicine (Houston, 2019) Introduction to Data Mining, a three-hour workshop for Ph.D. students (Ljubljana, 2017) A Gentle Introduction to Data Science, a two-hour workshop at GIS Ostrava (Ostrava, 2019) Single-Cell Gene Expression Analysis, a three-hour workshop at Baylor College of Medicine (Houston, 2019)  The lecture notes teach concepts in machine learning and data science. For example, here is a snapshot of one of the pages from our lecture notes.\n We tend to explain this through examples, and most often avoid any heavy mathematics. Our aim is the democratization of data science and would like to see Orange as an enabling tool that reaches a broad audience and complements toolboxes in R and Python that are intended for more tech savvy.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-02-08-lecture-notes-small.png",
	"shortExcerpt" : "Lecture notes for Orange workshops on machine learning and data science are now available online.",
	"longExcerpt" :  "Lecture notes for Orange workshops on machine learning and data science are now available online." ,
	"author" : "Blaž Zupan",
	"summary" : "In the past, we, the developers of Orange at the University of Ljubljana, have carried out over fifty hands-on workshops. We carried out the workshops for students in secondary schools, universities, Ph.D. programs, employees of scientific institutes, companies, and government institutions. We carried out the courses around the world and lectured in places like Houston, Pavia, Hanover, Moscow, Verona, Montreal, Luxemburg, Kolkata, Liverpool, Bari, Ashburn, Sao Paolo, Trieste, Bled, Lisbon, Konstanz, Oslo, Belgrade, Ostrava, Melbourne, Ås, Bochum, and Ljubljana.",
	"date" : "Feb 8, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-01-29-machine-anthropology/",
	"title": "What is Machine Anthropology?",
	"categories": ["machine", "anthropology", "big data", "pivot table"],
	"description": "",
	"content": "For those unfamiliar with the field, cultural anthropology is the study of human cultures, practices and habits in a holistic and comparative manner. Its core method is ethnographic fieldwork, which means researchers spend a long time in the field with their subjects, live with them, talk with them, socialize with them, and observe relationships and behaviours. But recently, anthropology has begun to use also machine learning and data mining as a part of its method. The subdiscipline is called computational anthropology (combining ethnographic fieldwork with big data) or machine anthropology (ethnography as big data).\nRelated: Data Mining for Anthropologists\n Copenhagen Center for Social Data Science (SODAS) held a conference on machine anthropology just a few days ago. It was inspiring to see how the humanities benefit from computer sciences and vice versa! There, I presented my PhD research of sensor data from a smart building and used Orange to show how to detect patterns of behaviour in such data.\nHere I will show a different example by using a publicly available Philadelphia Crime data set that can be loaded in the Datasets widget. This data set reports crimes in the city of Philadelphia from 2006 to 2016. We will have to create two new features, one for hour of the day and one for month. In the Feature Constructor we will write two Python expressions that parse month and hour from the datetime feature.\n How does our data look like now?\n Perfect. Now say we would like to establish the frequency of crimes by the hour. We will use Pivot Table to count the occurrences of each type of crime. We set rows to type, because we are interested in the differences between different types of crimes. Then we set hour as columns. This will create a new timeseries-like data table, where each column will represent a single hour of the day. Values can be set to anything, because we will not use any special type of aggregation, but simply count occurrences of crimes.\n We can observe the results of the pivot table in a Line Plot. We group by type and see how crime frequencies change with respect to the hour of the day. Unsurprisingly, the liquor law violations and prostitution go up late at night. Prostitution in particular, is a very frequent crime in Philadelphia (or at least was in the time of reporting).\n If we standardize the data with Preprocess (default normalization option), we see a more balanced picture, where homicides are relatively much more frequent early in the morning than at any other time of the day. Apparently, murderers are early birds.\n There you are, a workflow for observing simple timeseries patterns in the data. Of course, you can create much more complicated workflows in Orange or even write a custom Python script. If you have you own examples of anthropological, sociological, or any kind of socially-oriented data analysis in Orange, we\u0026rsquo;d love to hear about it!\n ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-01-29-MA-small.png",
	"shortExcerpt" : "At the recent Machine Anthropology workshop we used Orange to explore anthropological data.",
	"longExcerpt" :  "At the recent Machine Anthropology workshop we used Orange to explore anthropological data." ,
	"author" : "Ajda Pretnar",
	"summary" : "For those unfamiliar with the field, cultural anthropology is the study of human cultures, practices and habits in a holistic and comparative manner. Its core method is ethnographic fieldwork, which means researchers spend a long time in the field with their subjects, live with them, talk with them, socialize with them, and observe relationships and behaviours. But recently, anthropology has begun to use also machine learning and data mining as a part of its method.",
	"date" : "Jan 29, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-01-16-orange-cloud/",
	"title": "Orange in the Cloud",
	"categories": ["cloud", "server", "remote"],
	"description": "",
	"content": "Many problems are too big and require too much processing power to be efficiently processed on your laptop or PC. In such cases, the data is usually transferred to a remote server and processed using custom code, which is often time consuming. Now, there is a way to run Orange on a remote server so that you can keep using its interactive graphical interface. We will show you how to run Orange on the remote server so that you can use it through your web browser.\n \\\nIn a nutshell, you browser opens a remote desktop connection to an Orange instance that runs inside a docker container on a remote server. Now let us go into more details.\nIn our configuration we used several different technologies such as Docker and Apache Guacamole, which are shown in the diagram. First, you need to set up Nginx (or alternatively Apache web server), which is used to ensure that all communication to the server is SSL encrypted. This is very important, because remote desktop protocols such as Remote Desktop protocol (RDP) and Virtual network computing (VNC) are not encrypted. Failure to do so will expose your data to anyone listening on the network.\n \\\nNginx redirects you to the Apache Guacamole web application. In Guacamole you can manage multiple users and specify which of your Orange instances each user has access to. Guacamole then connects you to a selected Orange-docker container through an RDP or VNC connection. Once it is connected, you can see the remote desktop in your browser. You can use Orange just like on a local computer (see the image above), although you may need a few minutes to get used to the Linux environment.\nDockers allow you to run many lightweight isolated Linux containers on the same machine. We prepared a docker image that comes pre-configured with graphical desktop environment, a remote desktop server, Orange application and a few convenience applications (Libre Office, web browsers). You can run many isolated Orange-docker containers, so each user can work on a different project. When users collaborate on a project, they can connect to the same instance and share the same screen (read-only or full access). You can upload and download your data to and from the remote server using drag \u0026amp; drop feature or the side menu. Alternatively you can transfer the data with one of the web browsers that are provided in the container.\nThere are many other benefits to running Orange on the server infrastructure. First, Orange can stay open and continue to process the data even when you are offline. Second, you can access the same workflow from any computer. Third, multiple users can interactively collaborate on the same workflow. Finally, you do not have to keep the data on a local computer and you do not need to install Orange on a local computer. Note that this is a self-hosted solution, which means that all your data remains on the servers under your control.\nFor a complete installation guide and more details see orange-docker Github repository.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-01-16-orange-cloud.png",
	"shortExcerpt" : "Use Orange remotely by running it on a remote server as a docker container.",
	"longExcerpt" :  "Use Orange remotely by running it on a remote server as a docker container." ,
	"author" : "Andrej Čopar",
	"summary" : "Many problems are too big and require too much processing power to be efficiently processed on your laptop or PC. In such cases, the data is usually transferred to a remote server and processed using custom code, which is often time consuming. Now, there is a way to run Orange on a remote server so that you can keep using its interactive graphical interface. We will show you how to run Orange on the remote server so that you can use it through your web browser.",
	"date" : "Jan 16, 2020"
}

    
    , {
    "uri": "/blog/2020/2020-01-08-neighbors-images/",
	"title": "Look-alike Images",
	"categories": ["neighbors", "images"],
	"description": "",
	"content": "There is a cool and perhaps not often used widget in Orange called Neighbors. The widget accepts the data and a reference data item and outputs the nearest neighbors of that item.\nRelated: Image Analytics Workshop at AIUCD 2018\nHere I will show how to use it to display a set of images most similar to a selected reference image. I will use the following workflow:\n \\\nWe may use any collection of images, and for this blog, I have decided to pull out some image sets available from Datasets widget. To use your own collection of images, check out our YouTube video on image clustering to see how to prepare it. In the Dataset widget, filter the data sets to list only those that include images:\n \\\nI will use Bone Healing image set from our recent Nature Communications paper. In the workflow, we first embed the images in the vector space. We send all the embedded images to the Neighbors widget through its input data channel and display them in Image Viewer. In the viewer, we can select an image. Image Viewer sends its output \u0026ndash; the selected image \u0026ndash; to the reference channel of the Neighbors widget. I have instructed this widget to send out three data items that are the most similar to the item in the reference, and on the output excluded the reference item:\n \\\nImage Viewer (1) at the end of the pipeline displays three images that are most like the chosen image:\n \\\nAny other set of images works as well. Here is our image look-alike for traffic signs:\n \\\nWe skipped any details on image embedding, measuring distances and so on. For more on these, check out our recent paper in Nature Communications or see our set of image analytics videos on YouTube.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2020/2020-01-08-neighbors-images-small.png",
	"shortExcerpt" : "We show how to use Neighbors widget on image embedding space to find image look-alikes.",
	"longExcerpt" :  "We show how to use Neighbors widget on image embedding space to find image look-alikes." ,
	"author" : "Blaž Zupan",
	"summary" : "There is a cool and perhaps not often used widget in Orange called Neighbors. The widget accepts the data and a reference data item and outputs the nearest neighbors of that item.\nRelated: Image Analytics Workshop at AIUCD 2018\nHere I will show how to use it to display a set of images most similar to a selected reference image. I will use the following workflow:\n \\\nWe may use any collection of images, and for this blog, I have decided to pull out some image sets available from Datasets widget.",
	"date" : "Jan 8, 2020"
}

    
    , {
    "uri": "/blog/2019/2019-11-20-belgrade-workshop/",
	"title": "Explaining Models: Workshop in Belgrade",
	"categories": ["workshop", "belgrade", "classification", "nomogram", "naive bayes", "decision tree"],
	"description": "",
	"content": "On Monday, Blaž and I held a technical tutorial Data Mining through Visual Programming and Interactive Analytics in Orange at this year\u0026rsquo;s edition of Data Science Conference in Belgrade, Serbia. The tutorial explained how to quickly prototype standard data mining and machine learning workflows in Orange and how to interactively explore clustering and classification models. The final part raised an interesting question that we\u0026rsquo;re going to explore in continuation.\n \n\\\nFirst, let us load the titanic data with the Datasets widget. The data is a simplified version of Kaggle\u0026rsquo;s Titanic data set. Instead of 10 variables, we kept just 4 - gender (male or female), age (adult or child), status (first, second, third or crew) and the survival class label (yes or no).\n \n\\\nLet us build a simple decision tree model with a Tree widget and observe it in a Tree Viewer. I have hidden the bottom branches for simplicity. Let us observe, what gives me the highest probability of survival. Seems like it is best if I travel as female in the first or second class or as a member of the crew. This would give me 92% chance of surviving the trip.\n \n\\\nOk, what about another classifier. Let\u0026rsquo;s say Naive Bayes. Connect Naive Bayes to Datasets and then Nomogram to Naive Bayes. It seems like I have the best change of surviving the trip if I travel as female in the first class - the chance of surviving is 90%. But why does Naive Bayes say it is bad to travel as a female crew member, while Tree says it is ok?\n \n\\\nFirst, Naive Bayes does not say it is bad to travel as a female crew member. Naive Bayes assumes attribute independence, so it does not claim anything about being a female AND a member of the crew. It just says it is better to travel as a female and not so good if you are a member of the crew.\nSecond, if we observe the parameters of the Tree classifier, we see that we asked for a binary tree. So inevitably the variables will fall into two categories - third class (which has a low chance of survival) and everything else. If we uncheck this box, we see that the probability of surviving for female crew is slightly lower - 87%. That is still not too bad, but note that we only have 23 instances for this branch! There were not many female crew members on Titanic, so a sample is fairly small compared to other status values.\n \n\\\nThe key message of this part of the tutorial was that different models work differently and we have to understand what they are telling us and how they were constructed. Luckily, Orange enables us to explore certain models, so we can inspect them and draw conclusions from the best ranked attributes.\n ",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-11-20_workshop_small.jpg",
	"shortExcerpt" : "We explained how different models mean different things and how to interpret them at a recent tutorial in Belgrade.",
	"longExcerpt" :  "We explained how different models mean different things and how to interpret them at a recent tutorial in Belgrade." ,
	"author" : "Ajda Pretnar",
	"summary" : "On Monday, Blaž and I held a technical tutorial Data Mining through Visual Programming and Interactive Analytics in Orange at this year\u0026rsquo;s edition of Data Science Conference in Belgrade, Serbia. The tutorial explained how to quickly prototype standard data mining and machine learning workflows in Orange and how to interactively explore clustering and classification models. The final part raised an interesting question that we\u0026rsquo;re going to explore in continuation.",
	"date" : "Nov 20, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-11-15-telekom-workshop/",
	"title": "Explaining Customer Segments for Business",
	"categories": ["workshop", "telco", "clustering", "nomogram"],
	"description": "",
	"content": "Last month we held a workshop for a large Slovenian Telco company. Their two key questions were - what machine learning techniques can we use on our data and how do we explain the models afterwards. So the workshop focused on three use cases - product segmentation, modeling churn, and clustering customers. With any kind of models, especially unsupervised ones, we often get the question - but how can we explain the clusters? What do these clusters tell us about the customers?\n \n\\\nLet us see how we do this in Orange. We will use Telecom customer churn data set, which you can load with the Datasets widget. But we will not be interested in predicting churn, but rather how to segment customers. Orange already ignores target variable for clustering, but we can remove it with Select Columns to make the example clearer.\n \n\\\nNext, let us design a standard hierarchical clustering workflow. Pass the data to Distances and then to Hierarchical Clustering. Our data set is quite big and 20 variables is not negligible, but for the sake of simplicity we can use Euclidean distance in Distances and Average linkage in Hierarchical Clustering. Finally, let us select three clusters. You can of course try different parameters - say cosine distance and Ward linkage.\n \n\\\nNow for the fun part. Connect another Select Columns to Hierarchical Clustering and put the Cluster variable into the target variable section. Then connect Naive Bayes to Select Columns and Nomogram to Naive Bayes. Like so:\n \n\\\nBasically, we will use cluster labels as target variable and try to predict clusters with Naive Bayes. Nomogram then helps us explain the model. Remember, Naive Bayes assumes independence between variables, so it doesn\u0026rsquo;t take correlation into account. Logistic Regression does, so feel free to use it instead of Naive Bayes. Anyhow, Nomogram lists the top 10 variables that are important for defining the selected cluster. It seems that cluster C1 does not use internet (or at least does not buy internet from us). C3 on the other hand normally has the whole package.\n \n\n\n\\\nWhat does this mean from a business perspective? Well, that you should focus your marketing on cluster C1 and offer discounted internet packages. Marketing to C3 would be essentially useless. This is how Orange can help you identify business opportunities and understand you customer base better.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-11-15_ws-title.JPG",
	"shortExcerpt" : "Explaining customer base enables businesses to make informed decisions. We present the case for Telco companies.",
	"longExcerpt" :  "Explaining customer base for businesses to make informed decisions. We present the case for Telco companies." ,
	"author" : "Ajda Pretnar",
	"summary" : "Last month we held a workshop for a large Slovenian Telco company. Their two key questions were - what machine learning techniques can we use on our data and how do we explain the models afterwards. So the workshop focused on three use cases - product segmentation, modeling churn, and clustering customers. With any kind of models, especially unsupervised ones, we often get the question - but how can we explain the clusters?",
	"date" : "Nov 15, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-09-29-of-carrots-horses-and-fear-of-heights/",
	"title": "Of Carrots and Horses and the Fear of Heights",
	"categories": ["statistical significance", "hypothesis testing", "p-value", "multiple hypothesis testing"],
	"description": "",
	"content": "A word of warning. When teaching in the US, I am careful enough to not say things like \u0026ldquo;consider a binary variable, for instance, gender\u0026rdquo;. But I\u0026rsquo;m writing this from Russia where considering gender as a binary variable is not only an acceptable but rather a desired, official position. In Russia you can even get away with some mild swearing. While I never use strong curse words, neither in the classroom nor in private, I occasionally say \u0026ldquo;shit\u0026rdquo;, which is, as I have been warned, not acceptable in US classrooms (while on corridors you can f\u0026hellip; all you want). This blog entry targets Russian audience.\n* * * A(n imaginary) friend of mine has this hobby of searching for all kinds of weird relations. Basically to annoy people, and me in particular. Like \u0026ldquo;you know, I discovered that people who wear glasses tend to like horses\u0026rdquo;. Yes, sure they do. \u0026ldquo;No, seriously, I collected some data. One hundred people, actually. See? It\u0026rsquo;s totally scientific.\u0026rdquo; Like hell it is. \u0026ldquo;Prove me wrong then!\u0026rdquo; he said. Pathetic.\nOne day I\u0026rsquo;ve had enough and built a BS-detector, short for \u0026ldquo;bullshit detector\u0026rdquo;. I invented a formula that computes the similarity between two columns of 0\u0026rsquo;s and 1\u0026rsquo;s. It\u0026rsquo;s normalized so that it gives me a 1 if the columns are the same, a -1 if they are exactly opposite, and 0 if one tells us nothing about the other, that is, they are the same in exactly half of cases. The score can also be any other number in between, of course - the more different from 0, the stronger the relation. At first I called the formula \u0026ldquo;Demšar\u0026rsquo;s bullshit score\u0026rdquo;, but the next day it sounded too much like being about \u0026ldquo;Demšar\u0026rsquo;s bullshit\u0026rdquo; rather than \u0026ldquo;bullshit score\u0026rdquo;. So I made up a better name: \u0026ldquo;Pearson correlation coefficient\u0026rdquo;. After watching all that Star Trek series featuring all kinds of \u0026ldquo;rotating field harmonics\u0026rdquo;, I became rather proficient at inventing fancy names myself. Pearson correlation coefficient sounds really convincing.\nWhen the imaginary friend (children need them for company and professors need them for making up stories) brought me his next relation, I computed the so-called \u0026ldquo;Pearson correlation coefficient\u0026rdquo;, got a result of 0.012 and explained what it meant. In his concrete example it meant that drinking apple juice has nothing to do with fear of heights.\nOf course he complained that 0.012 is not zero, but I was prepared for this. I already realized that the numbers I\u0026rsquo;ll get for his relations won\u0026rsquo;t be neither 0 nor 1. So I needed to get some intuition into what kind of numbers to expect from his random (to my belief) relations. I produced some random data myself. I pretended to have a sample of one hundred people for which I recorded two properties. Both binary. One may be a tendency to scratch head, and the other binary variable may be, for instance, gender. Except that I of course had no actual data and just made up two random columns of zeros and ones. And computed my formula.\nTo do so, I opened Orange and created the following workflow. Widget Random Data from add-on \u0026ldquo;Orange3-Educational\u0026rdquo; provides random data drawn from different distributions. I set it up to prepare two variables drawn from Bernoulli distribution, which is a fancy name for a random column of 0\u0026rsquo;s and 1\u0026rsquo;s. The two variables are discrete, so I attached its output to Continuize to make them numeric. Widget Correlations computes the Demšar\u0026rsquo;s \u0026hellip; I mean \u0026ldquo;Pearson correlation coefficient\u0026rdquo;.\n  I checked the coefficient for my pair of random variables. It gave me 0.084, which is more than the score of his \u0026ndash; to his claim related \u0026ndash; pair, 0.012. I went back to Random Data, generated a different sample, and checked the correlation. And repeated it again. And again. Always larger than his 0.012. I told him that I can run this for 1000 times, but then I saw it\u0026rsquo;s more practical to just construct 45 variables instead of 2 by changing parameter Variables in Random data to 45.\nThis gave me 990 pairs. A large majority of them was larger than 0.012 that he presented. Actually, I told him (after scrolling way down through the widget) that apple juice and fear of heights were so unrelated that 914 out of (almost) 1000 random relations were stronger than this.\nI opened a beer about two minutes later, after closing the doors behind him. I earned it.\nNext day he came again. And the next. And the following day, too. He just wouldn\u0026rsquo;t give up. And I kept opening beers until his fifth day\u0026rsquo;s relation indeed had a BS score of 0.22 (and luckily made some sense \u0026ndash; I indeed believe that women can distinguish between more colors than men, who recognize about six or seven). I argued that 0.22 is still low, but he forced me to look into my correlations and, damn, only 32 out of 1000 random relations were so strongly correlated. I had to yield this time: I confessed that random, uncorrelated variables were unlikely to give such strong BS scores.\nWe agreed that even some of 1000 random (non-)relations are bound to be related by chance, so I couldn\u0026rsquo;t request that he beats all my random pairs. We decided that we will consider a relation significant, if its Pearson correlation coefficient is so large that only 5 % of random relations exceed it. With his samples of size 100, my experiments showed that this value was 0.204. So 5 % it was. I thought it safe enough, he thought it fair enough.\nWe now had a perfect test. One that we would both trust.\nNothing happened for a few days. I believed I essentially won. He knew that we had a method for detecting his bullshit so he won\u0026rsquo;t bother me again (so soon and so often).\nOr so I thought. Several days later some devil brought him again, with some more relations to check. Total rubbish, but some were really strong. Then he confessed he made up the data for those. It made no sense why he would do this \u0026ndash; and then even confess. But when I looked into the data I realized he was trying to guess my secret formula! By the looks of it, he actually succeeded. I made a stand: I made him swear he would never ever cheated by falsifed data. Ever. Again. Or it was the last time I talk with him.\nA day passed. He came grinning so I knew something was up. Eating carrots helps against missing a bus. It has beaten all except the top 2% of my random relations.\nSo I opened a beer \u0026ndash; with him. After the second one he opened up. He actually thanked me for the formula! He said it makes his life so much easier. I realized that I haven\u0026rsquo;t won but shot myself in the foot. He now uses my formula methodically. Yes, he actually calls it a method. He even gave it a name, he calls it \u0026ldquo;null-hypothesis significance testing\u0026rdquo;. I didn\u0026rsquo;t know he was a fellow Star Trek fan.\nHe collected a huge number of his stupid measurements over the years. All on the same group of one hundred individuals. He still believes that some variables are related. And I gave him a method for checking them. We agreed that any pair that beats at least 95% of my random pairs (for which it needs a score of 0.204) is truly correlated. So his method now is to simply compute my formula (which he knows under the name Pearson correlation coefficient) on all pairs of features. He picked one out of many pairs with correlation above 0.204 and brought it to me for testing. Hence the grin.\nThe worst thing was that he wasn\u0026rsquo;t doing this (only) to tease me. He sincerely believed in the method. We agreed that this would be the way to test his relations and he was serious about it. If my formula gives the number above 0.204 for some relation, the relation is true.\nHe thanked for the beer and promised to come again tomorrow. And the day after, too. Now that we are collaborators.\nЧёрт! (Another confession: I\u0026rsquo;m writing this at Patriarch ponds, sitting on a bench with my face towards the pond and my back towards Malaya bronnaya.)\nI had some serious thinking to do.\nNext day he came as promised. He told me about the latest relation he found. I forgot what it was; I didn\u0026rsquo;t pay attention. Neither did I bother to check it \u0026ndash; because I knew he has. I instead asked him how many variables he has. Like, for every person he knows whether she or he smokes, wears socks at work, speaks any Hungarian, reads Russian grotesque novels \u0026hellip; how many different things? He said 154.\n\u0026ldquo;Oh, this is quite a collection. Good work,\u0026rdquo; I kept chatting while typing something into a calculator. \u0026ldquo;So you must have identified around 590 \u0026lsquo;statistically significant\u0026rsquo; relations in your list, haven\u0026rsquo;t you?\u0026rdquo; He was significantly less happy after hearing this. \u0026ldquo;Actually 592,\u0026rdquo; he said. \u0026ldquo;But \u0026ndash; how did you know?\u0026rdquo; \u0026ldquo;People with strange beards like mine tend to have telepathic abilities,\u0026rdquo; I said seriosly. \u0026ldquo;What?\u0026rdquo; \u0026ldquo;Не шалю, никого не трогаю, починяю примус.\u0026rdquo;\n* * * I hate spoilers and told him nothing. But I\u0026rsquo;ll tell you. It\u0026rsquo;s easy. We can rather safely assume that all his correlations are random. So we can replicate his data without actually collecting it. Open the Random data widget and generate 154 variables instead of 45, so that you simulate his data. Go to Correlations and check how many pairs of random variables have a correlation above 0.204. You\u0026rsquo;ll see there are around 590. Quite close to the number he got.\nBut I wrote that I typed the numbers into a calculator. Sure. I don\u0026rsquo;t need Orange for this. With 154 variables, there are 154 * 153 / 2 = 11781 pairs. We agreed that he can show me any relation that beats all but top 5 % of random relations. His relations are random so there are 11781 * 0.05 = 590 relations in the top 5 %.\n* * * The story of course follows the dogma of hypothesis testing. We test a relation by computing some statistics, like Pearson correlation coefficient, and compare it with the value of this statistics that we would get on random data of this kind. This can be done like here. We generate a great number of random cases and compute the statistics for each. If the tested hypothesis beats all except 3 % of random ones, we say p = 0.03, meaning that \u0026ldquo;there is only 3% probability that a randomly generated hypothesis would reach such a high score\u0026rdquo;.\nThere is of course a faster way to test the significance of the Pearson correlation coefficient. Pearson was a smart guy (and my calling his correlation coefficient a \u0026ldquo;bullshit score\u0026rdquo; shouldn\u0026rsquo;t be taken out of the context of the story). Distribution of Pearson correlations for random data is known, so critical values (like 0.204 in our case) can be computed analytically, without simulation. We nevertheless used the simulation in this story to make it more evident what significance testing actually does.\nThe story is not without flaws. The 11891 hypotheses are not unrelated. More importantly, when I wrote that the guy went home unhappy (though grudging), I implied that I proved the lack of correlation. With the common null-hypotheses testing you cannot prove the null-hypothesis (in my case: the hypothesis that there is no correlation). Applied properly, he wins if my test proves the correlation, or we call it a draw if it doesn\u0026rsquo;t. I can\u0026rsquo;t win.\nThe moral of the story is that statistical tests should not be used in the way we use them here. They are not a magic bullet for testing the correctness of hypotheses. There is not just one but two problems here.\nIf the test requires beating 95 % of my random relations, it will incorrectly recognize 5 % of relations as significant. This is pretty obvious: the top 5 % of my relations beat the remaining 95 %. His random relations will be equally successful, by pure chance. I knew the test was not failsafe. But he cheated: he checked all his relations and brought me those that passed the test. This is the skill of the Texas sharpshooter: shooting first, and drawing targets around holes next. He just found the relations that were successful by chance. For this reason, statistical hypothesis testing requires that you form the hypothesis first, and only then test it on the data - instead of forming them form the data, like he did.\nBut even if he wouldn\u0026rsquo;t cheat and he would pick pairs at random, with 5 % threshold, 1 of 20 hypothesis would still be successful by chance, so he would be happy once every full moon, assuming he\u0026rsquo;d take weekends off. In statistics, this is called multiple hypotheses testing. You have to account for randomly \u0026ldquo;successful\u0026rdquo; hypotheses by using various corrections. Let us mention just the Bonferroni correction as the simplest example.\n* * * This being an Orange blog, let us conclude by showing a problem with (careless use of) Orange. Connect Distributions widget to Random data. A dialog will pop-up so you can decide which Correlation\u0026rsquo;s output to use. Connect output \u0026ldquo;Correlations\u0026rdquo; to input \u0026ldquo;Data\u0026rdquo;. Correlations now shows the distribution of Pearson correlation coefficients on this data. Correlation coefficient is significant if it\u0026rsquo;s in the 5 % tail of this distribution. You can fit some curve to this if you wish.\n \\\nNow connect Sieve to Random data. Open it and click Score Combinations. It will order the pairs according to their significance. For every pair we select, Sieve will also show its chi-square and the corresponding p-value (at the bottom left). Predictably, the first 50 pairs have p-values below 0.05. By using Score Combinations and picking the top ones, we are making the same mistake as my imaginary friend. (Sieve does not compute Pearson coefficients but chi squares, yet in this context the two statistics are completely related. You can check that Correlations and VizRank show the same order of pairs.)\nSo using Score Combinations (or equivalent buttons in other widgets) and then claiming that you found and proved some relation, is exactly what my friend was doing.\nDoes this mean that we shouldn\u0026rsquo;t use Score Combinations? Why does Orange have such buttons then? They are safe to use for as long as we do not consider relation that we found in this way as \u0026ldquo;proven\u0026rdquo;. Automated tools for forming hypotheses can, well, form hypotheses. To prove them, you need to check them on another data (and still risk a 5 % chance of being successful by pure luck, if you use statistical tests and require p \u0026lt; 0.05) or, preferably, you should find the underlying reason for the correlation.\nEating a lot of carrots decreases the chances of missing a bus because being more like a rabbit helps you run faster.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-09-29-horses.jpg",
	"shortExcerpt" : "A cautionary tale of imaginary friend who made too many hypotheses to test - and how Orange is no different",
	"longExcerpt" :  "A cautionary tale of imaginary friend who made too many hypotheses to test - and how Orange is no different" ,
	"author" : "Janez Demšar",
	"summary" : "A word of warning. When teaching in the US, I am careful enough to not say things like \u0026ldquo;consider a binary variable, for instance, gender\u0026rdquo;. But I\u0026rsquo;m writing this from Russia where considering gender as a binary variable is not only an acceptable but rather a desired, official position. In Russia you can even get away with some mild swearing. While I never use strong curse words, neither in the classroom nor in private, I occasionally say \u0026ldquo;shit\u0026rdquo;, which is, as I have been warned, not acceptable in US classrooms (while on corridors you can f\u0026hellip; all you want).",
	"date" : "Sep 29, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-09-28-on-expected-vomiting-time/",
	"title": "On Expected Vomiting Time",
	"categories": ["model performance", "confusion matrix", "education"],
	"description": "",
	"content": "We just finished a lecture and a student came with a question. The lecture was, in short, about how predictive models compute probabilities; to use a model for making decisions, we must find a threshold that will minimize the cost, maximize the profit, strike a good balance between sensitivity and specificity, or increase the general human well-being in some other way. So this student came with a question about something I didn\u0026rsquo;t explain well, and she gave me one last chance.\nI had some data that looked like this.\n   ID predicted prob.  аctual class     1 0.984 p   2 0.907 p   3 0.881 n   4 0.865 n   5 0.815 p   6 0.741 p   7 0.735 p   8 0.635 n   9 0.582 n   10 0.480 p   11 0.413 n   12 0.317 n   13 0.287 n   14 0.225 n   15 0.216 p   16 0.183 n     The data is made up, so I had to make up its meaning to form an example. In the rush of a moment, I succumbed to classics: \u0026ldquo;For each of those 16 people you predicted the probability for being sick. Whom are you going to give the cure?\u0026rdquo; Her answer was in line with the best manners of modern medical practice (who cares about bacteria becoming resistant to antibiotics?): \u0026ldquo;To all!\u0026rdquo; I didn\u0026rsquo;t expect this, but anyway planned to continue with emphasizing that this medicine is not harmless: \u0026ldquo;People who take it will vomit for one week.\u0026rdquo; She turned by 180 degrees: \u0026ldquo;Oh, no. To nobody!\u0026rdquo; I didn\u0026rsquo;t expect this either, but it still led nicely to my next condition: \u0026ldquo;But if a person is indeed sick and you don\u0026rsquo;t give him the drug, he\u0026rsquo;ll vomit for four weeks.\u0026rdquo;\nDecision making becomes much more interesting when both options have consequences.\nSo, to restate:\n if you give the medicine to anybody, sick or healthy, he vomits for a week; if you don\u0026rsquo;t give the cure to a sick person, he vomits for four weeks \u0026ndash; and you can no longer help him after he starts.  Optimal threshold To solve the problem, we had to compute the expected vomiting time. This being a fairly new medical term, we need to define it. For instance, what would be the expected vomiting time for a person who is sick with a probability of 0.865? If we have 1000 people with such probability of being sick, 865 will vomit (for four weeks) and 135 won\u0026rsquo;t.\n If we don\u0026rsquo;t give these 1000 people the medicine, 865 will vomit for 4 weeks, hence the total vomiting time will be 4 x 865 = 3460 weeks, or 3.460 weeks per person, where the average includes those who are not vomiting. This is of course the same as if we just multiplied 4 x 0.865, without multiplying and then dividing by 1000. If we give these 1000 people the medicine, all 1000 will vomit for a week, thus totaling 1000 weeks of vomiting or, obviously, 1 week per person.  The latter option is better. Administering the medicine will reduce the expected vomiting time (per person) from 3.460 weeks to 1 week. Well done.\nWhat about people with 0.216 probability of being sick? Expected vomiting time of such persons is 4 x 0.216 = 0.864, which is less than a week. No drug for them. (The poor bloke with the 0.216 probability is actually sick. But we don\u0026rsquo;t know this yet. S..t happens. I mean, vomit happens.)\nObviously, the optimal threshold here is 0.250, which gives the expected vomiting time of 1 week, with or without the drug. We should administer the drug to all except the last three people in the list.\nWe could have computed this threshold even before having the model. It is a property of the disease and the drug, not the model.\nFor an extra task, compute the expected vomiting time with a threshold of 0.25. Note that you can\u0026rsquo;t assume that 25 % of untreated people will vomit for 4 weeks: some of those below the 0.25 threshold may also have just 12 % or 3 % chance of being sick. Assume a uniform probability distribution.\nOptimal threshold for the model We have so far used just probabilities and ignored the information whether somebody is sick (p) or not (n), which is given in the third column. We also assumed that predicted probabilities were correct because we had nothing else to go by.\nNow let us take the true status into account. It will allow us to compute the actual vomiting times when using this concrete model at particular thresholds.\nIf we indeed used the threshold of 0.25 and administered the drug to the first 13 people, we\u0026rsquo;d have 13 people vomiting for 1 week, and the poor guy #15 suffering for 4, which gives us (well, which gives them) the total vomiting time of 13 + 4 = 17 weeks, or 17/16 = 1.0625 per person.\nIf we instead used a threshold of 0.2, 15 people would vomit for 1 week and nobody for 4 weeks. This reduces total vomiting time to 15 weeks.\n(The actual story we developed with the student was slightly different, but roughly at this point, a student who was doing something else and has just started listening to our debate about expected and total vomiting times, interrupted us with: \u0026ldquo;Who are all these people and why are they vomiting?\u0026rdquo; We had to explain that nobody is vomiting and that we (that is: I) just have a sick sense for examples.)\nBut there\u0026rsquo;s an even better threshold. If we give the drug only to people whose probability is at least 0.48, 10 people will vomit for one week and the poor guy #15 for four weeks, giving us a total of 14 weeks of vomiting. Or 14/16 = 0.875 weeks per person.\nWe shouldn\u0026rsquo;t go any higher. Every sick person whom we don\u0026rsquo;t give a drug needs to be counterbalanced by at least three healthy people being freed from a one-week ordeal, and there are not enough healthy persons left above 0.48.\nIn this way, we simulated what would happen if we used this model on the population represented by this sample of people. So if we use it in practice and our goal is to reduce the total (or average) vomiting time, we know to administer the drug to everybody with p ≥ 0.48.\nWhy the discrepancy? Why isn\u0026rsquo;t the threshold that we computed above, 0.250, also the optimal threshold? 0.250 would be the optimal threshold if predicted probabilities were true probabilities. Model\u0026rsquo;s may be badly calibrated. While there exist methods for fixing their calibration (though not with so little data), we here took a more direct approach and computed the threshold for probabilities as predicted by our model.\nPedagogical Moral The nice thing about the story is that we didn\u0026rsquo;t care about classification accuracy. Oh, yes, after fixing the threshold to 0.48, we can compute it (it\u0026rsquo;s 11/16 \u0026ndash; just count the p\u0026rsquo;s above and n\u0026rsquo;s below the threshold). We also see that the probability of administering a drug to a healthy person (and making him needlessly hug the toilet for seven days) is 40 %, because in our sample, we had 4 such cases out of 10 whom we gave the drug. We call such poor victims false positives, and 40 % is the false discovery rate. The miserables whom we don\u0026rsquo;t give the drug though we should, are false negatives. The false omission rate (being sick if you\u0026rsquo;re not given a drug) is 1/6. The probability of not being given the drug if you\u0026rsquo;re sick (miss rate or false negative rate) is 1/7. I\u0026rsquo;m of course just copying this from Wikipedia. Nobody knows the entire list of names.\nAs a lecturer, I believe that emphasizing this list of names too much may do more harm than good. I usually show them the list just to say that all these things have names, but then try to compute something meaningful and not fancy-named. Try forcing them to learn the list and then give them a task like above. They\u0026rsquo;ll likely spend the entire exam guessing whether you want them to compute specificity or critical success index \u0026ndash; instead of simply computing the expected vomiting time. Which is not even on Wikipedia.\nYet.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-09-28-vomiting-pumpkin.jpg",
	"shortExcerpt" : "Computing meaningful performance scores of models should be creative",
	"longExcerpt" :  "A report of an interesting ending to a lecture about setting probability thresholds for predictive models" ,
	"author" : "Janez Demšar",
	"summary" : "We just finished a lecture and a student came with a question. The lecture was, in short, about how predictive models compute probabilities; to use a model for making decisions, we must find a threshold that will minimize the cost, maximize the profit, strike a good balance between sensitivity and specificity, or increase the general human well-being in some other way. So this student came with a question about something I didn\u0026rsquo;t explain well, and she gave me one last chance.",
	"date" : "Sep 28, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-08-27-pivot-table/",
	"title": "Aggregate, Group By and Pivot with... Pivot Table!",
	"categories": ["pivot table", "aggregate", "data", "groupby"],
	"description": "",
	"content": "Orange recently welcomed its new Pivot Table widget, which offers functionalities for data aggregation, grouping and, well, pivot tables. The widget is a one-stop-shop for pandas' aggregate, groupby and pivot_table functions.\n \\\nLet us see how to achieve these tasks in Orange. For all of the below examples we will be using the heart_disease.tab data.\npandas.DataFrame.agg The first task is computing a simple mean for the column age.\nIn pandas:\n\u0026gt;\u0026gt;\u0026gt;df['age'].agg('mean') 54.43894389438944 \\\nIn Orange:\nIn Pivot Table we set Values to age and set aggregations to mean. There is no way to turn off splitting by rows in Pivot Table, but the values in Total report the mean value for the chosen attribute.\n \\\nAn even better way to observe simple statistics is in Feature Statistics widget. The widget also reports on the mean value of each attribute with handy distributions plots included.\n \\\nYet another way to observe a mean value is in a Box Plot.\n \n\\\npandas.DataFrame.groupby The second task is grouping the data by a discrete column. Say we want to group by gender and report the mean value for each column.\nIn pandas:\n\u0026gt;\u0026gt;\u0026gt; df.groupby(['gender']).mean() age rest SBP ... ST by exer. major ves. col. diameter narrowing gender female 55.721649 133.340206 ... 0.867010 0.546392 0.257732 male 53.834951 130.912621 ... 1.120874 0.732673 0.553398 [2 rows x 9 columns] In Orange:\nIn Pivot Table set Rows to gender and aggregation method to mean. The Values option in this example has no effect. Now, connect Data Table to Pivot Table. Finally, reset the connections. Pivot Table outputs three types of data - Pivot Table, Filtered Data, and Grouped Data. The output we are looking for here is Grouped Data.\n \\\n \\\nThis is our data table. Exactly what we were looking for.\n \n\\\npandas.DataFrame.pivot_table The third, final task is constructing a pivot table where rows are values of diameter narrowing, columns are values of gender and the values in cells is the mean of age for each subgroup. In other words, we want to see the average age of females with diameter narrowing, males with diameter narrowing, females without diameter narrowing and males without diameter narrowing.\nIn pandas:\n\u0026gt;\u0026gt;\u0026gt; pd.pivot_table(df, values='age', index=['diameter narrowing'], columns=['gender'], aggfunc=np.mean) gender female male diameter narrowing 0 54.555556 51.043478 1 59.080000 56.087719 In Orange:\nIn Pivot Table set Rows to diameter narrowing, Columns to gender, Values to age and aggregation method to mean. The widget already offers a view of the final data table, but we can also output it and use it in other Orange widgets.\n \n\\\nPivot Table widget really versatile - like a Swiss knife for data transformation.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-08-27_pivot-preview.png",
	"shortExcerpt" : "Orange's brand new Pivot Table widget with many aggregation and grouping functionalities.",
	"longExcerpt" :  "Orange has a brand new Pivot Table widget with many aggregation and grouping functionalities. It can be used to transform the data on-the-fly and use the output for downstream analysis." ,
	"author" : "Ajda Pretnar",
	"summary" : "Orange recently welcomed its new Pivot Table widget, which offers functionalities for data aggregation, grouping and, well, pivot tables. The widget is a one-stop-shop for pandas' aggregate, groupby and pivot_table functions.\n \\\nLet us see how to achieve these tasks in Orange. For all of the below examples we will be using the heart_disease.tab data.\npandas.DataFrame.agg The first task is computing a simple mean for the column age.\nIn pandas:",
	"date" : "Aug 27, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-07-26-doctoral_summer_school/",
	"title": "Doctoral Summer School",
	"categories": ["workshop", "education", "data science", "summer school"],
	"description": "",
	"content": "For the second year in a row, the Orange team was a part of the Ljubljana Doctoral Summer School, which is organized by the School of Economics and Business, University of Ljubljana. Our course, called Pratical Introduction to Machine Learning and Data Analytics, was aimed at presenting the nuts and bolts of data science methods and concepts with the help of visual programming. In Orange, of course.\n \n\\\nOne important lesson is comparing model accuracy. Let us load the heart_disease data with the File widget. This is the data on the presence of diameter narrowing (a sign of heart disease) in 303 patients. Now let us sample the data into the training and testing data. We will use Data Sampler for this.\n \\\nNow let\u0026rsquo;s check model accuracy on, say, Tree and Naive Bayes, one of the first two classifiers that we introduce. Connect both outputs of Data Sampler to Test \u0026amp; Score, the first as Data and the second as Test Data.\n \n\\\nFirst, check the model accuracy with the Test on train data option. Great scores! Seems like Tree performs almost flawlessly. It must be the better classifier of the two!\n \n\\\nOk, just to be sure, let us check the scores on a separate test data set by selecting Test on test data. Oh. This doesn\u0026rsquo;t look good. Tree now performs much worse than Naive Bayes. Why is that?\n  \n\\\nWe won\u0026rsquo;t answer this directly, but this is one of the homeworks we give the students in order to introduce cross-validation. This is such an important concept that we make sure everyone in the class really understand why we need separate training and testing data.\nFeel free to use this exercise in any of your future classes!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-07-26_ekonomska_small.jpg",
	"shortExcerpt" : "This year's Data Science course at the Doctoral Summer School.",
	"longExcerpt" :  "For the second year in a row we took part in the Ljubljana Doctoral Summer School, organized by the School of Economics and Business." ,
	"author" : "Ajda Pretnar",
	"summary" : "For the second year in a row, the Orange team was a part of the Ljubljana Doctoral Summer School, which is organized by the School of Economics and Business, University of Ljubljana. Our course, called Pratical Introduction to Machine Learning and Data Analytics, was aimed at presenting the nuts and bolts of data science methods and concepts with the help of visual programming. In Orange, of course.\n \n\\",
	"date" : "Jul 26, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-07-25-scorange/",
	"title": "Orange at ISMB/ECCB 2019",
	"categories": ["education", "bioinformatics"],
	"description": "",
	"content": "Our entry to this year\u0026rsquo;s largest bioinformatics conference was on the training of single-cell data analytics. We claim that with Orange and its new single-cell RNA analysis add-on, one can assemble a workshop to teach essential concepts from single-cell analytics in a single day.\n \\\nSingle-cell genomics is driven on revolutionary technology from molecular biology that allows us to peek into inner workings of the individual cell. Single-cell datasets feature thousands of genes and cells and benefit from the analysis that integrates this data with other knowledge sources, like those on the classification of genes, lists of pathways, and sets of cell-type markers. To support such analysis, we have been developing a single-cell add-on for Orange. We are also experimenting with packing this add-on, the add-on for bioinformatics, and the rest of Orange within a stand-alone application called scOrange. Installation of scOrange comes with a preinstalled single-cell add-on and workflows and videos that are specific to this area of research. Our single-cell RNA analysis tool still uses Orange\u0026rsquo;s widgets at the core, but additionally packs components for reading single-cell data, filtering cells and genes, preprocessing the data and handling peculiarities of single-cell analysis like batch effects and cell-type annotation.\nISMB/ECCB 2019 is the largest and most prestigious bioinformatics conference. It joins the meetings of International and European and Society for Computational Biology. Martin Stražar, a post-doc at our Bioinformatics Lab in Ljubljana and responsible for much of developments in scOrange, presented scOrange in light of its support for case-based teaching and hands-on workshops. During the talk, Martin showcased several scOrange\u0026rsquo;s workflows. The one I liked best includes automatic annotation of cell-types in point-based visualizations.\n Martin\u0026rsquo;s presentation was based on our ISMB/ECCB paper just published in Bioinformatics:\nMartin Stražar, Lan Žagar, Jaka Kokošar, Vesna Tanko, Aleš Erjavec, Pavlin G Poličar, Anže Starič, Janez Demšar, Gad Shaulsky, Vilas Menon, Andrew Lemire, Anup Parikh, Blaž Zupan (2019) scOrange—a tool for hands-on training of concepts from single-cell data analytics, Bioinformatics 35(14):i4–i12.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/scorange-annotation-thumb.png",
	"shortExcerpt" : "One of Orange's latest features is its add-on for single-cell data analytics.",
	"longExcerpt" :  "Our entry to this year\u0026rsquo;s largest bioinformatics conference was on the training of single-cell data analytics. We claim that with Orange and its new single-cell RNA analysis add-on, one can assemble a workshop to teach essential concepts from single-cell analytics in a single day.\n \\\nSingle-cell genomics is driven on revolutionary technology from molecular biology that allows us to peek into inner workings of the individual cell. Single-cell datasets feature thousands of genes and cells and benefit from the analysis that integrates this data with other knowledge sources, like those on the classification of genes, lists of pathways, and sets of cell-type markers." ,
	"author" : "Blaž Zupan",
	"summary" : "Our entry to this year\u0026rsquo;s largest bioinformatics conference was on the training of single-cell data analytics. We claim that with Orange and its new single-cell RNA analysis add-on, one can assemble a workshop to teach essential concepts from single-cell analytics in a single day.\n \\\nSingle-cell genomics is driven on revolutionary technology from molecular biology that allows us to peek into inner workings of the individual cell. Single-cell datasets feature thousands of genes and cells and benefit from the analysis that integrates this data with other knowledge sources, like those on the classification of genes, lists of pathways, and sets of cell-type markers.",
	"date" : "Jul 25, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-07-02-data-science-made-easy/",
	"title": "Data Science Made Easy: How To Identify Hate Comments with AI",
	"categories": ["education", "text mining", "workshop"],
	"description": "",
	"content": "The IdeenExpo is a biennial participatory event for children, adolescents and young adults taking place in Hanover, Germany. Companies, research organizations, schools and universities participate to show young people the possibilities of the modern working world and gain their interest in technologies and natural sciences. As a part of one of the biggest research-computing-centers in North Germany the GWDG (Gesellschaft für wissenschaftliche Datenverarbeitung mbh Göttingen) took a part in that event to present the possibilities of Data Science and how its methods can be used in different areas.\nRelated: Text Workshops in Ljubljana\nOur goal was to give the 9th grade students a 60-minute hands-on introduction to some possible real-life use cases. As we were working with Orange3 now for some time, we decided to use it in our workshop, because it has the great benefit of being able to do data analysis without the need to write code, which wouldn\u0026rsquo;t have worked in a 60 minute workshop.\n \\\nWe started with some introductory talks and discussions about the presence of Big Data in our daily life to get some insights into the existing knowledge of the students. A quick question about who is active in social media made it clear that everyone in that workshop has one or more accounts on social media platforms. Next we asked about hate comments on social media and everyone was aware about that topic as well.\nAfter displaying some hate comments and discussing about how we as humans identify them, we split the group in two parts. One group had then to write hate comments and insults, whereas the other group wrote neutral comments concerning a certain topic we decided on.\nWe then collected that comments in a GoogleSheets Table, labeled them and every student opened Orange3 on his or her laptop. The students were asked to explain in their own words how they would train an \u0026ldquo;AI\u0026rdquo; to learn to distinguish between hate and neutral comments and we showed them how this can be translated in an Orange workflow. This way the students already learned that we have to do some preprocessing to filter out uninformative words for example.\n \\\nWith just the tweets we made up in our session we gained a precision value of 0.66 in the first session and after we appended the tweets from the second group, we already gained a value of 0.76. Afterwards the students were asked to made up 4 other tweets the model was not trained on and used the Predictions widget to see how well our model performed. Well, we just got the results we would have thought of, if we would have had to classify them on our own!\nOrange3 made it possible to develop a model for detecting hate comments in just 60 minutes with students, who had no programming skills. Thanks to the Orange Team!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/twitter-police-workshop-picture.jpeg",
	"shortExcerpt" : "How to teach text mining and data science to the 9th grade students in 60 minutes?",
	"longExcerpt" :  "How to teach text mining and data science to the 9th grade students in 60 minutes? With Orange and the analysis of hate speech on social media, of course!" ,
	"author" : "Dr. Sven Bingert & Steffen Rörtgen",
	"summary" : "The IdeenExpo is a biennial participatory event for children, adolescents and young adults taking place in Hanover, Germany. Companies, research organizations, schools and universities participate to show young people the possibilities of the modern working world and gain their interest in technologies and natural sciences. As a part of one of the biggest research-computing-centers in North Germany the GWDG (Gesellschaft für wissenschaftliche Datenverarbeitung mbh Göttingen) took a part in that event to present the possibilities of Data Science and how its methods can be used in different areas.",
	"date" : "Jul 2, 2019"
}

    
    , {
    "uri": "/blog/2019/2019-06-28-bled-econference/",
	"title": "Orange at 32nd Bled eConference",
	"categories": ["workshop", "education", "data science"],
	"description": "",
	"content": "At the invitation of dr. Mirjana Kljajić, we participated in the 32nd Bled eConference. The conference is one of the most important in the region on trends and technologies for electronic communication and this year a short workshop on Data Science with Orange was included in its programme.\n \\\nWe covered many topics, including data loading, visualization, construction of a predictive modeling workflow, exploration of decision trees, overfitting, model scoring and of course finally predicting with cross-validated model on a new data set. One of the classic exercises for such courses includes using the Attrition - Train and Attrition - Predict data sets from the Datasets widget and answering the following questions:\n Find one or two interesting variables in a Box Plot and describe what they show. Use all known models in Test and Score. Which one has the highest classification accuracy? Which three variables are the most important predictors for Logistic Regression? On a new data set, identify the person that is most likely to quit. What would be your recommendations to the HR department, considering the Nomogram?  \n\n\\\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/2019-06-28_bled-small.png",
	"shortExcerpt" : "We held a short workshop for the participants of the 32nd Bled eConference.",
	"longExcerpt" :  "We held a short workshop covering the basics of Data Science for the participants of the 32nd Bled eConference. We focused on predictive modeling and explorations of predictions." ,
	"author" : "Ajda Pretnar",
	"summary" : "At the invitation of dr. Mirjana Kljajić, we participated in the 32nd Bled eConference. The conference is one of the most important in the region on trends and technologies for electronic communication and this year a short workshop on Data Science with Orange was included in its programme.\n \\\nWe covered many topics, including data loading, visualization, construction of a predictive modeling workflow, exploration of decision trees, overfitting, model scoring and of course finally predicting with cross-validated model on a new data set.",
	"date" : "Jun 28, 2019"
}

    
    , {
    "uri": "/blog/2019/6/gene-expression-profiles-with-line-plot/",
	"title": "Gene Expression Profiles with Line Plot",
	"categories": ["bioinformatics", "gene expression", "line plot"],
	"description": "",
	"content": "Line Plot is one of our recent additions to the visualization widgets. It shows data profiles, meaning it plots values for all features in the data set. Each data instance in a line plot is a line or a \u0026lsquo;profile\u0026rsquo;.\nThe widget can show four types of information – individual data profiles (lines), data range, mean profile and error bars. It has the same cool features of other Orange visualizations – it is interactive, meaning you can select a subset of data instances from the plot, it allows grouping by a discrete variable, and it highlights an incoming data subset.\nRelated: Scatter Plot: The Tour\nLet us check a simple example. We will use brown-selected data, which is a data on gene expression of baker\u0026rsquo;s yeast. To observe gene expression profiles, we will use Line Plot.\n\n\\\n \n\\\nSince the data has class, which represents a function of the gene, Line Plot will automatically group by class variable. It seems like protease, respiratory and ribosome genes have quite distinctive profiles! Let us select the most interesting region in the plot by selecting the zoom tool and dragging across the area of interest. \\\n \n\\\nWe see that spo-mid feature distinguishes really well between protease and two other gene types and that values of protease are normally high for spo-mid.\nAnother thing we can do is select a subset from the plot. If we press the ‘rectangle’ icon on the left, our plot will be automatically resized to the original size. Then we press the ‘arrow’ icon, which will put us back to the selecting mode. Now let us select Lines instead of Range and Mean for display. This will show individual expression profiles. \\\n \n\\\nIf we click and drag across an area of interest, instances under the thick black line will be selected. We can connect, say a Box Plot to the Line Plot and observe the distribution of the selected subset. Unsurprisingly, the genes we have selected are mostly protease. \\\n \n\\\n \n\\\nThis is it. Line Plot is really simple to use and can reveal many interesting things not only for biologists, but for any kind of data analyst. Next week we will talk about how to work with timeseries data in combination with the Line Plot.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/6/3/gene-blog.png",
	"shortExcerpt" : "We show how to explore gene expression profiles with the new Line Plot widget.",
	"longExcerpt" :  "Line Plot shows profiles of data instances – each instance is a line in the plot and its profile are values across all variables in the data. We show how to explore gene expression profiles." ,
	"author" : "Ajda Pretnar",
	"summary" : "Line Plot is one of our recent additions to the visualization widgets. It shows data profiles, meaning it plots values for all features in the data set. Each data instance in a line plot is a line or a \u0026lsquo;profile\u0026rsquo;.\nThe widget can show four types of information – individual data profiles (lines), data range, mean profile and error bars. It has the same cool features of other Orange visualizations – it is interactive, meaning you can select a subset of data instances from the plot, it allows grouping by a discrete variable, and it highlights an incoming data subset.",
	"date" : "Jun 3, 2019"
}

    
    , {
    "uri": "/blog/2019/5/18/business-case-studies-with-orange/",
	"title": "Business Case Studies with Orange",
	"categories": ["business intelligence", "HR", "logistic regression", "nomogram", "predictive models"],
	"description": "",
	"content": "Previous week Blaž, Robert and I visited Wärtsilä in the lovely Dolina near Trieste, Italy. Wärtsilä is one of the leading designers of lifecycle power solutions for the global marine and energy markets and its subsidiary in Trieste is one of the largest Wärtsilä Group engine production plants. We were there to hold a one-day workshop on data mining and machine learning with the aim to identify relevant use cases in business and show how to address them.\n\\\n \n\\\nRelated: Data Mining for Business and Public Administration\nOne such important use case is employee attrition. It is vital for any company to retain its most valuable workers, so they must learn how to identify dissatisfied employees and provide incentive from the to stay. It is easy to construct a workflow in Orange that helps us with this.\nFirst, let us load Attrition – Train data set from the Datasets widget. This is a synthetic data set from IBM Watson that has 1470 instances (employees) and 18 features describing them. Our target variable is Attrition, where Yes means the person left the company and No means it stayed.\n\\\n \n\\\nNow our goal is to construct a predictive model that will successfully predict the likelihood of a person leaving. Let us connect a couple of classifiers and the data set to Test and Score and see which model performs best.\n\\\n \nSeems like Logistic Regression is the winner here, since its AUC score it the highest of the three.\n\\\n \nA great thing about Logistic Regression is that it is interpretable. We can connect the data from Datasets to Logistic Regression and the resulting model from LR to Nomogram. Nomogram shows the top ten features, ranked by their contribution to the final probability of a class.\n\\\n \n\\\nThe length of a line corresponds to the relative importance of the attribute. Seems like recently hired employees are more likely to leave (YearsAtCompany goes towards 0). We also should consider promoting those that haven’t been promoted in a while (YearsSinceLastPromotion goes towards 15) and cut the overtime (OverTime is Yes). Model inspection helps us identify relevant attributes and interpret their values. So useful for HR departments!\n\\\n \n\\\nFinally, we can take new data and predict the likelihood for leaving. Put another Datasets on the canvas and load Attrition – Predict data. This one contains only three instances – say the data for three employees we have forgotten to consider in our training data.\n\\\n \n\\\nSo who is more likely to leave? We obviously cannot afford to promote everyone, because this costs money. We need to optimize our decisions so that we both increase the satisfaction of employees while keep our costs low. This is where we can use predictive modeling. Connect Logistic Regression to Predictions widget. Then connect the second Datasets widget with the new data to Predictions as well.\n\\\n \nSeems like John is most likely to leave. He has been at the company for only a year and he works overtime.\n\\\n \n\\\nThis is something HR department can work with to design proper policies and keep best talent. The same workflow can be used for churn prediction, process optimization and predicting success of a new product.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/5/18/wartsila-blog.jpg",
	"shortExcerpt" : "At the latest workshop we demonstrated how to predict, which employees are most likely to resign in the future.",
	"longExcerpt" :  "At the latest workshop in Italy we taught the participants how to identify relevant business use cases and how to predict, which employees are most likely to resign in the future." ,
	"author" : "Ajda Pretnar",
	"summary" : "Previous week Blaž, Robert and I visited Wärtsilä in the lovely Dolina near Trieste, Italy. Wärtsilä is one of the leading designers of lifecycle power solutions for the global marine and energy markets and its subsidiary in Trieste is one of the largest Wärtsilä Group engine production plants. We were there to hold a one-day workshop on data mining and machine learning with the aim to identify relevant use cases in business and show how to address them.",
	"date" : "May 18, 2019"
}

    
    , {
    "uri": "/blog/2019/4/24/orange-at-gis-ostrava/",
	"title": "Orange at GIS Ostrava",
	"categories": ["geo", "GIS", "hierarchical clustering"],
	"description": "",
	"content": "Ostrava is a city in the north-east of the Czech Republic and is the capital of the Moravian-Silesian Region. GIS Ostrava is a yearly conference organized by Jiří Horák and his team at the Technical University of Ostrava. University has a nice campus with a number of new developments. I have learned that this is the largest university campus in central and eastern Europe, as most of the universities, like mine, are city universities with buildings dispersed around the city.\nDuring the conference, I gave an invited talk on \u0026ldquo;Data Science for Everyone\u0026rdquo; and showed how Orange can be used to teach basic data science concepts in a few hours so that the trainee can gain some intuition about what data science is and then, preferably, use the software on their own data. To prove this concept, I gave an example workshop during the next day of the conference. The workshop was also attended by several teachers that are thinking of incorporating Orange within their data science curricula.\n\\\n \n\\\nAdmittedly, there was not much GIS in my presentations, as I – as planned – focused more on data science. But I did include an example of how to project the data in Orange to geographical maps. The example involved the analysis of Human Development Index data and clustering. When projected to the map, the results of clustering could be unexpected if we select only the features that address quality of life: check out the map below and try to figure out what is wrong.\n\\\n \n\\\nHere, I would like to thank Igor Ivan and Jiří Horák for the invitation, and their group and specifically Michal Kacmarik for the hospitality.\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/4/24/ostrava-workshop.jpg",
	"shortExcerpt" : "In late March 2019, we have been invited to present Orange during a GIS Ostrava conference.",
	"longExcerpt" :  "In late March 2019, we have been invited to present Orange during a GIS Ostrava conference. We have shown how to work with geospatial data in Orange." ,
	"author" : "Blaz Zupan",
	"summary" : "Ostrava is a city in the north-east of the Czech Republic and is the capital of the Moravian-Silesian Region. GIS Ostrava is a yearly conference organized by Jiří Horák and his team at the Technical University of Ostrava. University has a nice campus with a number of new developments. I have learned that this is the largest university campus in central and eastern Europe, as most of the universities, like mine, are city universities with buildings dispersed around the city.",
	"date" : "Apr 24, 2019"
}

    
    , {
    "uri": "/blog/2019/3/8/the-changing-status-bar/",
	"title": "The Changing Status Bar",
	"categories": ["release", "Status Bar"],
	"description": "",
	"content": "Every week on Friday, when the core team of Orange developers meets, we are designing new improvements of Orange\u0026rsquo;s graphical interface. This time, it was the status bar. Well, actually, it was the status bar quite a while ago and required the change of the core widget library, but it is materializing these days and you will see the changes in the next release.\nConsider the Neighbors widget. The widget considers the input data and reference data items, and outputs instance form input data that are most similar to the references. Like, if the dolphin is a reference, we would like to know which are the three most similar animals. But this is not what want I wanted to write about. I would only like to say that we are making a slight change in the user interface. Below is the Neighbors widget in the current release of Orange, and the upcoming one.\n\\\n \n\\\nSee the difference? We are getting rid of the infobox on the top of the control tab, and moving it to the status bar. In the infobox widgets typically display what is in their input and what is on the output after the data has been processed. Moving this information to the status bar will make widgets more compact and less cluttered. We will similarly change the infoboxes in this way in all of the widgets.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/3/8/changing-blog.png",
	"shortExcerpt" : "We are constantly optimizing Orange's look-and-feel. New features in the status bar will simplify the user interface.",
	"longExcerpt" :  "We are constantly optimizing Orange's look-and-feel. New features in the status bar will simplify the user interface. We are getting rid of the infobox on the top of the control tab, and moving it to the status bar." ,
	"author" : "Blaz Zupan",
	"summary" : "Every week on Friday, when the core team of Orange developers meets, we are designing new improvements of Orange\u0026rsquo;s graphical interface. This time, it was the status bar. Well, actually, it was the status bar quite a while ago and required the change of the core widget library, but it is materializing these days and you will see the changes in the next release.\nConsider the Neighbors widget. The widget considers the input data and reference data items, and outputs instance form input data that are most similar to the references.",
	"date" : "Mar 8, 2019"
}

    
    , {
    "uri": "/blog/2019/3/1/single-cell-data-science-for-everyone/",
	"title": "Single-Cell Data Science for Everyone",
	"categories": ["gene ontology", "genomics", "RNA-seq", "scOrange", "single cell"],
	"description": "",
	"content": "Molecular biologists have in the past twenty years invented technologies that can collect abundant experimental data. One such technique is single-cell RNA-seq, which, very simplified, can measure the activity of genes in possibly large collections of cells. The interpretation of such data can tell us about the heterogeneity of cells, cell types, or provide information on their development.\nTypical analysis toolboxes for single-cell data are available in R and Python and, most notably, include Seurat and scanpy, but they lack interactive visualizations and simplicity of Orange. Since the fall of 2017, we have been developing an extension of Orange, which is now (almost) ready. It has even been packed into its own installer. The first real test of the software was in early 2018 through a one day workshop at Janelia Research Campus. On March 6, and with a much more refined version of the software, we have now repeated the hands-on workshop at the University of Pavia.\n\\\n \n\\\nThe five-hour workshop covered both the essentials of data analysis and single cell analytics. The topics included data preprocessing, clustering, and two-dimensional embedding, as well as working with marker genes, differential expression analysis, and interpretation of clusters through gene ontology analysis.\nI want to thank Prof. Dr. Riccardo Bellazzi and his team for the organization, and Erasmus program for financial support. I have been a frequent guest to Pavia, and learn something new every time I am there. Besides meeting new students and colleagues that attended the workshop and hearing about their data analysis challenges, this time I have also learned about a dish I had never had before in all my Italian travels. For one of the dinners (thank you, Michela) we had Pizzoccheri. Simply great!\n\\\n \n\\\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2019/3/1/2019-pavia-blog.jpg",
	"shortExcerpt" : "We have been in Pavia, Italy, to carry out an introductory workshop on single-cell data science.",
	"longExcerpt" :  "We have been in Pavia, Italy, to carry out a five-hour workshop covered both the essentials of data analysis and single cell analytics. The topics included working with marker genes, differential expression analysis, and interpretation of clusters through gene ontology analysis." ,
	"author" : "Blaz Zupan",
	"summary" : "Molecular biologists have in the past twenty years invented technologies that can collect abundant experimental data. One such technique is single-cell RNA-seq, which, very simplified, can measure the activity of genes in possibly large collections of cells. The interpretation of such data can tell us about the heterogeneity of cells, cell types, or provide information on their development.\nTypical analysis toolboxes for single-cell data are available in R and Python and, most notably, include Seurat and scanpy, but they lack interactive visualizations and simplicity of Orange.",
	"date" : "Mar 1, 2019"
}

    
    , {
    "uri": "/blog/2019/1/28/the-mystery-of-test-and-score/",
	"title": "The Mystery of Test &amp; Score",
	"categories": ["cross validation", "leave one out", "LOO", "test and score"],
	"description": "",
	"content": "Test \u0026amp; Score is surely one the most used widgets in Orange. Fun fact: it is the fourth in popularity, right after Data Table, File and Scatter Plot. So let us dive into the nuts and bolts of the Test \u0026amp; Score widget.\nThe widget generally accepts two inputs – Data and Learner. Data is the data set that we will be using for modeling, say, iris.tab that is already pre-loaded in the File widget. Learner is any kind of learning algorithm, for example, Logistic Regression. You can only use those learners that support your type of task. If you wish to do classification, you cannot use Linear Regression and for regression you cannot use Logistic Regression. Most other learners support both tasks. You can connect more than one learner to Test \u0026amp; Score.\n\\\n \n\\\nTest \u0026amp; Score will now use each connected Learner and the Data to build a predictive model. Models can be built in different ways. The most typical procedure is cross validation, which splits the data into k folds and uses k – 1 folds for training and the remaining fold for testing. This procedure is repeated, so that each fold has been used for testing exactly once. Test \u0026amp; Score will then report on the average accuracy of the model.\nYou can also use Random Sampling, which will split the data into two sets with predefined proportions (e.g. 66% : 34%), build a model on the first set and test it on the second. This is similar to CV, except that each data instance can be used more than once for testing.\nLeave one out is again very similar to the above two methods, but it only takes one data instance for testing each time. If you have a 100 data instances, then 99 will be used for training and 1 for testing, and the procedure will be repeated a 100 times until every data instance was used once for testing. As you can imagine, this is a very time-intensive procedure and it is recommended for smaller data sets only.\nTest on train data uses the whole data set for training and again the same data for testing. Because of overfitting, this will usually overestimate the performance! Test on test data requires an additional data input (Test Data) and allows the user to control both data sets (training and testing) used for evaluation.\nFinally, you can also use cross validation by feature. Sometimes, you would have pre-defined folds for a procedure, that you wish to replicate. Then you can use Cross validation by feature to ensure data instances are split into the same folds every time. Just make sure the feature you are using for defining folds is a categorical variable and located in meta attributes.\nAnother scenario is when you have several examples from the same object, for example several measurements of the same patient or several images of the same plant. Then you absolutely want to make sure that all data instances for a particular object are in the same fold. Otherwise, your model would probably report severely overfitted scores.\n\\\n \n\\\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/1/28/ts-blog.png",
	"shortExcerpt" : "Test & Score widget is used for evaluating model performance, but what do the methods do? We explain each of them in a few lines.",
	"longExcerpt" :  "Test \u0026 Score widget is used for evaluating model performance, but what do the methods do? We explain cross validation, random sampling, leave one out and cross validation by feature in a few lines." ,
	"author" : "Ajda Pretnar",
	"summary" : "Test \u0026amp; Score is surely one the most used widgets in Orange. Fun fact: it is the fourth in popularity, right after Data Table, File and Scatter Plot. So let us dive into the nuts and bolts of the Test \u0026amp; Score widget.\nThe widget generally accepts two inputs – Data and Learner. Data is the data set that we will be using for modeling, say, iris.tab that is already pre-loaded in the File widget.",
	"date" : "Jan 28, 2019"
}

    
    , {
    "uri": "/blog/2019/1/4/how-to-abuse-p-values-in-correlations/",
	"title": "How to Abuse p-Values in Correlations",
	"categories": ["correlations", "NHTS", "null hypothesis", "p-value", "statistics"],
	"description": "",
	"content": "In a parallel universe, not so far from ours, Orange’s Correlation widget looks like this.\n\\\n \n\\\nQuite similar to ours, except that this one shows p-values instead of correlation coefficients. Which is actually better, isn’t it? I mean, we have all attended Statistics 101, and we know that you can never trust correlation coefficients without looking at p-values to check that these correlations are real, right? So why on Earth doesn’t Orange show them?\nFirst a side note. It was Christmas not long ago. Let’s call a ceasefire on the frequentist vs. Bayesian war. Let us, for Christ’s sake, pretend, pardon, agree that null-hypothesis testing is not wrong per se.\nThe mantra of null-hypothesis significance testing goes like this:\n Form hypothesis. Collect data. Test hypothesis.  In contrast, the parallel-universe Correlation widget is (ab)used like this:\n Collect data. Test all possible hypotheses. Cherry pick those that are confirmed.  This is like the Texas sharpshooter who fires first and then draws targets around the shots. You should never formulate hypothesis based on some data and then use this same data to prove it. Because it usually (surprise!) works.\n\\\n \n\\\nBack to the above snapshot. It shows correlations between 100 vegetables based on 100 different measurements (Ca and Mg content, their consumption in Finland, number of mentions in Star Trek DS9 series, likelihood of finding it on the Mars, and so forth). In other words, it’s all made it up. Just a 100×100 matrix of random numbers with column labels from the simple Wikipedia list of vegetables. Yet the similarity between mung bean and sunchokes surely cannot be dismissed (p \u0026lt; 0.001). Those who like bell pepper should try cilantro, too, because it’s basically one and the same thing (p = 0.001). And I honestly can’t tell black bean from wasabi (p = 0.001).\nHere are the p-values for the top 100 most correlated pairs.\nimport numpy as np import scipy as sp a = np.random.random((100, 100)) sorted(stats.pearsonr(a[i], a[j])[1] for i in range(100) for j in range(i))[:100] [0.0002774329730584203, 0.0004158786523819104, 0.0005008536192579852, 0.0007211022164265075, 0.0008268675086438253, 0.0010265740674904762, (...91 values omitted to reduce the nonsense) 0.01844720610938738, 0.018465602922746942, 0.018662079618069056] First 100 correlations are highly significant.  To learn a lesson we may have failed to grasp at the NHST 101 class, consider that there are 100 * 99 / 2 pairs. What is the significance of the pair at 5-th percentile?\ncorrelations = sorted(stats.pearsonr(a[i], a[j])[1] for i in range(100) for j in range(i)) npairs = 100 * 99 / 2 print(correlations[int(pairs * 0.05)] 0.0496868751692227  Roughly 0.05. This is exactly what should have happened, because:\ncorrelations[int(npairs * 0.10)] 0.10004180592217532 correlations[int(npairs * 0.15)] 0.15236602574520097 correlations[int(npairs * 0.30)] 0.3026816170584785  This proves only that p-values for the Pearson correlation coefficient are well calibrated (and that Mersenne twister that is used to generate random numbers in numpy works well). In theory, the p-value for a certain statistics (like Pearson’s r) is the probability of getting such or even more extreme value if the null-hypothesis (of no correlation, in our case) is actually true. 5 % of random hypotheses should have a p-value below 0.05, 10 % a value below 10, and 23 % a value below 23.\nImagine what they can do with the Correlations widget in the parallel universe! They compute correlations between all pairs, print out the first 5 % of them and start writing a paper without bothering to look at p-values at all. They know they should be statistically significant even if the data is random.\nWhich is precisely the reason why our widget must not compute p-values: because people would use it for Texas sharpshooting. P-values make sense only in the context of the proper NHST procedure (still pretending for the sake of Christmas ceasefire). They cannot be computed using the data on which they were found.\nIf so, why do we have the Correlation widget at all if it’s results are unpublishable? We can use it to find highly correlated pairs in a data sample. But we can’t just attach p-values to them and publish them. By finding these pairs (with assistance of Correlation widget) we just formulate hypotheses. This is only step 1 of the enshrined NHST procedure. We can’t skip the other two: the next step is to collect some new data (existing data won’t do!) and then use it to test the hypotheses (step 3).\nFollowing this procedure doesn’t save us from data dredging. There are still plenty of ways to cheat. It is the most tempting to select the first 100 most correlated pairs (or, actually, any 100 pairs), (re)compute correlations on some new data and publish the top 5 % of these pairs. The official solution for this is a patchwork of various corrections for multiple hypotheses testing, but… Well, they don’t work, but we should say no more here. You know, Christmas ceasefire.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/1/4/correlations.png",
	"shortExcerpt" : "Why doesn't Orange show p-values for correlations coefficients? To save you from data dredging and Texas sharpshooter fallacy...",
	"longExcerpt" :  "We have all attended Statistics 101, and we know that you can never trust correlation coefficients without looking at p-values to check that these correlations are real, right? So why on Earth doesn’t Orange show them?" ,
	"author" : "Ajda Pretnar",
	"summary" : "In a parallel universe, not so far from ours, Orange’s Correlation widget looks like this.\n\\\n \n\\\nQuite similar to ours, except that this one shows p-values instead of correlation coefficients. Which is actually better, isn’t it? I mean, we have all attended Statistics 101, and we know that you can never trust correlation coefficients without looking at p-values to check that these correlations are real, right? So why on Earth doesn’t Orange show them?",
	"date" : "Jan 4, 2019"
}

    
    , {
    "uri": "/blog/2018/12/21/scatter-plots-the-tour/",
	"title": "Scatter Plots: the Tour",
	"categories": ["interactive visualization", "scatter plot", "visualization"],
	"description": "",
	"content": "Scatter plots are surely one of the best loved visualizations in Orange. Very often, when we teach, people go back to scatter plots over and over again to see their data. We took people’s love for scatter plots to the heart and we redesigned them a bit to make them even more friendly.\nOur favorite still remains the Informative Projections button. This button helps you find interesting visualizations from all the combinations of your data variables. But what does interesting mean? Well, let us look at an example. Which of the two visualizations tells you more about the data?\n  We’d say it is the right one. Why? Because now we know that the combination of petal length and petal width nicely separates the classes!\nOf course, Informative Projections button will only work when you have set a class (target) variable.\nIn scatter plot, you can set also the color of the data points (class is selected by default), the size of the points and the shape. This means you can add three new layers of information to your data, but we warn you not to overuse them. This usually looks very incomprehensible, even though it packs a lot of information.\n You might notice, that in the current version of Orange, you can no longer select discrete attributes in Scatter Plot. This is entirely intentional. Scatter plots are best at showing the relationship between two numeric variables, such as in the two examples above. Categorical variables are much better represented with Box Plots, histograms (in Distributions) or in Mosaic Display.\n Above, we have presented the same information for titanic data set in different visualizations, that are particularly suitable for categorical variables.\nScatter plot also enables so cool tricks. Just like in most visualizations in Orange, I can select a part of the data and observe the subset downstream. Or the other way around. I have a particular subset I wish to observe and I can pass it to Scatter Plot widget, which will highlight selected data instances.\n This is also true for all other point-based visualizations in Orange, namely t-SNE, MDS, Radviz, Freeviz, and Linear Projection.\nYou can see there are many great thing you can do with Scatter Plot. Finally, we have added a nice touch to the visualization.\n Yes, setting the size of the attribute is now animated! 🙂\nHappy holidays, everyone!\n",
	"image" : "<no value>",
	"thumbImage" : "/blog_img/2018/12/21/scatter-blog.png",
	"shortExcerpt" : "Scatter Plot has recently been renovated and it is time to present some essential tricks for working with the widget!",
	"longExcerpt" :  "Scatter Plot has recently been renovated (under the hood and in GUI), so now it is time to present some essential tricks for working with the cool visualization!" ,
	"author" : "Ajda Pretnar",
	"summary" : "Scatter plots are surely one of the best loved visualizations in Orange. Very often, when we teach, people go back to scatter plots over and over again to see their data. We took people’s love for scatter plots to the heart and we redesigned them a bit to make them even more friendly.\nOur favorite still remains the Informative Projections button. This button helps you find interesting visualizations from all the combinations of your data variables.",
	"date" : "Dec 21, 2018"
}

    
    , {
    "uri": "/blog/2018/11/22/orange-is-getting-smarter/",
	"title": "Orange is Getting Smarter",
	"categories": ["analysis", "interface", "orange3"],
	"description": "",
	"content": "In the past few months, Orange has been getting smarter and sleeker.\nSince version 3.15.0, Orange remembers which distinct widgets users like to connect, adjusting the sorting on the widget search menu accordingly. Additionally, there is a new look for the Edit Links window coming soon.\nOrange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.\nWord cloud of widget popularity in Orange.\nThe information is collected anonymously for the users that opted-in. We will use this data to improve the widget suggestion system. Furthermore, the data provides us the first insight into how users interact with Orange. Let\u0026rsquo;s see what we\u0026rsquo;ve found out from the data recorded in the past few weeks.\nThere are four different ways of adding a widget to the canvas,\n clicking it in the sidebar, dragging it from the sidebar, searching for it by right-clicking on canvas, extending the workflow by dragging the communication channel from a widget.  A workflow extend action.\nAmong Orange users, the most popular way of adding a new widget is by dragging the communication line from the output widget – we think this is the most efficient way of using Orange too. However, the patterns vary among different widgets.\nHow users add widgets to canvas, from 20,775 add widget events.\nUsers tend to add root nodes such as File via a click or drag from the sidebar, while adding leaf nodes such as Data Table via extension from another widget.\nHow users add File to canvas.\n How users add Data Table to canvas.\n    The widget popularity contest goes to: Data Table! Rightfully so, one should always check their data with Data Table.\nWidget popularity visualization in Box Plot.\n52% of sessions tracked consisted of no widgets being added (the application just being opened and closed). While some people might really like watching the loading screen, most of these are likely due to the fact that usage is not tracked until the user explicitly opts in.\nEach bit of collected data comes at a cost to the privacy of the user. Care was put into minimizing the intrusiveness of data collection methods, while maximizing the usefulness of the collected data.\nInitially, widget addition events were planned to include a \u0026lsquo;time since application start\u0026rsquo; value, in order to be able to plot a user\u0026rsquo;s actions as a function of time. While this would be cool, it was ultimately decided that its usefulness is outweighed by the privacy cost to users.\nFor the keen, data is gathered per canvas session, in the following structure:\n  Date\n  Orange version\n  Operating system\n  Widget addition events, each entailing:\n Widget name Type of addition (Click, Drag, Search or Extend) (Other widget name), if type is Extend (Query), if type is Search or Extend    ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "Orange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.",
	"longExcerpt" :  "In the past few months, Orange has been getting smarter and sleeker.\nSince version 3.15.0, Orange remembers which distinct widgets users like to connect, adjusting the sorting on the widget search menu accordingly. Additionally, there is a new look for the Edit Links window coming soon.\nOrange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.\nWord cloud of widget popularity in Orange." ,
	"author" : "IRGOLIC",
	"summary" : "In the past few months, Orange has been getting smarter and sleeker.\nSince version 3.15.0, Orange remembers which distinct widgets users like to connect, adjusting the sorting on the widget search menu accordingly. Additionally, there is a new look for the Edit Links window coming soon.\nOrange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.\nWord cloud of widget popularity in Orange.",
	"date" : "Nov 22, 2018"
}

    
    , {
    "uri": "/blog/2018/11/06/data-mining-for-anthropologists/",
	"title": "Data Mining for Anthropologists?",
	"categories": ["education", "text mining", "workshop"],
	"description": "",
	"content": "This weekend we were in Lisbon, Portugal, at the Why the World Needs Anthropologists conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry. I was there to hold a workshop on Data Ethnography, an approach that tries to combine methods from data science and anthropology into a fruitful interdisciplinary mix!\nData Ethnography workshop at this year\u0026rsquo;s Why the World Needs Anthropologists conference.\nData ethnography is a novel methodological approach that tries to view social phenomena from two different points of view - qualitative and quantitative. The quantitative approach is using data mining and machine learning methods on anthropological data (say from sensors, wearables, social media, online fora, field notes and so on) trying to find interesting patterns and novel information. The qualitative approach uses ethnography to substantiate the analytical findings with context, motivations, values, and other external data to provide a complete account of the studied phenomenon.\nAt the workshop, I presented a couple of approaches I use in my own research, namely text mining, clustering, visualization of patterns, image analytics, and predictive modeling. Data ethnography can be used, not only in its native field of computational anthropology, but also in museology, digital anthropology, medical anthropology, and folkloristics (the list is probably not exhaustive). There are so many options just waiting for the researchers to dig in!\nRelated: Text Analysis Workshop at Digital Humanities 2017\nHowever, having data- and tech-savvy anthropologists does not only benefit the research, but opens a platform for discussing the ethics of data science, human relationships with technology, and overcoming model bias. Hopefully, the workshop inspired some of the participants to join me on a journey through the amazing expanses of data science.\nTo get you inspired, here are two contributions that present some option for computational anthropological research: Data Mining Workspace Sensors: A New Approach to Anthropology and Power of Algorithms for Cultural Heritage Classification: The Case of Slovenian Hayracks.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "This weekend we were in Lisbon, Portugal, at the [Why the World Needs Anthropologists](https://www.applied-anthropology.com/) conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry.",
	"longExcerpt" :  "This weekend we were in Lisbon, Portugal, at the Why the World Needs Anthropologists conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry. I was there to hold a workshop on Data Ethnography, an approach that tries to combine methods from data science and anthropology into a fruitful interdisciplinary mix!\nData Ethnography workshop at this year\u0026rsquo;s Why the World Needs Anthropologists conference." ,
	"author" : "AJDA",
	"summary" : "This weekend we were in Lisbon, Portugal, at the Why the World Needs Anthropologists conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry. I was there to hold a workshop on Data Ethnography, an approach that tries to combine methods from data science and anthropology into a fruitful interdisciplinary mix!\nData Ethnography workshop at this year\u0026rsquo;s Why the World Needs Anthropologists conference.",
	"date" : "Nov 6, 2018"
}

    
    , {
    "uri": "/blog/2018/10/05/orange-now-speaks-50-languages/",
	"title": "Orange Now Speaks 50 Languages",
	"categories": ["preprocessing", "text mining"],
	"description": "",
	"content": "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian\u0026hellip; Language support was most evident in the list of stopwords, normalization and POS tagging.\nRelated: Text Workshops in Ljubljana\nStopwords come from NLTK library, so we can only offer whatever is available there. However, TF-IDF already implicitly considers stopwords, so the functionality is already implemented. For POS tagging, we would rely on Stanford POS tagger, that already has pre-trained models available.\nThe main issue was with normalization. While English can do without lemmatization and stemming for simple tasks, morphologically rich languages, such as Slovenian, perform poorly on un-normalized tokens. Cases and declensions present a problem for natural language processing, so we wanted to provide a tool for normalization in many different languages. Luckily, we found UDPipe, a Czech initiative that offers trained lemmatization models for 50 languages. UDPipe is actually a preprocessing pipeline and we are already thinking about how to bring all of its functionality to Orange, but let us talk a bit about the recent improvements for normalization.\nLet us load a simple corpus from Corpus widget, say grimm-tales-selected.tab that contain 44 tales from the Grimm Brothers. Now, pass them through Preprocess Text and keep just the defaults, namely lowercase transformation, tokenization by words, and removal of stopwords. Here we see that we have came as quite a frequent word and come as a bit less frequent. But semantically, they are the same word from the verb to come. Shouldn\u0026rsquo;t we consider them as one word?\nWe can. This is what normalization does - it transforms all words into their lemmas or basic grammatical form. Came and come will become come, sons and son will become son, pretty and prettier will become pretty. This will result in less tokens that capture the text better, semantically speaking.\nWe can see that came became come with 435 counts. Went became go. Said became say. And so on. As we said, this doesn\u0026rsquo;t work only on verbs, but on all word forms.\nOne thing to note here. UDPipe has an internal tokenizer, that works with sentences instead of tokens. You can enable it by selecting UDPipe tokenizer option. What is the difference? A quicker version would be to tokenize all the words and just look up their lemma. But sometimes this can be wrong. Consider the sentence:\nI am wearing a tie to work.\nNow the word tie is obviously a piece of clothing, which is indicated by the word wearing before it. But tie alone can also be the verb to tie. So the UDPipe tokenizer will consider the entire sentence and correctly lemmatize this word, while lemmatization on regular tokens might not. While UDPipe works better, it is also slower, so you might want to work with regular tokenization to speed up the analysis.\nIn Preprocess Text, you turn on the Normalization button on the right, then select UDPipe Lemmatizer and select the language you wish to use. Finally, if you wish to go with the better albeit slower UDPipe tokenizer, tick the UDPipe tokenizer box.\nFinally, UDPipe does not remove punctuation, so you might end up with words like rose. and away., with the full stop at the end. This you can fix with using regular tokenization and also by select the Regex option in Filtering, which will remove pure punctuation.\nFinal workflow, where we compared the results of no normalization and UDPipe normalization in a word cloud.\nThis is it. UDPipe contains lemmatization models for 50 languages and only when you click on a particular language in the Language option, will the resource be loaded, so your computer won\u0026rsquo;t be flooded with models for languages you won\u0026rsquo;t ever use. The installation of UDPipe could also be a little tricky, but after some initial obstacles, we have managed to prepare packages for both pip (OSX and Linux) and conda (Windows).\nWe hope you enjoy the new possibilities of a freshly multilingual Orange!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian... ",
	"longExcerpt" :  "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian\u0026hellip; Language support was most evident in the list of stopwords, normalization and POS tagging.\nRelated: Text Workshops in Ljubljana\nStopwords come from NLTK library, so we can only offer whatever is available there." ,
	"author" : "AJDA",
	"summary" : "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian\u0026hellip; Language support was most evident in the list of stopwords, normalization and POS tagging.\nRelated: Text Workshops in Ljubljana\nStopwords come from NLTK library, so we can only offer whatever is available there.",
	"date" : "Oct 5, 2018"
}

    
    , {
    "uri": "/blog/2018/09/21/orange-in-space/",
	"title": "Orange in Space",
	"categories": ["addons", "infraorange", "infrared spectra", "python", "spectroscopy"],
	"description": "",
	"content": "Did you know that Orange has already been to space? Rosario Brunetto (IAS-Orsay, France) has been working on the analysis of infrared images of asteroid Ryugu as a member of the JAXA Hayabusa2 team. The Hayabusa2 asteroid sample-return mission aims to retrieve data and samples from the near-Earth Ryugu asteroid and analyze its composition. Hayabusa2 arrived at Ryugu on June 27 and while the spacecraft will return to Earth with a sample only in late 2020, the mission already started collecting and sending back the data. And of course, a part of the analysis of Hayabusa\u0026rsquo;s space data has been done in Orange!\nAn image of the asteroid Ryugu acquired by the Hayabusa2 (©JAXA).\nWithin the Hayabusa2 project, near-infrared spectral data will be collected in three series: the first part is the macro data from remote sensing measurements that are being collected at different altitudes from the asteroid by the Japanese spectrometer NIRS3 (©JAXA). The second part is surface infrared imaging at the micron scale that will soon be performed (October 2018) by the French MicrOmega instrument on the lander MASCOT (DLR-CNES). The third part are the samples that will be analyzed upon return. Among the techniques that will be used in different laboratories around the world in 2021 to analyze the returned samples are the hyperspectral imaging and micro-tomography with an infrared imaging FPA microscope, that will be performed by the IAS team at SMIS-SOLEIL. This means the data will contain satellite spectral images as well as microscope measurements.\nDr. Brunetto is currently working with the first part of the data, namely the macro hyperspectral images of the asteroid. Several tens of thousands of spectra over 70 spectral channels have already been acquired. The main goal of this initial exploration was to constrain the surface composition.\nOnce the data was preprocessed and cleaned in Python, separate surface regions were extracted in Orange with k-Means and PCA and plotted with the HyperSpectra widget, which comes as a part of the Spectroscopy package. So why was Orange chosen over other tools? Dr. Brunetto says Orange is an easy and friendly tool for complicated things, such as exploring the compositional diversity of the asteroid at the different scales. There are many clustering techniques he can use in Orange and he likes how he can interactively change the number of clusters and the changes immediately show in the plot. This enables the researchers to determine the level of granularity of the analysis, while they can also immediately inspect how each cluster looks like in a hyperspectra plot.\nMoreover, one can quickly test methods and visualize the effects and at the same time have a good overview of the workflow. Workflows can also be reused once the new data comes in or, if the pipeline is standard, used on a completely different data set!\nA simple workflow for the analysis of spectral data. 😁 A great thing about Orange is that you can label parts of the workflow and explore a different aspect of the data in each branch!\nWe would of course love to show you the results of the asteroid analysis, but as the project is still ongoing, the data is not yet available to the public. Instead, we asked Zélia Dionnet, dr. Brunetto\u0026rsquo;s PhD student, to share the results of her work on the organic and mineralogic heterogeneity of the Paris meteorite, which were already published.\nShe analyzed the composition of the Paris meteorite, which was discovered in 2008 in a statue. The story of how the meteorite was found is quite interesting in itself, but we wanted to know more on how the sample was analyzed in Orange. Dionnet had a slightly larger data set, with 16,000 spectra and 1600 wavenumbers. Just like dr. Brunetto, she used k-Means to discover interesting regions in the sample and Hyperspectra widget to plot the results.\nk-Means clusters plotted in the HyperSpectra widget.\nAt the top, you can see a 2D map of the meteorite sample showing the distribution of the clusters that were identified with k-Means. At the bottom, you see cluster averages for the spectra. The green region is the most interesting one and it shows crystalline minerals, which formed billions of years ago as the hydrothermal processes in the asteroid parent body of the meteorite turned amorphous silicates into phyllosilicates. The purple, on the contrary, shows different micro-sized minerals.\nThis is how to easily identify the compositional structure of samples with just a couple of widgets. Orange seems to love going to space and can\u0026rsquo;t wait to get its hands dirty with more astro-data!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Did you know that Orange has already been to space? Rosario Brunetto (IAS-Orsay, France) has been working on the analysis of infrared images of asteroid Ryugu as a member of the JAXA Hayabusa2 team. The Hayabusa2 asteroid sample-return mission aims to retrieve data and samples from the near-Earth Ryugu asteroid and analyze its composition. Hayabusa2 arrived at Ryugu on June 27 and while the spacecraft will return to Earth with a sample only in late 2020, the mission already started collecting and sending back the data." ,
	"author" : "AJDA",
	"summary" : "Did you know that Orange has already been to space? Rosario Brunetto (IAS-Orsay, France) has been working on the analysis of infrared images of asteroid Ryugu as a member of the JAXA Hayabusa2 team. The Hayabusa2 asteroid sample-return mission aims to retrieve data and samples from the near-Earth Ryugu asteroid and analyze its composition. Hayabusa2 arrived at Ryugu on June 27 and while the spacecraft will return to Earth with a sample only in late 2020, the mission already started collecting and sending back the data.",
	"date" : "Sep 21, 2018"
}

    
    , {
    "uri": "/blog/2018/09/11/text-workshops-in-ljubljana/",
	"title": "Text Workshops in Ljubljana",
	"categories": ["addons", "text mining", "update", "workshop"],
	"description": "",
	"content": "In the past month, we had two workshops that focused on text mining. The first one, Faksi v praksi, was organized by the University of Ljubljana Career Centers, where high school students learned about what we do at the Faculty of Computer and Information Science. We taught them what text mining is and how to group a collection of documents in Orange. The second one took on a more serious note, as the public sector employees joined us for the third set of workshops from the Ministry of Public Affairs. This time, we did not only cluster documents, but also built predictive models, explored predictions in nomogram, plotted documents on a map and discovered how to find the emotion in a tweet.\nThese workshops gave us a lot of incentive to improve the Text add-on. We really wanted to support more languages and add extra functionalities to widgets. In the upcoming week, we will release the 0.5.0 version, which introduces support for Slovenian in Sentiment Analysis widget, adds concordance output option to Concordances and, most importantly, implements UDPipe lemmatization, which means Orange will now support about 50 languages! Well, at least for normalization. 😇\nToday, we will briefly introduce sentiment analysis for Slovenian. We have added the KKS 1.001 opinion corpus of Slovene web commentaries, which is a part of the CLARIN infrastructure. You can access it in the Corpus widget. Go to Browse documentation corpora and look for slo-opinion-corpus.tab. Let\u0026rsquo;s have a quick view in a Corpus Viewer.\nThe data comes from comment sections of Slovenian online media and contains a fairly expressive language. Let us observe, whether a post is negative or positive. We will use Sentiment Analysis widget and select the Liu Hu method for Slovenian. This is a dictionary based method, where the algorithm sums the positive words and subtracts the sum of negative words. This gives a final score of the post.\nWe will have to adjust the attributes for a nicer view in a Select Columns widget. Remove all attributes other than sentiment.\nFinally, we can observe the results in a Heat Map. The blue lines are the negative posts, while the yellow ones are positive. Let us select the most positive tweets and see, what they are about.\nLooks like Slovenians are happy, when petrol gets cheaper and sports(wo)men are winning. We can relate.\nOf course, there are some drawbacks of lexicon-based methods. Namely, they don\u0026rsquo;t work well with phrases, they often don\u0026rsquo;t consider modern language (see \u0026lsquo;Jupiiiiiii\u0026rsquo; or \u0026lsquo;Hooooooraaaaay!\u0026rsquo;, where the more the letters, the more expressive the word is) and they fail with sarcasm. Nevertheless, even such crude methods give us a nice glimpse into the corpus and enable us to extract interesting documents.\nStay tuned for the information on the release date and the upcoming post on UDPipe infrastructure!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "In the past month, we had two workshops that focused on text mining. The first one, Faksi v praksi, was organized by the University of Ljubljana Career Centers, where high school students learned about what we do at the Faculty of Computer and Information Science. We taught them what text mining is and how to group a collection of documents in Orange. The second one took on a more serious note, as the public sector employees joined us for the third set of workshops from the Ministry of Public Affairs." ,
	"author" : "AJDA",
	"summary" : "In the past month, we had two workshops that focused on text mining. The first one, Faksi v praksi, was organized by the University of Ljubljana Career Centers, where high school students learned about what we do at the Faculty of Computer and Information Science. We taught them what text mining is and how to group a collection of documents in Orange. The second one took on a more serious note, as the public sector employees joined us for the third set of workshops from the Ministry of Public Affairs.",
	"date" : "Sep 11, 2018"
}

    
    , {
    "uri": "/blog/2018/08/27/explaining-kickstarter-success/",
	"title": "Explaining Kickstarter Success",
	"categories": ["addons", "orange3", "prediction", "widget"],
	"description": "",
	"content": "On Kickstarter most app ideas don\u0026rsquo;t get funded. But why is that? When we are looking for possible explanations, it is easy to ascribe the failure to the type of the idea.\nBut what about those rare cases, where an app idea gets funded? Can we figure out why a particular idea succeeded? Our new widget Explain Predictions can do just that - explain why they will succeed. Or at least, explain why the classifier thinks they will.\nFirst, let us load the Kickstarter data from the Datasets widget and inspect it in a Data Table.\nSelect the data instance you wish to explore in a Data Table.\nNow, let\u0026rsquo;s see why the app Create Games \u0026amp; Apps Without Any Coding got funded.\nExplain Predictions needs 3 inputs. Our data set, a classifier and a data sample that we wish to inspect. Connect File widget with Explain Predictions. Then add the classifier, say, Logistic Regression. Finally, select Create Games \u0026amp; Apps Without Any Coding in the Data Table and connect it to the widget.\nExplain Predictions needs three inputs.\nThe highest ranking attributes are those that contributed the most (high Score value). The fact that there were 11 pledge levels, 13 images, many connections to other projects and the length of the project description - all of these attributes add something positive to the funding. On the other side, we see how the duration of the project, description length, maximal pledge tiers and the type of the idea work against the decision to fund the project. Lastly, not having a Facebook page or a video amounts to almost nothing in the making of the final prediction.\nHigh score means the attribute contributed positively to the the final decision (Funded: yes), while low scores contributed negatively.\nWhen explaining the decision of the classifier, we look at the values of the attributes for our sample and how they interact. We do that by approximating Shapely value, since calculating it exactly would sometimes take more then a lifetime. That means customized explanations for every individual case, while treating classifier like a black box. You could do the same for any model the Orange offers, including Neural Networks!\nAnd there you have it, an easy way to know what makes your Kickstarter campaign succeed, cell be classified as healthy, or a bank loan approved.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "On Kickstarter most app ideas don\u0026rsquo;t get funded. But why is that? When we are looking for possible explanations, it is easy to ascribe the failure to the type of the idea.\nBut what about those rare cases, where an app idea gets funded? Can we figure out why a particular idea succeeded? Our new widget Explain Predictions can do just that - explain why they will succeed. Or at least, explain why the classifier thinks they will." ,
	"author" : "ANDREJA",
	"summary" : "On Kickstarter most app ideas don\u0026rsquo;t get funded. But why is that? When we are looking for possible explanations, it is easy to ascribe the failure to the type of the idea.\nBut what about those rare cases, where an app idea gets funded? Can we figure out why a particular idea succeeded? Our new widget Explain Predictions can do just that - explain why they will succeed. Or at least, explain why the classifier thinks they will.",
	"date" : "Aug 27, 2018"
}

    
    , {
    "uri": "/blog/2018/07/17/data-mining-and-machine-learning-for-economists/",
	"title": "Data Mining and Machine Learning for Economists",
	"categories": ["addons", "clustering", "education", "geo", "workshop"],
	"description": "",
	"content": "Last week Blaž, Marko and I held a week long introductory Data Mining and Machine Learning course at the Ljubljana Doctoral Summer School 2018. We got a room full of dedicated students and we embarked on a journey through standard and advanced machine learning techniques, all presented of course in Orange. We have covered a wide array of topics, from different clustering techniques (hierarchical clustering, k-means) to predictive models (logistic regression, naive Bayes, decision trees, random forests), regression and regularization, projections, text mining and image analytics.\nRelated: Data Mining for Business and Public Administration\nDefinitely the biggest crowd-pleaser was the Geo add-on in combination with the HDI data set. First, we got the HDI data from Datasets. A quick glimpse into a data table to check the output. We have information on some key performance indicators gathered by the United Nations for 188 countries. Now we would like to know which countries are similar based on the reported indicators. We will use Distances with Euclidean distance and use Ward linkage in Hierarchical Clustering.\nIn Datasets widget we have selected the HDI data set.\nThe HDI data set contains information on 188 countries, which are described with 66 features. The data set can be used for regression, but we will perform clustering to discover countries, similar by the proposed parameters.\nWe got our results in a dendrogram. Interestingly, the United States seems similar to Cuba. Let us select this cluster and inspect what the most significant feature for this cluster. We will use the Data output of Hierarchical Clustering which append a column indicating whether the data instances was selected or not. Then we will use Box Plot, group by Selected and check Order by relevance. It seems like these countries have the longest life expectancy at age 59. Go ahead and inspect other clusters by yourself!\nSelect an interesting cluster in Hierarchical Clustering.\nAnd inspect the results in a box plot. Seems like the selected cluster stands out from the other countries by high life expectancy.\nOf course, when we are talking about countries one naturally wants to see them on a map! That is easy. We will use the Geo add-on. First, we need to convert all the country names to geographical coordinates. We will do this with Geocoding, where we will encode column Country to latitude and longitude. Remember to use the same output as before, that is Data to Data.\n \\\nUse Encode to convert a column with region identifiers (in our case Country) to latitude/longitude pairs.\nNow, let us display these countries on a map with Choropleth widget. Beautiful. It is so easy to explore country data, when you see it on a map. You can try coloring also by HDI or any other feature.\nChoropleth shows us which countries were in the selected cluster (red). We used Selected as attribute and colored by Mode.\nThe final workflow:\nWe always try to keep our workshops fresh and interesting and visualizations are the best way to achieve this. Till the next workshop!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Last week Blaž, Marko and I held a week long introductory Data Mining and Machine Learning course at the Ljubljana Doctoral Summer School 2018. We got a room full of dedicated students and we embarked on a journey through standard and advanced machine learning techniques, all presented of course in Orange. We have covered a wide array of topics, from different clustering techniques (hierarchical clustering, k-means) to predictive models (logistic regression, naive Bayes, decision trees, random forests), regression and regularization, projections, text mining and image analytics." ,
	"author" : "AJDA",
	"summary" : "Last week Blaž, Marko and I held a week long introductory Data Mining and Machine Learning course at the Ljubljana Doctoral Summer School 2018. We got a room full of dedicated students and we embarked on a journey through standard and advanced machine learning techniques, all presented of course in Orange. We have covered a wide array of topics, from different clustering techniques (hierarchical clustering, k-means) to predictive models (logistic regression, naive Bayes, decision trees, random forests), regression and regularization, projections, text mining and image analytics.",
	"date" : "Jul 17, 2018"
}

    
    , {
    "uri": "/blog/2018/06/21/girls-go-data-mining/",
	"title": "Girls Go Data Mining",
	"categories": ["clustering", "education", "interactive data visualization", "workshop"],
	"description": "",
	"content": "This week we held our first Girls Go Data Mining workshop. The workshop brought together curious women and intuitively introduced them to essential data mining and machine learning concepts. Of course, we used Orange to explore visualizations, build predictive models, perform clustering and dive into text analysis. The workshop was supported by NumFocus through their small development grant initiative and we hope to repeat it next year with even more ladies attending!\nRelated: Text Analysis for Social Scientists\nIn two days, we covered many topics. On day one, we got to know Orange and the concept of visual programming, where the user construct analytical workflow by stacking visual components. Then we got to know several useful visualizations, such as box plot, scatter plot, distributions, and mosaic display, which give us an initial overview of the data and the potentially interesting patterns. Finally, we got our hands dirty with predictive modeling. We learnt about decision trees, logistic regression, and naive Bayes classifiers, and observed the models in tree viewer and nomogram. It is great having interpretable models and we had great fun exploring what is in the model!\nOn the second day, we tried to uncover groups in our data with clustering. First, we tried hierarchical clustering and explored the discovered clusters with box plot. Then we also tried k-means and learnt, why this method is better than hierarchical clustering. In the final part, we talked about the methods for text mining, how to do preprocessing, construct a bag of words and perform the machine learning on corpora. We used both clustering and classification and tried to find interesting information about Grimm tales.\nOne of our workflows, where we explored the data in many different ways, including inspecting misclassifications in a scatter plot!\nOne thing that always comes up as really useful in our workshops is Orange\u0026rsquo;s ability to output different types of data. For example, in Hierarchical Clustering, we can select the similarity cutoff at the top and output clusters. Our data table will have an additional column Cluster, with cluster labels for each data instance.\nHierarchial Clustering outputs data with an additional Cluster column.\nWe can explore clusters by connecting a Box Plot to Hierarchical Clustering, selecting Cluster in Subgroups and using Order by relevance option. This sorts the variables in Box Plot by how well they separate between clusters or, in other words, what is typical of each cluster.\nWe have selected Cluster in Subgroups section and ticked \u0026lsquo;Order by relevance\u0026rsquo; to sort the variables. Variables at the top are the most interesting ones. Looks like giving milk is an exclusive property of cluster C1.\nWe used zoo.tab and made the cutoff at three clusters. It looks like the first cluster gives milk. Could these be a cluster of mammals?\nWe said giving milk is a property of cluster C1. By selecting type as our variable, we can see that C1 is a cluster of mammals.\nIndeed it is!\nAnother option is to select a specific cluster in the dendrogram. Then, we have to rewire the connection between Hierarchical Clustering and Box Plot by setting it to Data. Data option will output the entire data set, with an extra column showing whether the data instance was selected or not. In our case, there would be a Yes if the instance is in the selected cluster and No if it is not.\nTo rewire the connection, double-click on it and drag a line from Data to Data.\nWe have selected one cluster in the dendrogram, rewired the connection to transmit Data (instead of Selected Data) and observed the results in a Data Table. We see an additional Selected column, which shows whether a data instance was selected in the visualization or not.\nThen we can use Box Plot to observe what is particular for our selected cluster.\nIn this Box Plot we have used Selected in the Subgroups section and kept \u0026lsquo;Order by relevance\u0026rsquo; on. The suggested distinctive feature of our selected cluster is having feathers.\nIt looks like animals from our selected cluster have feathers. Probably, this is a cluster of birds. We can check this with the same procedure as above.\nIn summary, most Orange visualizations have two outputs - Selected Data and Data. Selected Data will output a subset of data instances selected in the visualization (or selected clusters in the case of hierarchical clustering), while Data will output the entire data table with a column defining whether a data instance was selected or not. This is very useful if we want to inspect what is typical of an interesting group in our data, inspect clusters or even manually define groups.\nOverall, this was another interesting workshop and we hope to continue our fruitful partnership with NumFocus and keep offering free educational events for beginners and experts alike!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This week we held our first Girls Go Data Mining workshop. The workshop brought together curious women and intuitively introduced them to essential data mining and machine learning concepts. Of course, we used Orange to explore visualizations, build predictive models, perform clustering and dive into text analysis. The workshop was supported by NumFocus through their small development grant initiative and we hope to repeat it next year with even more ladies attending!" ,
	"author" : "AJDA",
	"summary" : "This week we held our first Girls Go Data Mining workshop. The workshop brought together curious women and intuitively introduced them to essential data mining and machine learning concepts. Of course, we used Orange to explore visualizations, build predictive models, perform clustering and dive into text analysis. The workshop was supported by NumFocus through their small development grant initiative and we hope to repeat it next year with even more ladies attending!",
	"date" : "Jun 21, 2018"
}

    
    , {
    "uri": "/blog/2018/06/12/from-surveys-to-orange/",
	"title": "From Surveys to Orange",
	"categories": ["data", "dataloading", "orange3", "workshop"],
	"description": "",
	"content": "Today we have finished a series of workshops for the Ministry of Public Affairs. This was a year-long cooperation and we had many students asking many different questions. There was however one that we talked about a lot. If I have a survey, how do I get it into Orange?\nRelated: Analyzing Surveys\nWe are using EnKlik Anketa service, which is a great Slovenian product offering a wide array of options for the creation of surveys. We have created one such simple survey to use as a test. I am now inside EnKlik Anketa online service and I can see my survey has been successfully filled out.\nNow I have to create a public link to my survey in order to access the data in Orange. I have to click on an icon in the top right part and select \u0026lsquo;Public link\u0026rsquo;.\nA new window opens, where I select \u0026lsquo;Add new public link\u0026rsquo;. This will generate a public connection to my survey results. But be careful, the type of the connection needs to be Data, not Analysis! Orange can\u0026rsquo;t read already analyzed data, it needs raw data from Data pane.\nNow, all I have to do is open Orange, place EnKlik Anketa widget from the Prototypes add-on onto the canvas, enter the public link into the \u0026lsquo;Public link URL\u0026rsquo; fields and press Enter. If your data has loaded successfully, the widget will display available variables and information in the Info pane.\nFrom here on you can continue your analysis just like you would with any other data source!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Today we have finished a series of workshops for the Ministry of Public Affairs. This was a year-long cooperation and we had many students asking many different questions. There was however one that we talked about a lot. If I have a survey, how do I get it into Orange?\nRelated: Analyzing Surveys\nWe are using EnKlik Anketa service, which is a great Slovenian product offering a wide array of options for the creation of surveys." ,
	"author" : "AJDA",
	"summary" : "Today we have finished a series of workshops for the Ministry of Public Affairs. This was a year-long cooperation and we had many students asking many different questions. There was however one that we talked about a lot. If I have a survey, how do I get it into Orange?\nRelated: Analyzing Surveys\nWe are using EnKlik Anketa service, which is a great Slovenian product offering a wide array of options for the creation of surveys.",
	"date" : "Jun 12, 2018"
}

    
    , {
    "uri": "/blog/2018/05/30/spectroscopy-workshop-at-biospec-and-how-to-merge-data/",
	"title": "Spectroscopy Workshop at BioSpec and How to Merge Data",
	"categories": ["addons", "bioinformatics", "data", "education", "spectroscopy", "workshop"],
	"description": "",
	"content": "Last week Marko and I visited the land of the midnight sun - Norway! We held a two-day workshop on spectroscopy data analysis in Orange at the Norwegian University of Life Sciences. The students from BioSpec lab were yet again incredible and we really dug deep into Orange.\nRelated: Orange with Spectroscopy Add-on\nA class full of dedicated scientists.\nOne thing we did was see how to join data from two different sources. It would often happen that you have measurements in one file and the labels in the other. Or in our case, we wanted to add images to our zoo.tab data. First, find the zoo.tab in the File widget under Browse documentation datasets. Observe the data in the Data Table.\nOriginal zoo data set.\nThis data contains 101 animal described with 16 different features (hair, aquatic, eggs, etc.), a name and a type. Now we will manually create the second table in Excel. The first column will contain the names of the animals as they appear in the original file. The second column will contain links to images of animals. Open your favorite browser and find a couple of images corresponding to selected animals. Then add links to images below the image column. Just like that:\nExtra data that we want to add to the original data.\nRemember, you need a three-row header to define the column that contains images. Under the image column add string in the second and type=image in the third row. This will tell Orange where to look for images. Now, we can check our animals in Image Viewer.\nA quick glance at an Image Viewer will tell us whether our images got loaded correctly.\nFinally, it is time to bring in the images to the existing zoo data set. Connect the original File to Merge Data. Then add the second file with animal images to Merge Data. The default merging method will take the first data input as original data and the second data as extra data. The column to match by is defined in the widget. In our case, it is the name column. This means Orange will look at the first name column and find matching instances in the second name column.\nA quick look at the merged data shows us an additional image column that we appended to the original file.\nMerged data with a new column.\nThis is the final workflow. Merge Data now contains a single data table on the output and you can continue your analysis from there.\nFind out more about spectroscopy for Orange on our YouTube channel or contribute to the project on Github.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Last week Marko and I visited the land of the midnight sun - Norway! We held a two-day workshop on spectroscopy data analysis in Orange at the Norwegian University of Life Sciences. The students from BioSpec lab were yet again incredible and we really dug deep into Orange.\nRelated: Orange with Spectroscopy Add-on\nA class full of dedicated scientists.\nOne thing we did was see how to join data from two different sources." ,
	"author" : "AJDA",
	"summary" : "Last week Marko and I visited the land of the midnight sun - Norway! We held a two-day workshop on spectroscopy data analysis in Orange at the Norwegian University of Life Sciences. The students from BioSpec lab were yet again incredible and we really dug deep into Orange.\nRelated: Orange with Spectroscopy Add-on\nA class full of dedicated scientists.\nOne thing we did was see how to join data from two different sources.",
	"date" : "May 30, 2018"
}

    
    , {
    "uri": "/blog/2018/05/15/python-script-managing-data-on-the-fly/",
	"title": "Python Script: Managing Data on the Fly",
	"categories": ["orange3", "python", "scripting"],
	"description": "",
	"content": "Python Script is this mysterious widget most people don\u0026rsquo;t know how to use, even those versed in Python. Python Script is the widget that supplements Orange functionalities with (almost) everything that Python can offer. And it\u0026rsquo;s time we unveil some of its functionalities with a simple example.\nExample: Batch Transform the Data There might be a time when you need to apply a function to all your attributes. Say you wish to log-transform their values, as it is common in gene expression data. In theory, you could do this with Feature Constructor, where you would log-transform every attribute individually. Sounds laborious? It\u0026rsquo;s because it is. Why else we have computers if not to reduce manual labor for certain tasks? Let\u0026rsquo;s do it the fast way - with Python Script.\nFirst, open File widget and load geo-gds360.tab from Browse documentation data sets. This data set has 9485 features, so imagine having to transform each feature individually.\nInstead, we will connect Python Script to File and use a simple script to apply the same transformation to all attributes.\n\u0026lt;code\u0026gt;import numpy as np from Orange.data import Table new_X = np.log(in_data.X) out_data = Table(in_data.domain, new_X, in_data.Y, in_data.metas) \u0026lt;/code\u0026gt;  This is really simple. Use in_data.X, which accesses all features in the data set, to transform the data with np.log (or any other numpy function). Set out_data to new_X and, voila, the transformed data is on the output. In a few lines we have instantly handled all 9485 features.\nYou can inspect the data before and after transformation in a Data Table widget.\nOriginal data.\nLog-transformed data.\nThis is it. Now we can do our standard analysis on the transformed data. Even better! We can save our script and use it in Python Script widget any time we want.\nFor your convenience we have created a repository of Orange scripts, so you can download and use it instantly!\nHave a more interesting example with Python Script? We\u0026rsquo;d love to hear about it!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Python Script is this mysterious widget most people don\u0026rsquo;t know how to use, even those versed in Python. Python Script is the widget that supplements Orange functionalities with (almost) everything that Python can offer. And it\u0026rsquo;s time we unveil some of its functionalities with a simple example.\nExample: Batch Transform the Data There might be a time when you need to apply a function to all your attributes. Say you wish to log-transform their values, as it is common in gene expression data." ,
	"author" : "AJDA",
	"summary" : "Python Script is this mysterious widget most people don\u0026rsquo;t know how to use, even those versed in Python. Python Script is the widget that supplements Orange functionalities with (almost) everything that Python can offer. And it\u0026rsquo;s time we unveil some of its functionalities with a simple example.\nExample: Batch Transform the Data There might be a time when you need to apply a function to all your attributes. Say you wish to log-transform their values, as it is common in gene expression data.",
	"date" : "May 15, 2018"
}

    
    , {
    "uri": "/blog/2018/05/09/clustering-of-monet-and-manet/",
	"title": "Clustering of Monet and Manet",
	"categories": ["addons", "image analytics", "orange3", "tutorial", "youtube"],
	"description": "",
	"content": "Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet\u0026rsquo;s masterpiece is indeed very similar to Manet\u0026rsquo;s! Use Image Analytics add-on and play with it. Here\u0026rsquo;s how:   ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet\u0026rsquo;s masterpiece is indeed very similar to Manet\u0026rsquo;s! Use Image Analytics add-on and play with it. Here\u0026rsquo;s how:   " ,
	"author" : "AJDA",
	"summary" : "Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet\u0026rsquo;s masterpiece is indeed very similar to Manet\u0026rsquo;s! Use Image Analytics add-on and play with it. Here\u0026rsquo;s how:   ",
	"date" : "May 9, 2018"
}

    
    , {
    "uri": "/blog/2018/05/03/data-mining-course-at-higher-school-of-economics-moscow/",
	"title": "Data Mining Course at Higher School of Economics, Moscow",
	"categories": ["analysis", "business intelligence", "classification", "education", "examples", "python", "scripting", "workshop"],
	"description": "",
	"content": "Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master\u0026rsquo;s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.\nIt was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best.\nOne of the things we did was compute minimum cost of misclassifications. The story goes like this. Sara is a doctor and has data on 303 patients with heart disease (Orange\u0026rsquo;s heart-disease.tab data set). She used some classifiers and now has to decide how many patients to send for further tests. Naive Bayes classifier, for example, returned probabilities of a patient being sick (column Naive Bayes 1). For each threshold in probabilites, she will compute how many false positives (patients declared sick when healthy) and how many false negatives (patients declared healthy when sick) a classifiers returns. Each mistake is associated with a cost. Now she wants to find out, how many patients to send for tests (what probability threshold to choose) so that her cost is the lowest.\nFirst, import all the libraries we will need:\nimport matplotlib.pyplot as plt import numpy as np from Orange.data import Table from Orange.classification import NaiveBayesLearner, TreeLearner from Orange.evaluation import CrossValidation  Then load heart disease data (and print a sample).\nheart = Table(\u0026quot;heart_disease\u0026quot;) print(heart[:5])  Now, train classifiers and select probabilities of Naive Bayes for a patient being sick.\nscores = CrossValidation(heart, [NaiveBayesLearner(), TreeLearner()]) #take probabilites of class 1 (sick) of NaiveBayesLearner p1 = scores.probabilities[0][:, 1] #take actual class values y = scores.actual #cost of false positive (patient classified as sick when healthy) fp_cost = 500 #cost of false negative (patient classified as healthy when sick) fn_cost = 800  Set counts, where we declare 0 patients being sick (threshold \u0026gt;1).\nfp = 0 #start with threshold above 1 (no one is sick) fn = np.sum(y)  For each threshold, compute the cost associated with each type of mistake.\nps = [] costs = [] #compute costs of classifying i patients as sick for i in np.argsort(p1)[::-1]: if y[i] == 0: fp += 1 else: fn -= 1 ps.append(p1[i]) costs.append(fp * fp_cost + fn * fn_cost)  In the end, we get a list of probability thresholds and associated costs. Now let us find the minimum cost and its probability of a patient being sick.\ncosts = np.array(costs) #find probability of a patient being sick at lowest cost print(ps[costs.argmin()])  This means the threshold that minimizes our cost for a given classifier is 0.620655. Sara would send all the patients with a probability of being sick higher or equal than 0.620655 for further tests.\nAt the end, we can also plot the cost to patients sent curve.\nfig, ax = plt.subplots() plt.plot(ps, costs) ax.set_xlabel('Patients sent') ax.set_ylabel('Cost')  You can download the IPython Notebook here: [download id=\u0026ldquo;2053\u0026rdquo;].\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master\u0026rsquo;s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.\nIt was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best." ,
	"author" : "AJDA",
	"summary" : "Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master\u0026rsquo;s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.\nIt was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best.",
	"date" : "May 3, 2018"
}

    
    , {
    "uri": "/blog/2018/04/23/installing-add-ons-works-again/",
	"title": "Installing Add-ons Works Again",
	"categories": ["addons", "download", "pypi", "release", "update"],
	"description": "",
	"content": "Dear Orange users,\nSome of you might have an issue installing add-ons with the following issue popping up:\nxmlrpc.client.Fault: \u0026lt;Fault -32601: 'server error; requested method not found'\u0026gt;\nThis is the result of the migration to a new infrastructure at PyPi, which provides the installation of add-ons. Our team has rallied to adjust the add-on installer so it works with the new and improved service.\nIn order to make the add-on installer work (again), please download the latest version of Orange (3.13.0).\nWe apologize for any inconvenience and wish you a fruitful data analysis in the future.\nYours truly,\nOrange team\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Dear Orange users,\nSome of you might have an issue installing add-ons with the following issue popping up:\nxmlrpc.client.Fault: \u0026lt;Fault -32601: 'server error; requested method not found'\u0026gt;\nThis is the result of the migration to a new infrastructure at PyPi, which provides the installation of add-ons. Our team has rallied to adjust the add-on installer so it works with the new and improved service.\nIn order to make the add-on installer work (again), please download the latest version of Orange (3." ,
	"author" : "AJDA",
	"summary" : "Dear Orange users,\nSome of you might have an issue installing add-ons with the following issue popping up:\nxmlrpc.client.Fault: \u0026lt;Fault -32601: 'server error; requested method not found'\u0026gt;\nThis is the result of the migration to a new infrastructure at PyPi, which provides the installation of add-ons. Our team has rallied to adjust the add-on installer so it works with the new and improved service.\nIn order to make the add-on installer work (again), please download the latest version of Orange (3.",
	"date" : "Apr 23, 2018"
}

    
    , {
    "uri": "/blog/2018/04/05/unfreezing-orange/",
	"title": "Unfreezing Orange",
	"categories": ["neuralnetwork", "orange3", "parallelization", "performance", "programming", "python", "qt"],
	"description": "",
	"content": "Have you ever tried Orange with data big enough that some widgets ran for more than a second? Then you have seen it: Orange froze. While the widget was processing, the interface would not respond to any inputs, and there was no way to stop that widget.\nNot all the widgets freeze, though! Some widgets, like Test \u0026amp; Score, k-Means, or Image Embedding, do not block. While they are working, we are free to build other parts of the workflow, and these widgets also show their progress. Some, like Image Embedding, which work with lots of images, even allow interruptions.\nWhy does Orange freeze? Most widgets process users' actions directly: after an event (click, pressed key, new input data) some code starts running: until it finishes, the interface can not respond to any new events. This is a reasonable approach for short tasks, such as making a selection in a Scatter Plot. But with longer tasks, such as building a Support Vector Model on big data, Orange gets unresponsive.\nTo make Orange responsive while it is processing, we need to start the task in a new thread. As programmers we have to consider the following:\n Starting the task. We have to make sure that other (older) tasks are not running. Showing results when the task has finished. Periodic communication between the task and the interface for status reports (progress bars) and task stopping.  Starting the task and showing the results are straightforward and well documented in a tutorial for writing widgets. Periodic communication with stopping is harder: it is completely task-dependent and can be either trivial, hard, or even impossible. Periodic communication is, in principle, unessential for responsiveness, but if we do not implement it, we will be unable to stop the running task and progress bars would not work either.\nTaking care of periodic communication was the hardest part of making the Neural Network widget responsive. It would have been easy, had we implemented neural networks ourselves. But we use the scikit-learn implementation, which does not expose an option to make additional function calls while fitting the network (we need to run code that communicates with the interface). We had to resort to a trick: we modified fitting so that a change to an attribute called n_iters_ called a function (see pull request). Not the cleanest solution, but it seems to work.\nFor now, only a few widgets work so that the interface remains responsive. We are still searching for the best way to make existing widgets behave nicely, but responsiveness is now one of our priorities.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Have you ever tried Orange with data big enough that some widgets ran for more than a second? Then you have seen it: Orange froze. While the widget was processing, the interface would not respond to any inputs, and there was no way to stop that widget.\nNot all the widgets freeze, though! Some widgets, like Test \u0026amp; Score, k-Means, or Image Embedding, do not block. While they are working, we are free to build other parts of the workflow, and these widgets also show their progress." ,
	"author" : "MARKO",
	"summary" : "Have you ever tried Orange with data big enough that some widgets ran for more than a second? Then you have seen it: Orange froze. While the widget was processing, the interface would not respond to any inputs, and there was no way to stop that widget.\nNot all the widgets freeze, though! Some widgets, like Test \u0026amp; Score, k-Means, or Image Embedding, do not block. While they are working, we are free to build other parts of the workflow, and these widgets also show their progress.",
	"date" : "Apr 5, 2018"
}

    
    , {
    "uri": "/blog/2018/03/28/orange-with-spectroscopy-add-on-workshop/",
	"title": "Orange with Spectroscopy Add-on Workshop",
	"categories": ["addons", "education", "examples", "infrared spectra", "workshop"],
	"description": "",
	"content": "We have just concluded our enhanced Introduction to Data Science workshop, which included several workflows for spectroscopy analysis. Spectroscopy add-on is intended for the analysis of spectral data and it is just as fun as our other add-ons (if not more!).\nWe will prove it with a simple classification workflow. First, install Spectroscopy add-on from Options - Add-ons menu in Orange. Restart Orange for the add-on to appear. Great, you are ready for some spectral analysis!\nUse Datasets widget and load Collagen spectroscopy data. This data contains cells measured with FTIR and annotated with the major chemical compound at the imaged part of a cell. A quick glance in a Data Table will give us an idea how the data looks like. Seems like a very standard spectral data set.\nCollagen data set from Datasets widget.\nNow we want to determine, whether we can classify cells by type based on their spectral profiles. First, connect Datasets to Test \u0026amp; Score. We will use 10-fold cross-validation to score the performance of our model. Next, we will add Logistic Regression to model the data. One final thing. Spectral data often needs some preprocessing. Let us perform a simple preprocessing step by applying Cut (keep) filter and retaining only the wave numbers between 1500 and 1800. When we connect it to Test \u0026amp; Score, we need to keep in mind to connect the Preprocessor output of Preprocess Spectra.\nPreprocessor that keeps a part of the spectra cut between 1500 and 1800. No data is shown here, since we are using only the preprocessing procedure as the input for Test \u0026amp; Score.\nLet us see how well our model performs. Not bad. A 0.99 AUC score. Seems like it is almost perfect. But is it really so?\n10-fold cross-validation on spectral data. Our AUC and CA scores are quite impressive.\nConfusion Matrix gives us a detailed picture. Our model fails almost exclusively on DNA cell type. Interesting.\nConfusion Matrix shows DNA is most often misclassified. By selecting the misclassified instances in the matrix, we can inspect why Logistic Regression couldn\u0026rsquo;t model these spectra\nWe will select the misclassified DNA cells and feed them to Spectra to inspect what went wrong. Instead of coloring by type, we will color by prediction from Logistic Regression. Can you find out why these spectra were classified incorrectly?\nMisclassified DNA spectra colored by the prediction made by Logistic Regression.\nThis is one of the simplest examples with spectral data. It is basically the same procedure as with standard data - data is fed as data, learner (LR) as learner and preprocessor as preprocessor directly to Test \u0026amp; Score to avoid overfitting. Play around with Spectroscopy add-on and let us know what you think! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have just concluded our enhanced Introduction to Data Science workshop, which included several workflows for spectroscopy analysis. Spectroscopy add-on is intended for the analysis of spectral data and it is just as fun as our other add-ons (if not more!).\nWe will prove it with a simple classification workflow. First, install Spectroscopy add-on from Options - Add-ons menu in Orange. Restart Orange for the add-on to appear. Great, you are ready for some spectral analysis!" ,
	"author" : "AJDA",
	"summary" : "We have just concluded our enhanced Introduction to Data Science workshop, which included several workflows for spectroscopy analysis. Spectroscopy add-on is intended for the analysis of spectral data and it is just as fun as our other add-ons (if not more!).\nWe will prove it with a simple classification workflow. First, install Spectroscopy add-on from Options - Add-ons menu in Orange. Restart Orange for the add-on to appear. Great, you are ready for some spectral analysis!",
	"date" : "Mar 28, 2018"
}

    
    , {
    "uri": "/blog/2018/03/05/single-cell-analytics-workshop-at-hhmi-janelia/",
	"title": "Single cell analytics workshop at HHMI | Janelia",
	"categories": ["orange3"],
	"description": "",
	"content": "HHMI | Janelia is one of the prettiest researcher campuses I have ever visited. Located in Ashburn, VA, about 20 minutes from Washington Dulles airport, it is conveniently located yet, in a way, secluded from the buzz of the capital. We adored the guest house with a view of the lake, tasty Janelia-style breakfast (hash-browns with two eggs and sausage, plus a bagel with cream cheese) in the on-campus pub, beautifully-designed interiors to foster collaborations and interactions, and late-evening discussions in the in-house pub.\nAll these thanks to the invitation of Andrew Lemire, a manager of a shared high-throughput genomics resource, and Dr. Vilas Menon, a mathematician specializing in quantitative genomics. With Andy and Vilas, we have been collaborating in the past few months on trying to devise a simple and intuitive tool for analysis of single-cell gene expression data. Single cell high-throughput technology is one of the latest approaches that allow us to see what is happening within a single cell, and it does that by simultaneously scanning through potentially thousands of cells. That generates loads of data, and apparently, we have been trying to fit Orange for single-cell data analysis task.\nNamely, in the past half a year, we have been perfecting an add-on for Orange with components for single-cell analysis. This endeavor became so vital that we have even designed a new installation of Orange, called scOrange. With everything still in prototype stage, we had enough courage to present the tool at Janelia, first through a seminar, and the next day within a five-hour lecture that I gave together with Martin Strazar, a PhD student and bioinformatics expert from my lab. Many labs are embarking on single cell technology at Janelia, and by the crowd that gathered at both events, it looks like that everyone was there.\nOrange, or rather, scOrange, worked as expected, and hands-on workshop was smooth, despite testing the software on some rather large data sets. Our Orange add-on for single-cell analytics is still in early stage of development, but already has some advanced features like biomarker discovery and tools for characterization of cell clusters that may help in revealing hidden relations between genes and phenotypes. Thanks to Andy and Vilas, Janelia proved an excellent proving ground for scOrange, and we are looking forward to our next hands-on single-cell analytics workshop in Houston.\nRelated: Hands-On Data Mining Course in Houston\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "HHMI | Janelia is one of the prettiest researcher campuses I have ever visited. Located in Ashburn, VA, about 20 minutes from Washington Dulles airport, it is conveniently located yet, in a way, secluded from the buzz of the capital. We adored the guest house with a view of the lake, tasty Janelia-style breakfast (hash-browns with two eggs and sausage, plus a bagel with cream cheese) in the on-campus pub, beautifully-designed interiors to foster collaborations and interactions, and late-evening discussions in the in-house pub." ,
	"author" : "BLAZ",
	"summary" : "HHMI | Janelia is one of the prettiest researcher campuses I have ever visited. Located in Ashburn, VA, about 20 minutes from Washington Dulles airport, it is conveniently located yet, in a way, secluded from the buzz of the capital. We adored the guest house with a view of the lake, tasty Janelia-style breakfast (hash-browns with two eggs and sausage, plus a bagel with cream cheese) in the on-campus pub, beautifully-designed interiors to foster collaborations and interactions, and late-evening discussions in the in-house pub.",
	"date" : "Mar 5, 2018"
}

    
    , {
    "uri": "/blog/2018/02/16/how-to-enable-sql-widget-in-orange/",
	"title": "How to enable SQL widget in Orange",
	"categories": ["data", "pypi", "sql"],
	"description": "",
	"content": "A lot of you have been interested in enabling SQL widget in Orange, especially regarding the installation of a psycopg backend that makes the widget actually work. This post will be slightly more technical, but I will try to keep it to a minimum. Scroll to the bottom for installation instructions.\nRelated: SQL for Orange\n Why won\u0026rsquo;t Orange recognize psycopg? The main issue for some people was that despite having installed the psycopg module in their console, the SQL widget still didn\u0026rsquo;t work. This is because Orange uses a separate virtual environment and most of you installed psycopg in the default (system) Python environment. For psycopg to be recognized in Orange, it needs to be installed in the same virtual environment, which is normally located in C:\\Users\\\u0026lt;usr\u0026gt;\\Anaconda3\\envs\\orange3 (on Windows). For the installation to work, you\u0026rsquo;d have to run it with the proper pip, namely:\nC:\\Users\\\u0026lt;usr\u0026gt;\\Anaconda3\\envs\\orange3\\Scripts\\pip.exe install psycopg2\n Installation instructions But there is a much easier way to do it. Head over to psycopg\u0026rsquo;s pip website and download the latest wheel for your platform. Py version has to be cp34 or higher (latest Orange from Anaconda comes with Python 3.6, so look for cp36).\nFor OSX, you would for example need: psycopg2-2.7.4-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\nFor 64-bit Windows: psycopg2-2.7.4-cp36-cp36m-win_amd64.whl\nAnd for Linux: psycopg2-2.7.4-cp36-cp36m-manylinux1_x86_64.whl\nThen open the add-on dialog in Orange (Options \u0026ndash;\u0026gt; Add-ons) and drag and drop the downloaded wheel into the add-on list. At the bottom, you will see psycopg2 with the tick next to it.\nClick OK to run the installation. Then re-start Orange and connect to your database with SQL widget. If you have any questions, drop them in the comment section!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "A lot of you have been interested in enabling SQL widget in Orange, especially regarding the installation of a psycopg backend that makes the widget actually work. This post will be slightly more technical, but I will try to keep it to a minimum. Scroll to the bottom for installation instructions.\nRelated: SQL for Orange\n Why won\u0026rsquo;t Orange recognize psycopg? The main issue for some people was that despite having installed the psycopg module in their console, the SQL widget still didn\u0026rsquo;t work." ,
	"author" : "AJDA",
	"summary" : "A lot of you have been interested in enabling SQL widget in Orange, especially regarding the installation of a psycopg backend that makes the widget actually work. This post will be slightly more technical, but I will try to keep it to a minimum. Scroll to the bottom for installation instructions.\nRelated: SQL for Orange\n Why won\u0026rsquo;t Orange recognize psycopg? The main issue for some people was that despite having installed the psycopg module in their console, the SQL widget still didn\u0026rsquo;t work.",
	"date" : "Feb 16, 2018"
}

    
    , {
    "uri": "/blog/2018/02/02/image-analytics-workshop-at-aiucd-2018/",
	"title": "Image Analytics Workshop at AIUCD 2018",
	"categories": ["addons", "analysis", "conference", "embedding", "images", "visualization", "workshop"],
	"description": "",
	"content": "This week, Primož and I flew to the south of Italy to hold a workshop on Image Analytics through Data Mining at AIUCD 2018 conference. The workshop was intended to familiarize digital humanities researchers with options that visual programming environments offer for image analysis.\nIn about 5 hours we discussed image embedding, clustering, finding closest neighbors and classification of images. While it is often a challenge to explain complex concepts in such a short time, it is much easier when working with Orange.\nRelated: Image Analytics: Clustering\nOne of the workflows we learned at the workshop was the one for finding the most similar image in a set of images. This is better explained with an example.\nWe had 15 paintings from different authors. Two of them were painted by Claude Monet, a famous French impressionist painter. Our task was, given a reference image of Monet, to find his other painting in a collection.\nA collection of images. It includes two Monet paintings.\nFirst, we loaded our data set with Import Images. Then we sent our images to Image Embedding. We selected Painters embedder since it was specifically trained to recognize authors of paintings.\nWe used Painters embedder here.\nOnce we have described our paintings with vectors (embeddings), we can compare them by similarity. To find the second Monet in a data set, we will have to compute the similarity of paintings and find the one most similar one to our reference painting.\nRelated: Video on image clustering\nLet us connect Image Embedding to Neighbors from Prototypes add-on. Neighbors widget is specifically intended to find a number of closest neighbors given a reference data point.\nWe will need to adjust the widget a bit. First, we will need cosine distance, since we will be comparing images by the content, not the magnitude of features. Next, we will tick off Exclude reference, in order to receive the reference image on the output. We do this just for visualization purposes. Finally, we set the number of neighbors to 2. Again, this is just for a nicer visualization, since we know there are only two Monet\u0026rsquo;s paintings in the data set.\nNeighbors was set to provide a nice visualization. Hence we ticked off Exclude references and set Neighbors to 2.\nThen we need to give Neighbors a reference image, for which we want to retrieve the neighbors. We do this by adding Data Table to Image Embedding, selecting one of Monet\u0026rsquo;s paintings in the spreadsheet and then connecting the Data Table to Neighbors. The widget will automatically consider the second input as a reference.\nMonet.jpg is our reference painting. We select it in Data Table.\nNow, all we need to do is to visualize the output. Connect Image Viewer to Neighbors and open it.\nVoila! The widget has indeed found the second Monet\u0026rsquo;s painting. So useful when you have thousands of images in your archive!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This week, Primož and I flew to the south of Italy to hold a workshop on Image Analytics through Data Mining at AIUCD 2018 conference. The workshop was intended to familiarize digital humanities researchers with options that visual programming environments offer for image analysis.\nIn about 5 hours we discussed image embedding, clustering, finding closest neighbors and classification of images. While it is often a challenge to explain complex concepts in such a short time, it is much easier when working with Orange." ,
	"author" : "AJDA",
	"summary" : "This week, Primož and I flew to the south of Italy to hold a workshop on Image Analytics through Data Mining at AIUCD 2018 conference. The workshop was intended to familiarize digital humanities researchers with options that visual programming environments offer for image analysis.\nIn about 5 hours we discussed image embedding, clustering, finding closest neighbors and classification of images. While it is often a challenge to explain complex concepts in such a short time, it is much easier when working with Orange.",
	"date" : "Feb 2, 2018"
}

    
    , {
    "uri": "/blog/2018/01/26/visualizing-multiple-variables-freeviz/",
	"title": "Visualizing multiple variables: FreeViz",
	"categories": ["analysis", "features", "interactive data visualization", "visualization"],
	"description": "",
	"content": "Scatter plots are great! But sometimes, we need to plot more than two variables to truly understand the data. How can we achieve this, knowing humans can only grasp up to three dimensions? With an optimization of linear projection, of course!\nOrange recently re-introduced FreeViz, an interactive visualization for plotting multiple variables on a 2-D plane.\nLet\u0026rsquo;s load zoo.tab data with File widget and connect FreeViz to it. Zoo data has 16 features describing animals of different types - mammals, amphibians, insects and so on. We would like to use FreeViz to show us informative features and create a visualization that separates well between animal types.\nFreeViz with initial, un-optimized plot.\nWe start with un-optimized projection, where data points are scattered around features axes. Once we click Optimize, we can observe optimization process in real-time and at the end see the optimized projection.\nFreeViz with optimized projection.\nThis projection is much more informative. Mammals are nicely grouped together within a pink cluster that is characterized by hair, milk, and toothed features. Conversely, birds are charaterized by eggs, feathers and airborne, while fish are aquatic. Results are as expected, which means optimization indeed found informative features for each class value.\nFreeViz with Show class density option.\nSince we are working with categorical class values, we can tick Show class density to color the plot by majority class values. We can also move anchors around to see how data points change in relation to a selected anchor.\nFinally, as in most Orange visualizations, we can select a subset of data points and explore them further. For example, let us observe which amphibians are characterized by being aquatic in a Data Table. A newt, a toad and two types of frogs, one venomous and one not.\nData exploration is always much easier with clever visualizations!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Scatter plots are great! But sometimes, we need to plot more than two variables to truly understand the data. How can we achieve this, knowing humans can only grasp up to three dimensions? With an optimization of linear projection, of course!\nOrange recently re-introduced FreeViz, an interactive visualization for plotting multiple variables on a 2-D plane.\nLet\u0026rsquo;s load zoo.tab data with File widget and connect FreeViz to it. Zoo data has 16 features describing animals of different types - mammals, amphibians, insects and so on." ,
	"author" : "AJDA",
	"summary" : "Scatter plots are great! But sometimes, we need to plot more than two variables to truly understand the data. How can we achieve this, knowing humans can only grasp up to three dimensions? With an optimization of linear projection, of course!\nOrange recently re-introduced FreeViz, an interactive visualization for plotting multiple variables on a 2-D plane.\nLet\u0026rsquo;s load zoo.tab data with File widget and connect FreeViz to it. Zoo data has 16 features describing animals of different types - mammals, amphibians, insects and so on.",
	"date" : "Jan 26, 2018"
}

    
    , {
    "uri": "/blog/2018/01/05/stack-everything/",
	"title": "Stack Everything!",
	"categories": ["classification", "examples", "widget"],
	"description": "",
	"content": "We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.\nStacking enables you to combine several trained models into one meta model and use it in Test\u0026amp;Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let\u0026rsquo;s see an example.\nWe start with something as complex as this. We used Paint Data to create a complex data set, where classes somewhat overlap. This is naturally an artificial example, but you can try the same on your own, real life data.\nWe used 4 classes and painted a complex, 2-dimensional data set.\nThen we add several kNN models with different parameters, say 5, 10 and 15 neighbors. We connect them to Test\u0026amp;Score and use cross validation to evaluate their performance. Not bad, but can we do even better?\nScores without staking, using only 3 different kNN classifiers.\nLet us try stacking. We will connect all three classifiers to the Stacking widget and use Logistic Regression as an aggregate, a method that aggregates the three models into a single meta model. Then we connect connect the stacked model into Test\u0026amp;Score and see whether our scores improved.\nScores with stacking. Stack reports on improved performance.\nAnd indeed they have. It might not be anything dramatic, but in real life, say medical context, even small improvements count. Now go and try the procedure on your own data. In Orange, this requires only a couple of minutes.\nFinal workflow with channel names. Notice that Logistic Regression is used as Aggregate, not a Learner.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.\nStacking enables you to combine several trained models into one meta model and use it in Test\u0026amp;Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let\u0026rsquo;s see an example." ,
	"author" : "AJDA",
	"summary" : "We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.\nStacking enables you to combine several trained models into one meta model and use it in Test\u0026amp;Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let\u0026rsquo;s see an example.",
	"date" : "Jan 5, 2018"
}

    
    , {
    "uri": "/blog/2017/12/23/speeding-up-network-visualization/",
	"title": "Speeding Up Network Visualization",
	"categories": ["addons", "network", "visualization"],
	"description": "",
	"content": "The Orange3 Network add-on contains a convenient Network Explorer widget for network visualization. Orange uses an iterative force-directed method (a variation of the Fruchterman-Reingold Algorithm) to layout the nodes on the 2D plane.\nThe goal of force-directed methods is to draw connected nodes close to each other as if the edges that connect the nodes were acting as springs. We also don\u0026rsquo;t want all nodes crowded in a single point, but would rather have them spaced evenly. This is achieved by simulating a repulsive force, which decreases with the distance between nodes.\nThere are two types of forces acting on each node:\n the attractive force towards connected adjacent nodes, the repulsive force that is directed away from all other nodes.  We could say that such network visualization as a whole is rather repulsive. Let\u0026rsquo;s take for example the lastfm.net network that comes with Orange\u0026rsquo;s network add-on and which has around 1.000 nodes and 4.000 edges. In every iteration, we have to consider 4.000 attractive forces and 1.000.000 repulsive forces for every of 1.000 times 1.000 edges. It takes about 100 iterations to get a decent network layout. That\u0026rsquo;s a lot of repulsions, and you\u0026rsquo;ll have to wait a while before you get the final layout.\nFortunately, we found a simple hack to speed things up. When computing the repulsive force acting on some node, we only consider a 10% sample of other nodes to obtain an estimate. We multiply the result by 10 and hope it\u0026rsquo;s not off by too much. By choosing a different sample in every iteration we also avoid favoring some set of nodes.\nThe left layout is obtained without sampling while the right one uses a 10% sampling. The results are pretty similar, but the sampling method is 10 times faster!\nNow that the computation is fast enough, it is time to also speed-up the drawing. But that\u0026rsquo;s a task for 2018.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "The Orange3 Network add-on contains a convenient Network Explorer widget for network visualization. Orange uses an iterative force-directed method (a variation of the Fruchterman-Reingold Algorithm) to layout the nodes on the 2D plane.\nThe goal of force-directed methods is to draw connected nodes close to each other as if the edges that connect the nodes were acting as springs. We also don\u0026rsquo;t want all nodes crowded in a single point, but would rather have them spaced evenly." ,
	"author" : "THOCEVAR",
	"summary" : "The Orange3 Network add-on contains a convenient Network Explorer widget for network visualization. Orange uses an iterative force-directed method (a variation of the Fruchterman-Reingold Algorithm) to layout the nodes on the 2D plane.\nThe goal of force-directed methods is to draw connected nodes close to each other as if the edges that connect the nodes were acting as springs. We also don\u0026rsquo;t want all nodes crowded in a single point, but would rather have them spaced evenly.",
	"date" : "Dec 23, 2017"
}

    
    , {
    "uri": "/blog/2017/11/29/how-to-properly-test-models/",
	"title": "How to Properly Test Models",
	"categories": ["analysis", "classification", "education", "overfitting", "predictive  analytics", "scoring", "workshop"],
	"description": "",
	"content": "On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\nOne thing we discussed was how to properly test your model. Ok, we know never to test on the same data you\u0026rsquo;ve built your model with, but even training and testing on separate data is sometimes not enough. Say I\u0026rsquo;ve tested Naive Bayes, Logistic Regression and Tree. Sure, I can select the one that gives the best performance, but we could potentially (over)fit our model, too.\nTo account for this, we would normally split the data to 3 parts:\n training data for building a model validation data for testing which parameters and which model to use test data for estmating the accurracy of the model  Let us try this in Orange. Load heart-disease.tab data set from Browse documentation data sets in File widget. We have 303 patients diagnosed with blood vessel narrowing (1) or diagnosed as healthy (0).\nNow, we will split the data into two parts, 85% of data for training and 15% for testing. We will send the first 85% onwards to build a model.\nWe sampled by a fixed proportion of data and went with 85%, which is 258 out of 303 patients.\nWe will use Naive Bayes, Logistic Regression and Tree, but you can try other models, too. This is also a place and time to try different parameters. Now we will send the models to Test \u0026amp; Score. We used cross-validation and discovered Logistic Regression scores the highest AUC. Say this is the model and parameters we want to go with.\nNow it is time to bring in our test data (the remaining 15%) for testing. Connect Data Sampler to Test \u0026amp; Score once again and set the connection Remaining Data - Test Data.\nTest \u0026amp; Score will warn us we have test data present, but unused. Select Test on test data option and observe the results. These are now the proper scores for our models.\nSeems like LogReg still performs well. Such procedure would normally be useful when testing a lot of models with different parameters (say +100), which you would not normally do in Orange. But it\u0026rsquo;s good to know how to do the scoring properly. Now we\u0026rsquo;re off to report on the results in Nature\u0026hellip; ;)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\nOne thing we discussed was how to properly test your model. Ok, we know never to test on the same data you\u0026rsquo;ve built your model with, but even training and testing on separate data is sometimes not enough." ,
	"author" : "AJDA",
	"summary" : "On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\nOne thing we discussed was how to properly test your model. Ok, we know never to test on the same data you\u0026rsquo;ve built your model with, but even training and testing on separate data is sometimes not enough.",
	"date" : "Nov 29, 2017"
}

    
    , {
    "uri": "/blog/2017/11/17/data-mining-business-public-administration/",
	"title": "Data Mining for Business and Public Administration",
	"categories": ["business intelligence", "clustering", "examples", "workshop"],
	"description": "",
	"content": "We\u0026rsquo;ve been having a blast with recent Orange workshops. While Blaž was getting tanned in India, Anže and I went to the charming Liverpool to hold a session for business school professors on how to teach business with Orange.\nRelated: Orange in Kolkata, India\nObviously, when we say teach business, we mean how to do data mining for business, say predict churn or employee attrition, segment customers, find which items to recommend in an online store and track brand sentiment with text analysis.\nFor this purpose, we have made some updates to our Associate add-on and added a new data set to Data Sets widget which can be used for customer segmentation and discovering which item groups are frequently bought together. Like this:\n We load the Online Retail data set.\nSince we have transactions in rows and items in columns, we have to transpose the data table in order to compute distances between items (rows). We could also simply ask Distances widget to compute distances between columns instead of rows. Then we send the transposed data table to Distances and compute cosine distance between items (cosine distance will only tell us, which items are purchased together, disregarding the amount of items purchased).\n Finally, we observe the discovered clusters in Hierarchical Clustering. Seems like mugs and decorative signs are frequently bought together. Why so? Select the group in Hierarchical Clustering and observe the cluster in a Data Table. Consider this an exercise in data exploration. :)\nThe second workshop was our standard Introduction to Data Mining for Ministry of Public Affairs.\nRelated: Analyzing Surveys\nThis group, similar to the one from India, was a pack of curious individuals who asked many interesting questions and were not shy to challenge us. How does a Tree know which attribute to split by? Is Tree better than Naive Bayes? Or is perhaps Logistic Regression better? How do we know which model works best? And finally, what is the mean of sauerkraut and beans? It has to be jota!\nWorkshops are always fun, when you have a curious set of individuals who demand answers! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We\u0026rsquo;ve been having a blast with recent Orange workshops. While Blaž was getting tanned in India, Anže and I went to the charming Liverpool to hold a session for business school professors on how to teach business with Orange.\nRelated: Orange in Kolkata, India\nObviously, when we say teach business, we mean how to do data mining for business, say predict churn or employee attrition, segment customers, find which items to recommend in an online store and track brand sentiment with text analysis." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve been having a blast with recent Orange workshops. While Blaž was getting tanned in India, Anže and I went to the charming Liverpool to hold a session for business school professors on how to teach business with Orange.\nRelated: Orange in Kolkata, India\nObviously, when we say teach business, we mean how to do data mining for business, say predict churn or employee attrition, segment customers, find which items to recommend in an online store and track brand sentiment with text analysis.",
	"date" : "Nov 17, 2017"
}

    
    , {
    "uri": "/blog/2017/11/08/orange-in-kolkata-india/",
	"title": "Orange in Kolkata, India",
	"categories": ["education", "orange3", "workshop"],
	"description": "",
	"content": "We have just completed the hands-on course on data science at one the most famous Indian educational institutions, Indian Statistical Institute. A one week course was invited by Institute\u0026rsquo;s director Prof. Dr. Sanghamitra Bandyopadhyay, and financially supported by the founding of India\u0026rsquo;s Global Initiative of Academic Networks.\nIndian Statistical Institute lies in the hearth of old Kolkata. A peaceful oasis of picturesque campus with mango orchards and waterlily lakes was founded by Prof. Prasanta Chandra Mahalanobis, one of the giants of statistics. Today, the Institute researches statistics and computational approaches to data analysis and runs a grad school, where a rather small number of students are hand-picked from tens of thousands of applicants.\nThe course was hands-on. The number of participants was limited to forty, the limitation posed by the number of the computers in Institute\u0026rsquo;s largest computer lab. Half of the students came from Institute\u0026rsquo;s grad school, and another half from other universities around Kolkata or even other schools around India, including a few participants from another famous institution, India Institutes of Technology. While the lecture included some writing on the white-board to explain machine learning, the majority of the course was about exploring example data sets, building workflows for data analysis, and using Orange on practical cases.\nThe course was not one of the lightest for the lecturer (Blaž Zupan). About five full hours each day for five days in a row, extremely motivated students with questions filling all of the coffee breaks, the need for deeper dive into some of the methods after questions in the classroom, and much need for improvisation to adapt our standard data science course to possibly the brightest pack of data science students we have seen so far. We have covered almost a full spectrum of data science topics: from data visualization to supervised learning (classification and regression, regularization), model exploration and estimation of quality. Plus computation of distances, unsupervised learning, outlier detection, data projection, and methods for parameter estimation. We have applied these to data from health care, business (which proposal on Kickstarter will succeed?), and images. Again, just like in our other data science courses, the use of Orange\u0026rsquo;s educational widgets, such as Paint Data, Interactive k-Means, and Polynomial Regression helped us in intuitive understanding of the machine learning techniques.\nThe course was beautifully organized by Prof. Dr. Saurabh Das with the help of Prof. Dr. Shubhra Sankar Ray and we would like to thank them for their devotion and excellent organization skills. And of course, many thanks to participating students: for an educator, it is always a great pleasure to lecture and work with highly motivated and curious colleagues that made our trip to Kolkata fruitful and fun.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have just completed the hands-on course on data science at one the most famous Indian educational institutions, Indian Statistical Institute. A one week course was invited by Institute\u0026rsquo;s director Prof. Dr. Sanghamitra Bandyopadhyay, and financially supported by the founding of India\u0026rsquo;s Global Initiative of Academic Networks.\nIndian Statistical Institute lies in the hearth of old Kolkata. A peaceful oasis of picturesque campus with mango orchards and waterlily lakes was founded by Prof." ,
	"author" : "BLAZ",
	"summary" : "We have just completed the hands-on course on data science at one the most famous Indian educational institutions, Indian Statistical Institute. A one week course was invited by Institute\u0026rsquo;s director Prof. Dr. Sanghamitra Bandyopadhyay, and financially supported by the founding of India\u0026rsquo;s Global Initiative of Academic Networks.\nIndian Statistical Institute lies in the hearth of old Kolkata. A peaceful oasis of picturesque campus with mango orchards and waterlily lakes was founded by Prof.",
	"date" : "Nov 8, 2017"
}

    
    , {
    "uri": "/blog/2017/11/03/neural-network-is-back/",
	"title": "Neural Network is Back!",
	"categories": ["classification", "neuralnetwork", "orange3", "regression", "widget"],
	"description": "",
	"content": "We know you\u0026rsquo;ve missed it. We\u0026rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.\nNeural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.\nDo we make each layer a widget and then stack them? Do we use parallel processing or try to do something server-side? Theano or Keras? Tensorflow perhaps?\nWe were so determined to do things properly, that after the n-th iteration we still had no clue what to actually do.\nThen one day a silly novice programmer (a.k.a. me) had enough and just threw scikit-learn\u0026rsquo;s Multi-layer Perceptron model into a widget and called it a day. There you go. A Neural Network widget just like it was in Orange2 - a wrapper for a scikit\u0026rsquo;s function that works out-of-the-box. Nothing fancy, nothing powerful, but it does its job. It models things and it predicts things.\nJust like that:\nHave fun with the new widget!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We know you\u0026rsquo;ve missed it. We\u0026rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.\nNeural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.\nDo we make each layer a widget and then stack them?" ,
	"author" : "AJDA",
	"summary" : "We know you\u0026rsquo;ve missed it. We\u0026rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.\nNeural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.\nDo we make each layer a widget and then stack them?",
	"date" : "Nov 3, 2017"
}

    
    , {
    "uri": "/blog/2017/10/26/analyzing-surveys/",
	"title": "Analyzing Surveys",
	"categories": ["analysis", "clustering", "data", "dataloading", "orange3", "visualization", "workshop"],
	"description": "",
	"content": "Our streak of workshops continues. This time we taught professionals from public administration how they can leverage data analytics and machine learning to retrieve interesting information from surveys. Thanks to the Ministry of Public Administration, this is only the first in a line of workshops on data science we are preparing for public sector employees.\nFor this purpose, we have designed EnKlik Anketa widget, which you can find in Prototypes add-on. The widget reads data from a Slovenian online survey service OneClick Survey and imports the results directly into Orange.\nWe have prepared a test survey, which you can import by entering a public link to data into the widget. Here\u0026rsquo;s the link: https://www.1ka.si/podatki/141025/72F5B3CC/ . Copy it into the Public link URL line in the widget. Once you press Enter, the widget loads the data and displays retrieved features, just like the File widget.\nEnKlik Anketa widget is similar to the File widget. It also enables changing the attribute type and role.\nThe survey is in Slovenian, but we can use Edit Domain to turn feature names into English equivalent.\nWe renamed attributes in order as they appear in the survey. If you load the survey yourself, you can rename them just like you see here.\nAs always, we can check the data in a Data Table. We have 41 respondents and 7 questions. Each respondent chose a nickname, which makes it easier to browse the data.\nNow we can perform familiar clustering to uncover interesting groups in our data. Connect Distances to Edit Domain and Hierarchical Clustering to Distances.\nDistance from Pipi and Chad to other respondents is very high, which makes them complete outliers.\nWe have two outliers, Pipi and Chad. One is an excessive sportsman (100 h of sport per week) and the other terminally ill (general health -1). Or perhaps they both simply didn\u0026rsquo;t fill out the survey correctly. If we use the Data Table to filter out Pipi and Chad, we get a fairly good clustering.\nWe can use Box Plot, to observe what makes each cluster special. Connect Box Plot to Hierarchical Clustering (with the two groups selected), select grouping by_ Cluster_ and tick _Order by relevance_.\nBox Plot separates distributions by Cluster and orders attributes by how well they split selected subgroups.\nThe final workflow.\nSeems like our second cluster (C2) is the sporty one. If we are serving in the public administration, perhaps we can design initiatives targeting cluster C1 to do more sports. It is so easy to analyze the data in Orange!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Our streak of workshops continues. This time we taught professionals from public administration how they can leverage data analytics and machine learning to retrieve interesting information from surveys. Thanks to the Ministry of Public Administration, this is only the first in a line of workshops on data science we are preparing for public sector employees.\nFor this purpose, we have designed EnKlik Anketa widget, which you can find in Prototypes add-on." ,
	"author" : "AJDA",
	"summary" : "Our streak of workshops continues. This time we taught professionals from public administration how they can leverage data analytics and machine learning to retrieve interesting information from surveys. Thanks to the Ministry of Public Administration, this is only the first in a line of workshops on data science we are preparing for public sector employees.\nFor this purpose, we have designed EnKlik Anketa widget, which you can find in Prototypes add-on.",
	"date" : "Oct 26, 2017"
}

    
    , {
    "uri": "/blog/2017/10/13/diving-into-car-registration-data/",
	"title": "Diving Into Car Registration Data",
	"categories": ["addons", "analysis", "interactive data visualization", "orange3", "visualization"],
	"description": "",
	"content": "Last week, we presented Orange at the Festival of Open Data, a mini-conference organized by the Slovenian government, dedicated to the promotion of transparent access to government data. In a 10 minute presentation, we showed how Orange can be used to visualize and explore what kinds of vehicles were registered for the first time in Slovenia in 2017.\nThe original dataset is available at theOPSI portal and it consists of 73 files, one for each month since January 2012. For the presentation, we focused on the 2017 data. If you want to follow along, you can download the merged dataset (first 9 months of 2017 as a single file). The workflow I used to prepare the data is also available.\nWhen exploring the data, the first thing we do is take a look at distributions. If we observe the distribution of new and used cars bought by the gender of the buyer, we can see that men prefer used cars while women more often opt for a new car. Or we can observe the distribution by age to see that older people tend to buy newer cars.\nBut the true power of Orange can be seen if we visualize the data on a map. In order to do this, we need to first use Geocoding to map municipality names to regions which can be shown on a map by choosing the column that contains municipality name (C1.3-Obcina uporabnika) and clicking apply. Since municipalities in Slovenia are created all the time, not all of them can be matched. The right part of the widget allows us to map these small municipalities to the nearest region. Or we can just ignore them.\nThe geocoded data can be displayed with Choropleth. If we select attribute D.1-Znamka and aggregation by mode, we get a visualization showing the most frequently bought mode for each region. Care to guess which manufacturer corresponds to the pink(-ish) color? It\u0026rsquo;s Volkswagen, in some regions with Golf and in other regions with Passat. But the visualization gives us just the most frequent value for each municipality. What if we would like to know more? As is the case with all visualizations you can click on a specific region on a map to select it and get the corresponding data on the output. We can then use Purge Domain to ignore the models that were not sold in the selected region and Box Plot to visualize the distribution by the model or by the manufacturer.\nIn Box Plot, select D.1 Znamka as both the variable and Subgroup and you get an overview of the distribution of cars by manufacturers in the selected region. But that is just the first step. We can also take a look at the distribution of Fiat cars by adding another boxplot. Now you can select the manufacturer and get a detailed distribution of specific car models sold. If you take some care in positioning the windows, you can create an interactive explorer, where you click on regions and instantly see the detailed distributions in the connected boxplots.\nThe final workflow should look like this:\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Last week, we presented Orange at the Festival of Open Data, a mini-conference organized by the Slovenian government, dedicated to the promotion of transparent access to government data. In a 10 minute presentation, we showed how Orange can be used to visualize and explore what kinds of vehicles were registered for the first time in Slovenia in 2017.\nThe original dataset is available at theOPSI portal and it consists of 73 files, one for each month since January 2012." ,
	"author" : "ASTARIC",
	"summary" : "Last week, we presented Orange at the Festival of Open Data, a mini-conference organized by the Slovenian government, dedicated to the promotion of transparent access to government data. In a 10 minute presentation, we showed how Orange can be used to visualize and explore what kinds of vehicles were registered for the first time in Slovenia in 2017.\nThe original dataset is available at theOPSI portal and it consists of 73 files, one for each month since January 2012.",
	"date" : "Oct 13, 2017"
}

    
    , {
    "uri": "/blog/2017/09/22/understanding-voting-patterns-at-akos-workshop/",
	"title": "Understanding Voting Patterns  at AKOS Workshop",
	"categories": ["clustering", "interactive data visualization", "unsupervised", "visualization", "workshop"],
	"description": "",
	"content": "Two days ago we held another Introduction to Data Mining workshop at our faculty. This time the target audience was a group of public sector professionals and our challenge was finding the right data set to explain key data mining concepts. Iris is fun, but not everyone is a biologist, right? Fortunately, we found this really nice data set with ballot counts from the Slovenian National Assembly (thanks to Parlameter).\nRelated: Intro to Data Mining for Life Scientists\nWorkshop for the Agency for Communication Networks and Services (AKOS).\nThe data contains ballot counts, statistics, and description for 84 members of the parliament (MPs). First, we inspected the data in a Data Table. Each MP is described with 14 meta features and has 18 ballot counts recorded.\nOut data has 84 instances, 18 features (ballot counts) and 14 meta features (MP description).\nWe have some numerical features, which means we can also inspect the data in Scatter Plot. We will plot MPs' attendance vs. the number of their initiatives. Quite interesting! There is a big group of MPs who regularly attend the sessions, but rarely propose changes. Could this be the coalition?\nScatter plot of MPs' session attendance (in percentage) and the number of initiatives. Already an interesting pattern emerges.\nThe next question that springs to our mind is - can we discover interesting voting patterns from our data? Let us see. We first explored the data in Hierarchical Clustering. Looks like there are some nice clusters in our data! The blue cluster is the coalition, red the SDS party and green the rest (both from the opposition).\nRelated: Hierarchical Clustering: A Simple Explanation\nHierarchical Clustering visualizes a hierarchy of clusters. But it is hard to observe similarity of pairs of data instances. How similar are Luka Mesec and Branko Grims? It is hard to tell\u0026hellip;\nBut it is hard to inspect so many data instances in a dendrogram. For example, we have no idea how similar are the voting records of Eva Irgl and Alenka Bratušek. Surely, there must be a better way to explore similarities and perhaps verify that voting patterns exist at even a party-level\u0026hellip; Let us try MDS. MDS transforms multidimensional data into a 2D projection so that similar data instances lie close to each other.\nMDS can plot a multidimensional data in 2D so that similar data points lie close to each other. But sometimes this optimization is hard. This is why we have grey lines connecting the dots - the dots connected are similar at the selected cut-off level (Show similar pairs slider).\nAh, this is nice! We even colored data points by the party. MDS beautifully shows the coalition (blue dots) and the opposition (all other colors). Even parties are clustered together. But there are some outliers. Let us inspect Matej Tonin, who is quite far away from his orange group. Seems like he was missing at the last two sessions and did not vote. Hence his voting is treated differently.\nData Table is a handy tool for instant data inspection. It is always great to check, what is on the output of each widget.\nIt is always great to inspect discovered groups and outliers. This way an expert can interpret the clusters and also explain, what outliers mean. Sometimes it is simply a matter of data (missing values), but sometimes we could find shifting alliances. Perhaps an outlier could be an MP about to switch to another party.\nThe final workflow.\nYou can have fun with these data, too. Let us know if you discover something interesting!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Two days ago we held another Introduction to Data Mining workshop at our faculty. This time the target audience was a group of public sector professionals and our challenge was finding the right data set to explain key data mining concepts. Iris is fun, but not everyone is a biologist, right? Fortunately, we found this really nice data set with ballot counts from the Slovenian National Assembly (thanks to Parlameter)." ,
	"author" : "AJDA",
	"summary" : "Two days ago we held another Introduction to Data Mining workshop at our faculty. This time the target audience was a group of public sector professionals and our challenge was finding the right data set to explain key data mining concepts. Iris is fun, but not everyone is a biologist, right? Fortunately, we found this really nice data set with ballot counts from the Slovenian National Assembly (thanks to Parlameter).",
	"date" : "Sep 22, 2017"
}

    
    , {
    "uri": "/blog/2017/09/15/orange-at-station-houston/",
	"title": "Orange at Station Houston",
	"categories": ["clustering", "images", "orange3", "workshop"],
	"description": "",
	"content": "With over 262 member companies, Station Houston is the largest hub for tech startups in Houston.\nOne of its members is also Genialis, a life science data exploration company that emerged from our lab and is now delivering pipelines and user-friendly apps for analytics in systems biology.\nThanks to the invitation by the director of operations Alex de la Fuente, we gave a seminar on Data Science for Everyone. We spoke about how Orange can support anyone to learn about data science and then use machine learning on their own data.\nWe pushed on this last point: say you walk in downtown Houston, pick first three passersby, take them to the workshop and train them in machine learning. To the point where they could walk out from the training and use some machine learning at home. Say, cluster their family photos, or figure out what Kickstarter project features to optimize to get the funding.\nHow long would such workshop take? Our informed guess: three hours. And of course, we illustrated this point to seminar attendees by giving a demo of the clustering of images in Orange and showcasing Kickstarter data analysis.\nRelated: Image Analytics: Clustering\nSeminars at Station Houston need to finish with a homework. So we delivered one. Here it is:\n Open your browser. Find some images of your interest (mountains, cities, cars, fish, dogs, faces, whatever). Place images in a folder (Mac: just drag the thumbnails, Win: right click and Save Image). Download \u0026amp; install Orange. From Orange, install Image Analytics add-on (Options, Add-Ons). Use Orange to cluster images. Does clustering make sense?  Data science and startups aside: there are some beautiful views from Station Houston. From the kitchen, there is a straight sight to Houston\u0026rsquo;s medical center looming about 4 miles away.\nAnd on the other side, there is a great view of the downtown.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "With over 262 member companies, Station Houston is the largest hub for tech startups in Houston.\nOne of its members is also Genialis, a life science data exploration company that emerged from our lab and is now delivering pipelines and user-friendly apps for analytics in systems biology.\nThanks to the invitation by the director of operations Alex de la Fuente, we gave a seminar on Data Science for Everyone. We spoke about how Orange can support anyone to learn about data science and then use machine learning on their own data." ,
	"author" : "BLAZ",
	"summary" : "With over 262 member companies, Station Houston is the largest hub for tech startups in Houston.\nOne of its members is also Genialis, a life science data exploration company that emerged from our lab and is now delivering pipelines and user-friendly apps for analytics in systems biology.\nThanks to the invitation by the director of operations Alex de la Fuente, we gave a seminar on Data Science for Everyone. We spoke about how Orange can support anyone to learn about data science and then use machine learning on their own data.",
	"date" : "Sep 15, 2017"
}

    
    , {
    "uri": "/blog/2017/08/28/can-we-download-orange-faster/",
	"title": "Can We Download Orange Faster?",
	"categories": ["analysis", "download", "orange3"],
	"description": "",
	"content": "One day Blaž and Janez came to us and started complaining how slow Orange download is in the US. Since they hold a large course at Baylor College of Medicine every year, this causes some frustration.\nRelated: Introduction to Data Mining Course in Houston\nBut we have the data and we\u0026rsquo;ve promptly tried to confirm their complaints by analyzing them\u0026hellip; well, in Orange!\nFirst, let us observe the data. We have 4887 recorded download sessions with one meta feature reporting on the country of the download and four features with time, size, speed in bytes and speed in gigabytes of the download.\nData of Orange download statistics. We get reports on the country of download, the size and the time of the download. We have constructed speed and size in gigabytes ourselves with simple formulae.\nNow let us check the validity of Blaž\u0026rsquo;s and Janez\u0026rsquo;s complaint. We will use orange3-geo add-on for plotting geolocated data. For any geoplotting, we need coordinates - latitude and longitude. To retrieve them automatically, we will use Geocoding widget.\nWe instruct Geocoding to retrieve coordinates from our Country feature. Identifier type tells the widget in what format the region name appears.\nWe told the widget to use the ISO-compliant country code from Country attribute and encode it into coordinates. If we check the new data in a Data Table, we see our data is enhanced with new features.\nEnhanced data table. Besides latitude and longitude, Geocoding can also append country-level data (economy, continent, region\u0026hellip;).\nNow that we have coordinates, we can plot these data regionally - in Choropleth widget! This widget plots data on three levels - country, state/region and county/municipality. Levels correspond to the administrative division of each country.\nChoropleth widget offers 3 aggregation levels. We chose country (e.g. administrative level 0), but with a more detailed data one could also plot by state/county/municipality. Administrative levels are different for each country (e.g. Bundesländer for Germany, states for the US, provinces for Canada\u0026hellip;).\nIn the plot above, we have simply displayed the amount of people (Count) that downloaded Orange in the past couple of months. Seems like we indeed have most users in the US, so it might make sense to solve installation issues for this region first.\nNow let us check the speed of the download - it is really so slow in the US? If we take the mean, we can see that Slovenia is far ahead of the rest as far as download speed is concerned. No wonder - we are downloading via the local network. Scandinavia, Central Europe and a part of the Balkans seem to do quite ok as well.\nAggregation by mean.\nBut mean sometimes doesn\u0026rsquo;t show the right picture - it is sensitive to outliers, which would be the case of Slovenia here. Let us try median instead. Looks like 50% of American download at speed lower than 1.5MB/s. Quite average, but it could be better.\nAggregation by median.\nAnd the longest time someone was prepared to wait for the download? Over 3 hours. Kudos, mate! We appreciate it! 🙌\nThis simple workflow is all it took to do our analysis.\nSo how is your download speed for Orange compared to other things you are downloading? Better, worse? We\u0026rsquo;re keen to hear it! 👂\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One day Blaž and Janez came to us and started complaining how slow Orange download is in the US. Since they hold a large course at Baylor College of Medicine every year, this causes some frustration.\nRelated: Introduction to Data Mining Course in Houston\nBut we have the data and we\u0026rsquo;ve promptly tried to confirm their complaints by analyzing them\u0026hellip; well, in Orange!\nFirst, let us observe the data. We have 4887 recorded download sessions with one meta feature reporting on the country of the download and four features with time, size, speed in bytes and speed in gigabytes of the download." ,
	"author" : "AJDA",
	"summary" : "One day Blaž and Janez came to us and started complaining how slow Orange download is in the US. Since they hold a large course at Baylor College of Medicine every year, this causes some frustration.\nRelated: Introduction to Data Mining Course in Houston\nBut we have the data and we\u0026rsquo;ve promptly tried to confirm their complaints by analyzing them\u0026hellip; well, in Orange!\nFirst, let us observe the data. We have 4887 recorded download sessions with one meta feature reporting on the country of the download and four features with time, size, speed in bytes and speed in gigabytes of the download.",
	"date" : "Aug 28, 2017"
}

    
    , {
    "uri": "/blog/2017/08/11/its-sailing-time-again/",
	"title": "It&#39;s Sailing Time (Again)",
	"categories": ["classification", "tree"],
	"description": "",
	"content": "Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A. Olshen). And finally, because they make the base of random forests, one of the most accurate machine learning models for smaller and mid-size data sets.\nRelated: Introduction to Data Mining Course in Houston\nLecture on classification trees has to start with the data. Years back I have crafted a data set on sailing. Every data set has to have a story. Here is one:\nSara likes weekend sailing. Though, not under any condition. Past twenty Wednesdays I have asked her if she will have any company, what kind of boat she can rent, and I have checked the weather forecast. Then, on Saturday, I wrote down if she actually went to the Sea. Data on Sara\u0026rsquo;s sailing contains three attributes (Outlook, Company, Sailboat) and a class (Sail).\nThe data comes with Orange and you can get them from Data Sets widget (currently in Prototypes Add-On, but soon to be moved to core Orange). It takes time, usually two lecture hours, to go through probabilities, entropy and information gain, but at the end, the data analysis workflow we develop with students looks something like this:\nAnd here is the classification tree:\nTurns out that Sara is a social person. When the company is big, she goes sailing no matter what. When the company is smaller, she would not go sailing if the weather is bad. But when it is sunny, sailing is fun, even when being alone.\nRelated: Pythagorean Trees and Forests\nClassification trees are not very stable classifiers. Even with small changes in the data, the trees can change substantially. This is an important concept that leads to the use of ensembles like random forests. It is also here, during my lecture, that I need to demonstrate this instability. I use Data Sampler and show a classification tree under the current sampling. Pressing on Sample Data button the tree changes every time. The workflow I use is below, but if you really want to see this in action, well, try it in Orange.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A." ,
	"author" : "BLAZ",
	"summary" : "Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A.",
	"date" : "Aug 11, 2017"
}

    
    , {
    "uri": "/blog/2017/08/08/text-analysis-workshop-at-digital-humanities-2017/",
	"title": "Text Analysis Workshop at Digital Humanities 2017",
	"categories": ["classification", "conference", "education", "interactive data visualization", "workshop"],
	"description": "",
	"content": "How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?\nIt seems so, especially when Orange is involved.\nYesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers. But at the same time, everyone was also excited. The audience about the possibilities Orange offers for their future projects and the lecturers about the fantastic participants who even during the workshop were already experimenting with their own data.\nThe biggest challenge was presenting the inner workings of algorithms to a predominantly non-computer science crowd. Luckily, we had Tree Viewer and Nomogram to help us explain Classification Tree and Logistic Regression! Everything is much easier with vizualizations.\nClassification Tree splits first by the word \u0026lsquo;came\u0026rsquo;, since it results in the purest split. Next it splits by \u0026lsquo;strange\u0026rsquo;. Since we still don\u0026rsquo;t have pure nodes, it continues to \u0026lsquo;bench\u0026rsquo;, which gives a satisfying result. Trees are easy to explain, but can quickly overfit the data.\nLogistic Regression transforms word counts to points. The sum of points directly corresponds to class probability. Here, if you see 29 foxes in a text, you get a high probability of Animal Tales. If you don\u0026rsquo;t see any, then you get a high probability of the opposite class.\nAt the end, we were experimenting with explorative data analysis, where we had Hierarchical Clustering, Corpus Viewer, Image Viewer and Geo Map opened at the same time. This is how a researcher can interactively explore the dendrogram, read the documents from selected clusters, observe the corresponding images and locate them on a map.\nHierarchical Clustering, Image Viewer, Geo Map and Corpus Viewer opened at the same time create an interactive data browser.\nThe workshop was a nice kick-off to an exciting week full of interesting lectures and presentations at Digital Humanities 2017 conference. So much to learn and see!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?\nIt seems so, especially when Orange is involved.\nYesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers." ,
	"author" : "AJDA",
	"summary" : "How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?\nIt seems so, especially when Orange is involved.\nYesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers.",
	"date" : "Aug 8, 2017"
}

    
    , {
    "uri": "/blog/2017/08/04/text-analysis-new-features/",
	"title": "Text Analysis: New Features",
	"categories": ["analysis", "dataloading", "examples", "features", "orange3", "release", "text mining", "version", "widget", "workshop"],
	"description": "",
	"content": "As always, we\u0026rsquo;ve been working hard to bring you new functionalities and improvements. Recently, we\u0026rsquo;ve released Orange version 3.4.5 and Orange3-Text version 0.2.5. We focused on the Text add-on since we are lately holding a lot of text mining workshops. The next one will be at Digital Humanities 2017 in Montreal, QC, Canada in a couple of days and we simply could not resist introducing some sexy new features_._\nRelated: Text Preprocessing\nRelated: Rehaul of Text Mining Add-On\nFirst, Orange 3.4.5 offers better support for Text add-on. What do we mean by this? Now, every core Orange widget works with Text smoothly so you can mix-and-match the widgets as you like. Before, one could not pass the output of Select Columns (data table) to Preprocess Text (corpus), but now this is no longer a problem.\nOf course, one still needs to keep in mind that Corpus is a sparse data format, which does not work with some widgets by design. For example, Manifold Learning supports only t-SNE projection.\nSecond, we\u0026rsquo;ve introduced two new widgets, which have been long overdue. One is Sentiment Analysis, which enables basic sentiment analysis of corpora. So far it works for English and uses two nltk-supported techniques - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu computes a single normalized score of sentiment in the text (negative score for negative sentiment, positive for positive, 0 is neutral), while Vader outputs scores for each category (positive, negative, neutral) and appends a total sentiment score called a compound.\nLiu Hu score.\nVader scores.\nTry it with Heat Map to visualize the scores.\nYellow represent a high, positive score, while blue represent a low, negative score. Seems like Animal Tales are generally much more negative than Tales of Magic.\nThe second widget we\u0026rsquo;ve introduced is Import Documents. This widget enables you to import your own documents into Orange and outputs a corpus on which you can perform the analysis. The widget supports .txt, .docx, .odt, .pdf and .xml files and loads an entire folder. If the folder contains subfolders, they will be considered as class values. Here\u0026rsquo;s an example.\nThis is the structure of my Kennedy folder. I will load the folder with Import Documents. Observe, how Orange creates a class variable category with post-1962 and pre-1962 as class values.\nSubfolders are considered as class in the category column.\nNow you can perform your analysis as usual.\nFinally, some widgets have cool new updates. Topic Modelling, for example, colors words by their weights - positive weights are colored green and negative red. Coloring only works with LSI, since it\u0026rsquo;s the only method that outputs both positive and negative weights.\nIf there are many kings in the text and no birds, then the text belongs to Topic 2. If there are many children and no foxes, then it belongs to Topic 3.\nTake some time, explore these improvements and let us know if you are happy with the changes! You can also submit new feature requests to our issue tracker.\nThank you for working with Orange! 🍊\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "As always, we\u0026rsquo;ve been working hard to bring you new functionalities and improvements. Recently, we\u0026rsquo;ve released Orange version 3.4.5 and Orange3-Text version 0.2.5. We focused on the Text add-on since we are lately holding a lot of text mining workshops. The next one will be at Digital Humanities 2017 in Montreal, QC, Canada in a couple of days and we simply could not resist introducing some sexy new features_._\nRelated: Text Preprocessing" ,
	"author" : "AJDA",
	"summary" : "As always, we\u0026rsquo;ve been working hard to bring you new functionalities and improvements. Recently, we\u0026rsquo;ve released Orange version 3.4.5 and Orange3-Text version 0.2.5. We focused on the Text add-on since we are lately holding a lot of text mining workshops. The next one will be at Digital Humanities 2017 in Montreal, QC, Canada in a couple of days and we simply could not resist introducing some sexy new features_._\nRelated: Text Preprocessing",
	"date" : "Aug 4, 2017"
}

    
    , {
    "uri": "/blog/2017/07/28/support-orange-developers/",
	"title": "Support Orange Developers",
	"categories": ["orange3"],
	"description": "",
	"content": "Do you love Orange? Do you think it is the best thing since sliced bread? Want to thank all the developers for their hard work?\nNothing says thank you like a fresh supply of ice cream and now you can help us stock our fridge with your generous donations. 🍦🍦🍦\nDonate\nSupport open source software and the team behind Orange. We promise to squander all your contributions purely on ice cream. Can\u0026rsquo;t have a development sprint without proper refreshments! ;)\nThank you in advance for all the contributions, encouragement and support! It wouldn\u0026rsquo;t be worth it without you.\n🍊Orange team🍊\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Do you love Orange? Do you think it is the best thing since sliced bread? Want to thank all the developers for their hard work?\nNothing says thank you like a fresh supply of ice cream and now you can help us stock our fridge with your generous donations. 🍦🍦🍦\nDonate\nSupport open source software and the team behind Orange. We promise to squander all your contributions purely on ice cream." ,
	"author" : "AJDA",
	"summary" : "Do you love Orange? Do you think it is the best thing since sliced bread? Want to thank all the developers for their hard work?\nNothing says thank you like a fresh supply of ice cream and now you can help us stock our fridge with your generous donations. 🍦🍦🍦\nDonate\nSupport open source software and the team behind Orange. We promise to squander all your contributions purely on ice cream.",
	"date" : "Jul 28, 2017"
}

    
    , {
    "uri": "/blog/2017/07/14/miniconda-installer/",
	"title": "Miniconda Installer",
	"categories": ["distribution", "download", "orange3"],
	"description": "",
	"content": "Orange has a new friend! It\u0026rsquo;s Miniconda, Anaconda\u0026rsquo;s little sister.\nFor a long time, the idea was to utilize the friendly nature of Miniconda to install Orange dependencies, which often misbehaved on some platforms. Miniconda provides Orange with Python 3.6 and conda installer, which is then used to handle everything Orange needs for proper functioning. So sssssss-mooth!\nMiniconda Installer\nPlease know that our Miniconda installer is in a beta state, but we are inviting adventurous testers to try it and report any bugs they find to our issue tracker [there won\u0026rsquo;t be any of course! ;) ].\nHappy testing! 🐍|🍊\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange has a new friend! It\u0026rsquo;s Miniconda, Anaconda\u0026rsquo;s little sister.\nFor a long time, the idea was to utilize the friendly nature of Miniconda to install Orange dependencies, which often misbehaved on some platforms. Miniconda provides Orange with Python 3.6 and conda installer, which is then used to handle everything Orange needs for proper functioning. So sssssss-mooth!\nMiniconda Installer\nPlease know that our Miniconda installer is in a beta state, but we are inviting adventurous testers to try it and report any bugs they find to our issue tracker [there won\u0026rsquo;t be any of course!" ,
	"author" : "AJDA",
	"summary" : "Orange has a new friend! It\u0026rsquo;s Miniconda, Anaconda\u0026rsquo;s little sister.\nFor a long time, the idea was to utilize the friendly nature of Miniconda to install Orange dependencies, which often misbehaved on some platforms. Miniconda provides Orange with Python 3.6 and conda installer, which is then used to handle everything Orange needs for proper functioning. So sssssss-mooth!\nMiniconda Installer\nPlease know that our Miniconda installer is in a beta state, but we are inviting adventurous testers to try it and report any bugs they find to our issue tracker [there won\u0026rsquo;t be any of course!",
	"date" : "Jul 14, 2017"
}

    
    , {
    "uri": "/blog/2017/06/19/text-preprocessing/",
	"title": "Text Preprocessing",
	"categories": ["orange3", "preprocessing", "text mining", "visualization"],
	"description": "",
	"content": "In data mining, preprocessing is key. And in text mining, it is the key and the door. In other words, it\u0026rsquo;s the most vital step in the analysis.\nRelated: Text Mining add-on\nSo what does preprocessing do? Let\u0026rsquo;s have a look at an example. Place Corpus widget from Text add-on on the canvas. Open it and load Grimm-tales-selected. As always, first have a quick glance of the data in Corpus Viewer. This data set contains 44 selected Grimms' tales.\nNow, let us see the most frequent words of this corpus in a Word Cloud.\nUgh, what a mess! The most frequent words in these texts are conjunctions (\u0026lsquo;and\u0026rsquo;, \u0026lsquo;or\u0026rsquo;) and prepositions (\u0026lsquo;in\u0026rsquo;, \u0026lsquo;of\u0026rsquo;), but so they are in almost every English text in the world. We need to remove these frequent and uninteresting words to get to the interesting part. We remove the punctuation by defining our tokens. Regexp \\w+ will keep full words and omit everything else. Next, we filter out the uninteresting words with a list of stopwords. The list is pre-set by nltk package and contains frequently occurring conjunctions, prepositions, pronouns, adverbs and so on.\nOk, we did some essential preprocessing. Now let us observe the results.\nThis does look much better than before! Still, we could be a bit more precise. How about removing the words could, would, should and perhaps even said, since it doesn\u0026rsquo;t say much about the content of the tale? A custom list of stopwords would come in handy!\nOpen a plain text editor, such as Notepad++ or Sublime, and place each word you wish to filter on a separate line.\nSave the file and load it next to the pre-set stopword list.\nOne final check in the Word Cloud should reveal we did a nice job preparing our data. We can now see the tales talk about kings, mothers, fathers, foxes and something that is little. Much more informative!\nRelated: Workshop: Text Analysis for Social Scientists\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "In data mining, preprocessing is key. And in text mining, it is the key and the door. In other words, it\u0026rsquo;s the most vital step in the analysis.\nRelated: Text Mining add-on\nSo what does preprocessing do? Let\u0026rsquo;s have a look at an example. Place Corpus widget from Text add-on on the canvas. Open it and load Grimm-tales-selected. As always, first have a quick glance of the data in Corpus Viewer." ,
	"author" : "AJDA",
	"summary" : "In data mining, preprocessing is key. And in text mining, it is the key and the door. In other words, it\u0026rsquo;s the most vital step in the analysis.\nRelated: Text Mining add-on\nSo what does preprocessing do? Let\u0026rsquo;s have a look at an example. Place Corpus widget from Text add-on on the canvas. Open it and load Grimm-tales-selected. As always, first have a quick glance of the data in Corpus Viewer.",
	"date" : "Jun 19, 2017"
}

    
    , {
    "uri": "/blog/2017/06/09/workshop-text-analysis-for-social-scientists/",
	"title": "Workshop: Text Analysis for Social Scientists",
	"categories": ["education", "text mining", "workshop"],
	"description": "",
	"content": "Yesterday was no ordinary day at the Faculty of Computer and Information Science, University of Ljubljana - there was an unusually high proportion of Social Sciences students, researchers and other professionals in our classrooms. It was all because of a Text Analysis for Social Scientists workshop.\nRelated: Data Mining for Political Scientists\nText mining is becoming a popular method across sciences and it was time to showcase what it (and Orange) can do. In this 5-hour hands-on workshop we explained text preprocessing, clustering, and predictive models, and applied them in the analysis of selected Grimm\u0026rsquo;s Tales. We discovered that predictive models can nicely distinguish between animal tales and tales of magic and that foxes and kings play a particularly important role in separating between the two types.\nNomogram displays 6 most important words (attributes) as defined by Logistic Regression. Seems like the occurrence of the word \u0026lsquo;fox\u0026rsquo; can tell us a lot about whether the text is an animal tale or a tale of magic.\nRelated: Nomogram\nThe second part of the workshop was dedicated to the analysis of tweets - we learned how to work with thousands of tweets on a personal computer, we plotted them on a map by geolocation, and used Instagram images for image clustering.\nRelated: Image Analytics: Clustering\nFive hours was very little time to cover all the interesting topics in text analytics. But Orange came to the rescue once again. Interactive visualization and the possibility of close reading in Corpus Viewer were such a great help! Instead of reading 6400 tweets \u0026lsquo;by hand\u0026rsquo;, now the workshop participants can cluster them in interesting groups, find important words in each cluster and plot them in a 2D visualization.\nParticipants at work.\nHere, we\u0026rsquo;d like to thank NumFocus for providing financial support for the course. This enabled us to bring in students from a wide variety of fields (linguists, geographers, marketers) and prove (once again) that you don\u0026rsquo;t have to be a computer scientists to do machine learning!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Yesterday was no ordinary day at the Faculty of Computer and Information Science, University of Ljubljana - there was an unusually high proportion of Social Sciences students, researchers and other professionals in our classrooms. It was all because of a Text Analysis for Social Scientists workshop.\nRelated: Data Mining for Political Scientists\nText mining is becoming a popular method across sciences and it was time to showcase what it (and Orange) can do." ,
	"author" : "AJDA",
	"summary" : "Yesterday was no ordinary day at the Faculty of Computer and Information Science, University of Ljubljana - there was an unusually high proportion of Social Sciences students, researchers and other professionals in our classrooms. It was all because of a Text Analysis for Social Scientists workshop.\nRelated: Data Mining for Political Scientists\nText mining is becoming a popular method across sciences and it was time to showcase what it (and Orange) can do.",
	"date" : "Jun 9, 2017"
}

    
    , {
    "uri": "/blog/2017/06/05/nomogram/",
	"title": "Nomogram",
	"categories": ["analysis", "classification", "features", "interactive data visualization", "visualization"],
	"description": "",
	"content": "One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly.\nSo, how does it work? First, feed the Nomogram a classification model, say, Logistic Regression. We will use the Titanic survival data that comes with Orange for this example (in File widget, choose \u0026ldquo;Browse documentation datasets\u0026rdquo;).\nIn the nomogram, we see the top ranked attributes and how much they contribute to the target class. Seems like a male third class adult had a much lower survival rate than did female first class child.\nThe first box show the target class, in our case survived=no. The second box shows the most important attribute, sex, and its contribution to the probability of the target class (more for male, almost 0 for female). The final box shows the total probability of the target class for the selected values of attributes (blue dots).\nThe most important attribute, however, seems to be \u0026lsquo;sex\u0026rsquo;, where the chance for survival (target class = no) is lower for males than it is for females. How do I know? Grab the blue dot over the attribute and drag it from \u0026lsquo;male\u0026rsquo; to \u0026lsquo;female\u0026rsquo;. The total probability for dying on Titanic (survived=no) drops from 89% to 43%.\nThe same goes for all the other attributes - you can interactively explore how much a certain value contributes to the probability of a selected target class.\nBut it gets even better! Instead of dragging the blue dots in the nomogram, you can feed it the data. In the workflow below, we pass the data through the Data Table widget and then feed the selected data instance to the Nomogram. The Nomogram would then show what is the probability of the target class for this particular instance, and it would \u0026ldquo;explain\u0026rdquo; what are the magnitudes of contributions of individual attribute values.\nThis makes Nomogram a great widget for understanding the model and for interactive data exploration.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly." ,
	"author" : "AJDA",
	"summary" : "One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly.",
	"date" : "Jun 5, 2017"
}

    
    , {
    "uri": "/blog/2017/04/25/outliers-in-traffic-signs/",
	"title": "Outliers in Traffic Signs",
	"categories": ["addons", "analysis", "images", "interactive data visualization", "orange3", "visualization"],
	"description": "",
	"content": "Say I am given a collection of images of traffic signs, and would like to find which signs stick out. That is, which traffic signs look substantially different from the others. I would assume that the traffic signs are not equally important and that some were designed to be noted before the others.\nI have assembled a small set of regulatory and warning traffic signs and stored the references to their images in a traffic-signs-w.tab data set.\nRelated: Viewing images\nRelated: Video on image clustering\nRelated: Video on image classification\nThe easiest way to display the images is by loading this data file with File widget and then passing the data to the Image Viewer,\nOpening the Image Viewer allows me to see the images:\nNote that initially the data table we have loaded contains no valuable features on which we can do any machine learning. It includes just a category of traffic sign, its name, and the link to its image.\nWe will use deep-network embedding to turn these images into numbers to describe them with 2048 real-valued features. Then, we will use Silhouette Plot to find which traffic signs are outliers in their own group. We would like to select these and visualize them in the Image Viewer.\nRelated: All I see is silhouette\nOur final workflow, with selection of three biggest outliers (we used shift-click to select its corresponding silhouettes in the Silhouette Plot), is:\nIsn\u0026rsquo;t this great? Turns out that traffic signs were carefully designed, such that the three outliers are indeed the signs we should never miss. It is great that we can now reconfirm this design choice by deep learning-based embedding and by using some straightforward machine learning tricks such as Silhouette Plot.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Say I am given a collection of images of traffic signs, and would like to find which signs stick out. That is, which traffic signs look substantially different from the others. I would assume that the traffic signs are not equally important and that some were designed to be noted before the others.\nI have assembled a small set of regulatory and warning traffic signs and stored the references to their images in a traffic-signs-w." ,
	"author" : "BLAZ",
	"summary" : "Say I am given a collection of images of traffic signs, and would like to find which signs stick out. That is, which traffic signs look substantially different from the others. I would assume that the traffic signs are not equally important and that some were designed to be noted before the others.\nI have assembled a small set of regulatory and warning traffic signs and stored the references to their images in a traffic-signs-w.",
	"date" : "Apr 25, 2017"
}

    
    , {
    "uri": "/blog/2017/04/07/model-replaces-classify-and-regression/",
	"title": "Model replaces Classify and Regression",
	"categories": ["classification", "features", "interface", "orange3", "prediction", "predictive  analytics", "regression", "toolbox", "update"],
	"description": "",
	"content": "Did you recently wonder where did Classification Tree go? Or what happened to Majority?\nOrange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.\nThis, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing.tab and iris.tab!\nLeading up to this change, many algorithms were refactored so that regression and classification versions of the same method were merged into a single widget (and class in the underlying python API). For example, Classification Tree and Regression Tree have become simply Tree, which is capable of modelling categorical or numeric target variables. And similarly for SVM, kNN, Random Forest, …\nHave you ever searched for a widget by typing its name and were confused by multiple options appearing in the search box? Now you do not need to decide if you need Classification SVM or Regression SVM, you can just select SVM and enjoy the rest of the time doing actual data analysis!\nHere is a quick wrap-up:\n Majority and Mean became Constant. Classification Tree and Regression Tree became Tree. In the same manner, Random Forest and Regression Forest became Random Forest. SVM, SGD, AdaBoost and kNN now work for both classification and regression tasks. Linear Regression only works for regression. Logistic Regression, Naive Bayes and CN2 Rule Induction only work for classification.  Sorry about the last part, we really couldn’t do anything about the very nature of these algorithms! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Did you recently wonder where did Classification Tree go? Or what happened to Majority?\nOrange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.\nThis, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing." ,
	"author" : "AJDA",
	"summary" : "Did you recently wonder where did Classification Tree go? Or what happened to Majority?\nOrange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.\nThis, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing.",
	"date" : "Apr 7, 2017"
}

    
    , {
    "uri": "/blog/2017/04/03/image-analytics-clustering/",
	"title": "Image Analytics: Clustering",
	"categories": ["addons", "analysis", "clustering", "embedding", "images", "interactive  data visualization", "orange3", "unsupervised"],
	"description": "",
	"content": "Data does not always come in a nice tabular form. It can also be a collection of text, audio recordings, video materials or even images. However, computers can only work with numbers, so for any data mining, we need to transform such unstructured data into a vector representation.\nFor retrieving numbers from unstructured data, Orange can use deep network embedders. We have just started to include various embedders in Orange, and for now, they are available for text and images.\nRelated: Video on image clustering\nHere, we give an example of image embedding and show how easy is to use it in Orange. Technically, Orange would send the image to the server, where the server would push an image through a pre-trained deep neural network, like Google\u0026rsquo;s Inception v3. Deep networks were most often trained with some special purpose in mind. Inception v3, for instance, can classify images into any of 1000 image classes. We can disregard the classification, consider instead the penultimate layer of the network with 2048 nodes (numbers) and use that for image\u0026rsquo;s vector-based representation.\nLet\u0026rsquo;s see this on an example.\nHere we have 19 images of domestic animals. First, download the images and unzip them. Then use Import Images widget from Orange\u0026rsquo;s Image Analytics add-on and open the directory containing the images.\nWe can visualize images in Image Viewer widget. Here is our workflow so far, with images shown in Image Viewer:\nBut what do we see in a data table? Only some useless description of images (file name, the location of the file, its size, and the image width and height).\nThis cannot help us with machine learning. As I said before, we need numbers. To acquire numerical representation of these images, we will send the images to Image Embedding widget.\nGreat! Now we have the numbers we wanted. There are 2048 of them (columns n0 to n2047). From now on, we can apply all the standard machine learning techniques, say, clustering.\nLet us measure the distance between these images and see which are the most similar. We used Distances widget to measure the distance. Normally, cosine distance works best for images, but you can experiment on your own. Then we passed the distance matrix to Hierarchical Clustering to visualize similar pairs in a dendrogram.\nThis looks very promising! All the right animals are grouped together. But I can\u0026rsquo;t see the results so well in the dendrogram. I want to see the images - with Image Viewer!\nSo cool! All the cow family is grouped together! Now we can click on different branches of the dendrogram and observe which animals belong to which group.\nBut I know what you are going to say. You are going to say I am cheating. That I intentionally selected similar images to trick you.\nI will prove you wrong. I will take a new cow, say, the most famous cow in Europe - Milka cow.\nThis image is quite different from the other images - it doesn\u0026rsquo;t have a white background, it\u0026rsquo;s a real (yet photoshopped) photo and the cow is facing us. Will the Image Embedding find the right numerical representation for this cow?\nIndeed it has. Milka is nicely put together with all the other cows.\nImage analytics is such an exciting field in machine learning and now Orange is a part of it too! You need to install the Image Analytics add on and you are all set for your research!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Data does not always come in a nice tabular form. It can also be a collection of text, audio recordings, video materials or even images. However, computers can only work with numbers, so for any data mining, we need to transform such unstructured data into a vector representation.\nFor retrieving numbers from unstructured data, Orange can use deep network embedders. We have just started to include various embedders in Orange, and for now, they are available for text and images." ,
	"author" : "AJDA",
	"summary" : "Data does not always come in a nice tabular form. It can also be a collection of text, audio recordings, video materials or even images. However, computers can only work with numbers, so for any data mining, we need to transform such unstructured data into a vector representation.\nFor retrieving numbers from unstructured data, Orange can use deep network embedders. We have just started to include various embedders in Orange, and for now, they are available for text and images.",
	"date" : "Apr 3, 2017"
}

    
    , {
    "uri": "/blog/2017/03/17/k-means-silhouette-score/",
	"title": "k-Means and Silhouette Score",
	"categories": ["clustering", "tutorial", "unsupervised", "youtube"],
	"description": "",
	"content": "k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.\nBut\u0026hellip; have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers.\n#1 Constructing workflow with k-means\n  #2 How k-means works [interactive visualization]\n  #3 How silhouette score works and why it is useful\n  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.\nBut\u0026hellip; have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers." ,
	"author" : "AJDA",
	"summary" : "k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.\nBut\u0026hellip; have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers.",
	"date" : "Mar 17, 2017"
}

    
    , {
    "uri": "/blog/2017/03/09/why-orange/",
	"title": "Why Orange?",
	"categories": ["data", "examples", "youtube"],
	"description": "",
	"content": "Why is Orange so great? Because it helps people solve problems quickly and efficiently.\nSašo Jakljevič, a former student of the Faculty of Computer and Information Science at University of Ljubljana, created the following motivational videos for his graduation thesis. He used two belowed datasets, iris and zoo, to showcase how to tackle real-life problems with Orange.\n    ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Why is Orange so great? Because it helps people solve problems quickly and efficiently.\nSašo Jakljevič, a former student of the Faculty of Computer and Information Science at University of Ljubljana, created the following motivational videos for his graduation thesis. He used two belowed datasets, iris and zoo, to showcase how to tackle real-life problems with Orange.\n    " ,
	"author" : "AJDA",
	"summary" : "Why is Orange so great? Because it helps people solve problems quickly and efficiently.\nSašo Jakljevič, a former student of the Faculty of Computer and Information Science at University of Ljubljana, created the following motivational videos for his graduation thesis. He used two belowed datasets, iris and zoo, to showcase how to tackle real-life problems with Orange.\n    ",
	"date" : "Mar 9, 2017"
}

    
    , {
    "uri": "/blog/2017/03/08/workshop-on-infraorange/",
	"title": "Workshop on InfraOrange",
	"categories": ["infraorange", "infrared spectra", "interactive data visualization", "orange3", "workshop"],
	"description": "",
	"content": "Thanks to the collaboration with synchrotrons Elettra (Trieste) and Soleil (Paris), Orange is getting an add-on InfraOrange, with widgets for analysis of infrared spectra. Its primary users obviously come from these two institutions, hence we organized the first workshop for InfraOrange at one of them.\nSome 20 participants spent the first day of the workshop in Trieste learning the basics of Orange and its use for data mining. With Janez at the helm and Marko assisting in the back, we traversed the standard list of visual and statistical techniques and a bit of unsupervised and supervised learning. The workshop was perhaps a bit unusual as most attendees were already quite familiar with these methods, but most haven\u0026rsquo;t yet used them in such an interactive fashion.\nMarko explaining how to analyze spectral data with Orange.\nOn the second day Marko and Andrej took over and focused on the analysis of spectral data. We demonstrated the use of widgets specifically developed for infrared data and used them with data mining techniques we covered on the first day. After lunch the attendees tried to work on their own data sets, which was a real test for InfraOrange.\nOrange for spectral data.\nGroup photo!\nWe now have a lot of realistic feedback on what to improve. There is a lot of work to do still, but a week after the workshop the most often occurring bugs have already been fixed.\nThe future of InfraOrange looks bright and\u0026hellip;. khm\u0026hellip; well, colorful! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Thanks to the collaboration with synchrotrons Elettra (Trieste) and Soleil (Paris), Orange is getting an add-on InfraOrange, with widgets for analysis of infrared spectra. Its primary users obviously come from these two institutions, hence we organized the first workshop for InfraOrange at one of them.\nSome 20 participants spent the first day of the workshop in Trieste learning the basics of Orange and its use for data mining. With Janez at the helm and Marko assisting in the back, we traversed the standard list of visual and statistical techniques and a bit of unsupervised and supervised learning." ,
	"author" : "AJDA",
	"summary" : "Thanks to the collaboration with synchrotrons Elettra (Trieste) and Soleil (Paris), Orange is getting an add-on InfraOrange, with widgets for analysis of infrared spectra. Its primary users obviously come from these two institutions, hence we organized the first workshop for InfraOrange at one of them.\nSome 20 participants spent the first day of the workshop in Trieste learning the basics of Orange and its use for data mining. With Janez at the helm and Marko assisting in the back, we traversed the standard list of visual and statistical techniques and a bit of unsupervised and supervised learning.",
	"date" : "Mar 8, 2017"
}

    
    , {
    "uri": "/blog/2017/03/06/luxembourg-pavia-ljubljana/",
	"title": "Orange Workshops: Luxembourg, Pavia, Ljubljana",
	"categories": ["bioinformatics", "education", "embedding", "orange3", "workshop"],
	"description": "",
	"content": "February was a month of Orange workshops.\nLjubljana: Biologists We (Tomaž, Martin and I) have started in Ljubljana with a hands-on course for the COST Action FA1405 Systems Biology Training School. This was a four hour workshop with an introduction to classification and clustering, and then with application of machine learning to analysis of gene expression data on a plant called Arabidopsis. The organization of this course has even inspired us for a creation of a new widget GOMapMan Ontology that was added to Bioinformatics add-on. We have also experimented with workflows that combine gene expressions and images of mutant. The idea was to find genes with similar expression profile, and then show images of the plants for which these genes have stood out.\nLuxembourg: Statisticians This workshop took place at STATEC, Luxembourgh\u0026rsquo;s National Institute of Statistics and Economic Studies. We (Anže and I) got invited by Nico Weydert, STATEC\u0026rsquo;s deputy director, and gave a two day lecture on machine learning and data mining to a room full of experienced statisticians. While the purpose was to showcase Orange as a tool for machine learning, we have learned a lot from participants of the course: the focus of machine learning is still different from that of classical statistics.\nStatisticians at STATEC, like all other statisticians, I guess, value, above all, understanding of the data, where accuracy of the models does not count if it cannot be explained. Machine learning often sacrifices understanding for accuracy. With focus on data and model visualization, Orange positions itself somewhere in between, but after our Luxembourg visit we are already planning on new widgets for explanation of predictions.\nPavia: Engineers About fifty engineers of all kinds at University of Pavia. Few undergrads, then mostly graduate students, some postdocs and even quite a few of the faculty staff have joined this two day course. It was a bit lighter that the one in Luxembourg, but also covered essentials of machine learning: data management, visualization and classification with quite some emphasis on overfitting on the first day, and then clustering and data projection on the second day. We finished with a showcase on image embedding and analysis. I have in particular enjoyed this last part of the workshop, where attendees were asked to grab a set of images and use Orange to find if they can cluster or classify them correctly. They were all kinds of images that they have gathered, like flowers, racing cars, guitars, photos from nature, you name it, and it was great to find that deep learning networks can be such good embedders, as most students found that machine learning on their image sets works surprisingly well.\nRelated: BDTN 2016 Workshop on introduction to data science\nRelated: Data mining course at Baylor College of Medicine\nWe thank Riccardo Bellazzi, an organizer of Pavia course, for inviting us. Oh, yeah, the pizza at Rossopommodoro was great as always, though Michella\u0026rsquo;s pasta al pesto e piselli back at Riccardo\u0026rsquo;s home was even better.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "February was a month of Orange workshops.\nLjubljana: Biologists We (Tomaž, Martin and I) have started in Ljubljana with a hands-on course for the COST Action FA1405 Systems Biology Training School. This was a four hour workshop with an introduction to classification and clustering, and then with application of machine learning to analysis of gene expression data on a plant called Arabidopsis. The organization of this course has even inspired us for a creation of a new widget GOMapMan Ontology that was added to Bioinformatics add-on." ,
	"author" : "BLAZ",
	"summary" : "February was a month of Orange workshops.\nLjubljana: Biologists We (Tomaž, Martin and I) have started in Ljubljana with a hands-on course for the COST Action FA1405 Systems Biology Training School. This was a four hour workshop with an introduction to classification and clustering, and then with application of machine learning to analysis of gene expression data on a plant called Arabidopsis. The organization of this course has even inspired us for a creation of a new widget GOMapMan Ontology that was added to Bioinformatics add-on.",
	"date" : "Mar 6, 2017"
}

    
    , {
    "uri": "/blog/2017/02/23/my-first-orange-widget/",
	"title": "My First Orange Widget",
	"categories": ["examples", "orange3", "programming", "widget"],
	"description": "",
	"content": "Recently, I took on a daunting task - programming my first widget. I\u0026rsquo;m not a programmer or a computer science grad, but I\u0026rsquo;ve been looking at Orange code for almost two years now and I thought I could handle it.\nI set to create a simple Concordance widget that displays word contexts in a corpus (the widget will be available in the future release). The widget turned out to be a little more complicated than I originally anticipated, but it was a great exercise in programming.\nToday, I\u0026rsquo;ll explain how I got started with my widget development. We will create a very basic Word Finder widget, that just goes through the corpus (data) and tells you whether a word occurs in a corpus or not. This particular widget is meant to be a part of the Orange3-Text add-on (so you need the add-on installed to try it), but the basic structure is the same for all widgets.\nFirst, I have to set the basic widget class.\nclass OWWordFinder(OWWidget): name = \u0026quot;Word Finder\u0026quot; description = \u0026quot;Display whether a word is in a text or not.\u0026quot; icon = \u0026quot;icons/WordFinder.svg\u0026quot; priority = 1 inputs = [('Corpus', Table, 'set_data')] # This widget will have no output, but in case you want one, you define it as below. # outputs = [('Output Name', output_type, 'output_method')] want_control_area = False  This sets up the description of the widget, icon, inputs and so on. want_control_area is where we say we only want the main window. Both are on by default in Orange and this simply hides the empty control area on the widget\u0026rsquo;s left side. If your widget has any parameters and controls, leave the control area on and place the buttons there.\nIn __init__ we define widget properties (such as data and queried word) and set the view. I decided to go with a very simple design - I just put everything in the mainArea. For such a basic widget this might be ok, but otherwise you might want to dig deeper into models and use QTableView, QGraphicsScene or something similar. Here we will build just the bare bones of a functioning widget.\ndef __init__(self): super().__init__() self.corpus = None # input data self.word = \u0026quot;\u0026quot; # queried word # setting the gui gui.widgetBox(self.mainArea, orientation=\u0026quot;vertical\u0026quot;) self.input = gui.lineEdit(self.mainArea, self, '', orientation=\u0026quot;horizontal\u0026quot;, label='Query:') self.input.setFocus() # run method self.search on every text change self.input.textChanged.connect(self.search) # place a text label in the mainArea self.view = QLabel() self.mainArea.layout().addWidget(self.view)  Ok, this now sets the __init__: what the widget remembers and how it looks like. With our buttons in place, the widget needs some methods, too.\nThe first method will update the self.corpus attribute, when the widget receives an input.\ndef set_data(self, data=None): if data is not None and not isinstance(data, Corpus): self.corpus = Corpus.from_table(data.domain, data) self.corpus = data self.search()  At the end we called self.search() method, which we already met in __init__ above. This method is key to our widget, as it will run the search every time the word changes. Moreover, it will run the method on the same query word when the widget is provided with a new data set, which is why we set it also in set_data().\nOk, let\u0026rsquo;s finally write this method.\ndef search(self): self.word = self.input.text() # self.corpus.tokens will run a default tokenizer, if no tokens are provided on the input result = any(self.word in doc for doc in self.corpus.tokens) self.view.setText(str(result))  This is it. This is our widget. Good job. Creating a new widget can indeed be lot of fun. You can go from a quite basic widget to very intricate, depending on your sense of adventure.\nFinally, you can get the entire widget code in gist.\nHappy programming, everyone! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Recently, I took on a daunting task - programming my first widget. I\u0026rsquo;m not a programmer or a computer science grad, but I\u0026rsquo;ve been looking at Orange code for almost two years now and I thought I could handle it.\nI set to create a simple Concordance widget that displays word contexts in a corpus (the widget will be available in the future release). The widget turned out to be a little more complicated than I originally anticipated, but it was a great exercise in programming." ,
	"author" : "AJDA",
	"summary" : "Recently, I took on a daunting task - programming my first widget. I\u0026rsquo;m not a programmer or a computer science grad, but I\u0026rsquo;ve been looking at Orange code for almost two years now and I thought I could handle it.\nI set to create a simple Concordance widget that displays word contexts in a corpus (the widget will be available in the future release). The widget turned out to be a little more complicated than I originally anticipated, but it was a great exercise in programming.",
	"date" : "Feb 23, 2017"
}

    
    , {
    "uri": "/blog/2017/02/03/for-when-you-want-to-transpose-a-data-table/",
	"title": "For When You Want to Transpose a Data Table...",
	"categories": ["analysis", "bioinformatics", "feature engineering", "features", "orange3"],
	"description": "",
	"content": "Sometimes, you need something more. Something different. Something, that helps you look at the world from a different perspective. Sometimes, you simply need to transpose your data.\nSince version 3.3.9, Orange has a Transpose widget that flips your data table around. Columns become rows and rows become columns. This is often useful, if you have, say, biological data.\nRelated: Datasets in Orange Bioinformatics\nToday we will play around with brown-selected.tab, a data set on gene expression levels for 79 experiments. Our data table has genes in rows and experiments in columns, with gene expression levels recorded as values.\nThis representation focuses on exploring genes and allows them to be plotted or to construct a model to predict their functions. But what if I want to explore the experimental conditions and see how different external conditions influence the yeast cells? For this, it would be better to have experiments in rows and genes in columns. We can do this with Transpose.\nTranspose widget took our gene meta attribute and used it for the new column names (YGR270W, YIL075C, etc.). It also appended class values to columns (Proteas). Former columns names (alpha 0, alpha 7, etc.) became our new meta attribute called Feature name.\nOk, we have a transposed data table. Now we ask ourselves: \u0026ldquo;Do similar experiment types (e.g. heat, cold, alpha, \u0026hellip;) behave similarly?\u0026rdquo;\nLet\u0026rsquo;s use PCA to transform these many-dimensional experiment vectors into a 2-D representation. We are going to use Scatter Plot to observe experiments (not genes) in a 2-D space. We expect the same experiment types to lie closer together than other experiments. A scatter plot after a 2-D transformation looks like this:\nSpo5 11 lies quite far from the rest, so it could be an experiment to look out for. If we zoom in on the big cluster, we see that similar experiments indeed lie closer together.\nNow, if you are reproducing the result, you probably won\u0026rsquo;t see these nice colors for class.\nThis is because we used the Create Class widget to help us create new class values. Create Class already available in Orange3-Prototypes add-on and will be included in a future Orange release. You can learn more about it soon\u0026hellip; :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Sometimes, you need something more. Something different. Something, that helps you look at the world from a different perspective. Sometimes, you simply need to transpose your data.\nSince version 3.3.9, Orange has a Transpose widget that flips your data table around. Columns become rows and rows become columns. This is often useful, if you have, say, biological data.\nRelated: Datasets in Orange Bioinformatics\nToday we will play around with brown-selected.tab, a data set on gene expression levels for 79 experiments." ,
	"author" : "AJDA",
	"summary" : "Sometimes, you need something more. Something different. Something, that helps you look at the world from a different perspective. Sometimes, you simply need to transpose your data.\nSince version 3.3.9, Orange has a Transpose widget that flips your data table around. Columns become rows and rows become columns. This is often useful, if you have, say, biological data.\nRelated: Datasets in Orange Bioinformatics\nToday we will play around with brown-selected.tab, a data set on gene expression levels for 79 experiments.",
	"date" : "Feb 3, 2017"
}

    
    , {
    "uri": "/blog/2017/01/23/preparing-scraped-data/",
	"title": "Preparing Scraped Data",
	"categories": ["addons", "analysis", "data", "dataloading", "examples", "python", "scripting"],
	"description": "",
	"content": "One of the key questions of every data analysis is how to get the data and put it in the right form(at). In this post I\u0026rsquo;ll show you how to easily get the data from the web and transfer it to a file Orange can read.\nRelated: Creating a new data table in Orange through Python\nFirst, we\u0026rsquo;ll have to do some scripting. We\u0026rsquo;ll use a couple of Python libraries - urllib.requests fetching the data, BeautifulSoup for reading it, csv for writing it and regular expressions for extracting the right data.\nfrom urllib.request import urlopen from bs4 import BeautifulSoup import csv import re  Ok, we\u0026rsquo;ve imported the all the libraries we\u0026rsquo;ll need. Now we will scrape the data from our own blog to see how many posts we\u0026rsquo;ve written throughout the years.\nhtml = urlopen('http://blog.biolab.si') soup = BeautifulSoup(html.read(), 'lxml')  The first line opens the address of the site we want to scrape. In our case this is our blog. The second line retrieves a html response from the site, which is our raw text. It looks like this:\n\u0026lt;aside id=\u0026quot;archives-2\u0026quot; class=\u0026quot;widget widget_archive\u0026quot;\u0026gt;\u0026lt;h3 class=\u0026quot;widget-title\u0026quot;\u0026gt;Archives\u0026lt;/h3\u0026gt; \u0026lt;ul\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2017/01/'\u0026gt;January 2017\u0026lt;/a\u0026gt;\u0026amp;nbsp;(1)\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2016/12/'\u0026gt;December 2016\u0026lt;/a\u0026gt;\u0026amp;nbsp;(3)\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2016/11/'\u0026gt;November 2016\u0026lt;/a\u0026gt;\u0026amp;nbsp;(4)\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2016/10/'\u0026gt;October 2016\u0026lt;/a\u0026gt;\u0026amp;nbsp;(3)\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2016/09/'\u0026gt;September 2016\u0026lt;/a\u0026gt;\u0026amp;nbsp;(2)\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2016/08/'\u0026gt;August 2016\u0026lt;/a\u0026gt;\u0026amp;nbsp;(5)\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;\u0026lt;a href='http://blog.biolab.si/2016/07/'\u0026gt;July 2016\u0026lt;/a\u0026gt;\u0026amp;nbsp;(3)\u0026lt;/li\u0026gt;....  Ok, html is nice, but we can\u0026rsquo;t really do data analysis with this. We will have to transform this output into something sensible. How about .csv, a simple comma-demilited format Orange can recognize?\nwith open('scraped.csv', 'w') as csvfile: csvwriter = csv.writer(csvfile, delimiter=',')  We created a new file called \u0026lsquo;scraped.csv\u0026rsquo; to which we will write our content (\u0026lsquo;w\u0026rsquo; parameter means write). Then we defined the writer and set the delimiter to comma.\nNow we need to add the header row, so Orange will know what are the column names. We add this just after csvwriter variable.\ncsvwriter.writerow([\u0026quot;Date\u0026quot;, \u0026quot;No_of_Blogs\u0026quot;])  Now we have two columns, one named \u0026lsquo;Date\u0026rsquo; and the other \u0026lsquo;No_of_Blogs\u0026rsquo;. The final step is to extract the data. We have a bunch of lines in html, but the one we\u0026rsquo;re interested in is in a section \u0026lsquo;aside\u0026rsquo; and has an id \u0026lsquo;archives-2\u0026rsquo;. We will first extract only this section (.find(id=\u0026lsquo;archives-2\u0026rsquo;) and get all the lines of the archive with the tag \u0026lsquo;li\u0026rsquo; (.find_all(\u0026lsquo;li\u0026rsquo;)):\nfor item in soup.find(id=\u0026quot;archives-2\u0026quot;).find_all('li'):  This is the result of print(item).\n\u0026lt;li\u0026gt;\u0026lt;a href=\u0026quot;http://blog.biolab.si/2017/01/\u0026quot;\u0026gt;January 2017\u0026lt;/a\u0026gt; (1)\u0026lt;/li\u0026gt;  Now we need to get the actual content from each line. The first part we need is the date of the archived content. Orange can read dates, but they need to come in the right format. We will extract the date from href part with item.a.get(\u0026lsquo;href\u0026rsquo;). Then we need to extract only digits from it as we\u0026rsquo;re not interested in the rest of the link. We do this with Regex for finding digits:\ndate = re.findall(r'\\d+', item.a.get('href'))  Regex\u0026rsquo;s findall function returns a list, in our case containing two items - the year and month of archived content. The second part of our data is the number of blogs archived in a particular month. We will again extract this with a Regex digit search, but this time we will be extracting data from the actual content - \u0026lsquo;item.contents[1]\u0026rsquo;.\ndigits = re.findall(r'\\d+', item.contents[1])  Finally, we need to write each line to a .csv file we created above.\ncsvwriter.writerow([\u0026quot;%s-%s-01\u0026quot; % (date[0], date[1]), digits[0]])  Here, we formatted the date into an ISO-standard format Orange recognizes as time variable (\u0026quot;%s-%s-01\u0026quot; % (date[0], date[1])), while the second part is simply a count of our blog posts.\nThis is the entire code:\nfrom urllib.request import urlopen from bs4 import BeautifulSoup import csv import re html = urlopen('http://blog.biolab.si') soup = BeautifulSoup(html.read(), 'lxml') with open('scraped.csv', 'w') as csvfile: csvwriter = csv.writer(csvfile, delimiter=',') csvwriter.writerow([\u0026quot;Date\u0026quot;, \u0026quot;No_of_Blogs\u0026quot;]) for item in soup.find(id=\u0026quot;archives-2\u0026quot;).find_all('li'): date = re.findall(r'\\d+', item.a.get('href')) digits = re.findall(r'\\d+', item.contents[1]) csvwriter.writerow([\u0026quot;%s-%s-01\u0026quot; % (date[0], date[1]), digits[0]])  Related: Scripting with Time Variable\nNow let\u0026rsquo;s load this in Orange. File widget can easily read .csv formats and also correctly identifies the two column types, datetime and numeric. A quick glance into the Data Table\u0026hellip;\nEverything looks ok. We can use Timeseries add-on to inspect how many blogs we\u0026rsquo;ve written each month since 2010. Connect As Timeseries widget to File. Orange will automatically suggest to use Date as our time variable. Finally, we\u0026rsquo;ll plot the data with Line Chart. This is the curve of our blog activity.\nThe example is extremely simple. A somewhat proficient user can extract much more interesting data than a simple blog count, but one always needs to keep in mind the legal aspects of web scraping. Nevertheless, this is a popular and fruitful way to extract and explore the data!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One of the key questions of every data analysis is how to get the data and put it in the right form(at). In this post I\u0026rsquo;ll show you how to easily get the data from the web and transfer it to a file Orange can read.\nRelated: Creating a new data table in Orange through Python\nFirst, we\u0026rsquo;ll have to do some scripting. We\u0026rsquo;ll use a couple of Python libraries - urllib." ,
	"author" : "AJDA",
	"summary" : "One of the key questions of every data analysis is how to get the data and put it in the right form(at). In this post I\u0026rsquo;ll show you how to easily get the data from the web and transfer it to a file Orange can read.\nRelated: Creating a new data table in Orange through Python\nFirst, we\u0026rsquo;ll have to do some scripting. We\u0026rsquo;ll use a couple of Python libraries - urllib.",
	"date" : "Jan 23, 2017"
}

    
    , {
    "uri": "/blog/2017/01/13/data-preparation-for-machine-learning/",
	"title": "Data Preparation for Machine Learning",
	"categories": ["analysis", "business intelligence", "data", "feature engineering", "preprocessing"],
	"description": "",
	"content": "We\u0026rsquo;ve said it numerous times and we\u0026rsquo;re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there\u0026rsquo;s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly.\nOrange is well equipped for such tasks.\nWidget no. 1: Preprocess\nPreprocess is there to handle a big share of your preprocessing tasks.\n It can normalize numerical data variables. Say we have a fictional data set of people employed in your company. We want to know which employees are more likely to go on holiday, based on the yearly income, years employed in your company and total years of experience in the industry. If you plot this in heat map, you would see a bold yellow line at \u0026lsquo;yearly income\u0026rsquo;. This obviously happens because yearly income has much higher values than years of experience or years employed by your company. You would naturally like the wage not to overweight the rest of the feature set, so normalization is the way to go. Normalization will transform your values to relative terms, that is, say (depending on the type of normalization) on a scale from 0 to 1. Now Heat Map neatly shows that people who\u0026rsquo;ve been employed longer and have a higher wage more often go on holidays. (Yes, this is a totally fictional data set, but you see the point.)           no normalization normalized data     It can impute missing values. Average or most frequent missing value imputation might seem as overly simple, but it actually works most of the time. Also, all the learners that require imputation do it implicitly, so the user doesn\u0026rsquo;t have to configure yet another widget for that. If you want to compare your results against a randomly mixed data set, select \u0026lsquo;Randomize\u0026rsquo; or if you want to select relevant features, this is the widget for it.  Preprocessing needs to be used with caution and understanding of your data to avoid losing important information or, worse, overfitting the model. A good example is a case of paramedics, who usually don\u0026rsquo;t record pulse if it is normal. Missing values here thus cannot be imputed by an average value or random number, but as a distinct value (normal pulse). Domain knowledge is always crucial for data preparation.\nWidget no. 2: Discretize\nFor certain tasks you might want to resort to binning, which is what Discretize does. It effectively distributes your continuous values into a selected number of bins, thus making the variable discrete-like. You can either discretize all your data variables at once, using selected discretization type, or select a particular discretization method for each attribute. The cool thing is the transformation is already displayed in the widget, so you instantly know what you\u0026rsquo;re getting in the end. A good example of discretization would be having a data set of your customers with their age recorded. It would make little sense to segment customers by each particular age, so binning them into 4 age groups (young, young-adult, middle-aged, senior) would be a great solution. Also some visualizations require feature transformation - Sieve Diagram is currently one such widget. Mosaic Display, however, has the transformation already implemented internally.\noriginal data\nDiscretized data with \u0026lsquo;years employed\u0026rsquo; lower or higher then/equal to 8 (same for \u0026lsquo;yearly income\u0026rsquo; and \u0026lsquo;experience in the industry\u0026rsquo;.\nWidget no. 3: Continuize\nThis widget essentially creates new attributes out of your discrete ones. If you have, for example, an attribute with people\u0026rsquo;s eye color, where values can be either blue, brown or green, you would probably want to have three separate attributes \u0026lsquo;blue\u0026rsquo;, \u0026lsquo;green\u0026rsquo; and \u0026lsquo;brown\u0026rsquo; with 0 or 1 if a person has that eye color. Some learners perform much better if data is transformed in such a way. You can also only have attributes where you would presume 0 is a normal condition and would only like to have deviations from the normal state recorded (\u0026lsquo;target or first value as base\u0026rsquo;) or the normal state would be the most common value (\u0026lsquo;most frequent value as base\u0026rsquo;). Continuize widget offers you a lot of room to play. Best thing is to select a small data set with discrete values, connect it to Continuize and then further to Data Table and change the parameters. This is how you can observe the transformations in real time. It is useful for projecting discrete data points in Linear Projection.\nOriginal data.\nContinuized data with two new columns - attribute \u0026lsquo;position\u0026rsquo; was replaced by attributes \u0026lsquo;position=office worker\u0026rsquo; and \u0026lsquo;position=technical staff\u0026rsquo; (same for \u0026lsquo;gender\u0026rsquo;).\nWidget no. 4: Purge Domain\nGet a broom and sort your data! That\u0026rsquo;s what Purge Domain does. If all of the values of some attributes are constant, it will remove these attributes. If you have unused (empty) attributes in your data, it will remove them. Effectively, you will get a nice and comprehensive data set in the end.\nOriginal data.\nEmpty columns and columns with the same (constant) value were removed.\nOf course, don\u0026rsquo;t forget to include all these procedures into your report with the \u0026lsquo;Report\u0026rsquo; button! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We\u0026rsquo;ve said it numerous times and we\u0026rsquo;re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there\u0026rsquo;s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve said it numerous times and we\u0026rsquo;re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there\u0026rsquo;s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly.",
	"date" : "Jan 13, 2017"
}

    
    , {
    "uri": "/blog/2016/12/22/the-beauty-of-random-forest/",
	"title": "The Beauty of Random Forest",
	"categories": ["classification", "interactive data visualization", "orange3", "regression", "tree", "visualization"],
	"description": "",
	"content": "It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange. And even more beautiful then a single tree is Orange\u0026rsquo;s rendering of a forest, that is, a random forest.\nRelated: Pythagorean Trees and Forests\nHere are six trees in the random forest constructed on the housing data set: The random forest for annealing data set includes a set of smaller-sized trees: A Christmas-lit random forest inferred from pen digits data set looks somehow messy in trying to categorize to ten different classes: The power of beauty! No wonder random forests are one of the best machine learning tools. Orange renders them according to the idea of Fabian Beck and colleagues who proposed Pythagoras trees for visualizations of hierarchies. The actual implementation for classification and regression trees for Orange was created by Pavlin Policar.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange." ,
	"author" : "BLAZ",
	"summary" : "It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange.",
	"date" : "Dec 22, 2016"
}

    
    , {
    "uri": "/blog/2016/12/16/bdtn-2016-workshop-introduction-to-data-science/",
	"title": "BDTN 2016 Workshop: Introduction to Data Science",
	"categories": ["education", "interactive data visualization", "tutorial", "workshop"],
	"description": "",
	"content": "Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.\nRelated: Intro to Data Mining for Life Scientists\nThe task was to teach and explain basic data mining concepts and techniques in four hours. To complete beginners. Not daunting at all\u0026hellip;\nLuckily, we had Orange at hand. First, we showed how the program works and how to easily import data into the software. We created a poll using Google Forms on the fly and imported the results from Google Sheets into Orange.\nTo get the first impression of our data, we used Distributions and Scatter Plot. This was just to show how to approach the construction and simple visual exploration on any new data set. Then we delved deep into the workings of classification with Classification Tree and Tree Viewer and showed how easy it is to fall into the trap of overfitting (and how to avoid it). Another topic was clustering and how to relate similar data instances to one another. Finally, we had some fun with ImageAnalytics add-on and observed whether we can detect wrongly labelled microscopy images with machine learning.\nRelated: Data Mining Course in Houston #2\nThese workshops are not only fun, but an amazing learning opportunity for us as well, as they show how our users think and how to even further improve Orange.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.\nRelated: Intro to Data Mining for Life Scientists\nThe task was to teach and explain basic data mining concepts and techniques in four hours." ,
	"author" : "AJDA",
	"summary" : "Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.\nRelated: Intro to Data Mining for Life Scientists\nThe task was to teach and explain basic data mining concepts and techniques in four hours.",
	"date" : "Dec 16, 2016"
}

    
    , {
    "uri": "/blog/2016/12/12/dimensionality-reduction-by-manifold-learning/",
	"title": "Dimensionality Reduction by Manifold Learning",
	"categories": ["analysis", "embedding", "examples", "interactive data visualization", "orange3", "unsupervised", "visualization", "widget"],
	"description": "",
	"content": "The new Orange release (v. 3.3.9) welcomed a few wonderful additions to its widget family, including Manifold Learning widget. The widget reduces the dimensionality of the high-dimensional data and is thus wonderful in combination with visualization widgets.\nManifold Learning widget has a simple interface with powerful features.\nManifold Learning widget offers five embedding techniques based on scikit-learn library: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding. They each handle the mapping differently and also have a specific set of parameters.\nRelated: Principal Component Analysis (video)\nFor example, a popular t-SNE requires only a metric (e.g. cosine distance). In the demonstration of this widget, we output 2 components, since they are the easiest to visualize and make sense of.\nFirst, let\u0026rsquo;s load the data and open it in Scatter Plot. Not a very informative visualization, right? The dots from an unrecognizable square in 2D.\nS-curve data in Scatter Plot. Data points form an uninformative square.\nLet\u0026rsquo;s use embeddings to make things a bit more informative. This is how the data looks like with a t-SNE embedding. The data is starting to have a shape and the data points colored according to regression class reveal a beautiful gradient.\nt-SNE embedding shows an S shape of the data.\nOk, how about MDS? This is beyond our expectations!\nThere\u0026rsquo;s a plethora of options with embeddings. You can play around with ImageNet embeddings and plot them in 2D or use any of your own high-dimensional data and discover interesting visualizations! Although t-SNE is nowadays probably the most popular dimensionality reduction technique used in combination with scatterplot visualization, do not underestimate the value of other manifold learning techniques. For one, we often find that MDS works fine as well.\nGo, experiment!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "The new Orange release (v. 3.3.9) welcomed a few wonderful additions to its widget family, including Manifold Learning widget. The widget reduces the dimensionality of the high-dimensional data and is thus wonderful in combination with visualization widgets.\nManifold Learning widget has a simple interface with powerful features.\nManifold Learning widget offers five embedding techniques based on scikit-learn library: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding. They each handle the mapping differently and also have a specific set of parameters." ,
	"author" : "AJDA",
	"summary" : "The new Orange release (v. 3.3.9) welcomed a few wonderful additions to its widget family, including Manifold Learning widget. The widget reduces the dimensionality of the high-dimensional data and is thus wonderful in combination with visualization widgets.\nManifold Learning widget has a simple interface with powerful features.\nManifold Learning widget offers five embedding techniques based on scikit-learn library: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding. They each handle the mapping differently and also have a specific set of parameters.",
	"date" : "Dec 12, 2016"
}

    
    , {
    "uri": "/blog/2016/11/30/data-mining-for-political-scientists/",
	"title": "Data Mining for Political Scientists",
	"categories": ["analysis", "classification", "education", "examples", "orange3", "prediction", "predictive analytics", "preprocessing", "text mining", "tutorial", "widget"],
	"description": "",
	"content": "Being a political scientist, I did not even hear about data mining before I\u0026rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!\nBut hold on! There are still many social scientists out there who haven\u0026rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning. So I\u0026rsquo;ve made it my mission to spread the word. And that was the spirit that led me back to my former university - School of Political Sciences, University of Bologna.\nUniversity of Bologna is the oldest university in the world and has one of the best departments for political sciences in Europe. I held a lecture Digital Research - Data Mining for Political Scientists for MIREES students, who are specializing in research and studies in Central and Eastern Europe.\nLecture at University of Bologna\nThe main goal of the lecture was to lay out the possibilities that contemporary technology offers to researchers and to showcase a few simple text mining tasks in Orange. We analysed Trump\u0026rsquo;s and Clinton\u0026rsquo;s Twitter timeline and discovered that their tweets are highly distinct from one another and that you can easily find significant words they\u0026rsquo;re using in their tweets. Moreover, we\u0026rsquo;ve discovered that Trump is much better at social media than Clinton, creating highly likable and shareable content and inventing his own hashtags. Could that be a tell-tale sign of his recent victory?\nPerhaps. Our future, data-mining savvy political scientists will decide. Below, you can see some examples of the workflows presented at the workshop.\nAuthor predictions from Tweet content. Logistic Regression reports on 92% classification accuracy and AUC score. Confusion Matrix can output misclassified tweets to Corpus Viewer, where we can inspect these tweets further.\nWord Cloud from preprocessed tweets. We removed stopwords and punctuation to find frequencies for meaningful words only.\nWord Enrichment by Author. First we find Donald\u0026rsquo;s tweets with Select Rows and then compare them to the entire corpus in Word Enrichment. The widget outputs a ranked list of significant words for the provided subset. We do the same for Hillary\u0026rsquo;s tweets.\nFinding potential topics with LDA.\nFinally, we offered a sneak peek of our recent Tweet Profiler widget. Tweet Profiler is intended for sentiment analysis of tweets and can output classes. probabilities and embeddings. The widget is not yet officially available, but will be included in the upcoming release.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Being a political scientist, I did not even hear about data mining before I\u0026rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!\nBut hold on! There are still many social scientists out there who haven\u0026rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning." ,
	"author" : "AJDA",
	"summary" : "Being a political scientist, I did not even hear about data mining before I\u0026rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!\nBut hold on! There are still many social scientists out there who haven\u0026rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning.",
	"date" : "Nov 30, 2016"
}

    
    , {
    "uri": "/blog/2016/11/25/celebrity-lookalike-or-how-to-make-students-love-machine-learning/",
	"title": "Celebrity Lookalike or How to Make Students Love Machine Learning",
	"categories": ["education", "images", "interactive data visualization", "orange3"],
	"description": "",
	"content": "Recently we\u0026rsquo;ve been participating at Days of Computer Science, organized by the Museum of Post and Telecommunications and the Faculty of Computer and Information Science, University of Ljubljana, Slovenia. The project brought together pupils and students from around the country and hopefully showed them what computer science is mostly about. Most children would think programming is just typing lines of code. But it\u0026rsquo;s more than that. It\u0026rsquo;s a way of thinking, a way to solve problems creatively and efficiently. And even better, computer science can be used for solving a great variety of problems.\nRelated: On teaching data science with Orange\nOrange team has prepared a small demo project called Celebrity Lookalike. We found 65 celebrity photos online and loaded them in Orange. Next we cropped photos to faces and turned them black and white, to avoid bias in background and color. Next we inferred embeddings with ImageNet widget and got 2048 features, which are the penultimate result of the ImageNet neural network.\nWe find faces in photos and turn them to black and white. This eliminates the effect of the background and distinct colors for embeddings.\nStill, we needed a reference photo to find the celebrity lookalike for. Students could take a selfie and similarly extracted black and white face out of it. Embeddings were computed and sent to Neighbors widget. Neighbors finds n closest neighbors based on the defined distance measure to the provided reference. We decided to output 10 closest neighbors by cosine distance.\nCelebrity Lookalike workflow. We load photos, find faces and compute embeddings. We do the same for our Webcam Capture. Then we find 10 closest neighbors and observe the results in Lookalike widget.\nFinally, we used Lookalike widget to display the result. Students found it hilarious when curly boys were the Queen of England and girls with glasses Steve Jobs. They were actively trying to discover how the algorithm works by taking photo of a statue, person with or without glasses, with hats on or by making a funny face.\nHopefully this inspires a new generation of students to become scientists, researchers and to actively find solutions to their problems. Coding or not. :)\n_Note: _Most widgets we have designed for this projects (like Face Detector, Webcam Capture, and Lookalike) are available in Orange3-Prototypes and are not actively maintained. They can, however, be used for personal projects and sheer fun. Orange does not own the copyright of the images.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Recently we\u0026rsquo;ve been participating at Days of Computer Science, organized by the Museum of Post and Telecommunications and the Faculty of Computer and Information Science, University of Ljubljana, Slovenia. The project brought together pupils and students from around the country and hopefully showed them what computer science is mostly about. Most children would think programming is just typing lines of code. But it\u0026rsquo;s more than that. It\u0026rsquo;s a way of thinking, a way to solve problems creatively and efficiently." ,
	"author" : "AJDA",
	"summary" : "Recently we\u0026rsquo;ve been participating at Days of Computer Science, organized by the Museum of Post and Telecommunications and the Faculty of Computer and Information Science, University of Ljubljana, Slovenia. The project brought together pupils and students from around the country and hopefully showed them what computer science is mostly about. Most children would think programming is just typing lines of code. But it\u0026rsquo;s more than that. It\u0026rsquo;s a way of thinking, a way to solve problems creatively and efficiently.",
	"date" : "Nov 25, 2016"
}

    
    , {
    "uri": "/blog/2016/11/18/top-100-changemakers-in-central-and-eastern-europe/",
	"title": "Top 100 Changemakers in Central and Eastern Europe",
	"categories": ["article", "orange3"],
	"description": "",
	"content": "Recently Orange and one of its inventors, Blaž Zupan, have been recognized as one of the top 100 changemakers in the region. A 2016 New Europe 100 is an annual list of innovators and entrepreneurs in Central and Eastern Europe highlighting novel approaches to pressing problems.\nOrange has been recognized for making data more approachable, which has been our goal from the get-go. The tool is continually being developed with the end user in mind - someone who wants to analyze his/her data quickly, visually, interactively, and efficiently. We\u0026rsquo;re always thinking hard how to expose valuable information in the data, how to improve the user experience, which defaults are the most appropriate for the method, and, finally, how to intuitively teach people about data mining.\nThis nomination is a great validation of our efforts and it only makes us work harder. Because every research should be fruitful and fun!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Recently Orange and one of its inventors, Blaž Zupan, have been recognized as one of the top 100 changemakers in the region. A 2016 New Europe 100 is an annual list of innovators and entrepreneurs in Central and Eastern Europe highlighting novel approaches to pressing problems.\nOrange has been recognized for making data more approachable, which has been our goal from the get-go. The tool is continually being developed with the end user in mind - someone who wants to analyze his/her data quickly, visually, interactively, and efficiently." ,
	"author" : "AJDA",
	"summary" : "Recently Orange and one of its inventors, Blaž Zupan, have been recognized as one of the top 100 changemakers in the region. A 2016 New Europe 100 is an annual list of innovators and entrepreneurs in Central and Eastern Europe highlighting novel approaches to pressing problems.\nOrange has been recognized for making data more approachable, which has been our goal from the get-go. The tool is continually being developed with the end user in mind - someone who wants to analyze his/her data quickly, visually, interactively, and efficiently.",
	"date" : "Nov 18, 2016"
}

    
    , {
    "uri": "/blog/2016/11/02/orange-at-eurostats-big-data-workshop/",
	"title": "Orange at Eurostat&#39;s Big Data Workshop",
	"categories": ["conference", "education", "orange3"],
	"description": "",
	"content": "A Eurostat\u0026rsquo;s Big Data Workshop recently took place in Ljubljana. In a presentation we have showcased Orange as a tool to teach data science.\nThe meeting was organised by Statistical Office of Slovenia and by Eurostat, a Statistical Office of the European Union, and was a primary gathering of representatives from national statistical institutes joined within European Statistical System. The meeting discussed possibilities that big data offers to modern statistics and the role it could play in statistical offices around the world. Say, can one use twitter data to measure costumer satisfaction? Or predict employment rates? Or use traffic information to predict GDP?\nDuring the meeting, Philippe Nieuwbourg from Canada pointed out that the stack of tools for big data analysis, and actually the tool stack for data science, are rather big and are growing larger each day. There is no way that data owners can master data bases, warehouses, Python, R, web development stacks, and similar. Are we alienating the owners and users from their own sources of information?\nOf course not. We were invited to the workshop to show that there are data science tools that can actually connect users and data, and empower the users to explore the data in the ways they have never dreamed before. We claimed that these tools should\n spark the intuition, offer powerful and interactive visualizations, and offer flexibility in design of analysis workflows, say, through visual programming.  Related: Teaching data science with Orange\nWe claimed that with such tools, it takes only a few days to train users to master basic and intermediate concepts of data science. And we claimed that this could be done without diving into complex mathematics and statistics.\nPart of our presentation was a demo in Orange that showed few tricks we use in such training. The presentation included:\n a case study of interactive data exploration by building and visualizing classification tree and forests, and mapping parts of the model to the projection in a scatter plot, a demo how fun it is to draw a data set and then use it to teach about clustering, a presentation how trained deep model can be used to explore and cluster images.  Related: Data Mining Course at Baylor College of Medicine in Houston\nThe Eurostat meeting was very interesting and packed with new ideas. Our thanks to Boro Nikić for inviting us, and thanks to attendees of our session for the many questions and requests we have received during presentation and after the meeting.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "A Eurostat\u0026rsquo;s Big Data Workshop recently took place in Ljubljana. In a presentation we have showcased Orange as a tool to teach data science.\nThe meeting was organised by Statistical Office of Slovenia and by Eurostat, a Statistical Office of the European Union, and was a primary gathering of representatives from national statistical institutes joined within European Statistical System. The meeting discussed possibilities that big data offers to modern statistics and the role it could play in statistical offices around the world." ,
	"author" : "BLAZ",
	"summary" : "A Eurostat\u0026rsquo;s Big Data Workshop recently took place in Ljubljana. In a presentation we have showcased Orange as a tool to teach data science.\nThe meeting was organised by Statistical Office of Slovenia and by Eurostat, a Statistical Office of the European Union, and was a primary gathering of representatives from national statistical institutes joined within European Statistical System. The meeting discussed possibilities that big data offers to modern statistics and the role it could play in statistical offices around the world.",
	"date" : "Nov 2, 2016"
}

    
    , {
    "uri": "/blog/2016/10/17/tips-for-using-orange/",
	"title": "10 Tips and Tricks for Using Orange",
	"categories": ["documentation", "education", "features", "interface", "orange3"],
	"description": "",
	"content": "TIP #1: Follow tutorials and example workflows to get started.\nIt\u0026rsquo;s difficult to start using new software. Where does one start, especially a total novice in data mining? For this exact reason we\u0026rsquo;ve prepared Getting Started With Orange - YouTube tutorials for complete beginners. Example workflows on the other hand can be accessed via Help - Examples.\nTIP #2: Make use of Orange documentation.\nYou can access it in three ways:\n Press F1 when the widget is selected. This will open help screen. Select Widget - Help when the widget is selected. It works the same as above. Visit online documentation.  TIP #3:** Embed your help screen.**\nDrag and drop help screen to the side of your Orange canvas. It will become embedded in the canvas. You can also make it narrower, allowing for a full-size analysis while exploring the docs.\nTIP #4: Use right-click.\nRight-click on the canvas and a widget menu will appear. Start typing the widget you\u0026rsquo;re looking for and press Enter when the widget becomes the top widget. This will place the widget onto the canvas immediately. You can also navigate the menu with up and down.\nTIP #5: Turn off channel names.\nSometimes it is annoying to see channel names above widget links. If you\u0026rsquo;re already comfortable using Orange, you can turn them off in Options - Settings. Turn off \u0026lsquo;Show channel names between widgets\u0026rsquo;.\nTIP #6: Hide control pane.\nOnce you\u0026rsquo;ve set the parameters, you\u0026rsquo;d probably want to focus just on visualizations. There\u0026rsquo;s a simple way to do this in Orange. Click on the split between the control pane and visualization pane - you should see a hand appearing instead of a cursor. Click and observe how the control pane gets hidden away neatly. To make it reappear, click the split again.\nTIP #7: Label your data.\nSo you\u0026rsquo;ve plotted your data, but have no idea what you\u0026rsquo;re seeing. Use annotation! In some widgets you will see a drop-down menu called Annotation, while in others it will be called a Label. This will mark your data points with suitable labels, making your MDS plots and Scatter Plots much more informative. Scatter Plot also enables you to label only selected points for better clarity.\nTIP #8: Find your plot.\nScrolled around and lost the plot? Zoomed in too much? To re-position the plot click \u0026lsquo;Reset zoom\u0026rsquo; and the visualization will jump snugly into the visualization pane. Comes in handy when browsing the subsets and trying to see the bigger picture every now and then.\nTIP #9: Reset widget settings.\nOrange is geared to remember your last settings, thus assisting you in a rapid analysis. However, sometimes you need to start anew. Go to Options - Reset widget settings\u0026hellip; and restart Orange. This will return Orange to its original state.\nTIP #10: Use Educational add-on.\nTo learn about how some algorithms work, use Orange3-Educational add-on. It contains 4 widgets that will help you get behind the scenes of some famous algorithms. And since they\u0026rsquo;re interactive, they\u0026rsquo;re also a lot of fun!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "TIP #1: Follow tutorials and example workflows to get started.\nIt\u0026rsquo;s difficult to start using new software. Where does one start, especially a total novice in data mining? For this exact reason we\u0026rsquo;ve prepared Getting Started With Orange - YouTube tutorials for complete beginners. Example workflows on the other hand can be accessed via Help - Examples.\nTIP #2: Make use of Orange documentation.\nYou can access it in three ways:" ,
	"author" : "AJDA",
	"summary" : "TIP #1: Follow tutorials and example workflows to get started.\nIt\u0026rsquo;s difficult to start using new software. Where does one start, especially a total novice in data mining? For this exact reason we\u0026rsquo;ve prepared Getting Started With Orange - YouTube tutorials for complete beginners. Example workflows on the other hand can be accessed via Help - Examples.\nTIP #2: Make use of Orange documentation.\nYou can access it in three ways:",
	"date" : "Oct 17, 2016"
}

    
    , {
    "uri": "/blog/2016/10/10/the-story-of-shadow-and-orange/",
	"title": "The Story of Shadow and Orange",
	"categories": ["canvas", "interface", "oasys"],
	"description": "",
	"content": "This is a long story. I remember when started my PhD in Italy. There I met a researcher and he said to me: »You should do some simulations on x-ray optics beamline.« »Yes, but how should I do that?« He gave me a big tape, it was 1986. I soon realized it was all code. But it was a code called Shadow.\nI started to look at the code, to play with it, do some simulations… Soon my boss told me:\n»You should do a simulation with asymmetric crystals for monochromators.«\n»But asymmetric crystals are not foreseen in this code.«\n»Yes, think about how to do it. You should contact Franco Cerrina, he\u0026rsquo;s the author of Shadow.«\nI indeed contacted prof. Cerrina and at that time this was not easy, because there was no direct e-mail. What we had was called a digital deck net, Digital Computers Network. I had to go to another laboratory just to send him an e-mail. Soon, he replied: »Come to see me.« I managed to get some funding to go to the US and for the next two years I spent a good amount of time in Madison, Wisconsin.\nI started to work with prof. Cerrina and it was thanks to my work on Shadow that I was called by the European Synchrotron Facility and they offered me a position. But soon I stopped working on Shadow, because I was getting busy with other things.\nIt was only in 2009 that I contacted prof. Cerrina again. We needed to upgrade our software, so I went back to the US two or three times and started working on what is now Shadow3.\nIn 2010 I organized a trip to go visit again with my family for the summer. We booked the house, we booked the trip… And it was ten days before the departure that I learned that Cerrina died. And since everything was already organized, we decided to visit the US anyway.\nThere, I went to Cerrina\u0026rsquo;s laboratory and met his PhD student, who was keeping his possessions. I said to her:\n»Tell me everything you were doing recently and I will try to recover what I can.« And at that moment, she said many things were on this big old Mac. So I proposed to buy this Mac from her, but my home institution wasn\u0026rsquo;t happy, they saw no reason to buy a second-hand Mac. Even though it contained some important things Cerrina was working on!\nLuckily, I managed to get it and I was able to recover many things from it. Moreover, I kept maintaining the Shadow code, because it is a standard software in the community. At the very beginning, the source was not public. Then it was eventually published, but the code was very complicated and nobody managed to recompile that. Thus I decided to clean the code and finally we completed the new version of Shadow in 2011.\nThree years ago it was time to update Shadow again, especially the interface. One day I discovered Orange and I thought \u0026lsquo;it looked nice\u0026rsquo;. In that exact time I met Luca [Rebuffi] in Trieste. He got so excited about Orange that his PhD project became redesigning Shadow\u0026rsquo;s interface with Orange! And now we have OASYS, which is an adaptation of Orange for optical physics. So I hope that in the future, we will have many more users and also many more developers helping us bring simple tools to the scientific community.\n\u0026ndash; Manuel Sanchez del Rio\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This is a long story. I remember when started my PhD in Italy. There I met a researcher and he said to me: »You should do some simulations on x-ray optics beamline.« »Yes, but how should I do that?« He gave me a big tape, it was 1986. I soon realized it was all code. But it was a code called Shadow.\nI started to look at the code, to play with it, do some simulations… Soon my boss told me:" ,
	"author" : "AJDA",
	"summary" : "This is a long story. I remember when started my PhD in Italy. There I met a researcher and he said to me: »You should do some simulations on x-ray optics beamline.« »Yes, but how should I do that?« He gave me a big tape, it was 1986. I soon realized it was all code. But it was a code called Shadow.\nI started to look at the code, to play with it, do some simulations… Soon my boss told me:",
	"date" : "Oct 10, 2016"
}

    
    , {
    "uri": "/blog/2016/10/02/intro-to-data-mining-for-life-scientists/",
	"title": "Intro to Data Mining for Life Scientists",
	"categories": ["bioinformatics", "bioorange", "education", "orange3", "tutorial", "workshop"],
	"description": "",
	"content": "RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.\nWe have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding.\nRelated: Data Mining Course at Baylor College of Medicine in Houston\nIt’s not easy to pack so many new things on data analytics within four hours, but working with Orange helps. This was a hands-on workshop. Students brought their own laptops with Orange and several of its add-ons for bioinformatics and image analytics. We also showed how to prepare one’s own data using Google Forms and designed a questionary, augment it in a class, run it with students and then analyze the questionary with Orange.\nThe hard part of any short course that includes machine learning is how to explain overfitting. The concept is not trivial for data science newcomers, but it is so important it simply cannot be left out. Luckily, Orange has some cool widgets to help us understanding the overfitting. Below is a workflow we have used. We read some data (this time it was a yeast gene expression data set called brown-selected that comes with Orange), “destroyed the data” by randomly permuting the column with class values, trained a classification tree, and observed near perfect results when the model was checked on the training data.\nSure this works, you are probably saying. The models should have been scored on a separate test set! Exactly, and this is what we have done next with Data Sampler, which lead us to cross-validation and Test \u0026amp; Score widget.\nThis was a great and interesting short course and we were happy to contribute to the success of the student-run MLSC-2016 conference.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.\nWe have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding." ,
	"author" : "BLAZ",
	"summary" : "RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.\nWe have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding.",
	"date" : "Oct 2, 2016"
}

    
    , {
    "uri": "/blog/2016/09/23/text-mining-version-0-2-0/",
	"title": "Text Mining: version 0.2.0",
	"categories": ["addons", "clustering", "examples", "orange3", "preprocessing", "release", "text mining", "widget"],
	"description": "",
	"content": "Orange3-Text has just recently been polished, updated and enhanced! Our GSoC student Alexey has helped us greatly to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\nWikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries. It serves as an easy data gathering source and it\u0026rsquo;s great for exploring text mining techniques. Here we\u0026rsquo;ve simply queried Wikipedia for articles on Slovenia and Germany and displayed them in Corpus Viewer.\nQuery Wikipedia by entering your query word list in the widget. Put each query on a separate line and run Search.\nSimilarity Hashing widget computes similarity hashes for the given corpus, allowing the user to find duplicates, plagiarism or textual borrowing in the corpus. Here\u0026rsquo;s an example from Wikipedia, which has a pre-defined structure of articles, making our corpus quite similar. We\u0026rsquo;ve used Wikipedia widget and retrieved 10 articles for the query \u0026lsquo;Slovenia\u0026rsquo;. Then we\u0026rsquo;ve used Similarity Hashing to compute hashes for our text. What we got on the output is a table of 64 binary features (predefined in the SimHash widget), which denote a 64-bit hash size. Then we computed similarities in text by sending Similarity Hashing to Distances. Here we\u0026rsquo;ve selected cosine row distances and sent the output to Hierarchical Clustering. We can see that we have some similar documents, so we can select and inspect them in Corpus Viewer.\nOutput of Similarity Hashing widget.\nWe\u0026rsquo;ve selected the two most similar documents in Hierarchical Clustering and displayed them in Corpus Viewer.\nTopic Modeling now includes three modeling algorithms, namely Latent Semantic Indexing (LSP), Latent Dirichlet Allocation (LDA), and Hierarchical Dirichlet Process (HDP). Let\u0026rsquo;s query Twitter for the latest tweets from Hillary Clinton and Donald Trump. First we preprocess the data and send the output to Topic Modeling. The widget suggests 10 topics, with the most significant words denoting each topic, and outputs topic probabilities for each document.\nWe can inspect distances between the topics with Distances (cosine) and Hierarchical Clustering. Seems like topics are not extremely author specific, since Hierarchical Clustering often puts Trump and Clinton in the same cluster. We\u0026rsquo;ve used Average linkage, but you can play around with different linkages and see if you can get better results.\nExample of comparing text by topics.\nNow we connect Corpus Viewer to Preprocess Text. This is nothing new, but Corpus Viewer now displays also tokens and POS tags. Enable POS Tagger in Preprocess Text. Now open Corpus Viewer and tick the checkbox Show Tokens \u0026amp; Tags. This will display tagged token at the bottom of each document.\nCorpus Viewer can now display tokens and POS tags below each document.\nThis is just a brief overview of what one can do with the new Orange text mining functionalities. Of course, these are just exemplary workflows. If you did textual analysis with great results using any of these widgets, feel free to share it with us! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange3-Text has just recently been polished, updated and enhanced! Our GSoC student Alexey has helped us greatly to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\nWikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries." ,
	"author" : "AJDA",
	"summary" : "Orange3-Text has just recently been polished, updated and enhanced! Our GSoC student Alexey has helped us greatly to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\nWikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries.",
	"date" : "Sep 23, 2016"
}

    
    , {
    "uri": "/blog/2016/09/15/data-mining-in-houston-2/",
	"title": "Data Mining Course in Houston #2",
	"categories": ["orange3", "tutorial", "workshop"],
	"description": "",
	"content": "This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.\nThe course was designed for students with little or no experience in data science. It consisted of seven two-hour lectures, each followed by a homework assignment. We (Blaz and Janez) lectured on data visualization, classification, regression, clustering, data projection and image analytics. We paid special attention to the problems of overfitting, use of regularization, and proper ways of testing and scoring of modeling methods.\nThe course was hands-on. The lectures were practical. They typically started with some data set and explained data mining techniques through designing data analysis workflows in Orange. Besides some standard machine learning and bioinformatics data sets, we have also painted the data to explore, say, the benefits of different classification techniques or design data sets where k-means clustering would fail.\nThis year, the course benefited from several new Orange widgets. The recently published interactive k-means widget was used to explain the inner working of this clustering algorithm, and polynomial classification widget was helpful in discussion of decision boundaries of classification algorithms. Silhouette plot was used to show how to evaluate and explore the results of clustering. And finally, we explained concepts from deep learning using image embedding to show how already trained networks can be used for clustering and classification of images.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.\nThe course was designed for students with little or no experience in data science." ,
	"author" : "BLAZ",
	"summary" : "This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.\nThe course was designed for students with little or no experience in data science.",
	"date" : "Sep 15, 2016"
}

    
    , {
    "uri": "/blog/2016/08/25/visualizing-gradient-descent/",
	"title": "Visualizing Gradient Descent",
	"categories": ["addons", "education", "gsoc2016", "interactive data visualization", "orange3"],
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nGradient Descent was implemented as a part of my Google Summer of Code project and it is available in the Orange3-Educational add-on. It simulates gradient descent for either Logistic or Linear regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set.\nGradient Descent widget takes data on input and outputs the model and its coefficients.\nThe widget displays the value of the cost function given two parameters of the model. For linear regression, we consider feature from the training set with the parameters being the intercept and the slope. For logistic regression, the widget considers two feature and their associated multiplicative parameters, setting the intercept to zero. Screenshot bellow shows gradient descent on a Iris data set, where we consider petal length and sepal width on the input and predict the probability that iris comes from the family of Iris versicolor.\n The type of the model used (either Logistic regression or Linear regression) Input features (one for X and one for Y axis) and the target class Learning rate is the step size of the gradient descent In a single iteration step, stochastic approach considers only a single data instance (instead of entire training set). Convergence in terms of iterations steps is slower, and we can instruct the widget to display the progress of optimization only after given number of steps (Step size) Step through the algorithm (steps can be reverted with step back button) Run optimization until convergence  Following shows gradient descent for linear regression using The Boston Housing Data Set when trying to predict the median value of a house given its age.\nOn the left we use the regular and on the right the stochastic gradient descent. While the regular descent goes straight to the target, the path of stochastic is not as smooth.\nWe can use the widget to simulate some dangerous, unwanted behavior of gradient descent. The following screenshots show two extreme cases with too high learning rate where optimization function never converges, and a low learning rate where convergence is painfully slow.\nThe two problems as illustrated above are the reason that many implementations of numerical optimization use adaptive learning rates. We can simulate this in the widget by modifying the learning rate for each step of the optimization.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nGradient Descent was implemented as a part of my Google Summer of Code project and it is available in the Orange3-Educational add-on. It simulates gradient descent for either Logistic or Linear regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set." ,
	"author" : "PRIMOZGODEC",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nGradient Descent was implemented as a part of my Google Summer of Code project and it is available in the Orange3-Educational add-on. It simulates gradient descent for either Logistic or Linear regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set.",
	"date" : "Aug 25, 2016"
}

    
    , {
    "uri": "/blog/2016/08/19/making-recommendations/",
	"title": "Making recommendations",
	"categories": ["addons", "business intelligence", "gsoc2016", "orange3", "recommender  system"],
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nRecommender systems are everywhere, we can find them on YouTube, Amazon, Netflix, iTunes,\u0026hellip; This is because they are crucial component in a competitive retail services.\nHow can I know what you may like if I have almost no information about you? The answer: taking Collaborative filtering (CF) approaches. Basically, this means to combine all the little knowledge we have about users and/or items in order to build a grid of knowledge with which we make recommendation.\nTo help you with that, Biolab has written Orange3-Recommendation - an add-on for Orange3 to train recommendation models, cross-validate them and make predictions.\n Input data First things first. Orange3-Recommendation can read files in native tab-delimited format, or can load data from any of the major standard spreadsheet file type, like CSV and Excel. Native format starts with a header row with feature (column) names. Second header row gives the attribute type, which can be continuous, discrete, string or time. The third header line contains meta information to identify dependent features (class), irrelevant features (ignore) or meta features (meta).\nHere are the first few lines from a data set:\n tid user movie score string discrete discrete continuous meta row=1 col=1 class 1 Breza HarrySally 2 2 Dana Cvetje 5 3 Cene Prometheus 5 4 Ksenija HarrySally 4 5 Albert Matrix 4 ...  The third row is mandatory in this kind of datasets*, in order to know which attributes correspond to the users (row=1) and which ones to the items (col=1). For the case of big datasets, users and items must be specified as continuous attributes due to efficiency issues. (*Note: If the meta attributes _row_ or _col_, some simple heuristics will be applied: users=_column 0_, items=_column 1_, class=_last column_)\nHere are the first few lines from a data set :\n user movie score tid continuous continuous continuous time row=1 col=1 class meta 196 242 3 881250949 186 302 3 891717742 22 377 1 878887116 244 51 2 880606923 166 346 1 886397596 298 474 4 884182806 ...   Training a model This step is pretty simple. To train a model we have to load the data as is described above and connect it to the learner. (Don\u0026rsquo;t forget to click apply)\n If the model uses side information, we only need to add an extra file.\n In addition, we can set the parameters of our model by double-clicking it:\n By using a fixed seed, we make random numbers predictable. Therefore, this feature is useful if we want to compare results in a deterministic way.\n Cross-validation This is as simple as it seems. The only thing to point out is that side information must be connected to the model.\nStill, cross-validation is a robust way to see how our model is performing. I consider that it\u0026rsquo;s a good idea to check how our model performs with respect to the baseline. This presents a negligible overload* in our pipeline and makes our analysis more solid. _(*For 1,000,000 ratings, it can take 0.027s)._\nWe can add a baseline leaner to Test\u0026amp;Score and select the model we want to apply.\n Making recommendations The prediction flow is exactly the same as in Orange3.\n Analyzing low-rank matrices Once we\u0026rsquo;ve output the low-rank matrices, we can play around the vectors in those matrices to discover hidden relations or understand the known ones. For instance, here we plot vector 1 and 2 from the item-feature matrix by simply connecting Data Table with selected instances to the widget Scatter Plot.\nUsing similar approaches we can discover pretty interesting things like similarity between movies or users, how movie genres relate with each other, changes in users' behavior, when the popularity of a movie has been raised due to a commercial campaign,\u0026hellip; and many others.\nFinally, a simple pipeline to do all of the above can be something like this:\nOn the left side we connected several models to Test\u0026amp;Score in order to cross-validate them. Later, we trained a SVD++ model, made some predictions, got the low-rank matrices learnt by the model and plotted some vectors of the Item-feature matrix.\nAnalysis (Advanced users) Here we\u0026rsquo;ve made a workflow (which can be downloaded here) to perform a really basic analysis on the results obtained through factoring the user and item feature matrices with BRISMF over the movielens100k dataset. (Note: Once downloaded, set the prepared datasets in the folder \u0026lsquo;orange\u0026rsquo;. Probably you\u0026rsquo;re gonna get a couple errors. Don\u0026rsquo;t worry, it\u0026rsquo;s normal. To solve it, apply the scripts sequentially but don\u0026rsquo;t forget previously to select all the rows in the related Table.)\nInstead of explaining how this pipeline works, the best thing you can do is to download it and play with it.\nOne of the analysis you can do, is to plot the most popular movies across two first vectors of the matrix descomposition. Later, you can try to find clusters, tweak it a bit and find crossed relations (e.g. male/female Vs._ action/drama)._\nNow let\u0026rsquo;s focus on the scripting part. Rating models In this tutorial we are going to train a BRISMF model.\n  First we import Orange and the learner that we want to use:\n import Orange from orangecontrib.recommendation import BRISMFLearner    After that, we have to load a dataset:\n data = Orange.data.Table('movielens100k.tab')    Then we set the learner parameters, and finally we train it passing the dataset as an argument (the returned value will be our model trained):\n learner = BRISMFLearner(num_factors=15, num_iter=25, learning_rate=0.07, lmbda=0.1) recommender = learner(data)    Finally, we can make predictions (in this case, for the first three pairs in the dataset):\n prediction = recommender(data[:3]) print(prediction) \u0026gt;\u0026gt;\u0026gt; [ 3.79505151 3.75096513 1.293013 ]     Ranking models At this point we can try something new, let\u0026rsquo;s make recommendations for a dataset in which only binary relevance is available. For this case, CLiMF is model that will suit our needs.\nimport Orange import numpy as np from orangecontrib.recommendation import CLiMFLearner # Load data data = Orange.data.Table('epinions_train.tab') # Train recommender learner = CLiMFLearner(num_factors=10, num_iter=10, learning_rate=0.0001, lmbda=0.001) recommender = learner(data) # Make recommendations recommender(X=5) \u0026gt;\u0026gt;\u0026gt; [ 494, 803, 180, ..., 25520, 25507, 30815]  Later, we can score the model. In this case we\u0026rsquo;re using the MeanReciprocalRank:\nimport Orange # Load test dataset testdata = Orange.data.Table('epinions_test.tab') # Sample users num_users = len(recommender.U) num_samples = min(num_users, 1000) # max. number to sample users_sampled = np.random.choice(np.arange(num_users), num_samples) # Compute Mean Reciprocal Rank (MRR) mrr, _ = recommender.compute_mrr(data=testdata, users=users_sampled) print('MRR: %.4f' % mrr) \u0026gt;\u0026gt;\u0026gt; MRR: 0.3975   SGD optimizers This add-on includes several configurations that can be used to modify the updates on the low rank matrices during the stochastic gradient descent optimization.\n SGD: Classical SGD update. Momentum: SGD with inertia. Nesterov momentum: A Momentum that \u0026ldquo;looks ahead\u0026rdquo;. AdaGrad: Optimizer that adapts its learning rating during the process. RMSProp: \u0026ldquo;Leaky\u0026rdquo; AdaGrad. AdaDelta: Extension of Adagrad that seeks to reduce its aggressive. Adam: Similar to AdaGrad and RMSProp but with an exponentially decaying average of past gradients. Adamax: Similar to Adam, but taking the maximum between the gradient and the velocity.  Do you want to learn more about this? Check our documentation!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nRecommender systems are everywhere, we can find them on YouTube, Amazon, Netflix, iTunes,\u0026hellip; This is because they are crucial component in a competitive retail services.\nHow can I know what you may like if I have almost no information about you? The answer: taking Collaborative filtering (CF) approaches. Basically, this means to combine all the little knowledge we have about users and/or items in order to build a grid of knowledge with which we make recommendation." ,
	"author" : "SALVACARRION",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nRecommender systems are everywhere, we can find them on YouTube, Amazon, Netflix, iTunes,\u0026hellip; This is because they are crucial component in a competitive retail services.\nHow can I know what you may like if I have almost no information about you? The answer: taking Collaborative filtering (CF) approaches. Basically, this means to combine all the little knowledge we have about users and/or items in order to build a grid of knowledge with which we make recommendation.",
	"date" : "Aug 19, 2016"
}

    
    , {
    "uri": "/blog/2016/08/16/polynomial-classification/",
	"title": "Visualization of Classification Probabilities",
	"categories": ["addons", "classification", "gsoc", "gsoc2016", "orange3", "widget"],
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nPolynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.\nHere is an example workflow. The data comes from the File widget. With no learner on input, the default is Logistic Regression. Widget outputs learners Coefficients, Classifier (model) and Learner.\nPolynomial Classification widget works on two continuous features only, all other features are ignored. The screenshot shows plot of classification for an Iris data set .\n Set name of the learner. This is the name of learner on output. Set features that logistic regression is performed on. Set class that is classified separately from other classes. Set the degree of a polynom that is used to transform an input data (1 means attributes are not transformed). Select whether see or not contour lines in chart. The density of contours is regulated by Contour step.  The classification for our case fails in separating Iris-versicolor from the other two classes. This is because logistic regression is a linear classifier, and because there is no linear combination of the chosen two attributes that would make for a good decision boundary. We can change that. Polynomial expansion adds features that are polynomial combinations of original ones. For example, if an input data contains features [a, b], polynomial expansion of degree two generates feature space [1, a, b, a2, a b, b2]. With this expansion, the classification boundary looks great.\nPolynomial Classification also works well with other learners. Below we have given it a Classification Tree. This time we have painted the input data using Paint Data, a great data generator used while learning about Orange and data science. The decision boundaries for the tree are all square, a well-known limitation for tree-based learners.\nPolynomial expansion if high degrees may be dangerous. Following example shows overfitting when degree is five. See the two outliers, a blue one on the top and the red one at the lower right of the plot? The classifier was unnecessary able to separate the outliers from the pack, something that will become problematic when classifier will be used on the new data.\nOverfitting is one of the central problems in machine learning. You are welcome to read our previous blog on this problem and possible solutions.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nPolynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.\nHere is an example workflow. The data comes from the File widget." ,
	"author" : "PRIMOZGODEC",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nPolynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.\nHere is an example workflow. The data comes from the File widget.",
	"date" : "Aug 16, 2016"
}

    
    , {
    "uri": "/blog/2016/08/12/interactive-k-means/",
	"title": "Interactive k-Means",
	"categories": ["addons", "clustering", "education", "gsoc", "gsoc2016", "orange3", "widget"],
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nAs a part of my Google Summer of Code project I started developing educational widgets and assemble them in an Educational Add-On for Orange. Educational widgets can be used by students to understand how some key data mining algorithms work and by teachers to demonstrate the working of these algorithms.\nHere I describe an educational widget for interactive k-means clustering, an algorithm that splits the data into clusters by finding cluster centroids such that the distance between data points and their corresponding centroid is minimized. Number of clusters in k-means algorithm is denoted with k and has to be specified manually.\nThe algorithm starts by randomly positioning the centroids in the data space, and then improving their position by repetition of the following two steps:\n Assign each point to the closest centroid. Move centroids to the mean position of points assigned to the centroid.  The widget needs the data that can come from File widget, and outputs the information on clusters (Annotated Data) and centroids:\nEducational widget for k-means works finds clusters based on two continuous features only, all other features are ignored. The screenshot shows plot of an Iris data set and clustering with k=3. That is partially cheating, because we know that iris data set has three classes, so that we can check if clusters correspond well to original classes:\n Select two features that are used in k-means Set number of centroids Randomize positions of centroids Show lines between centroids and corresponding points Perform the algorithm step by step. Reassign membership connects points to nearest centroid, Recompute centroids moves centroids. Step back in the algorithm Set speed of automatic stepping Perform the whole algorithm as fast preview  Anytime we can change number of centroids with spinner or with click in desired position in the graph.  If we want to see the correspondence of clusters that are denoted by k-means and classes, we can open Data Table widget where we see that all Iris-setosas are clustered in one cluster and but there are just few Iris-versicolor that are classified is same cluster together with Iris-virginica and vice versa.\nInteractive k-means works great in combination with Paint Data. There, we can design data sets where k-mains fails, and observe why.\nWe could also design data sets where k-means fails under specific initialization of centroids. Ah, I did not tell you that you can freely move the centroids and then restart the algorithm. Below we show the case of centroid initialization and how this leads to non-optimal clustering.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nAs a part of my Google Summer of Code project I started developing educational widgets and assemble them in an Educational Add-On for Orange. Educational widgets can be used by students to understand how some key data mining algorithms work and by teachers to demonstrate the working of these algorithms.\nHere I describe an educational widget for interactive k-means clustering, an algorithm that splits the data into clusters by finding cluster centroids such that the distance between data points and their corresponding centroid is minimized." ,
	"author" : "PRIMOZGODEC",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nAs a part of my Google Summer of Code project I started developing educational widgets and assemble them in an Educational Add-On for Orange. Educational widgets can be used by students to understand how some key data mining algorithms work and by teachers to demonstrate the working of these algorithms.\nHere I describe an educational widget for interactive k-means clustering, an algorithm that splits the data into clusters by finding cluster centroids such that the distance between data points and their corresponding centroid is minimized.",
	"date" : "Aug 12, 2016"
}

    
    , {
    "uri": "/blog/2016/08/05/rule-induction-part-i-scripting/",
	"title": "Rule Induction (Part I - Scripting)",
	"categories": ["classification", "gsoc2016", "orange3"],
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nWe’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3.3.8.\nRule induction from examples is recognised as a fundamental component of many machine learning systems. My goal was foremost to implement supervised rule induction algorithms and rule-based classification methods, but also to devise a more general framework of replaceable individual components that users could fine-tune to their needs. To this purpose, separate-and-conquer strategy was applied. In essence, learning instances are covered and removed following a chosen rule. The process is repeated while learning set examples remain. To evaluate found hypotheses and to choose the best rule in each iteration, search heuristics are used (primarily, rule class distribution is the decisive determinant).\nThe use of the created module is straightforward. New rule induction algorithms can be easily introduced, by either utilising predefined components or developing new ones (these include various search algorithms, search strategies, evaluators, and others). Several well-known rule induction algorithms have already been included. Let’s see how they perform!\nClassic CN2 inducer constructs a list of ordered rules (decision list). Here, we load the titanic data set and create a simple classifier, which can already be used to predict data.\nimport Orange data = Orange.data.Table('titanic') learner = Orange.classification.CN2Learner() classifier = learner(data)  Similarly, a set of unordered rules can be constructed using Unordered CN2 inducer. Rules are learnt for each class individually, in regard to the original learning data. To evaluate found hypotheses, Laplace accuracy measure is used. Having first initialised the learner, we then control the algorithm by modifying its parameters. The underlying components are available to us by accessing the rule finder.\ndata = Table('iris.tab') learner = CN2UnorderedLearner() # consider up to 10 solution streams at one time learner.rule_finder.search_algorithm.beam_width = 10 # continuous value space is constrained to reduce computation time learner.rule_finder.search_strategy.bound_continuous = True # found rules must cover at least 15 examples learner.rule_finder.general_validator.min_covered_examples = 15 # found rules must combine at most 2 selectors (conditions) learner.rule_finder.general_validator.max_rule_length = 2 classifier = learner(data)  Induced rules can be quickly reviewed and interpreted. They are each of the form ‘if cond then predict class”. That is, a conjunction of selectors followed by the predicted class.\nfor rule in classifier.rule_list: ... print(rule, rule.curr_class_dist.tolist()) \u0026gt;\u0026gt;\u0026gt; IF petal length\u0026lt;=3.0 AND sepal width\u0026gt;=2.9 THEN iris=Iris-setosa [49, 0, 0] \u0026gt;\u0026gt;\u0026gt; IF petal length\u0026gt;=3.0 AND petal length\u0026lt;=4.8 THEN iris=Iris-versicolor [0, 46, 3] \u0026gt;\u0026gt;\u0026gt; IF petal width\u0026gt;=1.8 AND petal length\u0026gt;=4.9 THEN iris=Iris-virginica [0, 0, 43] \u0026gt;\u0026gt;\u0026gt; IF TRUE THEN iris=Iris-virginica [50, 50, 50] # the default rule  If no other rules fire, default rule (majority classification) is used. Specific to each individual rule inducer, the application of the default rule varies.\nThough rule learning is most frequently used in the context of predictive induction, it can be adapted to subgroup discovery. In contrast, subgroup discovery aims at learning individual patterns or interesting population subgroups, rather than to maximise classification accuracy. Induced rules prove very valuable in terms of their descriptive power. To this end, CN2-SD algorithms were also implemented.\nHopefully, the addition to the Orange software suite will benefit both novice and expert users looking advance their knowledge in a particular area of study, through a better understanding of given predictions and underlying argumentation.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nWe’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3." ,
	"author" : "MATEVZKREN",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nWe’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3.",
	"date" : "Aug 5, 2016"
}

    
    , {
    "uri": "/blog/2016/07/29/pythagorean-trees-and-forests/",
	"title": "Pythagorean Trees and Forests",
	"categories": ["classification", "examples", "interactive data visualization", "orange3", "plot", "tree", "visualization"],
	"description": "",
	"content": "Classification Trees are great, but how about when they overgrow even your 27'' screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.\nPythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square. Hence Pythagorean Tree. Every square has the color of the prevalent, with opacity indicating the relative proportion of the majority class in the subset. Details are shown in hover balloons.\nClassification Tree with titanic.tab data set.\nPythagorean Tree with titanic.tab data set.\nWhen you hover over a square in Pythagorean Tree, a whole line of parent and child squares/nodes is highlighted. Clicking on a square/node outputs the selected subset, just like in Classification Tree.\nUpon hovering on the square in the tree, the lineage (parent and child nodes) is highlighted. Hover also displays information on the subset, represented by the square. The widget outputs the selected subset.\nAnother amazing addition to Orange\u0026rsquo;s Visualization set is Pythagorean Forest, which is a visualization of Random Forest algorithm. Random Forest takes N samples from a data set with N instances, but with replacement. Then a tree is grown for each sample, which alleviates the Classification Tree\u0026rsquo;s tendency to overfit the data. Pythagorean Forest is a concise visualization of Random Forest, with each Pythagorean Tree plotted side by side.\nDifferent trees are grown side by side. Parameters for the algorithm are set in Random Forest widget, then the whole forest is sent to Pythagorean Forest for visualization.\nThis makes Pythagorean Forest a great tool to explain how Random Forest works or to further explore each tree in Pythagorean Tree widget.\nPythagorean trees are a new addition to Orange. Their implementation has been inspired by a recent paper on Generalized Pythagoras Trees for Visualizing Hierarchies by Fabian Beck, Michael Burch, Tanja Munz, Lorenzo Di Silvestro and Daniel Weiskopf that was presented in at the 5th International Conference on Information Visualization Theory and Applications in 2014.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Classification Trees are great, but how about when they overgrow even your 27'' screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.\nPythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square." ,
	"author" : "AJDA",
	"summary" : "Classification Trees are great, but how about when they overgrow even your 27'' screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.\nPythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square.",
	"date" : "Jul 29, 2016"
}

    
    , {
    "uri": "/blog/2016/07/18/network-analysis-with-orange/",
	"title": "Network Analysis with Orange",
	"categories": ["addons", "analysis", "examples", "network", "orange3", "visualization"],
	"description": "",
	"content": "Visualizing relations between data instances can tell us a lot about our data. Let\u0026rsquo;s see how this works in Orange. We have a data set on machine learning and data mining conferences and journals, with the number of shared authors for each publication venue reported. We can estimate similarity between two conferences using the author profile of a conference: two conference would be similar if they attract the same authors. The data set is already 9 years old, but obviously, it\u0026rsquo;s about the principle. :) We\u0026rsquo;ve got two data files: one is a distance file with distance scores already calculated by Jaccard index and the other is a standard conferences.tab file.\nConferences.tab data file with the type of the publication venue (conference or journal) and average number of authors and published papers.\nWe load .tab file with the File widget (data set already comes with Orange) and .dst file with the Distance File widget (select \u0026lsquo;Browse documentation data sets\u0026rsquo; and choose conferences.dst).\nYou can find conferences.dst in \u0026lsquo;Browse documentation data sets\u0026rsquo;.\nNow we would like to create a graph from the distance file. Connect Distance File to Network from Distances. In the widget, we\u0026rsquo;ve selected a high distance threshold, because we would like to get more connections between nodes. We\u0026rsquo;ve also checked \u0026lsquo;Include also closest neighbors\u0026rsquo; to see each node connected with at least one other node.\nWe\u0026rsquo;ve set a high distance threshold, since we wanted to display connections between most of our nodes.\nWe can visualize our graph in Network Explorer. What we get is a quite uninformative network of conferences with labelled nodes. Now for the fun part. Connect the File widget with Network Explorer and set the link type to \u0026lsquo;Node Data\u0026rsquo;. This will match the two domains and display additional labelling options in Network Explorer.\nRemove the \u0026lsquo;Node Subset\u0026rsquo; link and connect Data to Node Data. This will display other attributes in Network Explorer by which you can label and color your network nodes.\nNodes are colored by event type (conference or journal) and adjusted in size by the average number of authors per event (bigger nodes represent larger events).\nWe\u0026rsquo;ve colored the nodes by type and set the size of the nodes to the number of authors per conference/paper. Finally, we\u0026rsquo;ve set the node label to \u0026lsquo;name\u0026rsquo;. Seems like International Conference on AI and Law and AI and Law journal are connected through the number of shared authors. Same goes for AI in Medicine in Europe conference and AI and Medicine journal. Connections indeed make sense.\nThe entire workflow.\nThere are many other things you can do with the Networks add-on in Orange. You can color nodes by predictions, highlight misclassifications or output only nodes with certain network parameters. But for today, let this be it.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Visualizing relations between data instances can tell us a lot about our data. Let\u0026rsquo;s see how this works in Orange. We have a data set on machine learning and data mining conferences and journals, with the number of shared authors for each publication venue reported. We can estimate similarity between two conferences using the author profile of a conference: two conference would be similar if they attract the same authors. The data set is already 9 years old, but obviously, it\u0026rsquo;s about the principle." ,
	"author" : "AJDA",
	"summary" : "Visualizing relations between data instances can tell us a lot about our data. Let\u0026rsquo;s see how this works in Orange. We have a data set on machine learning and data mining conferences and journals, with the number of shared authors for each publication venue reported. We can estimate similarity between two conferences using the author profile of a conference: two conference would be similar if they attract the same authors. The data set is already 9 years old, but obviously, it\u0026rsquo;s about the principle.",
	"date" : "Jul 18, 2016"
}

    
    , {
    "uri": "/blog/2016/07/05/rehaul-of-text-mining-add-on/",
	"title": "Rehaul of Text Mining Add-On",
	"categories": ["addons", "analysis", "business intelligence", "classification", "examples", "orange3", "preprocessing", "text mining"],
	"description": "",
	"content": "Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we\u0026rsquo;re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We\u0026rsquo;ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content.\nTwitter widget offers many functionalities. Since we wanted to get tweets from specific authors, we entered their Twitter handles as queries and set \u0026lsquo;Search by Author\u0026rsquo;. We only included Author, Content and Date in the query parameters, as we want to predict the author only on the basis of text.\n Provide API keys. Insert queries separated by newline. Search by content, author or both. Set date (1 week limit from tweepy module). Select language you want your tweets to be in. If ‘Max tweets’ is checked, you can set the maximum number of tweets you want to query. Otherwise the widget will provide all tweets matching the query. If ‘Accumulate results’ is checked, new queries will be appended to the old ones. Select what kind of data you want to retrieve. Tweet count. Press ‘Search’ to start your query.  We got 208 tweets on the output. Not bad. Now we need to preprocess them first, before we do any predictions. We transformed all the words into lowercase and split (tokenized) them by words. We didn\u0026rsquo;t use any normalization (below turned on just as an example) and applied a simple stopword removal.\n Information on the input and output. Transformation applies basic modifications of text. Tokenization splits the corpus into tokens according to the selected method (regexp is set to extract only words by default). Normalization lemmatizes words (do, did, done –\u0026gt; do). Filtering extracts only desired tokens (without stopwords, including only specified words, or by frequency).  Then we passed the tokens through a Bag of Words and observed the results in a Word Cloud.\nThen we simply connected Bag of Words to Test \u0026amp; Score and used several classifiers to see which one works best. We used Classification Tree and Nearest Neighbors since they are easy to explain even to teenagers. Especially Classification Tree offers a nice visualization in Classification Tree Viewer that makes the idea of the algorithm easy to understand. Moreover we could observe the most distinctive words in the tree.\nDo these make any sense? You be the judge. :)\nWe checked classification results in Test\u0026amp;Score, counted misclassifications in Confusion Matrix and finally observed them in Corpus Viewer. k-NN seems to perform moderately well, while Classification Tree fails miserably. Still, this was trained on barely 200 tweets. Perhaps accumulating results over time might give us much better results. You can now certainly try it on your own! Update your Orange3-Text add-on or install it via \u0026lsquo;pip install Orange3-Text\u0026rsquo;!\nAbove is the final workflow. Preprocessing on the left. Testing and scoring on the right bottom. Construction of classification tree right and above.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we\u0026rsquo;re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We\u0026rsquo;ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content." ,
	"author" : "AJDA",
	"summary" : "Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we\u0026rsquo;re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We\u0026rsquo;ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content.",
	"date" : "Jul 5, 2016"
}

    
    , {
    "uri": "/blog/2016/06/10/scripting-with-time-variable/",
	"title": "Scripting with Time Variable",
	"categories": ["data", "examples", "scripting", "orange3"],
	"description": "",
	"content": "It\u0026rsquo;s always fun to play around with data. And since Orange can, as of a few months ago, read temporal data, we decided to parse some data we had and put it into Orange.\nTimeVariable is an extended class of continuous variable and it works with properly formated ISO standard datetime (Y-M-D h:m:s). Oftentimes our original data is not in the right format and needs to be edited first, so Orange can read it. Python\u0026rsquo;s own datetime module is of great help. You can give it any date format and tell it how to interpret it in the argument.\nimport datetime date = \u0026quot;13.03.2013 13:13:31\u0026quot; new_date = str(datetime.datetime.strptime(date, \u0026quot;%d.%m.%Y %H:%M:%S\u0026quot;)) \u0026gt;\u0026gt;\u0026gt; '2013-03-13 13:13:31'  Do this for all your datetime attributes. This will transform them into strings that Orange\u0026rsquo;s TimeVariable can read. Then create a new data table:\nimport Orange from Orange.data import Domain, TimeVariable domain = Domain([TimeVariable(\u0026quot;timestamp\u0026quot;)]) timestamps = [\u0026quot;2013-03-13 13:13:31\u0026quot;, \u0026quot;2014-04-14 14:14:41\u0026quot;, \u0026quot;2015-05-15 15:15:51\u0026quot;] #create a new TimeVariable object time_var = TimeVariable() #it's important to parse strings into floats with var.parse(i) #list(zip(data)) then transforms the list into a 2d list of lists time_data = Orange.data.Table(domain, list(zip(time_var.parse(i) for i in timestamps)))  Now say you have some original data you want to append your new data to.\ndata = Orange.data.Table.concatenate([original_data, time_data]) Table.save(data, \u0026quot;data.tab\u0026quot;)  But what if you want to select only a few attributes from the original data? It can be arranged.\noriginal_data = Orange.data.Table(\u0026quot;original_data.tab\u0026quot;) new_domain = Domain([\u0026quot;attribute_1\u0026quot;, \u0026quot;attribute_2\u0026quot;], source=original_data.domain) new_data = Orange.data.Table(new_domain, original_data)  Then concatenate again:\ndata = Orange.data.Table.concatenate([new_data, time_data]) Table.save(data, \u0026quot;selected_data.tab\u0026quot;)  Remember, if your data has string variables, they will always be in meta attributes.\ndomain = Domain([\u0026quot;some_attribute1\u0026quot;, \u0026quot;other_attribute2\u0026quot;], metas=[\u0026quot;some_string_variable\u0026quot;])  Have fun scripting!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "It\u0026rsquo;s always fun to play around with data. And since Orange can, as of a few months ago, read temporal data, we decided to parse some data we had and put it into Orange.\nTimeVariable is an extended class of continuous variable and it works with properly formated ISO standard datetime (Y-M-D h:m:s). Oftentimes our original data is not in the right format and needs to be edited first, so Orange can read it." ,
	"author" : "AJDA",
	"summary" : "It\u0026rsquo;s always fun to play around with data. And since Orange can, as of a few months ago, read temporal data, we decided to parse some data we had and put it into Orange.\nTimeVariable is an extended class of continuous variable and it works with properly formated ISO standard datetime (Y-M-D h:m:s). Oftentimes our original data is not in the right format and needs to be edited first, so Orange can read it.",
	"date" : "Jun 10, 2016"
}

    
    , {
    "uri": "/blog/2016/05/20/oasys-orange-canvas-applied-to-optical-physics/",
	"title": "Oasys: Orange Canvas applied to Optical Physics",
	"categories": ["elettra", "esrf", "oasys", "orangecanvas", "physics", "synchrotron"],
	"description": "",
	"content": "This week we’re hosting experts in optical physics from Elettra Sincrotrone Trieste and European Synchrotron Radiation Facility in our laboratory. For a long time they have been interested in developing a user interface that integrates different simulation tools and data analysis software within one environment. It all came true with Orange Canvas and the OASYS system. We’ve already written about this two years ago, when the idea first came up. Now the actual software is ready and is being used by researchers for everyday analysis and prototyping.\nOASYS is basically pure Orange Canvas (Orange but no data mining widgets) that is reconfigured for the needs of optical physicists. What our partners from Italy did (with the help of our lab), was bring optic simulation software used in synchrotron facilities into a single graphical user interface. What is especially incredible is that they managed to transform Orange into a simulation platform for building synchrotron beamlines.\nIn essence, researches in synchrotrons experiment with actual physical objects, such as mirrors and crystals of different shape and size to transmit photons from several light sources of the synchrotron to the experimental endstations. They measure a broad array of material properties through the interaction with the synchrotron light and are trying to simulate different experiment settings before actually building a real-life experiment in the synchrotron. And this is where OASYS truly shines.\nWidgets in this case become parts of the simulation pipeline. Each widget has an input and output beam of light, just like real life devices, and the parameters within the widget are physical properties of a particular experimental object. Thus scientists can model the experiment in advance and do it much quicker and easier than before.\nFurthermore, Orange and OASYS provide a user-friendly GUI that domain experts can quickly get used to. There are anecdotal evidences of renowned physicists, who preferred to do their analysis with outdated simulation tools. However, after using OASYS for just a few days, they were already completely comfortable and could reproduce previously calculated results in a software without any problem. Moreover, they did it within several days instead of weeks as before.\nThis is the power of visual programming - providing a user-friendly interface for automating complicated calculations and quick prototyping.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This week we’re hosting experts in optical physics from Elettra Sincrotrone Trieste and European Synchrotron Radiation Facility in our laboratory. For a long time they have been interested in developing a user interface that integrates different simulation tools and data analysis software within one environment. It all came true with Orange Canvas and the OASYS system. We’ve already written about this two years ago, when the idea first came up. Now the actual software is ready and is being used by researchers for everyday analysis and prototyping." ,
	"author" : "AJDA",
	"summary" : "This week we’re hosting experts in optical physics from Elettra Sincrotrone Trieste and European Synchrotron Radiation Facility in our laboratory. For a long time they have been interested in developing a user interface that integrates different simulation tools and data analysis software within one environment. It all came true with Orange Canvas and the OASYS system. We’ve already written about this two years ago, when the idea first came up. Now the actual software is ready and is being used by researchers for everyday analysis and prototyping.",
	"date" : "May 20, 2016"
}

    
    , {
    "uri": "/blog/2016/04/25/association-rules-in-orange/",
	"title": "Association Rules in Orange",
	"categories": ["addons", "analysis", "association rules", "business intelligence", "examples", "orange3", "toolbox"],
	"description": "",
	"content": "Orange is welcoming back one of its more exciting add-ons: Associate! Association rules can help the user quickly and simply discover the underlying relationships and connections between data instances. Yeah!\nThe add-on currently has two widgets: one for Association Rules and the other for Frequent Itemsets. With Frequent Itemsets we first check frequency of items and itemsets in our transaction matrix. This tell us which items (products) and itemsets are the most frequent in our data, so it would make a lot of sense focusing on these products. Let\u0026rsquo;s use this widget on real Foodmart 2000 data set.\nFirst let\u0026rsquo;s check our data set. We have 62560 instances with 102 features. That\u0026rsquo;s a whole lot of transactions. Now we connect Frequent Itemsets to our File widget and observe the results. We went with a quite low minimal support due to the large number of transactions.\nCollapse All will display the most frequent items, so these will be our most important products (\u0026lsquo;bestsellers\u0026rsquo;). Our clients seem to be buying a whole lot of fresh vegetables and fresh fruit. Call your marketing department - you could become the ultimate place to buy fruits and veggies from.\nIf there\u0026rsquo;s a little arrow on the left side of the item, you can expand it to see all the other items connected to the selected attribute. So if a person buy fresh vegetables, it is most likely to buy fresh fruits as an accompanying product group. Now you can explore frequent itemsets to understand what really sells in your store.\nOk. Now how about some transaction flows? We\u0026rsquo;re mostly interested in the action-consequence relationship here. In other words, if a person buys one item, what is the most likely second item she will buy? Association Rules will help us discover that.\nOur parameters will again be adjusted for our data set. We probably want low support, since it will be hard to find a few prevailing rules for 62,000+ transactions. However, you want the discovered rules to be true most of the time, so increase the confidence.\nThe table on the right displays a list of rules with 6 different measures of association rule quality:\n support: how often a rule is applicable to a given data set (rule/data) confidence: how frequently items in Y appear in transactions with X or in other words how frequently the rule is true (support for a rule/support of antecedent) coverage: how often antecedent item is found in the data set (support of antecedent/data) strength: (support of consequent/support of antecedent) lift: how frequently a rule is true per consequent item (data * confidence/support of consequent) leverage: the difference between two item appearing in a transaction and the two items appearing independently (support*data - antecedent support * consequent support/data2)  Orange will rank the rules automatically. Now give a quick look at the rules. How about these two rules?\nfresh vegetables, plastic utensils, deli meats, wine \u0026ndash;\u0026gt; dried fruit\nfresh vegetables, plastic utensils, bologna, soda \u0026ndash;\u0026gt; chocolate candy\nThese seem to picnickers, clients who don\u0026rsquo;t want to spend a whole lot of time preparing their food. The first group is probably more gourmet, while the second seems to enjoy sweets. A logical step would be to place dried fruit closer to the wine section and the candy bars closer to sodas. What do you say? This already happened in your local supermarket? Coincidence? I don\u0026rsquo;t think so. :)\nAssociation rules are a powerful way to improve your business by organizing your actual or online store, adjusting marketing strategies to target suitable groups, providing product recommendations and generally understanding your client base better. Just another way Orange can be used as a business intelligence tool!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange is welcoming back one of its more exciting add-ons: Associate! Association rules can help the user quickly and simply discover the underlying relationships and connections between data instances. Yeah!\nThe add-on currently has two widgets: one for Association Rules and the other for Frequent Itemsets. With Frequent Itemsets we first check frequency of items and itemsets in our transaction matrix. This tell us which items (products) and itemsets are the most frequent in our data, so it would make a lot of sense focusing on these products." ,
	"author" : "AJDA",
	"summary" : "Orange is welcoming back one of its more exciting add-ons: Associate! Association rules can help the user quickly and simply discover the underlying relationships and connections between data instances. Yeah!\nThe add-on currently has two widgets: one for Association Rules and the other for Frequent Itemsets. With Frequent Itemsets we first check frequency of items and itemsets in our transaction matrix. This tell us which items (products) and itemsets are the most frequent in our data, so it would make a lot of sense focusing on these products.",
	"date" : "Apr 25, 2016"
}

    
    , {
    "uri": "/blog/2016/04/14/univariate-gsoc-success/",
	"title": "Univariate GSoC Success",
	"categories": ["analysis", "data", "distribution", "gsoc", "gsoc2016", "plot", "visualization"],
	"description": "",
	"content": "Google Summer of Code application period has come to an end. We\u0026rsquo;ve received 34 applications, some of which were of truly high quality. Now it\u0026rsquo;s upon us to select the top performing candidates, but before that we wanted to have an overlook of the candidate pool. We\u0026rsquo;ve gathered data from our Google Form application and gave it a quick view in Orange.\nFirst, we needed to preprocess the data a bit, since it came in a messy form of strings. Feature Constructor to the rescue! We wanted to extract the OS usage across users. So we first made three new variables named \u0026lsquo;uses linux\u0026rsquo;, \u0026lsquo;uses windows\u0026rsquo; and \u0026lsquo;uses osx\u0026rsquo; to represent our three new columns. For each column we searched through \u0026lsquo;OS_of_choice_and_why\u0026rsquo;, looked up the value of the column, converted it to string, put the string in lowercase, found mentions of either \u0026lsquo;linux\u0026rsquo;, \u0026lsquo;windows\u0026rsquo; or \u0026lsquo;osx\u0026rsquo;, and voila\u0026hellip;. if a mention occurred in the string, we marked the column with 1, else with 0.\nThe expression is just a logical statement in Python and works with booleans (0 if False and 1 if True):\n'linux' in str(OS_of_choice_and_why_.value).lower() or 'ubuntu' in str(OS_of_choice_and_why_.value).lower()  Another thing we might want to do is create three discrete values for \u0026lsquo;\u0026lsquo;Dogs or cats\u0026rsquo;\u0026rsquo; question. We want Orange to display \u0026lsquo;dogs\u0026rsquo; for someone who replied \u0026lsquo;dogs\u0026rsquo;, \u0026lsquo;cats\u0026rsquo; for someone who replied \u0026lsquo;cats\u0026rsquo; and \u0026lsquo;?\u0026rsquo; if the questions was a blank or very creative (we had people who wanted to be elephants and butterflies :) ).\nTo create three discrete values you would write:\n0 if 'dogs' in str(Dogs_or_cats_.value).lower() else 1 if 'cats' in str(Dogs_or_cats_.value).lower() else 2  Since we have three values, we need to assign them the corresponding indexes. So if there is \u0026lsquo;dogs\u0026rsquo; in the reply, we would get 0 (which we converted to \u0026lsquo;dogs\u0026rsquo; in the Feature Constructor\u0026rsquo;s \u0026lsquo;Values\u0026rsquo; box), 1 if there\u0026rsquo;s \u0026lsquo;cats\u0026rsquo; in the reply and 2 if none of the above apply.\nOk, the next step was to sift through a big pile of attributes. We removed personal information for privacy concerns and selected the ones we cared about the most. For example programming skills, years of experience, contributions to OSS and of course whether someone is a dog or a cat person. :) Select Columns sorts the problem. Here you can download a mock-up workflow (same as above, but without sensitive data).\nNow for some lovely charts. Enjoy!\nPython is our lingua franca, experts wanted!\n20 years of programming experience? Hello outlier!\nOSS all the way!\nSome people love dogs and some love cats. Others prefer elephants and butterflies.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Google Summer of Code application period has come to an end. We\u0026rsquo;ve received 34 applications, some of which were of truly high quality. Now it\u0026rsquo;s upon us to select the top performing candidates, but before that we wanted to have an overlook of the candidate pool. We\u0026rsquo;ve gathered data from our Google Form application and gave it a quick view in Orange.\nFirst, we needed to preprocess the data a bit, since it came in a messy form of strings." ,
	"author" : "AJDA",
	"summary" : "Google Summer of Code application period has come to an end. We\u0026rsquo;ve received 34 applications, some of which were of truly high quality. Now it\u0026rsquo;s upon us to select the top performing candidates, but before that we wanted to have an overlook of the candidate pool. We\u0026rsquo;ve gathered data from our Google Form application and gave it a quick view in Orange.\nFirst, we needed to preprocess the data a bit, since it came in a messy form of strings.",
	"date" : "Apr 14, 2016"
}

    
    , {
    "uri": "/blog/2016/04/01/version-3-3-1-updates-and-features/",
	"title": "Version 3.3.1 - Updates and Features",
	"categories": ["distribution", "orange3", "release", "version"],
	"description": "",
	"content": "About a week ago we issued an updated stable release of Orange, version 3.3.1. We\u0026rsquo;ve introduced some new functionalities and improved a few old ones.\nHere\u0026rsquo;s what\u0026rsquo;s new in this release:\n  New widgets: Distance Matrix for visualizing distance measures in a matrix, Distance Transformation for normalization and inversion of distance matrix, Save Distance Matrix and Distance File for saving and loading distances. Last week we also mentioned a really amazing Silhouette Plot, which helps you visually assess cluster quality.\n  Orange can now read datetime variables in its Time Variable format.\n  Rank outputs scores for each scoring method.\n  Report function had been added to Linear Regression, Univariate Regression, Stochastic Gradient Descent and Distance Transformation widgets.\n  FCBF algorithm has been added to Rank for feature scoring and ReliefF now supports missing target values.\n  Graphs in Classification Tree Viewer can be saved in .dot format.\n  You can view the entire changelog here. :) Enjoy the improvements!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "About a week ago we issued an updated stable release of Orange, version 3.3.1. We\u0026rsquo;ve introduced some new functionalities and improved a few old ones.\nHere\u0026rsquo;s what\u0026rsquo;s new in this release:\n  New widgets: Distance Matrix for visualizing distance measures in a matrix, Distance Transformation for normalization and inversion of distance matrix, Save Distance Matrix and Distance File for saving and loading distances. Last week we also mentioned a really amazing Silhouette Plot, which helps you visually assess cluster quality." ,
	"author" : "AJDA",
	"summary" : "About a week ago we issued an updated stable release of Orange, version 3.3.1. We\u0026rsquo;ve introduced some new functionalities and improved a few old ones.\nHere\u0026rsquo;s what\u0026rsquo;s new in this release:\n  New widgets: Distance Matrix for visualizing distance measures in a matrix, Distance Transformation for normalization and inversion of distance matrix, Save Distance Matrix and Distance File for saving and loading distances. Last week we also mentioned a really amazing Silhouette Plot, which helps you visually assess cluster quality.",
	"date" : "Apr 1, 2016"
}

    
    , {
    "uri": "/blog/2016/03/23/all-i-see-is-silhouette/",
	"title": "All I See is Silhouette",
	"categories": ["analysis", "classification", "clustering", "examples", "forestlearner", "orange3", "plot", "visualization"],
	"description": "",
	"content": "Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn\u0026rsquo;t wait to get it into Orange3. And now we did.\nWhat this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster. Instances with silhouette scores close to 0 are on the border between two clusters. Overall, the quality of the clustering could be assessed by the average silhouette scores of the data instances. But here, we are more interested in the individual silhouettes and their visualization in the silhouette plot.\nUsing the good old iris data set, we are going to assess the silhouettes for each of the data instances. In k-means we set the number of clusters to 3 and send the data to Silhouette plot. Good clusters should include instances with higher silhouette scores. But we\u0026rsquo;re doing the opposite. In Orange, we are selecting instances with scores close to 0 from the silhouette plot and pass them to other widgets for exploration. No surprise, they are at the periphery of two clusters. This is so perfectly demonstrated in the scatter plot.\nLet\u0026rsquo;s do something wild now. We\u0026rsquo;ll use the silhouette on a class attribute of Iris (no clustering here, just using the original class values from the data set). Here is our hypothesis: the data instances with low silhouette values are also those that will be misclassified by some learning algorithm. Say, by a random forest.\nWe will use ten-fold cross validation in Test\u0026amp;Score, send the evaluation results to confusion matrix and select misclassified instances in the widget. Then we will explore the inclusion of these misclassifications in the set of low-silhouette instances in the Venn diagram. The agreement (i.e. the intersection in Venn) between the two techniques is quite high.\nFinally, we can observe these instances in the Scatter Plot. Classifiers indeed have problems with borderline data instances. Our hypothesis was correct.\nSilhouette plot is yet another one of the great visualizations that can help you with data analysis or with understanding certain machine learning concepts. What did we say? Fruitful and fun!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn\u0026rsquo;t wait to get it into Orange3. And now we did.\nWhat this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster." ,
	"author" : "AJDA",
	"summary" : "Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn\u0026rsquo;t wait to get it into Orange3. And now we did.\nWhat this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster.",
	"date" : "Mar 23, 2016"
}

    
    , {
    "uri": "/blog/2016/03/12/overfitting-and-regularization/",
	"title": "Overfitting and Regularization",
	"categories": ["analysis", "education", "examples", "orange3", "overfitting", "plot", "regression", "tutorial"],
	"description": "",
	"content": "A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit. This widget can also expand the data set by adding columns with powers of original attribute x, thereby augmenting the training set with x^p, where x is our original attribute and p an integer going from 2 to K. The polynomial expansion of data sets allows linear regression model to nicely fit the data, and with higher K to overfit it to extreme, especially if the number of data points in the training set is low.\nWe have already blogged about this experiment a while ago, showing that it is easy to see that linear regression coefficients blow out of proportion with increasing K. This leads to the idea that linear regression should not only minimize the squared error when predicting the value of dependent variable in the training set, but also keep model coefficients low, or better, penalize any high value of coefficients. This procedure is called regularization. Based on the type of penalty (sum of coefficient squared or sum of absolute values), the regularization is referred to L1 or L2, or, ridge and lasso regression.\nIt is quite easy to play with regularized models in Orange by attaching a Linear Regression widget to Polynomial Regression, in this way substituting the default model used in Polynomial Regression with the one designed in Linear Regression widget. This makes available different kinds of regularization. This workflow can be used to show that the regularized models less overfit the data, and that the overfitting depends on the regularization coefficient which governs the degree of penalty stemming from the value of coefficients of the linear model.\nI also use this workflow to show the difference between L1 and L2 regularization. The change of the type of regularization is most pronounced in the table of coefficients (Data Table widget), where with L1 regularization it is clear that this procedure results in many of those being 0. Try this with high value for degree of polynomial expansion, and a data set with about 10 data points. Also, try changing the regularization regularization strength (Linear Regression widget).\nWhile the effects of overfitting and regularization are nicely visible in the plot in Polynomial Regression widget, machine learning models are really about predictions. And the quality of predictions should really be estimated on independent test set. So at this stage of the lecture I needed to introduce the model scoring, that is, a measure that tells me how well my model inferred on the training set performs on the test set. For simplicity, I chose to introduce root mean squared error (RMSE) and then crafted the following workflow.\nHere, I draw the data set (Paint Data, about 20 data instances), assigned y as the target variable (Select Columns), split the data to training and test sets of approximately equal sizes (Data Sampler), and pass training and test data and linear model to the Test \u0026amp; Score widget. Then I can use linear regression with no regularization, and expect how RMSE changes with changing the degree of the polynomial. I can alternate between Test on train data and Test on test data (Test \u0026amp; Score widget). In the class I have used the blackboard to record this dependency. For the data from the figure, I got the following table:\n Poly K, RMSE Train, RMSE Test 0, 0.147, 0.138 1, 0.155, 0.192 2, 0.049, 0.063 3, 0.049, 0.063 4, 0.049, 0.067 5, 0.040, 0.408 6, 0.040, 0.574 7, 0.033, 2.681 8, 0.001, 5.734 9, 0.000, 4.776  That\u0026rsquo;s it. For the class of computer scientists, one may do all this in scripting, but for any other audience, or for any introductory lesson, explaining of regularization with Orange widgets is a lot of fun.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit." ,
	"author" : "BLAZ",
	"summary" : "A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit.",
	"date" : "Mar 12, 2016"
}

    
    , {
    "uri": "/blog/2016/03/03/orange-at-google-summer-of-code-2016/",
	"title": "Orange at Google Summer of Code 2016",
	"categories": ["gsoc", "gsoc2016", "orange3"],
	"description": "",
	"content": "Orange team is extremely excited to be a part of this year\u0026rsquo;s Google Summer of Code! GSoC is a great opportunity for students around the world to spend their summer contributing to an open-source software, gaining experience and earning money.\nAccepted students will help us develop Orange (or other chosen OSS project) from May to August. Each student is expected to select and define a project of his/her interest and will be ascribed a mentor to guide him/her through the entire process.\nApply here:\nhttps://summerofcode.withgoogle.com/\nOrange\u0026rsquo;s project proposals (we accept your own ideas as well!):\nhttps://github.com/biolab/orange3/wiki/GSoC-2016\nOur GSoC community forum:\nhttps://groups.google.com/forum/#!forum/orange-gsoc\nSpread the word! (and don\u0026rsquo;t forget to apply ;) )\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange team is extremely excited to be a part of this year\u0026rsquo;s Google Summer of Code! GSoC is a great opportunity for students around the world to spend their summer contributing to an open-source software, gaining experience and earning money.\nAccepted students will help us develop Orange (or other chosen OSS project) from May to August. Each student is expected to select and define a project of his/her interest and will be ascribed a mentor to guide him/her through the entire process." ,
	"author" : "AJDA",
	"summary" : "Orange team is extremely excited to be a part of this year\u0026rsquo;s Google Summer of Code! GSoC is a great opportunity for students around the world to spend their summer contributing to an open-source software, gaining experience and earning money.\nAccepted students will help us develop Orange (or other chosen OSS project) from May to August. Each student is expected to select and define a project of his/her interest and will be ascribed a mentor to guide him/her through the entire process.",
	"date" : "Mar 3, 2016"
}

    
    , {
    "uri": "/blog/2016/02/26/getting-started-series-pt2/",
	"title": "Getting Started Series: Part Two",
	"categories": ["tutorial", "youtube"],
	"description": "",
	"content": "We\u0026rsquo;ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.\nThe first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:\n  while the second video introduces classification trees and predictive modelling:\n  The seventh video in the series will address how to score classification and regression models by different evaluation methods. Fruits and vegetables data set can be found here.\nIf you have an idea what you\u0026rsquo;d like to see in the upcoming videos, let us know!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We\u0026rsquo;ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.\nThe first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:\n  while the second video introduces classification trees and predictive modelling:\n  The seventh video in the series will address how to score classification and regression models by different evaluation methods." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.\nThe first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:\n  while the second video introduces classification trees and predictive modelling:\n  The seventh video in the series will address how to score classification and regression models by different evaluation methods.",
	"date" : "Feb 26, 2016"
}

    
    , {
    "uri": "/blog/2016/01/29/tips-and-tricks-for-data-preparation/",
	"title": "Tips and Tricks for Data Preparation",
	"categories": ["data", "dataloading"],
	"description": "",
	"content": "Probably the most crucial step in your data analysis is purging and cleaning your data. Here are a couple of cool tricks that will make your data preparation a bit easier.\n  Use a smart text editor. We can recommend Sublime Text as it an extremely versatile editor that supports a broad variety of programming languages and markups, but there are other great tools out there as well. One of the best things you\u0026rsquo;ll keep coming back to in your editor is \u0026lsquo;Replace\u0026rsquo; function that allows you to replace specified values with different ones. You can also use regex to easily find and replace parts of text.\nWe can replace all instances of \u0026lsquo;male\u0026rsquo; with \u0026lsquo;man\u0026rsquo; in one click.\n  Apply simple hacks. Sometimes when converting files to different formats data can get some background information appended that you cannot properly see. A cheap and dirty trick is to manually select the cells and rows and copy-paste them to a new sheet. This will start with a clean slate and you data will be read properly.\n  Check your settings. When reading .csv files in Excel, you might see all your data squished in one column and literally separated with commas. This can be easily solved with Data \u0026ndash;\u0026gt; From Text (Get external data) and a new window will appear. In a Text Import Wizard you can set whether your data is delimited or not (in our case it is), how it is delimited (comma, tab, etc.), whether you have a header or not, what qualifies as text (\u0026quot; is a recommended option), what is your encoding and so on.\n  Manually annotate the data. Orange loves headers and the easiest way to assure your data gets read properly is to set the header yourself. Add two extra rows under your feature names. In the first row, set your variable type and in the second one, your kind. Here\u0026rsquo;s how to do it properly.\n  Exploit the widgets in Orange. Select Columns is your go-to widget for organizing what gets read as a meta attribute, what is your class variable and which features you want to use in your analysis. Another great widget is Edit domain, where you can set the way the values are displayed in the analysis (say you have \u0026ldquo;grey\u0026rdquo; in your data, but you want it to say \u0026ldquo;gray\u0026rdquo;). Moreover, you can use Concatenate and Merge widgets to put your data together.\nSet domain with Edit domain widget.\n  What\u0026rsquo;s your preferred trick?\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Probably the most crucial step in your data analysis is purging and cleaning your data. Here are a couple of cool tricks that will make your data preparation a bit easier.\n  Use a smart text editor. We can recommend Sublime Text as it an extremely versatile editor that supports a broad variety of programming languages and markups, but there are other great tools out there as well. One of the best things you\u0026rsquo;ll keep coming back to in your editor is \u0026lsquo;Replace\u0026rsquo; function that allows you to replace specified values with different ones." ,
	"author" : "AJDA",
	"summary" : "Probably the most crucial step in your data analysis is purging and cleaning your data. Here are a couple of cool tricks that will make your data preparation a bit easier.\n  Use a smart text editor. We can recommend Sublime Text as it an extremely versatile editor that supports a broad variety of programming languages and markups, but there are other great tools out there as well. One of the best things you\u0026rsquo;ll keep coming back to in your editor is \u0026lsquo;Replace\u0026rsquo; function that allows you to replace specified values with different ones.",
	"date" : "Jan 29, 2016"
}

    
    , {
    "uri": "/blog/2016/01/22/predictive-analytics-with-orange/",
	"title": "Making Predictions",
	"categories": ["analysis", "data", "examples", "predictive analytics", "widget"],
	"description": "",
	"content": "One of the cool things about being a data scientist is being able to predict. That is, predict before we know the actual outcome. I am not talking about verifying your favorite classification algorithm here, and I am not talking about cross-validation or classification accuracies or AUC or anything like that. I am talking about the good old prediction. This is where our very own Predictions widget comes to help.\nPredictions workflow.\nWe will be exploring the Iris data set again, but we\u0026rsquo;re going to add a little twist to it. Since we\u0026rsquo;ve worked so much with it already, I\u0026rsquo;m sure you know all about this data. But now we got three new flowers in the office and of course there\u0026rsquo;s no label attached to tell us what species of Iris these flowers are. [sigh\u0026hellip;.] Obviously, we will be measuring petals and sepals and contrasting the results with our data.\nOur new data on three flowers. We have used Google Sheets to enter the data and the copied the sharable link and pasted the link to the File widget.\nBut surely you don\u0026rsquo;t want to go through all 150 flowers to properly match the three new Irises? So instead, let\u0026rsquo;s first train a model on the existing data set. We connect the File widget to the chosen classifier (we went with Classification Tree this time) and feed the results into Predictions. Now we write down the measurements for our new flowers into Google Sheets (just like above), load it into Orange with a new File widget and input the fresh data into Predictions. We can observe the predicted class directly in the widget itself.\nPredictions made by classification tree.\nIn the left part of the visualization we have the input data set (our measurements) and in the right part the predictions made with classification tree. By default you see probabilities for all three class values and the predicted class. You can of course use other classifiers as well - it would probably make sense to first evaluate classifiers on the existing data set, find the best one for your and then use it on the new data.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One of the cool things about being a data scientist is being able to predict. That is, predict before we know the actual outcome. I am not talking about verifying your favorite classification algorithm here, and I am not talking about cross-validation or classification accuracies or AUC or anything like that. I am talking about the good old prediction. This is where our very own Predictions widget comes to help." ,
	"author" : "AJDA",
	"summary" : "One of the cool things about being a data scientist is being able to predict. That is, predict before we know the actual outcome. I am not talking about verifying your favorite classification algorithm here, and I am not talking about cross-validation or classification accuracies or AUC or anything like that. I am talking about the good old prediction. This is where our very own Predictions widget comes to help.",
	"date" : "Jan 22, 2016"
}

    
    , {
    "uri": "/blog/2016/01/04/orange-youtube-tutorials/",
	"title": "Orange YouTube Tutorials",
	"categories": ["analysis", "data", "examples", "orange3", "tutorial", "youtube"],
	"description": "",
	"content": "It\u0026rsquo;s been a long time coming, but finally we\u0026rsquo;ve created out our first set of YouTube tutorials. In a series \u0026lsquo;Getting Started with Orange\u0026rsquo; we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy. Below are the first three videos from this series, more are coming in the following weeks.\n      We are also preparing a series called \u0026lsquo;Data Science with Orange\u0026rsquo;, which will take you on a journey through the world of data mining and machine learning by explaining predictive modeling, classification, regression, model evaluation and much more.\nFeel free to let us know what tutorials you\u0026rsquo;d like to see and we\u0026rsquo;ll do our best to include it in one of the two series. :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "It\u0026rsquo;s been a long time coming, but finally we\u0026rsquo;ve created out our first set of YouTube tutorials. In a series \u0026lsquo;Getting Started with Orange\u0026rsquo; we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy." ,
	"author" : "AJDA",
	"summary" : "It\u0026rsquo;s been a long time coming, but finally we\u0026rsquo;ve created out our first set of YouTube tutorials. In a series \u0026lsquo;Getting Started with Orange\u0026rsquo; we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy.",
	"date" : "Jan 4, 2016"
}

    
    , {
    "uri": "/blog/2015/12/28/color-it/",
	"title": "Color it!",
	"categories": ["orange3", "plot", "visualization", "widget"],
	"description": "",
	"content": "Holiday season is upon us and even the Orange team is in a festive mood. This is why we made a Color widget!\nThis fascinating artsy widget will allow you to play with your data set in a new and exciting way. No more dull visualizations and default color schemes! Set your own colors the way YOU want it to! Care for some magical cyan-to-magenta? Or do you prefer a more festive red-to-green? How about several shades of gray? Color widget is your go-to stop for all things color (did you notice it’s our only widget with a colorful icon?). :)\nColoring works with most visualization widgets, such as scatter plot, distributions, box plot, mosaic display and linear projection. Set the colors for discrete values and gradients for continuous values in this widget, and the same palletes will be used in all downstream widgets. As a bonus, the Color widget also allows you to edit the names of variables and values.\nRemember - the (blue) sky is the limit.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Holiday season is upon us and even the Orange team is in a festive mood. This is why we made a Color widget!\nThis fascinating artsy widget will allow you to play with your data set in a new and exciting way. No more dull visualizations and default color schemes! Set your own colors the way YOU want it to! Care for some magical cyan-to-magenta? Or do you prefer a more festive red-to-green?" ,
	"author" : "AJDA",
	"summary" : "Holiday season is upon us and even the Orange team is in a festive mood. This is why we made a Color widget!\nThis fascinating artsy widget will allow you to play with your data set in a new and exciting way. No more dull visualizations and default color schemes! Set your own colors the way YOU want it to! Care for some magical cyan-to-magenta? Or do you prefer a more festive red-to-green?",
	"date" : "Dec 28, 2015"
}

    
    , {
    "uri": "/blog/2015/12/19/model-based-feature-scoring/",
	"title": "Model-Based Feature Scoring",
	"categories": ["analysis", "classification", "features", "regression", "scoring"],
	"description": "",
	"content": "Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression. Same holds for feature weight: any change of parameters of the modeling techniques would change the resulting feature scores.\nIt would thus be great if we could observe these changes and compare feature ranking provided by various machine learning methods. For this purpose, the Rank widget recently got a new input channel called scorer. We can attach any learner that can provide feature scores to the input of Rank, and then observe the ranking in the Rank table.\nSay, for the famous voting data set (File widget, Browse documentation data sets), the last two feature score columns were obtained by random forest and logistic regression with L1 regularization (C=0.1). Try changing the regularization parameter and type to see changes in feature scores.\nFeature weights for logistic and linear regression correspond to the absolute value of coefficients of their linear models. To observe their untransformed values in the table, these widgets now also output a data table with feature weights. (At the time of the writing of this blog, this feature has been implemented for linear regression; other classifiers and regressors that can estimate feature weights will be updated soon).\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression." ,
	"author" : "BLAZ",
	"summary" : "Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression.",
	"date" : "Dec 19, 2015"
}

    
    , {
    "uri": "/blog/2015/12/11/report-is-back-and-better-than-ever/",
	"title": "Report is back! (and better than ever)",
	"categories": ["analysis", "data", "orange3", "report"],
	"description": "",
	"content": "I’m sure you’d agree that reporting your findings when analyzing the data is crucial. Say you have a couple of interesting predictions that you’ve tested with several methods many times and you’d like to share that with the world. Here’s how.\nSave Graph just got company - a Report button! Report works in most widgets, apart from the very obvious ones that simply transmit or display the data (Python Scripting, Edit Domain, Image Viewer, Predictions…).\nWhy is Report so great?\n  Display data and graphs used in your workflow. Whatever you do with your data will be put in the report upon a click of a button.\n  Write comments below each section in your workflow. Put down whatever matters for your research - pitfalls and advantages of a model, why this methodology works, amazing discoveries, etc.\n  Access your workflows. Every step of the analysis recorded in the Report is saved as a workflow and can be accessed by clicking on the Orange icon. Have you spent hours analyzing your data only to find out you made a wrong turn somewhere along the way? No problem. Report saves workflows for each step of the analysis. Perhaps you would like to go back and start again from Bo Plot? Click on the Orange icon next to Box Plot and you will be taken to the workflow you had when you placed that widget in the report. Completely stress-free!\n  Save your reports. The amazing new report that you just made can be saved as .html, .pdf or .report file. Html and PDF are pretty standard, but report format is probably the best thing since sliced bread. Why? Not only it saves your report file for later use, you can also send it to your colleagues and they will be able to access both your report and workflows used in the analysis.\n  Open report. To open a saved report file go to File → Open Report. To view the report you’re working on, go to Options → Show report view or click Shift+R.\n  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "I’m sure you’d agree that reporting your findings when analyzing the data is crucial. Say you have a couple of interesting predictions that you’ve tested with several methods many times and you’d like to share that with the world. Here’s how.\nSave Graph just got company - a Report button! Report works in most widgets, apart from the very obvious ones that simply transmit or display the data (Python Scripting, Edit Domain, Image Viewer, Predictions…)." ,
	"author" : "AJDA",
	"summary" : "I’m sure you’d agree that reporting your findings when analyzing the data is crucial. Say you have a couple of interesting predictions that you’ve tested with several methods many times and you’d like to share that with the world. Here’s how.\nSave Graph just got company - a Report button! Report works in most widgets, apart from the very obvious ones that simply transmit or display the data (Python Scripting, Edit Domain, Image Viewer, Predictions…).",
	"date" : "Dec 11, 2015"
}

    
    , {
    "uri": "/blog/2015/12/04/2uda/",
	"title": "2UDA",
	"categories": ["sql"],
	"description": "",
	"content": "In one of the previous blog posts we mentioned that installing the optional dependency psycopg2 allows Orange to connect to PostgreSQL databases and work directly on the data stored there. It is also possible to transfer a whole table to the client machine, keep it in the local memory, and continue working with it as with any other Orange data set loaded from a file. But the true power of this feature lies in the ability of Orange to leave the bulk of the data on the server, delegate some of the computations to the database, and transfer only the needed results. This helps especially when the connection is too slow to transfer all the data and when the data is too big to fit in the memory of the local machine, since SQL databases are much better equipped to work with large quantities of data residing on the disk.\nIf you want to test this feature it is now even easier to do so! A third party distribution called 2UDA provides a single installer for all major OS platforms that combines Orange and a PostgreSQL 9.5 server along with LibreOffice (optional) and installs all the needed dependencies. The database even comes with some sample data sets that can be used to start testing and using Orange out of the box. 2UDA is also a great way to get the very latest version of PostgreSQL, which is important for Orange as it relies heavily on its new TABLESAMPLE clause. It enables time-based sampling of tables, which is used in Orange to get approximate results quickly and allow responsive and interactive work with big data.\nWe hope this will help us reach an even wider audience and introduce Orange to a whole new group of people managing and storing their data in SQL databases. We believe that having lots of data is a great starting point, but the benefits truly kick in with the ability to easily extract useful information from it.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "In one of the previous blog posts we mentioned that installing the optional dependency psycopg2 allows Orange to connect to PostgreSQL databases and work directly on the data stored there. It is also possible to transfer a whole table to the client machine, keep it in the local memory, and continue working with it as with any other Orange data set loaded from a file. But the true power of this feature lies in the ability of Orange to leave the bulk of the data on the server, delegate some of the computations to the database, and transfer only the needed results." ,
	"author" : "LAN",
	"summary" : "In one of the previous blog posts we mentioned that installing the optional dependency psycopg2 allows Orange to connect to PostgreSQL databases and work directly on the data stored there. It is also possible to transfer a whole table to the client machine, keep it in the local memory, and continue working with it as with any other Orange data set loaded from a file. But the true power of this feature lies in the ability of Orange to leave the bulk of the data on the server, delegate some of the computations to the database, and transfer only the needed results.",
	"date" : "Dec 4, 2015"
}

    
    , {
    "uri": "/blog/2015/12/02/hierarchical-clustering-a-simple-explanation/",
	"title": "Hierarchical Clustering: A Simple Explanation",
	"categories": ["clustering", "education", "plot"],
	"description": "",
	"content": "One of the key techniques of exploratory data mining is clustering – separating instances into distinct groups based on some measure of similarity. We can estimate the similarity between two data instances through euclidean (pythagorean), manhattan (sum of absolute differences between coordinates) and mahalanobis distance (distance from the mean by standard deviation), or, say, through Pearson correlation or Spearman correlation.\nOur main goal when clustering data is to get groups of data instances where:\n each group (Ci) is a a subset of the training data (U): Ci ⊂ U an intersection of all the sets is an empty set: Ci ∩ Cj = 0 a union of all groups equals the train data: Ci ∪ Cj = U  This would be ideal. But we rarely get the data, where separation is so clear. One of the easiest techniques to cluster the data is hierarchical clustering. First, we take an instance from, say, 2D plot. Now we want to find its nearest neighbor. Nearest neighbor of course depends on the measure of distance we choose, but let’s go with euclidean for now as it is the easiest to visualize. First steps of hierarchical clustering.\nEuclidean distance is calculated as:\nNaturally, the shorter the distance the more similar the two instances are. In the beginning, all instances are in their own particular clusters. Then we seek for the closest instances of every instance in the plot. We pin down the closest instance and make a cluster of the original and the closest instance. Now we repeat the process again. What is the closest instances to our new cluster –\u0026gt; add it to the cluster –\u0026gt; find the closest instance. We repeat this procedure until all the instances are grouped in one single cluster.\nWe can write this down also in a form of a pseudocode:\nevery instance is in its own cluster repeat until instances are all in one group: find the closest instances to the group (distance has to be minimum) join closest instances with the group  Visualization of this procedure is called a dendrogram, which is what Hierarchical clustering widget displays in Orange.\nSingle, complete and average linkage.\nAnother thing to consider is the distance between instances when we have already two or more instances in a cluster. Do we go with the closest instance in a cluster or to the furthest one?\n Picture A shows the distances to the closest instance – single linkage. Picture B shows the distance to the furthest instance – complete linkage. Picture C shows the average of all distances in a cluster to the instance – average linkage.  Single vs complete linkage.\nThe downside of single linkage is, even by intuition, creating elongated, stretched clusters. Instances at the top part of the red C are in fact quite different from the lower part of the red C. Complete linkage does much better here as it centers clustering nicely. However, the downside of complete linkage is taking outliers too much into consideration. Naturally, each approach has its own pros and cons and it’s good to know how they work in order to use them correctly. One extra hint: single linkage works great for image recognition, exactly because it can follow the curve.\nThere’s a lot more we could say about hierarchical clustering, but to sum it up, let’s state pros and cons of this method:\n pros: sums up the data, good for small data sets cons: computationally demanding, fails on larger sets  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One of the key techniques of exploratory data mining is clustering – separating instances into distinct groups based on some measure of similarity. We can estimate the similarity between two data instances through euclidean (pythagorean), manhattan (sum of absolute differences between coordinates) and mahalanobis distance (distance from the mean by standard deviation), or, say, through Pearson correlation or Spearman correlation.\nOur main goal when clustering data is to get groups of data instances where:" ,
	"author" : "AJDA",
	"summary" : "One of the key techniques of exploratory data mining is clustering – separating instances into distinct groups based on some measure of similarity. We can estimate the similarity between two data instances through euclidean (pythagorean), manhattan (sum of absolute differences between coordinates) and mahalanobis distance (distance from the mean by standard deviation), or, say, through Pearson correlation or Spearman correlation.\nOur main goal when clustering data is to get groups of data instances where:",
	"date" : "Dec 2, 2015"
}

    
    , {
    "uri": "/blog/2015/11/27/mining-our-own-data/",
	"title": "Mining our own data",
	"categories": ["analysis", "data", "distribution", "orange3", "visualization"],
	"description": "",
	"content": "Recently we\u0026rsquo;ve made a short survey that was, upon Orange download, asking people how they found out about Orange, what was their data mining level and where do they work. The main purpose of this is to get a better insight into our user base and to figure out what is the profile of people interested in trying Orange.\nHere we have some preliminary results that we\u0026rsquo;ve managed to gather in the past three weeks or so. Obviously we will use Orange to help us make sense of the data.\nWe\u0026rsquo;ve downloaded our data from Typeform and appended some background information such as OS and browser. Let\u0026rsquo;s see what we\u0026rsquo;ve got in the Data Table widget.\nOk, this is our entire data table. Here we also have the data on people who completed the survey and who didn\u0026rsquo;t. First, let\u0026rsquo;s organize the data properly. We\u0026rsquo;ll do this with Select Columns widget.\nWe removed all the meta attributes as they are not very relevant for our analysis. Next we moved the \u0026lsquo;completed\u0026rsquo; attribute into target variable, thus making it our class variable.\nNow we would like to see some basic distributions from our data.\nInteresting. Most of our users are working on Windows, a few on Mac and very few on Linux.\nLet\u0026rsquo;s investigate further. Now we want to know more about those people who actually completed the survey. Let\u0026rsquo;s use Select Columns again, this time removing os_type, os_name, agent_name and completed from our data and keeping just the answers. We made \u0026ldquo;Where do you work?\u0026rdquo; our class variable, but we could use either one of the three. Another trick is to set in directly in Distributions widget under \u0026lsquo;Group by\u0026rsquo;.\nOk, let\u0026rsquo;s again use Distributions - this is such a simple way to get a good sense of your data.\nObviously out of those who found out about Orange in college, most are students, but what\u0026rsquo;s interesting here is that there are so many. We can also see that out of those who found us on the web, most come from the private sector, followed by academia and researchers. Good. How about the other question?\nAgain, results are not particularly shocking, but it\u0026rsquo;s great to confirm your hypothesis with real data. Out of beginner level data miners, most are students, while most intermediate users come from the industry.\nA quick look at the Mosaic Display will give us a good overview:\nYup, this sums it up quite nicely. We have lots of beginner levels users and not many expert ones (height of the box). Also most people found out about Orange on the web or in college (width of the box). A thin line on the left shows apriori distribution, thus making it easier to compare expected and actual number of instances. For example, there should be at least some people who are students and have found out about Orange at a conference. But there aren\u0026rsquo;t - a contrast between how much red there should be in the box (line on the left) and how much there actually is (bigger part of the box) is quite telling. We can even select all the beginner level users who found out about Orange in college and further inspect the data, but be it enough for now.\nOur final workflow:\nObviously, this is a very simple analysis. But even such simple tasks are never boring with good visualization tools such as Distributions and Mosaic Display. You could also use Venn Diagram to find common features of selected subsets or perhaps Sieve Diagram for probabilities.\nWe are very happy to get these data and we would like to thank everyone who completed the survey. If you wish to help us further, please fill out a longer survey that won\u0026rsquo;t actually take you more than 3 minutes of your time (we timed it!).\nHappy Friday everyone!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Recently we\u0026rsquo;ve made a short survey that was, upon Orange download, asking people how they found out about Orange, what was their data mining level and where do they work. The main purpose of this is to get a better insight into our user base and to figure out what is the profile of people interested in trying Orange.\nHere we have some preliminary results that we\u0026rsquo;ve managed to gather in the past three weeks or so." ,
	"author" : "AJDA",
	"summary" : "Recently we\u0026rsquo;ve made a short survey that was, upon Orange download, asking people how they found out about Orange, what was their data mining level and where do they work. The main purpose of this is to get a better insight into our user base and to figure out what is the profile of people interested in trying Orange.\nHere we have some preliminary results that we\u0026rsquo;ve managed to gather in the past three weeks or so.",
	"date" : "Nov 27, 2015"
}

    
    , {
    "uri": "/blog/2015/10/30/ghostbusters/",
	"title": "Ghostbusters",
	"categories": ["analysis", "data", "distribution", "orange3"],
	"description": "",
	"content": "Ok, we’ve just recently stumbled across an interesting article on how to deal with non normal (non-Gaussian distributed) data. We have an absolutely paranormal data set of 20 persons with weight, height, paleness, vengefulness, habitation and age attributes (download).\nLet’s check the distribution in Distributions widget.\nOur first attribute is “Weight” and we see a little hump on the left. Otherwise the data would be normally distributed. Ok, so perhaps we have a few children in the data set. Let’s check the age distribution. Whoa, what? Why is the hump now on the right? These distributions look scary. We seem to have a few reaaaaally old people here. What is going on? Perhaps we can figure this out with MDS. This widget projects the data into two dimensions so that the distances between the points correspond to differences between the data instances.\nAha! Now we see that three instances are quite different from all others. Select them and send them to the Data Table for final inspection.\nBusted! We have found three ghosts hiding in our data. They are extremely light (the sheet they are wearing must weight around 2kg), quite vengeful and old.\nNow, joke aside, what would this mean for a general non-normally distributed data? One thing is your data set might be too small. Here we only have 20 instances, thus 3 outlying ghosts have a great impact on the distribution. It is difficult to hide 3 ghosts among 17 normal persons.\nSecondly, why can’t we use Outliers widget to hunt for those ghosts? Again, our data set is too small. With just 20 instances, the estimation variance is so large that it can easily cover a few ghosts under its sheet. We don’t have enough “normal” data to define what is normal and thus detect the paranormal.\nHaven’t we just written two exactly opposite things? Perhaps.\nHappy Halloween everybody! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Ok, we’ve just recently stumbled across an interesting article on how to deal with non normal (non-Gaussian distributed) data. We have an absolutely paranormal data set of 20 persons with weight, height, paleness, vengefulness, habitation and age attributes (download).\nLet’s check the distribution in Distributions widget.\nOur first attribute is “Weight” and we see a little hump on the left. Otherwise the data would be normally distributed. Ok, so perhaps we have a few children in the data set." ,
	"author" : "AJDA",
	"summary" : "Ok, we’ve just recently stumbled across an interesting article on how to deal with non normal (non-Gaussian distributed) data. We have an absolutely paranormal data set of 20 persons with weight, height, paleness, vengefulness, habitation and age attributes (download).\nLet’s check the distribution in Distributions widget.\nOur first attribute is “Weight” and we see a little hump on the left. Otherwise the data would be normally distributed. Ok, so perhaps we have a few children in the data set.",
	"date" : "Oct 30, 2015"
}

    
    , {
    "uri": "/blog/2015/10/19/sql-for-orange/",
	"title": "SQL for Orange",
	"categories": ["data", "orange3", "sql"],
	"description": "",
	"content": "We bet you\u0026rsquo;ve always wanted to use your SQL data in Orange, but you might not be quite sure how to do it. Don\u0026rsquo;t worry, we\u0026rsquo;re coming to the rescue.\nThe key to SQL files is installation of \u0026lsquo;psycopg2\u0026rsquo; library in Python.\nWINDOWS\nGo to this website and download psycopg2 package. Once your .whl file has downloaded, go to the file directory and run command prompt. Enter “pip install [file name]” and run it.\nMAC OS X, LINUX\nIf you’re on Mac or Linux, install psycopg2 with this.\nUpon opening Orange, you will be able to see a lovely new icon - SQL Table. Then just connect to your server and off you go!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We bet you\u0026rsquo;ve always wanted to use your SQL data in Orange, but you might not be quite sure how to do it. Don\u0026rsquo;t worry, we\u0026rsquo;re coming to the rescue.\nThe key to SQL files is installation of \u0026lsquo;psycopg2\u0026rsquo; library in Python.\nWINDOWS\nGo to this website and download psycopg2 package. Once your .whl file has downloaded, go to the file directory and run command prompt. Enter “pip install [file name]” and run it." ,
	"author" : "AJDA",
	"summary" : "We bet you\u0026rsquo;ve always wanted to use your SQL data in Orange, but you might not be quite sure how to do it. Don\u0026rsquo;t worry, we\u0026rsquo;re coming to the rescue.\nThe key to SQL files is installation of \u0026lsquo;psycopg2\u0026rsquo; library in Python.\nWINDOWS\nGo to this website and download psycopg2 package. Once your .whl file has downloaded, go to the file directory and run command prompt. Enter “pip install [file name]” and run it.",
	"date" : "Oct 19, 2015"
}

    
    , {
    "uri": "/blog/2015/10/16/learners-in-python/",
	"title": "Learners in Python",
	"categories": ["classification", "examples", "orange3", "python", "scripting"],
	"description": "",
	"content": "We\u0026rsquo;ve already written about classifying instances in Python. However, it\u0026rsquo;s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.\nTRAINING THE CLASSIFIER\nWe start with simply importing Orange module into Python and loading our data set.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; import Orange \u0026gt;\u0026gt;\u0026gt;\u0026gt; data = Orange.data.Table(\u0026quot;titanic\u0026quot;)  We are using \u0026lsquo;titanic.tab\u0026rsquo; data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression). Now we want to train our classifier.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; learner = Orange.classification.LogisticRegressionLearner() \u0026gt;\u0026gt;\u0026gt;\u0026gt; classifier = learner(data) \u0026gt;\u0026gt;\u0026gt;\u0026gt; classifier(data[0])  Python returns the index of the value, as usual.\narray[0.]  To check what\u0026rsquo;s in the class variable we print:\n\u0026gt;\u0026gt;\u0026gt;\u0026gt;print(\u0026quot;Name of the variable: \u0026quot;, data.domain.class_var.name) \u0026gt;\u0026gt;\u0026gt;\u0026gt;print(\u0026quot;Class values: \u0026quot;, data.domain.class_var.values) \u0026gt;\u0026gt;\u0026gt;\u0026gt;print(\u0026quot;Value of our instance: \u0026quot;, data.domain.class_var.values[0]) Name of the variable: survived Class values: no, yes Value of our instance: no  PREDICTIONS\nIf you want to get predictions for the entire data set, just give the classifier the entire data set.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; classifier(data) array[0, 0, 0, ..., 1, 1, 1]  If we want to append predictions to the data table, first use classifier on the data, then create a new domain with an additional meta attribute and finally form a new data table with appended predictions:\nsvm = classifier(data) new_domain = Orange.data.Domain(data.domain.attributes, data.domain.class_vars, [data.domain.class_var]) table2 = Orange.data.Table(new_domain, data.X, data.Y, svm.reshape(-1, 1))  We use .reshape to transform vector data into a reshaped array. Then we print out the data.\nprint(table2)  PARAMETERS\nWant to use another classifier? The procedure is the same, simply use:\nOrange.classification.\u0026lt;algorithm-name\u0026gt;()  For most classifiers, you can set a whole range of parameters. Logistic Regression, for example, uses the following:\nlearner = Orange.classification.LogisticRegressionLearner(**penalty**='l2', **dual**=False, **tol**=0.0001, **C**=1.0, **fit_intercept**=True, **intercept_scaling**=1, **class_weight**=None, **random_state**=None **preprocessors**=None)  To check the parameters for the classifier, use:\nprint(Orange.classification.SVMLearner())  PROBABILITIES\nAnother thing you can check with classifiers are the probabilities.\nclassifier(data[0], Orange.classification.Model.ValueProbs) \u0026gt;\u0026gt;\u0026gt;(array([ 0.]), array([[ 1., 0.]]))  The first array is the value for your selected instance (data[0]), while the second array contains probabilities for class values (probability for ‘no’ is 1 and for ‘yes’ 0).\nCLASSIFIERS\nAnd because we care about you, we’re giving you here a full list of classifier names:\nLogisticRegressionLearner()\nNaiveBayesLearner()\nKNNLearner()\nTreeLearner()\nMajorityLearner()\nRandomForestLearner()\nSVMLearner()\nFor other learners, you can find all the parameters and descriptions in the documentation.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We\u0026rsquo;ve already written about classifying instances in Python. However, it\u0026rsquo;s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.\nTRAINING THE CLASSIFIER\nWe start with simply importing Orange module into Python and loading our data set.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; import Orange \u0026gt;\u0026gt;\u0026gt;\u0026gt; data = Orange.data.Table(\u0026quot;titanic\u0026quot;)  We are using \u0026lsquo;titanic.tab\u0026rsquo; data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression)." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve already written about classifying instances in Python. However, it\u0026rsquo;s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.\nTRAINING THE CLASSIFIER\nWe start with simply importing Orange module into Python and loading our data set.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; import Orange \u0026gt;\u0026gt;\u0026gt;\u0026gt; data = Orange.data.Table(\u0026quot;titanic\u0026quot;)  We are using \u0026lsquo;titanic.tab\u0026rsquo; data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression).",
	"date" : "Oct 16, 2015"
}

    
    , {
    "uri": "/blog/2015/10/09/data-mining-course-in-houston/",
	"title": "Data Mining Course in Houston",
	"categories": ["dataloading", "education", "orange3", "visualization", "workshop"],
	"description": "",
	"content": "We have just completed an Introduction to Data Mining, a graduate course at Baylor College of Medicine in Texas, Houston. The course was given in September and consisted of seven two-hour lectures, each one followed with a homework assignment. The course was attended by about 40 students and some faculty and research staff.\nThis was a challenging course. The audience was new to data mining, and we decided to teach them with the newest, third version of Orange. We also experimented with two course instructors (Blaz and Janez), who, instead of splitting the course into two parts, taught simultaneously, one on the board and the other one helping the students with hands-on exercises. To check whether this worked fine, we ran a student survey at the end of the course. We used Google Sheets and then examined the results with students in the class. Using Orange, of course.\nAnd the outcome? Looks like the students really enjoyed the course\nand the teaching style.\nThe course took advantage of several new widgets in Orange 3, including those for data preprocessing and polynomial regression. The core development team put a lot of effort during the summer to debug and polish this newest version of Orange. Also thanks to the financial support by AXLE EU FP7 and CARE-MI EU FP7** grants and grants by the Slovene Research agency, we were able to finish everything in time.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have just completed an Introduction to Data Mining, a graduate course at Baylor College of Medicine in Texas, Houston. The course was given in September and consisted of seven two-hour lectures, each one followed with a homework assignment. The course was attended by about 40 students and some faculty and research staff.\nThis was a challenging course. The audience was new to data mining, and we decided to teach them with the newest, third version of Orange." ,
	"author" : "BLAZ",
	"summary" : "We have just completed an Introduction to Data Mining, a graduate course at Baylor College of Medicine in Texas, Houston. The course was given in September and consisted of seven two-hour lectures, each one followed with a homework assignment. The course was attended by about 40 students and some faculty and research staff.\nThis was a challenging course. The audience was new to data mining, and we decided to teach them with the newest, third version of Orange.",
	"date" : "Oct 9, 2015"
}

    
    , {
    "uri": "/blog/2015/10/02/a-visit-from-the-tilburg-university/",
	"title": "A visit from the Tilburg University",
	"categories": ["education", "examples", "overfitting", "regression", "visualization"],
	"description": "",
	"content": "Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it. And then consult with statisticians and data mining expert (of course!).\nBiolab team with Marie and Eric, who is standing next to Ivan Cankar - the very serious guy in the middle.\nTo honor this occasion we invite you to check out the Polynomial regression widget, which is specially intended for educational use. There, you can showcase the problem of overfitting through visualization.\nFirst, we set up a workflow.\nThen we paint, say, at most 10 points into the Paint Data widget. (Why at most ten? You’ll see later.)\nNow we open our Polynomial Regression widget, where we play with polynomial degree. Polynomial Degree 1 gives us a line. With coefficient 2 we get a curve that fits only one point. However, with the coefficient 7 we fit all the points with one curve. Yay!\nBut hold on! The curve now becomes very steep. Would the lower end of the curve at about (0.9, -2.2) still be a realistic estimate of our data set? Probably not. Even when we look at the Data Table with coefficient values, they seem to skyrocket.\nThis is a typical danger of overfitting, which is often hard to explain, but with the help of these three widgets becomes as clear as day! Now go out and share the knowledge!!!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it." ,
	"author" : "AJDA",
	"summary" : "Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it.",
	"date" : "Oct 2, 2015"
}

    
    , {
    "uri": "/blog/2015/09/25/save-your-graphs/",
	"title": "Save your graphs!",
	"categories": ["analysis", "images", "visualization"],
	"description": "",
	"content": "If you are often working with Orange, you probably have noticed a small button at the bottom of most visualization widgets. “Save Graph” now enables you to export graphs, charts, and hierarchical trees to your computer and use them in your reports. Because people need to see it to believe it!\n\u0026ldquo;Save Graph\u0026rdquo; will save visualizations to your computer.\nSave Graph function is available in Paint Data, Image Viewer, all visualization widgets, and a few others (list is below).\nWidgets with the \u0026ldquo;Save Graph\u0026rdquo; option.\nYou can save visualizations in .png, .dot or .svg format. However - brace yourselves - our team is working on something even better, which will be announced in the following weeks.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "If you are often working with Orange, you probably have noticed a small button at the bottom of most visualization widgets. “Save Graph” now enables you to export graphs, charts, and hierarchical trees to your computer and use them in your reports. Because people need to see it to believe it!\n\u0026ldquo;Save Graph\u0026rdquo; will save visualizations to your computer.\nSave Graph function is available in Paint Data, Image Viewer, all visualization widgets, and a few others (list is below)." ,
	"author" : "AJDA",
	"summary" : "If you are often working with Orange, you probably have noticed a small button at the bottom of most visualization widgets. “Save Graph” now enables you to export graphs, charts, and hierarchical trees to your computer and use them in your reports. Because people need to see it to believe it!\n\u0026ldquo;Save Graph\u0026rdquo; will save visualizations to your computer.\nSave Graph function is available in Paint Data, Image Viewer, all visualization widgets, and a few others (list is below).",
	"date" : "Sep 25, 2015"
}

    
    , {
    "uri": "/blog/2015/09/11/hubbing-with-hub-widget/",
	"title": "Hubbing with the Hub widget",
	"categories": ["addons", "data", "download", "orange3", "widget"],
	"description": "",
	"content": "So you have painted two data sets and loaded another one from a file, and now you are testing predictions of logistic regression, classification trees and SVM on it? Tired of having to reconnect the Paint data widget and the File widget back and forth whenever you switch between them?\nSay no more! Look no further! Here is the new Hub widget!\nHub widget is the most versatile widget available so far. It accepts several inputs of any type and outputs them to as many other widgets as you want.\nThe Hub widget treats all types with the strictest equality.\n(It also adheres to all applicable EU policies with respect to gender equality, and does not use cookies.)\nThe Hub widget works like charm and is like the amazing cast-to-void-and-back-to-anything idiom in C. This strongful MacGyver of widgets can (almost) convert classification tree into data, or preprocessor into experimental results without ever touching the data. With its amazing capabilities, the Hub widget has the potential to cause an even greater havoc in your workflows than the famous Merge data widget.\nDownload, install - and start hubbing today !!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "So you have painted two data sets and loaded another one from a file, and now you are testing predictions of logistic regression, classification trees and SVM on it? Tired of having to reconnect the Paint data widget and the File widget back and forth whenever you switch between them?\nSay no more! Look no further! Here is the new Hub widget!\nHub widget is the most versatile widget available so far." ,
	"author" : "AJDA",
	"summary" : "So you have painted two data sets and loaded another one from a file, and now you are testing predictions of logistic regression, classification trees and SVM on it? Tired of having to reconnect the Paint data widget and the File widget back and forth whenever you switch between them?\nSay no more! Look no further! Here is the new Hub widget!\nHub widget is the most versatile widget available so far.",
	"date" : "Sep 11, 2015"
}

    
    , {
    "uri": "/blog/2015/09/04/updated-widget-documentation/",
	"title": "Updated Widget Documentation",
	"categories": ["documentation", "orange3", "widget"],
	"description": "",
	"content": "Happy news for all passionate Orange users! We’ve uploaded documentation for our Orange 3 widget selection.\nRight click and select \u0026ldquo;Help\u0026rdquo; or press F1.\n** **\nIt’s easy to use. To learn more about a particular wigdet, click on the widget. Either use right click and select “Help” or press F1. A new window will open with a widget description and an example for its use. There are also screenshots included as visual help.\nWidget documentation.\n** **\nWe are going to be updating documentation as the widgets continue to develop. Documentation for bioinformatics and data fusion add-ons is expected to be up and running in the following week.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Happy news for all passionate Orange users! We’ve uploaded documentation for our Orange 3 widget selection.\nRight click and select \u0026ldquo;Help\u0026rdquo; or press F1.\n** **\nIt’s easy to use. To learn more about a particular wigdet, click on the widget. Either use right click and select “Help” or press F1. A new window will open with a widget description and an example for its use. There are also screenshots included as visual help." ,
	"author" : "AJDA",
	"summary" : "Happy news for all passionate Orange users! We’ve uploaded documentation for our Orange 3 widget selection.\nRight click and select \u0026ldquo;Help\u0026rdquo; or press F1.\n** **\nIt’s easy to use. To learn more about a particular wigdet, click on the widget. Either use right click and select “Help” or press F1. A new window will open with a widget description and an example for its use. There are also screenshots included as visual help.",
	"date" : "Sep 4, 2015"
}

    
    , {
    "uri": "/blog/2015/08/28/scatter-plot-projection-rank/",
	"title": "Scatter Plot Projection Rank",
	"categories": ["orange3", "visualization", "widget"],
	"description": "",
	"content": "One of the nicest and surely most useful visualization widgets in Orange is Scatter Plot. The widget displays a 2-D plot, where x and y-axes are two attributes from the data.\n2-dimensional scatter plot visualization\nOrange 2.7 has a wonderful functionality called VizRank, that is now implemented also in Orange 3. Rank Projections functionality enables you to find interesting attribute pairs by scoring their average classification accuracy. Click ‘Start Evaluation’ to begin ranking.\nRank Projections before ranking is performed.\nThe functionality will also instantly adapt the visualization to the best scored pair. Select other pairs from the list to compare visualizations.\nRank Projections once the attribute pairs are scored.\nRank suggested petal length and petal width as the best pair and indeed, the visualization below is much clearer (better separated).\nScatter Plot once the visualization is optimized.\nHave fun trying out this and other visualization widgets!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One of the nicest and surely most useful visualization widgets in Orange is Scatter Plot. The widget displays a 2-D plot, where x and y-axes are two attributes from the data.\n2-dimensional scatter plot visualization\nOrange 2.7 has a wonderful functionality called VizRank, that is now implemented also in Orange 3. Rank Projections functionality enables you to find interesting attribute pairs by scoring their average classification accuracy. Click ‘Start Evaluation’ to begin ranking." ,
	"author" : "AJDA",
	"summary" : "One of the nicest and surely most useful visualization widgets in Orange is Scatter Plot. The widget displays a 2-D plot, where x and y-axes are two attributes from the data.\n2-dimensional scatter plot visualization\nOrange 2.7 has a wonderful functionality called VizRank, that is now implemented also in Orange 3. Rank Projections functionality enables you to find interesting attribute pairs by scoring their average classification accuracy. Click ‘Start Evaluation’ to begin ranking.",
	"date" : "Aug 28, 2015"
}

    
    , {
    "uri": "/blog/2015/08/14/classifying-instances-with-orange-in-python/",
	"title": "Classifying instances with Orange in Python",
	"categories": ["classification", "data", "examples", "orange3", "python", "tree"],
	"description": "",
	"content": "Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.\nFirst we’re going to create a new data table with 10 fruits as our instances.\nimport Orange from Orange.data import * color = DiscreteVariable(\u0026quot;color\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;yellow\u0026quot;])calories = ContinuousVariable(\u0026quot;calories\u0026quot;) fiber = ContinuousVariable(\u0026quot;fiber\u0026quot;) fruit = DiscreteVariable(\u0026quot;fruit\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;peach\u0026quot;]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [\u0026lt;/span\u0026gt; [\u0026quot;green\u0026quot;, 4, 1.2, \u0026quot;apple\u0026quot;], [\u0026quot;orange\u0026quot;, 5, 1.1, \u0026quot;orange\u0026quot;], [\u0026quot;yellow\u0026quot;, 4, 1.0, \u0026quot;peach\u0026quot;], [\u0026quot;orange\u0026quot;, 4, 1.1, \u0026quot;orange\u0026quot;], [\u0026quot;yellow\u0026quot;, 4, 1.1,\u0026quot;peach\u0026quot;], [\u0026quot;green\u0026quot;, 5, 1.3, \u0026quot;apple\u0026quot;], [\u0026quot;green\u0026quot;, 4, 1.3, \u0026quot;apple\u0026quot;], [\u0026quot;orange\u0026quot;, 5, 1.0, \u0026quot;orange\u0026quot;], [\u0026quot;yellow\u0026quot;, 4.5, 1.3, \u0026quot;peach\u0026quot;], [\u0026quot;green\u0026quot;, 5, 1.0, \u0026quot;orange\u0026quot;]]) print(data)  Now we have to select a model for classification. Among the many learners in Orange library, we decided to use the Tree Learner for this example. Since we’re dealing with fruits, we thought it’s only appropriate. :)\nLet’s create a learning algorithm and use it to induce the classifier from the data.\ntree_learner = Orange.classification.TreeLearner() tree = tree_learner(data)  Now we can predict what variety a green fruit with 3.5 calories and 2g of fiber is with the help of our model. To do this, simply call the model and use a list of new data as argument.\nprint(tree([\u0026quot;green\u0026quot;, 3.5, 2]))  Python returns index as a result:\n1  To check the index, we can call class variable values with the corresponding index:\ndomain.class_var.values[1]  Final result:\n\u0026quot;apple\u0026quot;  You can use your own data set to see how this model works for different data types. Let us know how it goes! :)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.\nFirst we’re going to create a new data table with 10 fruits as our instances.\nimport Orange from Orange.data import * color = DiscreteVariable(\u0026quot;color\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;yellow\u0026quot;])calories = ContinuousVariable(\u0026quot;calories\u0026quot;) fiber = ContinuousVariable(\u0026quot;fiber\u0026quot;) fruit = DiscreteVariable(\u0026quot;fruit\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;peach\u0026quot;]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [\u0026lt;/span\u0026gt; [\u0026quot;green\u0026quot;, 4, 1." ,
	"author" : "AJDA",
	"summary" : "Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.\nFirst we’re going to create a new data table with 10 fruits as our instances.\nimport Orange from Orange.data import * color = DiscreteVariable(\u0026quot;color\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;yellow\u0026quot;])calories = ContinuousVariable(\u0026quot;calories\u0026quot;) fiber = ContinuousVariable(\u0026quot;fiber\u0026quot;) fruit = DiscreteVariable(\u0026quot;fruit\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;peach\u0026quot;]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [\u0026lt;/span\u0026gt; [\u0026quot;green\u0026quot;, 4, 1.",
	"date" : "Aug 14, 2015"
}

    
    , {
    "uri": "/blog/2015/08/07/creating-a-new-data-table-in-orange-through-python/",
	"title": "Creating a new data table in Orange through Python",
	"categories": ["data", "examples", "python"],
	"description": "",
	"content": "IMPORT DATA\nOne of the first tasks in Orange data analysis is of course loading your data. If you are using Orange through Python, this is as easy as riding a bike:\nimport Orange data = Orange.data.Table(“iris”) print (data)  This will return a neat data table of the famous Iris data set in the console.\nCREATE YOUR OWN DATA TABLE\nWhat if you want to create your own data table from scratch? Even this is surprisingly simple. First, import the Orange data library.\nfrom Orange.data import *  Set all the attributes you wish to see in your data table. For discrete attributes call DiscreteVariable and set the name and the possible values, while for a continuous variable call ContinuousVariable and set only the attribute name.\ncolor = DiscreteVariable(“color”, values=[“orange”, “green”, “yellow”]) calories = ContinuousVariable(“calories”) fiber = ContinuousVariable(“fiber”)] fruit = DiscreteVariable(\u0026quot;fruit”, values=[”orange\u0026quot;, “apple”, “peach”])  Then set the domain for your data table. See how we set class variable with class_vars?\ndomain = Domain([color, calories, fiber], class_vars=fruit)  Time to input your data!\ndata = Table(domain, [ [“green”, 4, 1.2, “apple”], [\u0026quot;orange\u0026quot;, 5, 1.1, \u0026quot;orange\u0026quot;], [\u0026quot;yellow\u0026quot;, 4, 1.0, \u0026quot;peach\u0026quot;]])  And now print what you have created!\nprint(data)  One final step:\nTable.save(table, \u0026quot;fruit.tab\u0026quot;)  Your data is safely stored to your computer (in the Python folder)! Good job!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "IMPORT DATA\nOne of the first tasks in Orange data analysis is of course loading your data. If you are using Orange through Python, this is as easy as riding a bike:\nimport Orange data = Orange.data.Table(“iris”) print (data)  This will return a neat data table of the famous Iris data set in the console.\nCREATE YOUR OWN DATA TABLE\nWhat if you want to create your own data table from scratch?" ,
	"author" : "AJDA",
	"summary" : "IMPORT DATA\nOne of the first tasks in Orange data analysis is of course loading your data. If you are using Orange through Python, this is as easy as riding a bike:\nimport Orange data = Orange.data.Table(“iris”) print (data)  This will return a neat data table of the famous Iris data set in the console.\nCREATE YOUR OWN DATA TABLE\nWhat if you want to create your own data table from scratch?",
	"date" : "Aug 7, 2015"
}

    
    , {
    "uri": "/blog/2015/07/31/datasets-in-orange-bioinformatics-add-on/",
	"title": "Datasets in Orange Bioinformatics Add-On",
	"categories": ["addons", "analysis", "bioinformatics", "bioorange", "data", "dataloading"],
	"description": "",
	"content": "As you might know, Orange comes with several basic widget sets pre-installed. These allow you to upload and explore the data, visualize them, learn from them and make predictions. However, there are also some exciting add-ons available for installation. One of these is a bioinformatics add-on, which is our specialty.\nBioinformatics widget set allows you to pursue complex analysis of gene expression by providing access to several external libraries. There are four widgets intended specifically for this - dictyExpress, GEO Data Sets, PIPAx and GenExpress. GEO Data Sets are sourced from NCBI, PIPAx and dictyExpress from two Biolab projects, and finally GenExpress from Genialis. A lot of the data is freely accessible, while you will need a user account for the rest.\nOnce you open the widget, select the experiments you wish to use for your analysis and view it in the Data Table widget. You can compare these experiments in Data Profiles, visualize them in Volcano Plot, select the most relevant genes in Differential Expression widget and much more.\nThree widgets with experiment data libraries.\nThese databases enable you to start your research just by installing the bioinformatics add-on (Orange → Options → Add-ons…). The great thing is you can easily combine bioinformatics widgets with the basic pre-installed ones. What an easy way to immerse yourself in the exciting world of bioinformatics!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "As you might know, Orange comes with several basic widget sets pre-installed. These allow you to upload and explore the data, visualize them, learn from them and make predictions. However, there are also some exciting add-ons available for installation. One of these is a bioinformatics add-on, which is our specialty.\nBioinformatics widget set allows you to pursue complex analysis of gene expression by providing access to several external libraries. There are four widgets intended specifically for this - dictyExpress, GEO Data Sets, PIPAx and GenExpress." ,
	"author" : "AJDA",
	"summary" : "As you might know, Orange comes with several basic widget sets pre-installed. These allow you to upload and explore the data, visualize them, learn from them and make predictions. However, there are also some exciting add-ons available for installation. One of these is a bioinformatics add-on, which is our specialty.\nBioinformatics widget set allows you to pursue complex analysis of gene expression by providing access to several external libraries. There are four widgets intended specifically for this - dictyExpress, GEO Data Sets, PIPAx and GenExpress.",
	"date" : "Jul 31, 2015"
}

    
    , {
    "uri": "/blog/2015/07/24/visualizing-misclassifications/",
	"title": "Visualizing Misclassifications",
	"categories": ["analysis", "classification", "visualization"],
	"description": "",
	"content": "In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.\nIn Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification. Below we use Iris data set for simplicity, but the same procedure works just as well on all kinds of data sets.\nHere we have three confusion matrices for Naive Bayes (top), Classification Tree (middle) and Logistic Regression (bottom).\nThree misclassification matrices (Naive Bayes, Classification Tree and Logistic Regression)\nWe see that Classification Tree did the best with only 9 misclassified instances. To see which instances were assigned a false class, we select ‘Misclassified’ option in the widget, which highlights misclassifications and feeds them to the Scatter Plot widget. In the graph we thus see the entire data set presented with empty dots and the selected misclassifications with full dots.\nVisualization of misclassified instances in scatter plot.\nFeel free to switch between learners in Confusion Matrix to see how the visualization changes for each of them.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.\nIn Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification." ,
	"author" : "AJDA",
	"summary" : "In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.\nIn Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification.",
	"date" : "Jul 24, 2015"
}

    
    , {
    "uri": "/blog/2015/07/20/explorative-data-analysis-with-hierarchical-clustering/",
	"title": "Explorative data analysis with Hierarchical Clustering",
	"categories": ["analysis", "clustering", "orange3", "visualization", "principal component analysis", "visualization", "workflow"],
	"description": "",
	"content": "Today we will write about cluster analysis with Hierarchical Clustering widget. We use a well-known Iris data set, which contains 150 Iris flowers, each belonging to one of the three species (setosa, versicolor and virginica). To an untrained eye the three species are very alike, so how could we best tell them apart? The data set contains measurements of sepal and petal dimensions (width and length) and we assume that these gives rise to interesting clustering. But is this so?\nHierarchical Clustering workflow\nTo find clusters, we feed the data from the File widget to Distances and then into Hierarchical Clustering. The last widget in our workflow visualizes hierarchical clustering dendrogram. In the dendrogram, let us annotate the branches with the corresponding Iris species (Annotation = Iris). We see that not all the clusters are composed of the same actual class - there are some mixed clusters with both virginicas and versicolors.\nSelected clusters in Hierarchical Clustering widget\nTo see these clusters, we select them in Hierarchical Clustering widget by clicking on a branch. Selected data will be fed into the output of this widget. Let us inspect the data we have selected by adding Scatter Plot and PCA widgets. If we draw a Data Table directly from Hierarchical Clustering, we see the selected instances and the clusters they belong to. But if we first add the PCA widget, which decomposes the data into principal components, and then connect it to Scatter Plot, we will see the selected instances in the adjusted scatter plot (where principal components are used for x and y-axis).\nSelect other clusters in Hierarchical Clustering widget to see how the scatter plot visualization changes. This allows for an interesting explorative data analysis through a combination of widgets for unsupervised learning and visualizations.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Today we will write about cluster analysis with Hierarchical Clustering widget. We use a well-known Iris data set, which contains 150 Iris flowers, each belonging to one of the three species (setosa, versicolor and virginica). To an untrained eye the three species are very alike, so how could we best tell them apart? The data set contains measurements of sepal and petal dimensions (width and length) and we assume that these gives rise to interesting clustering." ,
	"author" : "AJDA",
	"summary" : "Today we will write about cluster analysis with Hierarchical Clustering widget. We use a well-known Iris data set, which contains 150 Iris flowers, each belonging to one of the three species (setosa, versicolor and virginica). To an untrained eye the three species are very alike, so how could we best tell them apart? The data set contains measurements of sepal and petal dimensions (width and length) and we assume that these gives rise to interesting clustering.",
	"date" : "Jul 20, 2015"
}

    
    , {
    "uri": "/blog/2015/07/10/learn-with-paint-data/",
	"title": "Learn with Paint Data",
	"categories": ["classification", "clustering", "data", "examples", "plot", "visualization"],
	"description": "",
	"content": "Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).\nThe widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph.\nSeveral painting tools allow you to manage your data set according to your specific needs; brush will paint several data instances at once, while put allows you paint a single data instance. Select a data subset and view it in the Data Table widget or zoom in to see the position of your points up close. Jitter and magnet are converse tools which allow either to spread the instances or draw them closer together.\nThe data will be represented in a data table with two attributes, where their instances correspond to coordinates in the system. Such data set is great for demonstrating k-means and hierarchical clustering methods. Just like we do below. In the screenshot we see that k-means, with our particular settings, recognizes clusters way better than hierarchical clustering. It returns a score rank, where the best score (the one with the highest value) means the most likely number of clusters. Hierarchical clustering, however, doesn’t even group the right classes together.\nPaint Data widget for comparing precision of k-means and hierarchical clustering methods.\nAnother way to use Paint Data is to observe the performance of classification methods, where we can alter the graph to demonstrate improvement or deterioration of prediction models. By painting the data points we can try to construct the data set, which would be difficult for one but easy for another classifier. Say, why does linear SVM fail on the data set below?\nUse Paint Data to compare prediction quality of several classifiers.\nHappy painting!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).\nThe widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph." ,
	"author" : "AJDA",
	"summary" : "Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).\nThe widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph.",
	"date" : "Jul 10, 2015"
}

    
    , {
    "uri": "/blog/2015/07/03/support-vectors-output-in-svm-widget/",
	"title": "Support vectors output in SVM widget",
	"categories": ["classification", "orange3", "visualization"],
	"description": "",
	"content": "Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.\nIdeally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance. This margin touches a small number of data instances that are called support vectors.\nIn Orange 3.0 you can set the SVM classification widget to output also the support vectors and visualize them. We used Iris data set in the File widget and classified data instances with SVM classifier. Then we connected both widgets with Scatterplot and selected Support Vectors in the SVM output channel. This allows us to see support vectors in the Scatterplot widget - they are represented by the bold dots in the graph.\nNow feel free to try it with your own data set!\nSupport vectors output of SVM widget with Iris data set.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.\nIdeally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance." ,
	"author" : "AJDA",
	"summary" : "Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.\nIdeally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance.",
	"date" : "Jul 3, 2015"
}

    
    , {
    "uri": "/blog/2015/06/19/435/",
	"title": "Orange workshops around the world",
	"categories": ["addons", "bioinformatics", "conference", "tutorial"],
	"description": "",
	"content": "Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion. Marinka also got an award for the poster, where data fusion was used to hunt for Dictyostelium bacterial-response genes. This week, we are in Pavia, Italy, also for Matrix Computations in Biomedical Informatics Workshop at AIME 2015, a Conference on Artificial Intelligence in Medicine. During the workshop, we are giving an invited talk on learning latent factor models by data fusion and we’ll also show Orange’s data fusion add-on. Thanks to the workshop organizers, Riccardo Bellazzi, Jimeng Sun and Ping Zhang, the workshop program looks great.\nBlaž with Riccardo and John in Pavia, Italy\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion." ,
	"author" : "AJDA",
	"summary" : "Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion.",
	"date" : "Jun 19, 2015"
}

    
    , {
    "uri": "/blog/2015/06/08/data-fusion-tutorial-at-the-bc2/",
	"title": "Data Fusion Tutorial at the [BC]^2",
	"categories": ["bioinformatics", "data-fusion", "orange3"],
	"description": "",
	"content": "We are excited to host a three-hour tutorial on data fusion at the Basel Computational Biology Conference. To this end we have prepared a series of short lectures notes that accompany the recently developed Data Fusion Add-on for Orange.\nWe design the tutorial for data mining researchers and molecular biologists with interest in large-scale data integration. In the tutorial we focus on collective latent factor models, a popular class of approaches for data fusion. We demonstrate the effectiveness of these approaches on several hands-on case studies from recommendation systems and molecular biology.\nThis is a high-risk event. I mean, for us, lecturers. Ok, no bricks will probably fall down. But, in the part of the tutorial, this is the first time we are showing Orange\u0026rsquo;s data fusion add-on. And not just showing: part of the tutorial is a hands-on session.\nWe would like to acknowledge Biolab members for pushing the widgets through the development pipeline under extreme time constraints. Special thanks to Anze, Ales, Jernej, Andrej, Marko, Aleksandar and all other members of the lab.\nThis post was contributed by Marinka and Blaz.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We are excited to host a three-hour tutorial on data fusion at the Basel Computational Biology Conference. To this end we have prepared a series of short lectures notes that accompany the recently developed Data Fusion Add-on for Orange.\nWe design the tutorial for data mining researchers and molecular biologists with interest in large-scale data integration. In the tutorial we focus on collective latent factor models, a popular class of approaches for data fusion." ,
	"author" : "MARINKAZ",
	"summary" : "We are excited to host a three-hour tutorial on data fusion at the Basel Computational Biology Conference. To this end we have prepared a series of short lectures notes that accompany the recently developed Data Fusion Add-on for Orange.\nWe design the tutorial for data mining researchers and molecular biologists with interest in large-scale data integration. In the tutorial we focus on collective latent factor models, a popular class of approaches for data fusion.",
	"date" : "Jun 8, 2015"
}

    
    , {
    "uri": "/blog/2015/06/05/data-fusion-add-on-for-orange/",
	"title": "Data Fusion Add-on for Orange",
	"categories": ["addons", "bioinformatics", "data-fusion", "orange3"],
	"description": "",
	"content": "Orange is about to get even more exciting! We have created a prototype add-on for data fusion, which will certainly be of interest to many users. Data fusion brings large heterogeneous data sets together to create sensible clusters of related data instances and provides a platform for predictive modelling and recommendation systems.\nThis widget set can be used either to recommend you the next movie to watch based on your demographic characteristics, movies you gave high scores to, your preferred genre, etc. or to suggest you a set of genes that might be relevant for a particular biological function or process. We envision the add-on to be useful for predictive modeling dealing with large heterogeneous data compendia, such as life sciences.\nThe prototype set will be available for download next week, but we are happy to give you a sneak peek below.\nData fusion workflow\n Movie Ratings widget is pre-set to offer data on movie ratings by users with 706 users and 855 movies (10% of the data selected as a subset). We add IMDb Actors to filter the data by matching movie ratings with actors. Then we add the Fusion Graph widget to fuse the data together. Here we have two object types, i.e. users and movies, and one relation between them, i.e. movie ratings. In Latent Factors we see latent data representation demonstrated by red squares at the side. Let’s select a latent matrix associated with Users as our input for the Data Table. In Data Table we see the latent data matrix of Users. The algorithm infers low-dimensional user profiles by collective consideration of entire data collection, i.e. movie ratings and actor information. In our scenario the algorithm has transformed 855 movie titles into 70 movie groupings, i.e. latent components.  Data fusion visualized\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange is about to get even more exciting! We have created a prototype add-on for data fusion, which will certainly be of interest to many users. Data fusion brings large heterogeneous data sets together to create sensible clusters of related data instances and provides a platform for predictive modelling and recommendation systems.\nThis widget set can be used either to recommend you the next movie to watch based on your demographic characteristics, movies you gave high scores to, your preferred genre, etc." ,
	"author" : "AJDA",
	"summary" : "Orange is about to get even more exciting! We have created a prototype add-on for data fusion, which will certainly be of interest to many users. Data fusion brings large heterogeneous data sets together to create sensible clusters of related data instances and provides a platform for predictive modelling and recommendation systems.\nThis widget set can be used either to recommend you the next movie to watch based on your demographic characteristics, movies you gave high scores to, your preferred genre, etc.",
	"date" : "Jun 5, 2015"
}

    
    , {
    "uri": "/blog/2015/05/29/excel-files-in-orange-3-0/",
	"title": "Excel files in Orange 3.0",
	"categories": ["data", "dataloading", "orange3"],
	"description": "",
	"content": "Orange 3.0 version comes with an exciting feature that will simplify reading your data. If the old Orange required conversion from Excel into either tab-delimited or comma-separated files, the new version allows you to open plain .xlsx format data sets in the program. Naturally, the .txt and .csv files are still readable in Orange, so feel free to use data sets in any of the above-mentioned formats.\nSince Orange 3.0 is still in the development mode, you will find a smaller selection of widgets available at the moment, but give it a go and see how it works for Excel type data and whether the existing widgets are sufficient for your data analysis. Please find the daily build for OSX here.\nOrange 3.0 can read Excel files.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange 3.0 version comes with an exciting feature that will simplify reading your data. If the old Orange required conversion from Excel into either tab-delimited or comma-separated files, the new version allows you to open plain .xlsx format data sets in the program. Naturally, the .txt and .csv files are still readable in Orange, so feel free to use data sets in any of the above-mentioned formats.\nSince Orange 3.0 is still in the development mode, you will find a smaller selection of widgets available at the moment, but give it a go and see how it works for Excel type data and whether the existing widgets are sufficient for your data analysis." ,
	"author" : "AJDA",
	"summary" : "Orange 3.0 version comes with an exciting feature that will simplify reading your data. If the old Orange required conversion from Excel into either tab-delimited or comma-separated files, the new version allows you to open plain .xlsx format data sets in the program. Naturally, the .txt and .csv files are still readable in Orange, so feel free to use data sets in any of the above-mentioned formats.\nSince Orange 3.0 is still in the development mode, you will find a smaller selection of widgets available at the moment, but give it a go and see how it works for Excel type data and whether the existing widgets are sufficient for your data analysis.",
	"date" : "May 29, 2015"
}

    
    , {
    "uri": "/blog/2015/05/22/orange-fridays/",
	"title": "Orange Fridays",
	"categories": ["orange3"],
	"description": "",
	"content": "You might think “casual Fridays” are the best thing since sliced bread. But what if I were to tell you we have “Orange Fridays” at our lab, where lab members focus solely on debugging Orange software and making improvements to existing features. This is because the new developing version of Orange (3.0) still needs certain widgets to be implemented, such as net explorer, radviz, and survey plot.\nBut there’s more. We are currently hosting an expert on data fusion from the University of Leuven, prof. dr. Yves Moreau, to discuss new venues and niches for the development of Orange. The big debate is how to scale the program to fit large data sets and make it possible to process such sets in a shorter period of time. If you have any ideas and suggestions, please feel free to share them on our community forum.\nprof. dr. Yves Moreau - Prioritization of candidate disease genes and drug—target interactions by genomic data fusion\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "You might think “casual Fridays” are the best thing since sliced bread. But what if I were to tell you we have “Orange Fridays” at our lab, where lab members focus solely on debugging Orange software and making improvements to existing features. This is because the new developing version of Orange (3.0) still needs certain widgets to be implemented, such as net explorer, radviz, and survey plot.\nBut there’s more. We are currently hosting an expert on data fusion from the University of Leuven, prof." ,
	"author" : "AJDA",
	"summary" : "You might think “casual Fridays” are the best thing since sliced bread. But what if I were to tell you we have “Orange Fridays” at our lab, where lab members focus solely on debugging Orange software and making improvements to existing features. This is because the new developing version of Orange (3.0) still needs certain widgets to be implemented, such as net explorer, radviz, and survey plot.\nBut there’s more. We are currently hosting an expert on data fusion from the University of Leuven, prof.",
	"date" : "May 22, 2015"
}

    
    , {
    "uri": "/blog/2015/05/05/working-with-sql-data-in-orange-3/",
	"title": "Working with SQL data in Orange 3",
	"categories": ["orange3", "sql", "visualization"],
	"description": "",
	"content": "Orange 3 is slowly, but steadily, gaining support for working with data stored in a SQL database. The main focus is to allow huge data sets that do not fit into RAM to be analyzed and visualized efficiently. Many widgets already recognize the type of input data and perform the necessary computations intelligently. This means that data is not downloaded from the database and analyzed locally, but is retained on the remote server, with the computation tasks translated into SQL queries and offloaded to the database engine. This approach takes advantage of the state-of-the-art optimizations relational databases have for working with data that does not fit into working memory, as well as minimizes the transfer of required information to the client.\nWe demonstrate how to explore and visualize data stored in a SQL table on a remote server in the following short video. It shows how to connect to the server and load the data with the SqlTable widget, manipulate the data (Select Columns, Select Rows), obtain the summary statistics (Box plot, Distributions), and visualize the data (Heat map, Mosaic Display).\n  The research leading to these results has received funding from the European Union’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no 318633\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange 3 is slowly, but steadily, gaining support for working with data stored in a SQL database. The main focus is to allow huge data sets that do not fit into RAM to be analyzed and visualized efficiently. Many widgets already recognize the type of input data and perform the necessary computations intelligently. This means that data is not downloaded from the database and analyzed locally, but is retained on the remote server, with the computation tasks translated into SQL queries and offloaded to the database engine." ,
	"author" : "LAN",
	"summary" : "Orange 3 is slowly, but steadily, gaining support for working with data stored in a SQL database. The main focus is to allow huge data sets that do not fit into RAM to be analyzed and visualized efficiently. Many widgets already recognize the type of input data and perform the necessary computations intelligently. This means that data is not downloaded from the database and analyzed locally, but is retained on the remote server, with the computation tasks translated into SQL queries and offloaded to the database engine.",
	"date" : "May 5, 2015"
}

    
    , {
    "uri": "/blog/2015/02/19/orange-in-pavia-italy/",
	"title": "Orange in Pavia, Italy",
	"categories": ["orange3", "python", "workshop"],
	"description": "",
	"content": "These days, we (Blaz Zupan and Marinka Zitnik, with full background support of entire Bioinformatics Lab) are running a three-day course on Data Mining in Python. Riccardo Bellazzi, a professor at University of Pavia, a world-renown researcher in biomedical informatics, and most of all, a great friend, has invited us to run the elective course for Pavia\u0026rsquo;s grad students. The enrollment was, he says, overwhelming, as with over 50 students this is by far the best attended grad course at Pavia\u0026rsquo;s faculty of engineering in the past years.\nWe have opted for the hands-on course and a running it as a workshop. The lectures include a new, development version of Orange 3, and mix it with numpy, scikit-learn, matplotlib, networkx and bunch of other libraries. Course themes are classification, clustering, data projection and network analysis.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "These days, we (Blaz Zupan and Marinka Zitnik, with full background support of entire Bioinformatics Lab) are running a three-day course on Data Mining in Python. Riccardo Bellazzi, a professor at University of Pavia, a world-renown researcher in biomedical informatics, and most of all, a great friend, has invited us to run the elective course for Pavia\u0026rsquo;s grad students. The enrollment was, he says, overwhelming, as with over 50 students this is by far the best attended grad course at Pavia\u0026rsquo;s faculty of engineering in the past years." ,
	"author" : "BLAZ",
	"summary" : "These days, we (Blaz Zupan and Marinka Zitnik, with full background support of entire Bioinformatics Lab) are running a three-day course on Data Mining in Python. Riccardo Bellazzi, a professor at University of Pavia, a world-renown researcher in biomedical informatics, and most of all, a great friend, has invited us to run the elective course for Pavia\u0026rsquo;s grad students. The enrollment was, he says, overwhelming, as with over 50 students this is by far the best attended grad course at Pavia\u0026rsquo;s faculty of engineering in the past years.",
	"date" : "Feb 19, 2015"
}

    
    , {
    "uri": "/blog/2015/02/12/towards-orange-3/",
	"title": "Towards Orange 3",
	"categories": ["orange3"],
	"description": "",
	"content": "We are rushing, full speed ahead, towards Orange 3. A complete revamp of Orange in Python 3 changes its data model to that of numpy, making Orange compatible with an array of Python-based data analytics. We are rewriting all the widgets for visual programming as well. We have two open fronts: the scripting part, and the widget part. So much to do, but it is going well: the closed tasks for widgets are those on the left of Anze (the board full of sticky notes), and those open, in minority, are on Anze\u0026rsquo;s right. Oh, by the way, it\u0026rsquo;s Anze who is managing the work and he looks quite happy.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We are rushing, full speed ahead, towards Orange 3. A complete revamp of Orange in Python 3 changes its data model to that of numpy, making Orange compatible with an array of Python-based data analytics. We are rewriting all the widgets for visual programming as well. We have two open fronts: the scripting part, and the widget part. So much to do, but it is going well: the closed tasks for widgets are those on the left of Anze (the board full of sticky notes), and those open, in minority, are on Anze\u0026rsquo;s right." ,
	"author" : "BLAZ",
	"summary" : "We are rushing, full speed ahead, towards Orange 3. A complete revamp of Orange in Python 3 changes its data model to that of numpy, making Orange compatible with an array of Python-based data analytics. We are rewriting all the widgets for visual programming as well. We have two open fronts: the scripting part, and the widget part. So much to do, but it is going well: the closed tasks for widgets are those on the left of Anze (the board full of sticky notes), and those open, in minority, are on Anze\u0026rsquo;s right.",
	"date" : "Feb 12, 2015"
}

    
    , {
    "uri": "/blog/2015/01/18/loading-your-data/",
	"title": "Loading your data",
	"categories": ["data", "dataloading", "orange3"],
	"description": "",
	"content": "By a popular demand, we have just published a tutorial on how to load the data table into Orange. Besides its own .tab format, Orange can load any tab or comma delimited data set. The details are though in writing header rows that tell Orange about the type and domain of each attribute. The tutorial is a step-by-step description on how to do this and how to transfer the data from popular spreadsheet programs like Excel.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "By a popular demand, we have just published a tutorial on how to load the data table into Orange. Besides its own .tab format, Orange can load any tab or comma delimited data set. The details are though in writing header rows that tell Orange about the type and domain of each attribute. The tutorial is a step-by-step description on how to do this and how to transfer the data from popular spreadsheet programs like Excel." ,
	"author" : "BLAZ",
	"summary" : "By a popular demand, we have just published a tutorial on how to load the data table into Orange. Besides its own .tab format, Orange can load any tab or comma delimited data set. The details are though in writing header rows that tell Orange about the type and domain of each attribute. The tutorial is a step-by-step description on how to do this and how to transfer the data from popular spreadsheet programs like Excel.",
	"date" : "Jan 18, 2015"
}

    
    , {
    "uri": "/blog/2014/10/24/hands-on-orange-at-functional-genomics-workshop/",
	"title": "Hands-on Orange at Functional Genomics Workshop",
	"categories": ["bioinformatics"],
	"description": "",
	"content": "Last week we have co-organized a Functional Genomics Workshop. At University of Ljubljana we have hosted an inspiring pack of scientists from the Donnelly Centre for Cellular and Biomolecular Research from Toronto. Part of the event was a hands-on workshop Data mining without programing, where we have used Orange to analyze data from systems biology. Data included a subset of Charlie Boone\u0026rsquo;s famous yeast interaction data and data from chemical genomics. For the program, info about the speakers, and panckages and šmorn check out workshop\u0026rsquo;s newspaper.\nIt is always a pleasure seeing a packed lecture room with all laptops running Orange. Attendees were assisted by members of the Biolab in Ljubljana. Hands-on program followed a set of short lectures we have crafted for intended audience – biologists. Everything ran smoothly. At the end, we got excited enough to promise a data import wizard for all those that have problems annotating the data with feature type tags. The deadline: two weeks from the end of the workshop.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Last week we have co-organized a Functional Genomics Workshop. At University of Ljubljana we have hosted an inspiring pack of scientists from the Donnelly Centre for Cellular and Biomolecular Research from Toronto. Part of the event was a hands-on workshop Data mining without programing, where we have used Orange to analyze data from systems biology. Data included a subset of Charlie Boone\u0026rsquo;s famous yeast interaction data and data from chemical genomics." ,
	"author" : "BLAZ",
	"summary" : "Last week we have co-organized a Functional Genomics Workshop. At University of Ljubljana we have hosted an inspiring pack of scientists from the Donnelly Centre for Cellular and Biomolecular Research from Toronto. Part of the event was a hands-on workshop Data mining without programing, where we have used Orange to analyze data from systems biology. Data included a subset of Charlie Boone\u0026rsquo;s famous yeast interaction data and data from chemical genomics.",
	"date" : "Oct 24, 2014"
}

    
    , {
    "uri": "/blog/2014/08/26/orange-canvas-applied-to-x-ray-optics/",
	"title": "Orange Canvas applied to x-ray optics",
	"categories": ["computervision"],
	"description": "",
	"content": "Orange Canvas is being appropriated by guys who would like to use it as graphical environment for simulating x-ray optics.\nManuel Sanchez del Rio, from The European Synchrotron Facility in Grenoble, France, and Luca Rebuffi from Elettra-Sincrotrone, Trieste, Italy, were looking for a tool that would help them integrate the various tools for x-ray optics simulations, like the popular SHADOW and SRW. They discovered that the data workflow paradigm, like the one used in Orange Canvas, fits their needs perfectly. They took Orange, and replaced the existing widgets with new widgets that represent sources of photons (bending magnets, in the case of ESRF), various optical elements, like lenses and mirrors, and detectors. The channels between the widgets no longer pass data tables, like in the standard Orange, but rays of photons. How cool is this?\nThe result is a system in which the user can arrange the elements in a system that resembles the actual physical system, and then run the simulations using the most powerful tools available in x-ray optics.\nThe tool prototype has been presented at the SPIE Optics + Photonic 2014 in San Diego, the largest meeting of its kind.\nWe\u0026rsquo;re really excited about this novel use of Orange Canvas.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange Canvas is being appropriated by guys who would like to use it as graphical environment for simulating x-ray optics.\nManuel Sanchez del Rio, from The European Synchrotron Facility in Grenoble, France, and Luca Rebuffi from Elettra-Sincrotrone, Trieste, Italy, were looking for a tool that would help them integrate the various tools for x-ray optics simulations, like the popular SHADOW and SRW. They discovered that the data workflow paradigm, like the one used in Orange Canvas, fits their needs perfectly." ,
	"author" : "BIOLAB",
	"summary" : "Orange Canvas is being appropriated by guys who would like to use it as graphical environment for simulating x-ray optics.\nManuel Sanchez del Rio, from The European Synchrotron Facility in Grenoble, France, and Luca Rebuffi from Elettra-Sincrotrone, Trieste, Italy, were looking for a tool that would help them integrate the various tools for x-ray optics simulations, like the popular SHADOW and SRW. They discovered that the data workflow paradigm, like the one used in Orange Canvas, fits their needs perfectly.",
	"date" : "Aug 26, 2014"
}

    
    , {
    "uri": "/blog/2014/05/29/orange-and-sql/",
	"title": "Orange and SQL",
	"categories": ["orange3"],
	"description": "",
	"content": "Orange 3.0 will also support working with data stored in a database.\nWhile we have already talked about this some time ago, we here describe some technical details for anybody interested. This is not a thorough tecnical report, its purpose is only to provide an impression about the architecture of the upcoming version of Orange.\nSo, data tables in Orange 3.0 can refer to data in the working memory or in the database. Any (properly written) code that uses tables should work the same with both storages. When the data is stored in the database, the table is implemented as a \u0026ldquo;proxy object\u0026rdquo; with the necessary meta-data for constructing the SQL query to retrieve the data when needed. Operations on the data only modify the meta-data without retrieving any actual data. For instance, construction of a new table with some selected data subset, say all instances that match a certain condition, creates a new proxy with additional conditions for the WHERE clause. Similarly, selecting a subset of features only changes the domain (the list of features), which is later reflected in the columns of the SELECT clause.\nFeatures in this model are no longer described just with their names but also with the part which goes into the query that retrieves or constructs their values. Discretization, for instance, constructs new features which wrap the representation of the continuous features into a CASE statement that assigns a value based on the boundaries of the bins.\nSince the goal was to make the code in modules and widgets oblivious to the storage, we also needed separate implementation of the operations that need to be aware of how the data is stored. For instance, the code that computes the average values of attributes needs to be different for the two storages: for the in-memory data we need to use the corresponding numpy functions and for databases the average is computed on the server.\nWe went through the code of Orange 2.7 and identified the common operations on the data. We found that all data access belongs into the following types:\n basic aggregates like mean, variance, median, minimal and maximal value, distributions of discrete and continuous variables, values at percentiles, contingency matrices, covariance matrices, filtering of rows based on various criteria, including random sampling, selection of columns, construction of variables from values of other variables, matrices of distances (e.g. Euclidean) between all row pairs, individual data rows.  Points 1 to 4 are typical examples of what cannot be done on client but can be efficiently done in the database. The storage (a class derived from Table) now provides specialized methods for computing aggregates, distributions and contingencies, which use numpy for in-memory data and SQL for the data on the database.\nPoints 5 to 7 are implemented “lazily”, by modifying the SQL query describing the data as described above.\nPoint 8 is difficult to implement efficiently in common relational databases and, besides, results in a data matrix that is larger than the actual data. Methods that require such a matrix will need to be reimplemented and be aware of the storage mechanism.\nPoint 9 requires some caution with regard to how the data is retrieved and what it is used for. Access to individual rows should be used sparingly. Sequential retrieval - especially of all rows - needs to be avoided. For efficiency, most methods that did so in the previous versions of Orange will need to be reimplemented to use aggregate data (possibly as approximations) or to be aware of the data storage and execute some operations directly through SQL.\nWe have already ported a number of visualizations and other widgets to the new Orange. Here is one nice example: Mosaic needs to discretize the variables and then compute contingency matrices for discrete variables. Within the above scheme, the widget does not care about the storage mechanism, yet its computation is still as efficient as possible.\nThe described activities were funded in part by the European Union\u0026rsquo;s Seventh Framework Programme (FP7/2007-2013) under grant agreement n° 318633.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange 3.0 will also support working with data stored in a database.\nWhile we have already talked about this some time ago, we here describe some technical details for anybody interested. This is not a thorough tecnical report, its purpose is only to provide an impression about the architecture of the upcoming version of Orange.\nSo, data tables in Orange 3.0 can refer to data in the working memory or in the database." ,
	"author" : "BIOLAB",
	"summary" : "Orange 3.0 will also support working with data stored in a database.\nWhile we have already talked about this some time ago, we here describe some technical details for anybody interested. This is not a thorough tecnical report, its purpose is only to provide an impression about the architecture of the upcoming version of Orange.\nSo, data tables in Orange 3.0 can refer to data in the working memory or in the database.",
	"date" : "May 29, 2014"
}

    
    , {
    "uri": "/blog/2014/05/26/workshops-at-baylor-college-of-medicine/",
	"title": "Workshops at Baylor College of Medicine",
	"categories": ["bioinformatics", "workshop"],
	"description": "",
	"content": "On May 22nd and May 23rd, we (Blaz Zupan and Janez Demsar, assisted by Marinka Zitnik and Balaji Santhanam) have given two hands-on workshops called Data Mining without Programming at Baylor College of Medicine in Houston, Texas.\nActually, there was a lot of programming, but no Python or alike. The workshop was designed for biomedical students and Baylor\u0026rsquo;s faculty members. We have presented a visual programming approach for development of data mining workflows for interactive data exploration. A three-hour workshop consisted of 15 data mining lessons on visual data exploration, classification, clustering, network analysis, and gene expression analytics. Each lesson focused on a particular data analysis task that the attendees solved with Orange.\nThe two workshops were organized by Baylor\u0026rsquo;s Computational and Integrative Biomedical Research Center. Over two days, the event was attended by a large audience of 120 attendees.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "On May 22nd and May 23rd, we (Blaz Zupan and Janez Demsar, assisted by Marinka Zitnik and Balaji Santhanam) have given two hands-on workshops called Data Mining without Programming at Baylor College of Medicine in Houston, Texas.\nActually, there was a lot of programming, but no Python or alike. The workshop was designed for biomedical students and Baylor\u0026rsquo;s faculty members. We have presented a visual programming approach for development of data mining workflows for interactive data exploration." ,
	"author" : "BIOLAB",
	"summary" : "On May 22nd and May 23rd, we (Blaz Zupan and Janez Demsar, assisted by Marinka Zitnik and Balaji Santhanam) have given two hands-on workshops called Data Mining without Programming at Baylor College of Medicine in Houston, Texas.\nActually, there was a lot of programming, but no Python or alike. The workshop was designed for biomedical students and Baylor\u0026rsquo;s faculty members. We have presented a visual programming approach for development of data mining workflows for interactive data exploration.",
	"date" : "May 26, 2014"
}

    
    , {
    "uri": "/blog/2014/04/29/viewing-images/",
	"title": "Viewing Images",
	"categories": ["clustering", "images", "visualization"],
	"description": "",
	"content": "I am lately having fun with Image Viewer. The widget has been recently updated and can display images stored locally or on the internet. But wait, what images? How on earth can Orange now display images if it can handle mere tabular or basket-based data?\nHere\u0026rsquo;s an example. I have considered a subset of animals from the [download id=\u0026ldquo;864\u0026rdquo;] data set (comes with Orange installation), and for demonstration purposes selected only a handful of attributes. I have added a new string attribute (\u0026ldquo;images\u0026rdquo;) and declared that this is a meta attribute of the type \u0026ldquo;image\u0026rdquo;. The values of this attribute are links to images on the web:\nHere is the resulting data set, [download id=\u0026ldquo;859\u0026rdquo;]. I have used this data set in a schema with hierarchical clustering, where upon selection of the part of the clustering tree I can display the associated images:\nTypically and just like above, you would use a string meta attribute to store the link to images. Images can be referred to using a HTTP address, or, if stored locally, using a relative path from the data file location to the image files.\nHere is another example, where all the images were local and we have associated them with a famous digits data set ( download id=\u0026ldquo;868\u0026rdquo; is a data set in the Orange format with the image files). The task for this data set is to classify handwritten digits based on their bitmap representation. In the schema below we wanted to find out which are the most frequent errors some classification algorithm would make, and how do the images of the misclassified digits look like. Turns out that SVM with RBF kernel most often misclassify the digit 9 and confuses it with a digit 3:\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "I am lately having fun with Image Viewer. The widget has been recently updated and can display images stored locally or on the internet. But wait, what images? How on earth can Orange now display images if it can handle mere tabular or basket-based data?\nHere\u0026rsquo;s an example. I have considered a subset of animals from the [download id=\u0026ldquo;864\u0026rdquo;] data set (comes with Orange installation), and for demonstration purposes selected only a handful of attributes." ,
	"author" : "BIOLAB",
	"summary" : "I am lately having fun with Image Viewer. The widget has been recently updated and can display images stored locally or on the internet. But wait, what images? How on earth can Orange now display images if it can handle mere tabular or basket-based data?\nHere\u0026rsquo;s an example. I have considered a subset of animals from the [download id=\u0026ldquo;864\u0026rdquo;] data set (comes with Orange installation), and for demonstration purposes selected only a handful of attributes.",
	"date" : "Apr 29, 2014"
}

    
    , {
    "uri": "/blog/2013/12/20/paint-your-data/",
	"title": "Paint Your Data",
	"categories": ["data", "visualization"],
	"description": "",
	"content": "One of the widgets I enjoy very much when teaching introductory course in data mining is the Paint Data widget. When painting in this widget I would intentionally include some clusters, or intentionally obscure them. Or draw them in any strange shape. Then I would discuss with students if these clusters are identified by k-means clustering or by hierarchical clustering. We would also discuss automatic scoring of the quality of clusters, come up with the idea of a silhouette (ok, already invented, but helps if you get this idea on your own as well). And then we would play with various data sets and clustering techniques and their parameters in Orange.\nLike in the following workflow where I drew three clusters which were indeed recognized by k-means clustering. Notice that silhouette scoring correctly identified even the number of clusters. And I also drew the clustered data in the Scatterplot to check if the clusters are indeed where they should be.\nOr like in the workflow below where k-means fails miserably (but someother clustering technique would not).\nPaint Data can also be used in supervised setting, for classification tasks. We can set the intended number of classes, and then chose any of these to paint the data. Below I have used it to create the datasets to check the behavior of several classifiers.\nThere are tons of other workflows where Paint Data can be useful. Give it a try!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "One of the widgets I enjoy very much when teaching introductory course in data mining is the Paint Data widget. When painting in this widget I would intentionally include some clusters, or intentionally obscure them. Or draw them in any strange shape. Then I would discuss with students if these clusters are identified by k-means clustering or by hierarchical clustering. We would also discuss automatic scoring of the quality of clusters, come up with the idea of a silhouette (ok, already invented, but helps if you get this idea on your own as well)." ,
	"author" : "BIOLAB",
	"summary" : "One of the widgets I enjoy very much when teaching introductory course in data mining is the Paint Data widget. When painting in this widget I would intentionally include some clusters, or intentionally obscure them. Or draw them in any strange shape. Then I would discuss with students if these clusters are identified by k-means clustering or by hierarchical clustering. We would also discuss automatic scoring of the quality of clusters, come up with the idea of a silhouette (ok, already invented, but helps if you get this idea on your own as well).",
	"date" : "Dec 20, 2013"
}

    
    , {
    "uri": "/blog/2013/10/09/brief-history-of-orange-praise-to-donald-michie/",
	"title": "Brief History of Orange, Praise to Donald Michie",
	"categories": ["history"],
	"description": "",
	"content": "Informatica has recently published our paper on the history of Orange. The paper is a post-publication from a Conference on 100 Years of Alan Turing and 20 Years of Slovene AI Society, where Janez Demšar gave a talk on the topics.\nHistory of Orange goes all the way back to 1997, when late Donald Michie had an idea that machine learning needs an open toolbox for machine learning. To spark the development, we co-organized WebLab97 at beautiful Bled, Slovenia. Workshop\u0026rsquo;s name reflected Michie\u0026rsquo;s idea that tool should be a web application where people can submit data mining code, procedures, testing scripts, and data and share them in the joint web workspace.\nDonald Michie, a pioneer of Artificial Intelligence, was always ahead of time. (Check out a great talk by Ivan Bratko on their friendship and adventures in chess and machine learning). At WebLab97, Michie was actually very, very ahead of time. But despite the presence of IBM\u0026rsquo;s Java team that could guide us in developments of the toolbox, the technology was not ripe and initiative of WebLab was gone as the conference ended. But, at least for us, the idea sparked interest of Janez and myself, and development of what is now Orange begun shortly after.\nOur paper gives brief account of Orange\u0026rsquo;s history and its developments since WebLab97. For reasons of brevity it does not mention that prior to Qt we have experimented with other GUI platforms. Prior to Qt, we laid our hopes to Pwm Python megawidgets, a library that helped us to construct the first Orange graphical user interface. The GUI part of Orange was called Orange*First. Its screenshot shows a tab for interactive discretisation, thanks to Noriaki Aoki who then proposed that this kind of visualisation should be useful in medical data analysis:\nPS Somehow, I have lost a latex file with a WebLab97 program. It should be on some backup tape, somewhere. The following scan of the first page (and a weblab97.pdf), left in some PPT presentation, is all that I can retrieve. The program of the second day is missing, with keynotes from Tom Mitchell, and much talk about then already a success story of R.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Informatica has recently published our paper on the history of Orange. The paper is a post-publication from a Conference on 100 Years of Alan Turing and 20 Years of Slovene AI Society, where Janez Demšar gave a talk on the topics.\nHistory of Orange goes all the way back to 1997, when late Donald Michie had an idea that machine learning needs an open toolbox for machine learning. To spark the development, we co-organized WebLab97 at beautiful Bled, Slovenia." ,
	"author" : "BLAZ",
	"summary" : "Informatica has recently published our paper on the history of Orange. The paper is a post-publication from a Conference on 100 Years of Alan Turing and 20 Years of Slovene AI Society, where Janez Demšar gave a talk on the topics.\nHistory of Orange goes all the way back to 1997, when late Donald Michie had an idea that machine learning needs an open toolbox for machine learning. To spark the development, we co-organized WebLab97 at beautiful Bled, Slovenia.",
	"date" : "Oct 9, 2013"
}

    
    , {
    "uri": "/blog/2013/10/03/jmlr-publishes-article-on-orange/",
	"title": "JMLR Publishes Article on Orange",
	"categories": ["article", "jmlr", "scripting", "toolbox"],
	"description": "",
	"content": "Journal of Machine Learning Research has just published our paper on Orange. In the paper we focus on its Python scripting part. We have last reported on Orange scripting at ECML/PKDD 2004. The manuscript was well received (over 270 citations on Google Scholar), but it is now entirely outdated. This was also our only formal publication on Orange scripting. With publication in JMLR this is now a current description of Orange and will be, for a while :-), Orange’s primary reference.\nHere\u0026rsquo;s a reference:\nDemšar, J., Curk, T., \u0026amp; Erjavec, A. et al. Orange: Data Mining Toolbox in Python; Journal of Machine Learning Research 14(Aug):2349−2353, 2013.\nand bibtex entry:\n@article{JMLR:demsar13a, author = {Janez Dem\\v{s}ar and Toma\\v{z} Curk and Ale\\v{s} Erjavec and \\v{C}rt Gorup and Toma\\v{z} Ho\\v{c}evar and Mitar Milutinovi\\v{c} and Martin Mo\\v{z}ina and Matija Polajnar andMarko Toplak and An\\v{z}e Stari\\v{c} and Miha \\v{S}tajdohar and Lan Umek and Lan \\v{Z}agar and Jure \\v{Z}bontar and Marinka \\v{Z}itnik and Bla\\v{z} Zupan}, title = {Orange: Data Mining Toolbox in Python}, journal = {Journal of Machine Learning Research}, year = {2013}, volume = {14}, pages = {2349-2353}, url = {http://jmlr.org/papers/v14/demsar13a.html} }  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Journal of Machine Learning Research has just published our paper on Orange. In the paper we focus on its Python scripting part. We have last reported on Orange scripting at ECML/PKDD 2004. The manuscript was well received (over 270 citations on Google Scholar), but it is now entirely outdated. This was also our only formal publication on Orange scripting. With publication in JMLR this is now a current description of Orange and will be, for a while :-), Orange’s primary reference." ,
	"author" : "BLAZ",
	"summary" : "Journal of Machine Learning Research has just published our paper on Orange. In the paper we focus on its Python scripting part. We have last reported on Orange scripting at ECML/PKDD 2004. The manuscript was well received (over 270 citations on Google Scholar), but it is now entirely outdated. This was also our only formal publication on Orange scripting. With publication in JMLR this is now a current description of Orange and will be, for a while :-), Orange’s primary reference.",
	"date" : "Oct 3, 2013"
}

    
    , {
    "uri": "/blog/2013/09/02/orange-and-axle-project/",
	"title": "Orange and AXLE project",
	"categories": ["dataloading", "features", "future", "orange3", "sql"],
	"description": "",
	"content": "Our group at University of Ljubljana is a partner in the EU 7FP project Advanced Analytics for Extremely Large European Databases (AXLE). The project is particularly interesting because of the diverse partners that cover the entire vertical, from studying hardware architectures that would better support extremely large databases (University of Manchester, Barcelona Supercomputing Center) to making the necessary adjustments related to speed and security of databases (2ndQuadrant) to data analytics (our group) to handling and analyzing real data and decision making (Portavita).\nAs a result of the project, Orange will be better connected with databases. Currently, all data is stored in working memory, while the forthcoming Orange 3.0 will be able to handle data that is stored in the database. We are working on a parallel computation architecture. Visualization of large data also presents a big challenge: we cannot transfer large amounts of data from the database to the desktop, and on the other hand it is difficult to provide a rich interactive experience if visualizations are created on the server-side. Also, most visualizations are intrinsically unsuitable for large data sets. For instance, the scatter plot represents each data instance with a symbol. Even when the datum is represented with a single pixel, only a few million data points fits on the computer screen. So in the context of big data, we will have to replace scatterplots with heatmaps.\nWhat have we got so far? Orange 3, which is in early stage of development, features a new architecture, which allows the data to be stored either in memory or on a database. In the latter case, selecting a subset of features or filtering the data does not copy the data but only modifies the queries that are used to access the data when needed. Computation of, for instance, distributions or contingency matrices is performed on the server, so only the minimal amount of data is transferred to the client.\nWe also already have a small suite of widgets that work with this new architecture. Just to wet your appetite, here is the new box plot widget.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Our group at University of Ljubljana is a partner in the EU 7FP project Advanced Analytics for Extremely Large European Databases (AXLE). The project is particularly interesting because of the diverse partners that cover the entire vertical, from studying hardware architectures that would better support extremely large databases (University of Manchester, Barcelona Supercomputing Center) to making the necessary adjustments related to speed and security of databases (2ndQuadrant) to data analytics (our group) to handling and analyzing real data and decision making (Portavita)." ,
	"author" : "BIOLAB",
	"summary" : "Our group at University of Ljubljana is a partner in the EU 7FP project Advanced Analytics for Extremely Large European Databases (AXLE). The project is particularly interesting because of the diverse partners that cover the entire vertical, from studying hardware architectures that would better support extremely large databases (University of Manchester, Barcelona Supercomputing Center) to making the necessary adjustments related to speed and security of databases (2ndQuadrant) to data analytics (our group) to handling and analyzing real data and decision making (Portavita).",
	"date" : "Sep 2, 2013"
}

    
    , {
    "uri": "/blog/2013/06/03/network-add-on-published-in-jss/",
	"title": "Network Add-on Published in JSS",
	"categories": ["addons", "network"],
	"description": "",
	"content": "NetExplorer, a widget for network exploration, was in orange for over 5 years. Several network analysis widgets were added to Orange since, and we decided to move the entire network functionality to an Orange Network add-on.\nWe recently published a paper Interactive Network Exploration with Orange in the Journal of Statistical Software. We invite you to read the tutorial on network exploration. It is aimed for beginners in this topic, and includes detailed explanation with images.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "NetExplorer, a widget for network exploration, was in orange for over 5 years. Several network analysis widgets were added to Orange since, and we decided to move the entire network functionality to an Orange Network add-on.\nWe recently published a paper Interactive Network Exploration with Orange in the Journal of Statistical Software. We invite you to read the tutorial on network exploration. It is aimed for beginners in this topic, and includes detailed explanation with images." ,
	"author" : "BIOLAB",
	"summary" : "NetExplorer, a widget for network exploration, was in orange for over 5 years. Several network analysis widgets were added to Orange since, and we decided to move the entire network functionality to an Orange Network add-on.\nWe recently published a paper Interactive Network Exploration with Orange in the Journal of Statistical Software. We invite you to read the tutorial on network exploration. It is aimed for beginners in this topic, and includes detailed explanation with images.",
	"date" : "Jun 3, 2013"
}

    
    , {
    "uri": "/blog/2013/05/25/orange-2-7/",
	"title": "Orange 2.7",
	"categories": ["update", "version"],
	"description": "",
	"content": "Orange 2.7 is out with a major update in the visual programming environment. Redesigned interface, new widgets, welcome screen with workflow browser. Text annotation and arrow lines in workspace. Preloaded workflows with annotations. Widget menu and search can now be activated through key press (open the Settings to make this option available). Extended or minimised widget tab. Improved widget browsing. Enjoy!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange 2.7 is out with a major update in the visual programming environment. Redesigned interface, new widgets, welcome screen with workflow browser. Text annotation and arrow lines in workspace. Preloaded workflows with annotations. Widget menu and search can now be activated through key press (open the Settings to make this option available). Extended or minimised widget tab. Improved widget browsing. Enjoy!" ,
	"author" : "BLAZ",
	"summary" : "Orange 2.7 is out with a major update in the visual programming environment. Redesigned interface, new widgets, welcome screen with workflow browser. Text annotation and arrow lines in workspace. Preloaded workflows with annotations. Widget menu and search can now be activated through key press (open the Settings to make this option available). Extended or minimised widget tab. Improved widget browsing. Enjoy!",
	"date" : "May 25, 2013"
}

    
    , {
    "uri": "/blog/2013/03/04/problems-with-orange-website/",
	"title": "Problems With Orange Website",
	"categories": ["website"],
	"description": "",
	"content": "Our servers crashed on Friday, March 1st due to technical problems. The Orange website was offline for several hours and Mac bundle was unaccessible until today.\nWe are still reviewing if our other services work. If you notice some problems, please ping us.\nStay tuned and fruitful downloading!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Our servers crashed on Friday, March 1st due to technical problems. The Orange website was offline for several hours and Mac bundle was unaccessible until today.\nWe are still reviewing if our other services work. If you notice some problems, please ping us.\nStay tuned and fruitful downloading!" ,
	"author" : "BIOLAB",
	"summary" : "Our servers crashed on Friday, March 1st due to technical problems. The Orange website was offline for several hours and Mac bundle was unaccessible until today.\nWe are still reviewing if our other services work. If you notice some problems, please ping us.\nStay tuned and fruitful downloading!",
	"date" : "Mar 4, 2013"
}

    
    , {
    "uri": "/blog/2013/02/14/new-canvas/",
	"title": "New canvas",
	"categories": ["canvas", "features"],
	"description": "",
	"content": "Orange Canvas, a visual programming environment for Orange, has been around for a while. Integrating new and new features degraded the quality of code to a point where further development proved to be a daunting task. With ever increasing number of widgets, the existing widget toolbar is becoming harder and harder to use, but improving it is really hard. For that reason, we decided Orange needs a new Canvas, a rewrite, that would keep all of the feature of the existing one, but introduce the needed structure and modularity to the source code.\nThe project started about a year ago, and more than 20 thousand lines of code later, we have something to show you. As of yesterday, the new canvas was merged to the main Orange repository, where it lives alongside the old one. At the moment, it still lacks a lot of testing, some features are not completely implemented, but the main functionality, i.e. visual programming with widgets and links, should work.\nIf you are feeling adventurous, you can try it out yourself. Download the latest version from our website and run:\nWindows:\nC:\\Python27\\python.exe -m Orange.OrangeCanvas.main  Mac OS X bundle:\n/Applications/Orange.app/Contents/MacOS/python -m Orange.OrangeCanvas.main  or, regardless of your operating system,\npython -m Orange.OrangeCanvas.main  with the python that has Orange installed.\nWhat to expect?\nNothing will explode, but short of that, anything might happen. If you stumble upon issues or have helpful suggestions, please post them on our issue tracker. There are some known problems we are aware of; you do not need to report those :).\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange Canvas, a visual programming environment for Orange, has been around for a while. Integrating new and new features degraded the quality of code to a point where further development proved to be a daunting task. With ever increasing number of widgets, the existing widget toolbar is becoming harder and harder to use, but improving it is really hard. For that reason, we decided Orange needs a new Canvas, a rewrite, that would keep all of the feature of the existing one, but introduce the needed structure and modularity to the source code." ,
	"author" : "BIOLAB",
	"summary" : "Orange Canvas, a visual programming environment for Orange, has been around for a while. Integrating new and new features degraded the quality of code to a point where further development proved to be a daunting task. With ever increasing number of widgets, the existing widget toolbar is becoming harder and harder to use, but improving it is really hard. For that reason, we decided Orange needs a new Canvas, a rewrite, that would keep all of the feature of the existing one, but introduce the needed structure and modularity to the source code.",
	"date" : "Feb 14, 2013"
}

    
    , {
    "uri": "/blog/2013/02/06/orange-nmf-add-on/",
	"title": "Orange NMF add-on",
	"categories": ["addons", "matrixfactorization", "nmf"],
	"description": "",
	"content": "Nimfa, a Python library for non-negative matrix factorization (NMF), which was part of Orange GSoC program back in 2011 got its own add-on.\nNimfa provides a plethora of initialization and factorization algorithms, quality measures along with examples on real-world and synthetic data sets. However, until now the analysis was possible only through Python scripting. A recent increase of interest in NMF techniques motivated Fajwel Fogel (a PhD student from INRIA, Paris, SIERRA team) to design and implement several widgets that deal with missing data in target matrices, their normalizations, viewing and assessing the quality of matrix factors returned by different matrix factorization algorithms. He also provided an implementation of robust singular value decomposition (rSVD). All NMF methods call Nimfa library.\nAbove is shown a simple scenario in Orange that applies LSNMF algorithm from Nimfa to decompose a non-negative target matrix and visualizes its basis matrix (W) and coefficient matrix (H) as heat maps. NMF finds a parts-based representation of the data due to the fact that only additive, not subtractive, combinations are allowed, which results in improved interpretability of matrix factors. That is possible because non-negativity constraints are imposed in the NMF model in contrast to SVD, PCA and ICA, which provide only holistic representations. The effect can be easily seen if we investigate heat maps produced by the scenario above. Below are shown the target, basis and coefficient matrices (from left to right, top down), respectively.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Nimfa, a Python library for non-negative matrix factorization (NMF), which was part of Orange GSoC program back in 2011 got its own add-on.\nNimfa provides a plethora of initialization and factorization algorithms, quality measures along with examples on real-world and synthetic data sets. However, until now the analysis was possible only through Python scripting. A recent increase of interest in NMF techniques motivated Fajwel Fogel (a PhD student from INRIA, Paris, SIERRA team) to design and implement several widgets that deal with missing data in target matrices, their normalizations, viewing and assessing the quality of matrix factors returned by different matrix factorization algorithms." ,
	"author" : "BIOLAB",
	"summary" : "Nimfa, a Python library for non-negative matrix factorization (NMF), which was part of Orange GSoC program back in 2011 got its own add-on.\nNimfa provides a plethora of initialization and factorization algorithms, quality measures along with examples on real-world and synthetic data sets. However, until now the analysis was possible only through Python scripting. A recent increase of interest in NMF techniques motivated Fajwel Fogel (a PhD student from INRIA, Paris, SIERRA team) to design and implement several widgets that deal with missing data in target matrices, their normalizations, viewing and assessing the quality of matrix factors returned by different matrix factorization algorithms.",
	"date" : "Feb 6, 2013"
}

    
    , {
    "uri": "/blog/2013/01/29/writing-orange-add-ons/",
	"title": "Writing Orange Add-ons",
	"categories": ["addons", "pypi"],
	"description": "",
	"content": "We officially supported add-ons in Orange 2.6. You should start by checking the list of available add-ons. We pull those automatically from the PyPi, which is our preferred distribution channel. Try to install an add-on by either:\n writing \u0026ldquo;pip install \u0026rdquo; in the terminal or from the Orange Canvas GUI. Select \u0026ldquo;Options / Add-ons\u0026hellip;\u0026rdquo; in the menu.  Everything should just work. Writing add-ons is as easy as writing your own Orange Widgets or Orange Scripts. Just follow this tutorial and you will have your brand-new Orange add-on on PyPi in no time (an hour at most).\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We officially supported add-ons in Orange 2.6. You should start by checking the list of available add-ons. We pull those automatically from the PyPi, which is our preferred distribution channel. Try to install an add-on by either:\n writing \u0026ldquo;pip install \u0026rdquo; in the terminal or from the Orange Canvas GUI. Select \u0026ldquo;Options / Add-ons\u0026hellip;\u0026rdquo; in the menu.  Everything should just work. Writing add-ons is as easy as writing your own Orange Widgets or Orange Scripts." ,
	"author" : "BIOLAB",
	"summary" : "We officially supported add-ons in Orange 2.6. You should start by checking the list of available add-ons. We pull those automatically from the PyPi, which is our preferred distribution channel. Try to install an add-on by either:\n writing \u0026ldquo;pip install \u0026rdquo; in the terminal or from the Orange Canvas GUI. Select \u0026ldquo;Options / Add-ons\u0026hellip;\u0026rdquo; in the menu.  Everything should just work. Writing add-ons is as easy as writing your own Orange Widgets or Orange Scripts.",
	"date" : "Jan 29, 2013"
}

    
    , {
    "uri": "/blog/2013/01/21/orange-2-6/",
	"title": "Orange 2.6",
	"categories": ["addons", "pypi"],
	"description": "",
	"content": "A new version of Orange, 2.6, has been uploaded to Python Package Index. Since the version on the Orange website is always up to date (we post daily builds), this may not affect you. Nevertheless, let us explain what we were working on for the last year.\nThe most important improvement to Orange is an implementation of add-on framework that is much more \u0026ldquo;standard pythonic\u0026rdquo;. As a consequence, the add-on installation procedure has been simplified for both individual users and system administrators. For developers, the new framework eases the development and distribution of add-ons. This enabled us to make first steps towards the goal of removing the rarely used parts of Orange from the core distribution, which will ultimately result in less external dependencies and less warnings on module import. Orange 2.6 lacks the modules for network analysis (Orange.network) and prediction reliability assesment (Orange.reliability), but fear not: you can get them back by installing the Orange-Network and Orange-Reliability add-ons.\nApart from that, we have been mostly squashing bugs. A fun spare time activity - you can join us anytime by cloning our repository and sending us a pull request. :)\nIf our version numbering system confuses you, let us try to explain. For the last (couple of) year(s), our version numbers have been a mess. Orange2.5a4 was uploaded to pypi almost a year ago, and was followed by a 2.6a2 release that was never available outisde our repository/daily builds. From this day forth, our versioning system should be as follows.\n If you install orange from pypi, the version (Orange.version.full_version) will be something like 2.6 or 2.6.1. If you use our daily builds or build orange yourself from the source available in our repository, your version will be 2.6.1.dev-8804fbc. (minor will be larger by one and .dev- suffix will show the source control revision that was used for the build)  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "A new version of Orange, 2.6, has been uploaded to Python Package Index. Since the version on the Orange website is always up to date (we post daily builds), this may not affect you. Nevertheless, let us explain what we were working on for the last year.\nThe most important improvement to Orange is an implementation of add-on framework that is much more \u0026ldquo;standard pythonic\u0026rdquo;. As a consequence, the add-on installation procedure has been simplified for both individual users and system administrators." ,
	"author" : "BIOLAB",
	"summary" : "A new version of Orange, 2.6, has been uploaded to Python Package Index. Since the version on the Orange website is always up to date (we post daily builds), this may not affect you. Nevertheless, let us explain what we were working on for the last year.\nThe most important improvement to Orange is an implementation of add-on framework that is much more \u0026ldquo;standard pythonic\u0026rdquo;. As a consequence, the add-on installation procedure has been simplified for both individual users and system administrators.",
	"date" : "Jan 21, 2013"
}

    
    , {
    "uri": "/blog/2013/01/06/new-scripting-tutorial/",
	"title": "New scripting tutorial",
	"categories": ["documentation", "examples", "tutorial"],
	"description": "",
	"content": "Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:\nclass SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name='small', m=5): self.name = name self.m = m self.base_learner = base_learner def __call__(self, data, weight=None): gain = Orange.feature.scoring.InfoGain() m = min(self.m, len(data.domain.features)) best = [f for _, f in sorted((gain(x, data), x) \\ for x in data.domain.features)[-m:]] domain = Orange.data.Domain(best + [data.domain.class_var]) model = self.base_learner(Orange.data.Table(domain, data), weight) return Orange.classification.PyClassifier(classifier=model, name=self.name)  The tutorial was first written for Python 2.3. Since, Python and Orange have changed a lot. And so did I. Most of the for loops have become one-liners, list and dictionary comprehension have become a must, and many new and great libraries have emerged. The (boring) tutorial code that used to read\n c = [0] * len(data.domain.classVar.values) for e in data: c[int(e.getclass())] += 1 print \u0026quot;Instances: \u0026quot;, len(data), \u0026quot;total\u0026quot;, r = [0.] * len(c) for i in range(len(c)): r[i] = c[i] * 100. / len(data) for i in range(len(data.domain.classVar.values)): print \u0026quot;, %d(%4.1f%s) with class %s\u0026quot; % (c[i], r[i], '%', data.domain.classVar.values[i]), print  is now replaced with\nprint Counter(str(d.get_class()) for d in data)  Ok. Pretty print is missing, but that, if not in the same line, could be done in another one.\nFor now, the tutorial focuses on data input and output, classification and regression. We plan to use other sections, but you can also give us a hint if there are any you would wish to be included.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:\nclass SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name='small', m=5): self." ,
	"author" : "BLAZ",
	"summary" : "Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:\nclass SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name='small', m=5): self.",
	"date" : "Jan 6, 2013"
}

    
    , {
    "uri": "/blog/2012/11/30/the-easy-way-to-install-add-ons/",
	"title": "The easy way to install add-ons",
	"categories": ["addons", "orange25", "pypi"],
	"description": "",
	"content": "The possibility of extending functionality of Orange through add-ons has been present for a long time. In fact, we never provided the toolbox for crunching bioinformatical data as an integral part of Orange; it has always been an add-on. The exact mechanism of distribution of add-ons has changed significantly in the last year to simplify the process for add-on authors and to make it more standards-compliant. Among other things, this enables system administrators to install add-ons system-wide directly from PyPi using easy_install or pip. Unfortunately there were also negative side effects of this process, notably the temporary breakage of the add-on management dialog within the Orange Canvas.\nWe are happy to report that this is now being taken care of and you are encouraged to test the functionality.\nSelect \u0026ldquo;Add-ons\u0026hellip;\u0026rdquo; in the Options menu. A dialog will open that will list and describe existing add-ons. You can see the same list on the appropriate part of Orange website, but there is more. In the dialog, you can simply pick the add-ons you wish to use, confirm the selection and you should be good to go: widgets that come with the selected add-ons will become available immediately.\nIn case you change your mind, on some systems you can also uninstall add-ons by removing the check marks in front of them. This only works if you have pip installed, which is uncommon on Windows systems.\nThis might be a good time to warn you that the described functionality is new and not thoroughly tested on all the platforms on which Orange runs. If you stumble upon any strange or unwanted behavior, please let us now on the Orange forum, preferrably in the Bugs section.\nNote that the Orange-Text add-on requires a compiler and appropriate libraries on your computer, and it as of now still refuses to be installed using the dialog. This is a known bug.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "The possibility of extending functionality of Orange through add-ons has been present for a long time. In fact, we never provided the toolbox for crunching bioinformatical data as an integral part of Orange; it has always been an add-on. The exact mechanism of distribution of add-ons has changed significantly in the last year to simplify the process for add-on authors and to make it more standards-compliant. Among other things, this enables system administrators to install add-ons system-wide directly from PyPi using easy_install or pip." ,
	"author" : "BIOLAB",
	"summary" : "The possibility of extending functionality of Orange through add-ons has been present for a long time. In fact, we never provided the toolbox for crunching bioinformatical data as an integral part of Orange; it has always been an add-on. The exact mechanism of distribution of add-ons has changed significantly in the last year to simplify the process for add-on authors and to make it more standards-compliant. Among other things, this enables system administrators to install add-ons system-wide directly from PyPi using easy_install or pip.",
	"date" : "Nov 30, 2012"
}

    
    , {
    "uri": "/blog/2012/11/27/coming-soon-oranges-new-interface/",
	"title": "Coming soon: Orange&#39;s new interface",
	"categories": ["interface"],
	"description": "",
	"content": "Orange will soon get entirely new interface. The GUI will feature new canvas and icons and new presentation of data flow. Orange will be upgraded with on-line help for widgets and tutorials. The prototype is now in testing and should replace the current version of Orange in early 2013.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange will soon get entirely new interface. The GUI will feature new canvas and icons and new presentation of data flow. Orange will be upgraded with on-line help for widgets and tutorials. The prototype is now in testing and should replace the current version of Orange in early 2013." ,
	"author" : "BLAZ",
	"summary" : "Orange will soon get entirely new interface. The GUI will feature new canvas and icons and new presentation of data flow. Orange will be upgraded with on-line help for widgets and tutorials. The prototype is now in testing and should replace the current version of Orange in early 2013.",
	"date" : "Nov 27, 2012"
}

    
    , {
    "uri": "/blog/2012/10/23/short-history-of-orange/",
	"title": "Short history of Orange",
	"categories": ["future", "history"],
	"description": "",
	"content": "Few weeks back we celebrated 20 years of Slovene Artificial Intelligence Society. I have much enjoyed Ivan Bratko\u0026rsquo;s talk on AI history, and his account of events as triggered by late Donald Michie. Many interesting talks followed, including highlights by Stephen Muggleton and Claude Sammut.\nThe last talk of the event was on Orange. Janez talked about its birth, history and future prospects. You can see his presentation on videolectures and check out the paper with lecture\u0026rsquo;s notes.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Few weeks back we celebrated 20 years of Slovene Artificial Intelligence Society. I have much enjoyed Ivan Bratko\u0026rsquo;s talk on AI history, and his account of events as triggered by late Donald Michie. Many interesting talks followed, including highlights by Stephen Muggleton and Claude Sammut.\nThe last talk of the event was on Orange. Janez talked about its birth, history and future prospects. You can see his presentation on videolectures and check out the paper with lecture\u0026rsquo;s notes." ,
	"author" : "BLAZ",
	"summary" : "Few weeks back we celebrated 20 years of Slovene Artificial Intelligence Society. I have much enjoyed Ivan Bratko\u0026rsquo;s talk on AI history, and his account of events as triggered by late Donald Michie. Many interesting talks followed, including highlights by Stephen Muggleton and Claude Sammut.\nThe last talk of the event was on Orange. Janez talked about its birth, history and future prospects. You can see his presentation on videolectures and check out the paper with lecture\u0026rsquo;s notes.",
	"date" : "Oct 23, 2012"
}

    
    , {
    "uri": "/blog/2012/06/15/computing-joint-entropy-in-python/",
	"title": "Computing joint entropy (in Python)",
	"categories": ["orange3", "python"],
	"description": "",
	"content": "How I wrote a beautiful, general, and super fast joint entropy method (in Python).\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u0026gt; 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).\nimport numpy as np def entropy(X): probs = [np.mean(X == c) for c in set(X)] return np.sum(-p * np.log2(p) for p in probs)  In my next version I extended it to compute the joint entropy of two variables:\ndef entropy(X, Y): probs = [] for c1 in set(X): for c2 in set(Y): probs.append(np.mean(np.logical_and(X == c1, Y == c2))) return np.sum(-p * np.log2(p) for p in probs)  Now wait a minute, it looks like we have a recursion here. I couldn\u0026rsquo;t stop myself of writing en extended general function to compute the joint entropy of n variables.\ndef entropy(*X, **kwargs): predictions = parse_arg(X[0]) H = kwargs[\u0026quot;H\u0026quot;] if \u0026quot;H\u0026quot; in kwargs else 0 v = kwargs[\u0026quot;v\u0026quot;] if \u0026quot;v\u0026quot; in kwargs else np.array([True] * len(predictions)) for c in set(predictions): if len(X) \u0026gt; 1: H = entropy(*X[1:], v=np.logical_and(v, predictions == c), H=H) else: p = np.mean(np.logical_and(v, predictions == c)) H += -p * np.log2(p) if p \u0026gt; 0 else 0 return H  It was the ugliest recursive function I\u0026rsquo;ve ever written. I couldn\u0026rsquo;t stop coding, I was hooked. Besides, this method was slow as hell and I need a faster version for my reasearch. I need my data tommorow, not next month. I googled if Python has something that would help me deal with the recursive part. I fould this great method: itertools.product, I\u0026rsquo;s just what we need. It takes lists and returns a cartesian product of their values. It\u0026rsquo;s the \u0026ldquo;nested for loops\u0026rdquo; in one function.\ndef entropy(*X): n_insctances = len(X[0]) H = 0 for classes in itertools.product(*[set(x) for x in X]): v = np.array([True] * n_insctances) for predictions, c in zip(X, classes): v = np.logical_and(v, predictions == c) p = np.mean(v) H += -p * np.log2(p) if p \u0026gt; 0 else 0 return H  No resursion, but still slow. It\u0026rsquo;s time to rewrite loops to the Python-like style. As a sharp eye has already noticed, the second for loop with the np.logical_and inside is perfect for the reduce method.\ndef entropy(*X): n_insctances = len(X[0]) H = 0 for classes in itertools.product(*[set(x) for x in X]): v = reduce(np.logical_and, (predictions, c for predictions, c in zip(X, classes))) p = np.mean(v) H += -p * np.log2(p) if p \u0026gt; 0 else 0 return H  Now, we have to remove just one more list comprehension and we have a beautiful, general, and super fast joint etropy method.\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u0026gt; 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "How I wrote a beautiful, general, and super fast joint entropy method (in Python).\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u0026gt; 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings)." ,
	"author" : "BIOLAB",
	"summary" : "How I wrote a beautiful, general, and super fast joint entropy method (in Python).\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u0026gt; 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).",
	"date" : "Jun 15, 2012"
}

    
    , {
    "uri": "/blog/2012/05/19/kdnuggets-is-asking-if-you-have-been-using-orange-lately/",
	"title": "KDnuggets is asking if you have been using Orange lately",
	"categories": ["orange3"],
	"description": "",
	"content": "KDnuggets, one of leading data mining community websites, is having its yearly poll asking its visitors which analytics/data mining software they used in the past 12 months. Among listed is also Orange, our fruity visually pleasing open source pythonic data mining suite. So we are asking you, have you been using Orange lately, that is, in the past 12 months? How do you feel about telling that to the world?\nIf so, we would also like to hear more about how you are using Orange in your projects, research, competitions, or data mining play. We would be glad to publish your story on our blog, or link to your blog post. Feel free to contact us if you are interested.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "KDnuggets, one of leading data mining community websites, is having its yearly poll asking its visitors which analytics/data mining software they used in the past 12 months. Among listed is also Orange, our fruity visually pleasing open source pythonic data mining suite. So we are asking you, have you been using Orange lately, that is, in the past 12 months? How do you feel about telling that to the world?" ,
	"author" : "BIOLAB",
	"summary" : "KDnuggets, one of leading data mining community websites, is having its yearly poll asking its visitors which analytics/data mining software they used in the past 12 months. Among listed is also Orange, our fruity visually pleasing open source pythonic data mining suite. So we are asking you, have you been using Orange lately, that is, in the past 12 months? How do you feel about telling that to the world?",
	"date" : "May 19, 2012"
}

    
    , {
    "uri": "/blog/2012/05/15/orange-gsoc-computer-vision-add-on-for-orange/",
	"title": "Orange GSoC: Computer vision add-on for Orange",
	"categories": ["computervision", "gsoc"],
	"description": "",
	"content": "This summer I got the chance to develop an add-on for Orange that will introduce basic computer vision functionality, as a part of Google Summer of Code.\nThe add-on will consist of a set of widgets, each with it\u0026rsquo;s own dedicated purpose, which can be seamlessly connected to provide most commonly used image preprocessing functionality.\nHere is a list of the widgets:\n Widget for viewing image files (add description) Widget for resizing an image Widget for rotation/flipping of the image Widget for converting the color mode (RGB, HSV, Grayscale etc.) Widget for changing the hue/saturation, brightness/contrast and inverting the image Widget for generic transformations through convolution with a matrix  Also, if there is enough time left throughout the GSoC period, a face detection widget will be built in order to demonstrate the power of the underlying libraries.\nThese are all things that have been implemented in Python before. Reimplementing them is of course a rather bad idea, so I will use an library called OpenCV. It is written in C++ and has Python bindings, and is the most widely used computer vision library, by far. So the core of the widgets will be written in it, and the GUI using PyQT, the library used for building the Orange Canvas.\nAlthough working with images is not Oranges' main thing, the knowledge gathered while developing the add-on will be used to improve in a number of ways: finding a general structure for add-ons developed in the future, improving the way they are distributed and the way they are tested.\nFinally, I want to thank the Orange core team for having faith in me and giving me the chance to spend the summer working on an idea I care about. I\u0026rsquo;m very grateful for that and I hope I\u0026rsquo;ll exceed their expectations.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This summer I got the chance to develop an add-on for Orange that will introduce basic computer vision functionality, as a part of Google Summer of Code.\nThe add-on will consist of a set of widgets, each with it\u0026rsquo;s own dedicated purpose, which can be seamlessly connected to provide most commonly used image preprocessing functionality.\nHere is a list of the widgets:\n Widget for viewing image files (add description) Widget for resizing an image Widget for rotation/flipping of the image Widget for converting the color mode (RGB, HSV, Grayscale etc." ,
	"author" : "BIOLAB",
	"summary" : "This summer I got the chance to develop an add-on for Orange that will introduce basic computer vision functionality, as a part of Google Summer of Code.\nThe add-on will consist of a set of widgets, each with it\u0026rsquo;s own dedicated purpose, which can be seamlessly connected to provide most commonly used image preprocessing functionality.\nHere is a list of the widgets:\n Widget for viewing image files (add description) Widget for resizing an image Widget for rotation/flipping of the image Widget for converting the color mode (RGB, HSV, Grayscale etc.",
	"date" : "May 15, 2012"
}

    
    , {
    "uri": "/blog/2012/05/06/orange-gsoc-a-fully-featured-neural-network-library-implementation-with-extension-for-deep-learning/",
	"title": "Orange GSoC: A Fully-Featured Neural Network Library Implementation with Extension for Deep Learning",
	"categories": ["gsoc", "neuralnetwork"],
	"description": "",
	"content": "This project aims to build a neural network library based on some great existing NN libraries, notably the Flood Library, which already provides a fully functional Multilayer Perceptron (MLP) implementation. The project starts with implementing a robust, efficient feed forward neural network library, and then will extend it in significant ways that add support for state-of-the-art deep learning techniques. Additional extensions include building a PCA framework and improving existing training algorithms and error functional.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This project aims to build a neural network library based on some great existing NN libraries, notably the Flood Library, which already provides a fully functional Multilayer Perceptron (MLP) implementation. The project starts with implementing a robust, efficient feed forward neural network library, and then will extend it in significant ways that add support for state-of-the-art deep learning techniques. Additional extensions include building a PCA framework and improving existing training algorithms and error functional." ,
	"author" : "BIOLAB",
	"summary" : "This project aims to build a neural network library based on some great existing NN libraries, notably the Flood Library, which already provides a fully functional Multilayer Perceptron (MLP) implementation. The project starts with implementing a robust, efficient feed forward neural network library, and then will extend it in significant ways that add support for state-of-the-art deep learning techniques. Additional extensions include building a PCA framework and improving existing training algorithms and error functional.",
	"date" : "May 6, 2012"
}

    
    , {
    "uri": "/blog/2012/04/30/orange-gsoc-multi-target-learning-for-orange/",
	"title": "Orange GSoC: Multi-Target Learning for Orange",
	"categories": ["classification", "gsoc", "multitarget"],
	"description": "",
	"content": "Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year\u0026rsquo;s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange\u0026rsquo;s SimpleTreeLearner and will be integrated with Orange 3.0.\nOnce the clustering trees are implemented and integrated, documentation and unit tests will be written. Additionally I intend to make an experimental study that will compare the effectiveness of clustering trees with established multi-target classifiers (like PLS and chain classifiers) on benchmark data-sets. I will also work on some additional tasks related to multi-target classification that I had not included in my original proposal but Orange\u0026rsquo;s team thinks would be useful to include. Among these is a chain classifier framework that Orange is currently missing.\nIf any reader is interested in learning more about clustering trees or chain classifiers these articles should cover the basics:\n Top-Down Induction of Clustering Trees (1998), by Hendrik Blockeel, Luc De Raedt, Jan Ramong Classiﬁer Chains for Multi-label Classiﬁcation (2009), by Jesse Read, Bernhard Pfahringer, Geoﬀ Holmes, Eibe Frank  I am a third year undergraduate student at the Faculty of Computer and Information Science in Ljubljana and my project will be mentored by prof. dr. Blaž Zupan. I thank him and the rest of the Orange team for advice and support.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year\u0026rsquo;s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange\u0026rsquo;s SimpleTreeLearner and will be integrated with Orange 3." ,
	"author" : "BIOLAB",
	"summary" : "Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year\u0026rsquo;s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange\u0026rsquo;s SimpleTreeLearner and will be integrated with Orange 3.",
	"date" : "Apr 30, 2012"
}

    
    , {
    "uri": "/blog/2012/04/25/orange-team-wins-jrs-2012-data-mining-competition/",
	"title": "Orange team wins JRS 2012 Data Mining Competition",
	"categories": ["competition", "prediction"],
	"description": "",
	"content": "Lead by Jure Žbontar, the team from University of Ljubljana wins over 126 other entrants in an international competition in predictive data analytics.\nJure’s team consisted of several Orange developers and computer science students: Miha Zidar, Blaž Zupan, Gregor Majcen, Marinka Žitnik in Matic Potočnik. To win, the team had to predict topics for 10.000 MedLine documents that were represented with over 25.000 algorithmically derived numerical features. Given was training set of another 10.000 documents in the same representation but each labeled with a set of topics. From the training set the task was to develop a model to predict labels for documents in the test set. A particular challenge was guessing the right number of topics to be associated with the documents, as these, at least in the training set, varied from one to a dozen.\nJRS 2012 is just one in a series of competitions recently organized on servers such as TunedIT and Kaggle. The price for winning was $1000 and a trip to Joint Rough Set Symposium in Chengdu, China, to present a winning strategy and developed data mining techiques.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Lead by Jure Žbontar, the team from University of Ljubljana wins over 126 other entrants in an international competition in predictive data analytics.\nJure’s team consisted of several Orange developers and computer science students: Miha Zidar, Blaž Zupan, Gregor Majcen, Marinka Žitnik in Matic Potočnik. To win, the team had to predict topics for 10.000 MedLine documents that were represented with over 25.000 algorithmically derived numerical features. Given was training set of another 10." ,
	"author" : "BLAZ",
	"summary" : "Lead by Jure Žbontar, the team from University of Ljubljana wins over 126 other entrants in an international competition in predictive data analytics.\nJure’s team consisted of several Orange developers and computer science students: Miha Zidar, Blaž Zupan, Gregor Majcen, Marinka Žitnik in Matic Potočnik. To win, the team had to predict topics for 10.000 MedLine documents that were represented with over 25.000 algorithmically derived numerical features. Given was training set of another 10.",
	"date" : "Apr 25, 2012"
}

    
    , {
    "uri": "/blog/2012/04/24/this-year-five-students-participate-in-google-summer-of-code/",
	"title": "This year five students participate in Google Summer of Code",
	"categories": ["gsoc"],
	"description": "",
	"content": "This year five students have been accepted to participate in Google Summer of Code and contribute to Orange in their summer time. Congratulations!\n Amela – Widgets for statistics Andrej T. – Computer vision add-on for Orange CoderWilliam – A Fully-Featured Neural Network Library Implementation Based On the Flood Library with Extension for Deep Learning Makarov Dmitry – Text mining add-on for Orange Miran Levar – Multi-Target Learning for Orange  Overall, 1,212 students have been accepted this year to various open source organizations from all around the world.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This year five students have been accepted to participate in Google Summer of Code and contribute to Orange in their summer time. Congratulations!\n Amela – Widgets for statistics Andrej T. – Computer vision add-on for Orange CoderWilliam – A Fully-Featured Neural Network Library Implementation Based On the Flood Library with Extension for Deep Learning Makarov Dmitry – Text mining add-on for Orange Miran Levar – Multi-Target Learning for Orange  Overall, 1,212 students have been accepted this year to various open source organizations from all around the world." ,
	"author" : "BIOLAB",
	"summary" : "This year five students have been accepted to participate in Google Summer of Code and contribute to Orange in their summer time. Congratulations!\n Amela – Widgets for statistics Andrej T. – Computer vision add-on for Orange CoderWilliam – A Fully-Featured Neural Network Library Implementation Based On the Flood Library with Extension for Deep Learning Makarov Dmitry – Text mining add-on for Orange Miran Levar – Multi-Target Learning for Orange  Overall, 1,212 students have been accepted this year to various open source organizations from all around the world.",
	"date" : "Apr 24, 2012"
}

    
    , {
    "uri": "/blog/2012/04/09/redesign-of-gui-icons/",
	"title": "Redesign of GUI icons",
	"categories": ["icons"],
	"description": "",
	"content": "Orange GUI is being redesigned. Expect a welcome screen with selection of preloaded widget schemes, simpler access to computational components, and integration with intelligent interface (widget suggestions). For the project we have engaged a designer Peter Čuhalev. To give you a taste of what is going on, here are some icons for widget sets that are being redesigned. There are in B/W, the color will be decided on and added in later stages. Below are just the icons - widget symbols with no frames. Current frames are rounded squares, while it looks like the widget frames for the new GUI will be circles. New icons are designed in a vector format.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange GUI is being redesigned. Expect a welcome screen with selection of preloaded widget schemes, simpler access to computational components, and integration with intelligent interface (widget suggestions). For the project we have engaged a designer Peter Čuhalev. To give you a taste of what is going on, here are some icons for widget sets that are being redesigned. There are in B/W, the color will be decided on and added in later stages." ,
	"author" : "BLAZ",
	"summary" : "Orange GUI is being redesigned. Expect a welcome screen with selection of preloaded widget schemes, simpler access to computational components, and integration with intelligent interface (widget suggestions). For the project we have engaged a designer Peter Čuhalev. To give you a taste of what is going on, here are some icons for widget sets that are being redesigned. There are in B/W, the color will be decided on and added in later stages.",
	"date" : "Apr 9, 2012"
}

    
    , {
    "uri": "/blog/2012/03/27/orange-is-again-participating-in-gsoc/",
	"title": "Orange is again participating in GSoC",
	"categories": ["gsoc"],
	"description": "",
	"content": "This year Orange is again participating in Google Summer of Code as a mentoring organization. Student proposal submission period is running and the deadline is on 6th April. We have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year but feel free to suggest your own ideas how you could contribute to Orange and make it even better.\nGoogle Summer of Code is a Google-sponsored program where Google stipends students working for a summer job on an open source projects from all around the world. Student is paid $5000 (and a t-shirt!) for approximately two months of work/contribution to the project. More about the program is available on its homepage.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This year Orange is again participating in Google Summer of Code as a mentoring organization. Student proposal submission period is running and the deadline is on 6th April. We have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year but feel free to suggest your own ideas how you could contribute to Orange and make it even better." ,
	"author" : "BIOLAB",
	"summary" : "This year Orange is again participating in Google Summer of Code as a mentoring organization. Student proposal submission period is running and the deadline is on 6th April. We have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year but feel free to suggest your own ideas how you could contribute to Orange and make it even better.",
	"date" : "Mar 27, 2012"
}

    
    , {
    "uri": "/blog/2012/02/05/random-decisions-behind-your-back/",
	"title": "Random decisions behind your back",
	"categories": ["tree"],
	"description": "",
	"content": "When Orange builds a decision tree, candidate attributes are evaluated and the best candidate is chosen. But what if two or more share the first place? Most machine learning systems don\u0026rsquo;t care about it and always take the first, which is unfair and, besides, has strange effects: the induced model and, consequentially, its accuracy depends upon the order of attributes. Which shouldn\u0026rsquo;t be.\nThis is not an isolated problem. Another instance is when a classifier has to choose between two equally probable classes when there is no additional information (such as classification costs) to help make the prediction. Or selecting random reference examples when computing ReliefF. Returning a modus of a distribution with two or more competing values\u0026hellip;\nThe old solution was to make a random selection in such cases. Take a random class (out of the most probable, of course), random attribute, random examples\u0026hellip; Although theoretically correct, it comes with a price: the only way to ensure repeatability of experiments is by setting the global random generator, which is not a good practice in component-based systems.\nWhat Orange does now is more cunning. When, for instance, choosing between n equally probable classes, Orange computes something like a hash value from the example to be classified. Its remainder at division by n is then used to select the class. Thus, the class will be random, but always the same for same example.\nA similar trick is used elsewhere. To choose an attribute when building a tree, it simply divides the number of learning examples at that node by the number of candidate attributes and the remainder is used again.\nWhen more random numbers are needed, for instance for selecting m random reference examples for computing ReliefF, the number of examples is used for a random seed for a temporary random generator.\nTo conclude: Orange will sometimes make decisions that will look random. The reason for this is that it is more fair than most of machine learning systems that pick the first (or the last) candidate. But whatever decision is taken, it will be the same if you run the program twice. The message is thus: be aware that this is happenning, but don\u0026rsquo;t care about it.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "When Orange builds a decision tree, candidate attributes are evaluated and the best candidate is chosen. But what if two or more share the first place? Most machine learning systems don\u0026rsquo;t care about it and always take the first, which is unfair and, besides, has strange effects: the induced model and, consequentially, its accuracy depends upon the order of attributes. Which shouldn\u0026rsquo;t be.\nThis is not an isolated problem. Another instance is when a classifier has to choose between two equally probable classes when there is no additional information (such as classification costs) to help make the prediction." ,
	"author" : "BIOLAB",
	"summary" : "When Orange builds a decision tree, candidate attributes are evaluated and the best candidate is chosen. But what if two or more share the first place? Most machine learning systems don\u0026rsquo;t care about it and always take the first, which is unfair and, besides, has strange effects: the induced model and, consequentially, its accuracy depends upon the order of attributes. Which shouldn\u0026rsquo;t be.\nThis is not an isolated problem. Another instance is when a classifier has to choose between two equally probable classes when there is no additional information (such as classification costs) to help make the prediction.",
	"date" : "Feb 5, 2012"
}

    
    , {
    "uri": "/blog/2012/02/02/new-in-orange-partial-least-squares-regression/",
	"title": "New in Orange: Partial least squares regression",
	"categories": ["multitarget", "pls", "regression"],
	"description": "",
	"content": "Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!\nYou can use PLS regression model on single-target or multi-target data sets. Simply load the data set multitarget-synthetic.tab and see that it contains three predictor variables and four responses using this code.\ndata = Orange.data.Table(\u0026quot;multitarget-synthetic.tab\u0026quot;) print \u0026quot;Input variables:\u0026quot; print data.domain.features print \u0026quot;Response variables:\u0026quot; print data.domain.class_vars  Output:\nInput variables: \u0026lt;FloatVariable 'X1', FloatVariable 'X2', FloatVariable 'X3'\u0026gt; Response variables: \u0026lt;FloatVariable 'Y1', FloatVariable 'Y2', FloatVariable 'Y3', FloatVariable 'Y4'\u0026gt;  As you can see, all variables in this data set are continuous. The PLS regression is intended forsuch situations although it can be used for discrete input variables as well (using 0-1 continuation). Currently, discrete response variables are not yet supported.\nLet\u0026rsquo;s try to fit the PLS regression model on our data set.\nlearner = Orange.multitarget.pls.PLSRegressionLearner() classifier = learner(data)\nThe classifier can be now used to predict values of the four responses based onthree predictors. Let\u0026rsquo;s see how it manages this task on the first data instance.\nactual = data[0].get_classes() predicted = classifier(data[0]) print \u0026quot;Actual\u0026quot;, \u0026quot;Predicted\u0026quot; for a, p in zip(actual, predicted): print \u0026quot;%6.3f %6.3f\u0026quot; % (a,p)  Output:\nActual Predicted 0.490 0.613 1.237 0.826 1.808 1.084 0.422 0.534  To test the usefulness of PLS as a multi-target method let\u0026rsquo;s compare it to a single-target method - linear regression. We did this by comparing Root mean squared error (RMSE) of predicted values for a single response variable. We constructed synthetic data sets and performed the RMSE analysis using this script. The results can be seen in the following output:\n Training set sizes 5 10 20 50 100 200 500 1000 Linear (single-target) 0.5769 0.3128 0.2703 0.2529 0.2493 0.2446 0.2436 0.2442 PLS (multi-target) 0.3663 0.2955 0.2623 0.2517 0.2487 0.2447 0.2441 0.2448  We can see that PLS regression outperforms linear regression when the number of training instances is low. Such situations (low number of instances compared to high number of variables) are quite common when analyzing data sets in bioinformatics. However, with increasing number of training instances, the advantages of PLS regression diminish.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!\nYou can use PLS regression model on single-target or multi-target data sets." ,
	"author" : "BIOLAB",
	"summary" : "Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!\nYou can use PLS regression model on single-target or multi-target data sets.",
	"date" : "Feb 2, 2012"
}

    
    , {
    "uri": "/blog/2012/01/23/orange-2-5a2-available/",
	"title": "Orange 2.5a2 available",
	"categories": ["gsoc", "pypi", "release"],
	"description": "",
	"content": "Orange 2.5a2 has been uploaded to PyPI. It now includes basic support for multi-label classification (developed during the Google Summer of Code 2011), some new widget icons and documentation for basket format. Release is also tagged on our Bitbucket repository.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange 2.5a2 has been uploaded to PyPI. It now includes basic support for multi-label classification (developed during the Google Summer of Code 2011), some new widget icons and documentation for basket format. Release is also tagged on our Bitbucket repository." ,
	"author" : "BIOLAB",
	"summary" : "Orange 2.5a2 has been uploaded to PyPI. It now includes basic support for multi-label classification (developed during the Google Summer of Code 2011), some new widget icons and documentation for basket format. Release is also tagged on our Bitbucket repository.",
	"date" : "Jan 23, 2012"
}

    
    , {
    "uri": "/blog/2012/01/09/multi-label-classification-and-multi-target-prediction-in-orange/",
	"title": "Multi-label classification (and Multi-target prediction) in Orange",
	"categories": ["classification", "gsoc", "mlc", "multilabel"],
	"description": "",
	"content": "The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been \u0026ldquo;hidden\u0026rdquo; in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet \u0026hellip;) and added a more general support for multi-target prediction.\nYou can load multi-label tab-delimited data (e.g. emotions.tab) just like any other tab-delimited data:\n\u0026gt;\u0026gt;\u0026gt; zoo = Orange.data.Table('zoo') # single-target \u0026gt;\u0026gt;\u0026gt; emotions = Orange.data.Table('emotions') # multi-label  The difference is that now zoo\u0026rsquo;s domain has a non-empty class_var field, while a list of emotions' labels can be obtained through it\u0026rsquo;s domain\u0026rsquo;s class_vars:\n\u0026gt;\u0026gt;\u0026gt; zoo.domain.class_var EnumVariable 'type' \u0026gt;\u0026gt;\u0026gt; emotions.domain.class_vars \u0026lt;EnumVariable 'amazed-suprised', EnumVariable 'happy-pleased', EnumVariable 'relaxing-calm', EnumVariable 'quiet-still', EnumVariable 'sad-lonely', EnumVariable 'angry-aggresive'\u0026gt;  A simple example of a multi-label classification learner is a \u0026ldquo;binary relevance\u0026rdquo; learner. Let\u0026rsquo;s try it out.\n\u0026gt;\u0026gt;\u0026gt; learner = Orange.multilabel.BinaryRelevanceLearner() \u0026gt;\u0026gt;\u0026gt; classifier = learner(emotions) \u0026gt;\u0026gt;\u0026gt; classifier(emotions[0]) [\u0026lt;orange.Value 'amazed-suprised'='0'\u0026gt;, \u0026lt;orange.Value 'happy-pleased'='0'\u0026gt;, \u0026lt;orange.Value 'relaxing-calm'='1'\u0026gt;, \u0026lt;orange.Value 'quiet-still'='1'\u0026gt;, \u0026lt;orange.Value 'sad-lonely'='1'\u0026gt;, \u0026lt;orange.Value 'angry-aggresive'='0'\u0026gt;] \u0026gt;\u0026gt;\u0026gt; classifier(emotions[0], Orange.classification.Classifier.GetProbabilities) [\u0026lt;1.000, 0.000\u0026gt;, \u0026lt;0.881, 0.119\u0026gt;, \u0026lt;0.000, 1.000\u0026gt;, \u0026lt;0.046, 0.954\u0026gt;, \u0026lt;0.000, 1.000\u0026gt;, \u0026lt;1.000, 0.000\u0026gt;]  Real values of label variables of emotions[0] instance can be obtained by calling emotions[0].get_classes(), which is analogous to the get_class method in the single-target case.\nFor multi-label classification, we can also perform testing like usual, however, specialised evaluation measures have to be used:\n\u0026gt;\u0026gt;\u0026gt; test = Orange.evaluation.testing.cross_validation([learner], emotions) \u0026gt;\u0026gt;\u0026gt; Orange.evaluation.scoring.mlc_hamming_loss(test) [0.2228780213603148]  In one of the following blog posts, a multi-target regression method PLS that is in the process of implementation will be described.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been \u0026ldquo;hidden\u0026rdquo; in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet \u0026hellip;) and added a more general support for multi-target prediction.\nYou can load multi-label tab-delimited data (e." ,
	"author" : "BIOLAB",
	"summary" : "The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been \u0026ldquo;hidden\u0026rdquo; in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet \u0026hellip;) and added a more general support for multi-target prediction.\nYou can load multi-label tab-delimited data (e.",
	"date" : "Jan 9, 2012"
}

    
    , {
    "uri": "/blog/2012/01/06/new-orange-icons/",
	"title": "New Orange icons",
	"categories": ["icons"],
	"description": "",
	"content": "As new and new widgets with new features are added to Orange, icons for them have to be drawn. Most of the time those are just some quick sketches or even missing altogether. But now we are starting to redraw and unify them. A few of them have already been made.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "As new and new widgets with new features are added to Orange, icons for them have to be drawn. Most of the time those are just some quick sketches or even missing altogether. But now we are starting to redraw and unify them. A few of them have already been made." ,
	"author" : "BIOLAB",
	"summary" : "As new and new widgets with new features are added to Orange, icons for them have to be drawn. Most of the time those are just some quick sketches or even missing altogether. But now we are starting to redraw and unify them. A few of them have already been made.",
	"date" : "Jan 6, 2012"
}

    
    , {
    "uri": "/blog/2012/01/03/parallel-orange/",
	"title": "Parallel Orange?",
	"categories": ["parallelization"],
	"description": "",
	"content": "We attended a NIPS 2011 workshop on processing and learning from large scale data. Various presenters showed different tools and frameworks that can be used when developing algorithms suitable for dealing with large scale data, but none of them were written in Python and as such, not useful for Orange. We have been looking for a framework that would help us run code in parallel for some time, but so far with no luck.\nWe would like to have a framework that is easy to use, can be used in C as well as in Python and supports multi-level map reduce (cross validation can be viewed as map reduce and random forest that is tested is another map-reduce). Prototypes we have created so far solve this problem by inspecting learners that are used in cross-validation and creating all \u0026ldquo;subtasks\u0026rdquo; at the same time. That results in really ugly code we don\u0026rsquo;t want to commit ;). If you know a framework that would suit our needs, want to implement support for parallel computation by yourself (we will apply to GSoC) or have an idea how to solve this problem, feel free to contact us ;).\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We attended a NIPS 2011 workshop on processing and learning from large scale data. Various presenters showed different tools and frameworks that can be used when developing algorithms suitable for dealing with large scale data, but none of them were written in Python and as such, not useful for Orange. We have been looking for a framework that would help us run code in parallel for some time, but so far with no luck." ,
	"author" : "BIOLAB",
	"summary" : "We attended a NIPS 2011 workshop on processing and learning from large scale data. Various presenters showed different tools and frameworks that can be used when developing algorithms suitable for dealing with large scale data, but none of them were written in Python and as such, not useful for Orange. We have been looking for a framework that would help us run code in parallel for some time, but so far with no luck.",
	"date" : "Jan 3, 2012"
}

    
    , {
    "uri": "/blog/2011/12/20/earth-multivariate-adaptive-regression-splines/",
	"title": "Earth - Multivariate adaptive regression splines",
	"categories": ["regression"],
	"description": "",
	"content": "There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.\nSo lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):\nimport Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange.data.Table(\u0026quot;data.tab\u0026quot;) earth_predictor = earth.EarthLearner(data) X, Y = data.to_numpy(\u0026quot;A/C\u0026quot;) pl.plot(X, Y, \u0026quot;.r\u0026quot;) linspace = numpy.linspace(min(X), max(X), 20) predictions = [earth_predictor([s, \u0026quot;?\u0026quot;]) for s in linspace] pl.plot(linspace, predictions, \u0026quot;-b\u0026quot;) pl.show()  which produces the following plot:\nWe can also print the model representation using\nprint earth_predictor  which outputs:\nY = 1.013 +1.198 * max(0, X - 0.485) -1.803 * max(0, 0.485 - X) -1.321 * max(0, X - 0.283) -1.609 * max(0, X - 0.640) +1.591 * max(0, X - 0.907)  See Orange.regression.earth reference for full documentation.\n(Edit: Added link to the dataset file)\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.\nSo lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):\nimport Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange." ,
	"author" : "BIOLAB",
	"summary" : "There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.\nSo lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):\nimport Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange.",
	"date" : "Dec 20, 2011"
}

    
    , {
    "uri": "/blog/2011/12/20/orange-2-5-code-conversion/",
	"title": "Orange 2.5: code conversion",
	"categories": ["orange25"],
	"description": "",
	"content": "Orange 2.5 unifies Orange\u0026rsquo;s C++ core and Python modules into a single module hierarchy. To use the new module hierarchy, import Orange instead of orange and accompanying orng* modules. While we will maintain backward compatibility in 2.* releases, we nevertheless suggest programmers to use the new interface. The provided conversion tool can help refactor your code to use the new interface.\nThe conversion script, orange2to25.py, resides in Orange\u0026rsquo;s main directory. To refactor accuracy8.py from the \u0026ldquo;Orange for beginners\u0026rdquo; tutorial runpython orange2to25.py -w -o accuracy8_25.py doc/ofb-rst/code/accuracy8.py.\nThe old code\nimport orange import orngTest, orngStat, orngTree # set up the learners bayes = orange.BayesLearner() tree = orngTree.TreeLearner(mForPruning=2) bayes.name = \u0026quot;bayes\u0026quot; tree.name = \u0026quot;tree\u0026quot; learners = [bayes, tree] # compute accuracies on data data = orange.ExampleTable(\u0026quot;voting\u0026quot;) res = orngTest.crossValidation(learners, data, folds=10) cm = orngStat.computeConfusionMatrices(res, classIndex=data.domain.classVar.values.index('democrat'))  is refactored to\nimport Orange # set up the learners bayes = Orange.classification.bayes.NaiveLearner() tree = Orange.classification.tree.TreeLearner(mForPruning=2) bayes.name = \u0026quot;bayes\u0026quot; tree.name = \u0026quot;tree\u0026quot; learners = [bayes, tree] # compute accuracies on data data = Orange.data.Table(\u0026quot;voting\u0026quot;) res = Orange.evaluation.testing.cross_validation(learners, data, folds=10) cm = Orange.evaluation.scoring.compute_confusion_matrices(res, classIndex=data.domain.classVar.values.index('democrat'))  Read more about the refactoring tool on the wiki and on the help page (python orange2to25.py \u0026ndash;help).\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange 2.5 unifies Orange\u0026rsquo;s C++ core and Python modules into a single module hierarchy. To use the new module hierarchy, import Orange instead of orange and accompanying orng* modules. While we will maintain backward compatibility in 2.* releases, we nevertheless suggest programmers to use the new interface. The provided conversion tool can help refactor your code to use the new interface.\nThe conversion script, orange2to25.py, resides in Orange\u0026rsquo;s main directory. To refactor accuracy8." ,
	"author" : "MARKO",
	"summary" : "Orange 2.5 unifies Orange\u0026rsquo;s C++ core and Python modules into a single module hierarchy. To use the new module hierarchy, import Orange instead of orange and accompanying orng* modules. While we will maintain backward compatibility in 2.* releases, we nevertheless suggest programmers to use the new interface. The provided conversion tool can help refactor your code to use the new interface.\nThe conversion script, orange2to25.py, resides in Orange\u0026rsquo;s main directory. To refactor accuracy8.",
	"date" : "Dec 20, 2011"
}

    
    , {
    "uri": "/blog/2011/12/08/random-forest-switches-to-simple-tree-learner-by-default/",
	"title": "Random forest switches to Simple tree learner by default",
	"categories": ["forestlearner", "simpletreelearner"],
	"description": "",
	"content": "Random forest classifiers now use Orange.classification.tree.SimpleTreeLearnerby default, which considerably shortens their construction times.\nUsing a random forest classifier is easy.\nimport Orange iris = Orange.data.Table('iris') forest = Orange.ensemble.forest.RandomForestLearner(iris, trees=200) for instance in iris: print forest(instance), instance.get_class()  The example above loads the iris dataset and trains a random forest classifier with 200 trees. The classifier is then used to label all training examples, printing its prediction alongside the actual class value.\nUsing SimpleTreeLearner insted of TreeLearner substantially reduces the training time. The image below compares construction times of Random Forest classifiers using a SimpleTreeLearner or a TreeLearner as the base learner.\nBy setting the base_learner parameter to TreeLearer it is possible to revert to the original behaviour:\ntree_learner = Orange.classification.tree.TreeLearner() forest_orig = Orange.ensemble.forest.RandomForestLearner(base_learner=tree_learner)  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Random forest classifiers now use Orange.classification.tree.SimpleTreeLearnerby default, which considerably shortens their construction times.\nUsing a random forest classifier is easy.\nimport Orange iris = Orange.data.Table('iris') forest = Orange.ensemble.forest.RandomForestLearner(iris, trees=200) for instance in iris: print forest(instance), instance.get_class()  The example above loads the iris dataset and trains a random forest classifier with 200 trees. The classifier is then used to label all training examples, printing its prediction alongside the actual class value." ,
	"author" : "BIOLAB",
	"summary" : "Random forest classifiers now use Orange.classification.tree.SimpleTreeLearnerby default, which considerably shortens their construction times.\nUsing a random forest classifier is easy.\nimport Orange iris = Orange.data.Table('iris') forest = Orange.ensemble.forest.RandomForestLearner(iris, trees=200) for instance in iris: print forest(instance), instance.get_class()  The example above loads the iris dataset and trains a random forest classifier with 200 trees. The classifier is then used to label all training examples, printing its prediction alongside the actual class value.",
	"date" : "Dec 8, 2011"
}

    
    , {
    "uri": "/blog/2011/10/26/gsoc-mentor-summit/",
	"title": "GSoC Mentor Summit",
	"categories": ["gsoc"],
	"description": "",
	"content": "On 22th and 23th October 2011 there was Google Summer of Code Mentor Summit in Mountain View, California. Google Summer of Code is Google\u0026rsquo;s program for encouraging students to work on open-source projects during their summer break. Because this year Orange participated in this program too, we decided to participate also in this summit and get to know other mentors, other open-source projects and organizations, exchange our experiences, learn something new, and improve our connections and collaborations with others.\n                                         We went to the meeting together with another Slovenian open-source project: wlan slovenija, an open wireless network initiative. Because the summit itself was held at Google\u0026rsquo;s premises, where taking photographs was forbidden, photos are mostly from the trip there itself and area around the buildings. There are some photos by others available.\nSummit really satisfied all expectations. We have experienced how it is at Google\u0026rsquo;s, meet many new people, sessions were great, presenting a lot of interesting issues within open-source deployment and IT in general, and giving some ideas how to solve them. We meet many other researchers doing open-source science and developing different programs, libraries and having similar problems. We have discussed ways of solving them: how to maintain libraries we all use, how to make our projects survive, once research is completed or funding ends and we move to some other research, etc. Cooperation is the key here, but there is often not much time to do that, as it requires extra time and energy, often not a part of research projects.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "On 22th and 23th October 2011 there was Google Summer of Code Mentor Summit in Mountain View, California. Google Summer of Code is Google\u0026rsquo;s program for encouraging students to work on open-source projects during their summer break. Because this year Orange participated in this program too, we decided to participate also in this summit and get to know other mentors, other open-source projects and organizations, exchange our experiences, learn something new, and improve our connections and collaborations with others." ,
	"author" : "BIOLAB",
	"summary" : "On 22th and 23th October 2011 there was Google Summer of Code Mentor Summit in Mountain View, California. Google Summer of Code is Google\u0026rsquo;s program for encouraging students to work on open-source projects during their summer break. Because this year Orange participated in this program too, we decided to participate also in this summit and get to know other mentors, other open-source projects and organizations, exchange our experiences, learn something new, and improve our connections and collaborations with others.",
	"date" : "Oct 26, 2011"
}

    
    , {
    "uri": "/blog/2011/09/13/debian-packages-support-multiple-python-versions-now/",
	"title": "Debian packages support multiple Python versions now",
	"categories": ["debian", "packaging", "python"],
	"description": "",
	"content": "We have created Debian packages for multiple Python versions. This means that they work now with both Python 2.6 and 2.7 out of the box, or if you compile them manually, with any (supported) version you have installed on your (Debian-based) system.\nPractically, this means that now you can install them without manual compiling on current Debian and Ubuntu systems. Give it a try, add our Debian package repository, apt-get install python-orange for Orange library/modules and/or orange-canvas for GUI. If you install the later package, type orange in the terminal and Orange canvas will pop-up.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have created Debian packages for multiple Python versions. This means that they work now with both Python 2.6 and 2.7 out of the box, or if you compile them manually, with any (supported) version you have installed on your (Debian-based) system.\nPractically, this means that now you can install them without manual compiling on current Debian and Ubuntu systems. Give it a try, add our Debian package repository, apt-get install python-orange for Orange library/modules and/or orange-canvas for GUI." ,
	"author" : "BIOLAB",
	"summary" : "We have created Debian packages for multiple Python versions. This means that they work now with both Python 2.6 and 2.7 out of the box, or if you compile them manually, with any (supported) version you have installed on your (Debian-based) system.\nPractically, this means that now you can install them without manual compiling on current Debian and Ubuntu systems. Give it a try, add our Debian package repository, apt-get install python-orange for Orange library/modules and/or orange-canvas for GUI.",
	"date" : "Sep 13, 2011"
}

    
    , {
    "uri": "/blog/2011/09/07/3d-visualizations-in-orange/",
	"title": "3D Visualizations in Orange",
	"categories": ["opengl", "visualization"],
	"description": "",
	"content": "Over the summer I worked (and still do) on several new 3D visualization widgets as well as a 3D plotting library they use, which will hopefully simplify making more widgets. The library is designed to be similar in terms of API to the new Qt plotting library Noughmad is working on.\nThe library uses OpenGL 2/3: since Khronos deprecated parts of the old OpenGL API (particularly immediate mode and fixed-function functionality) care has been taken to use only capabilities less likely to go away in the years to come. All the drawing is done using shaders; geometry data is fed to the graphics hardware using Vertex Buffers. The library is fully functional under OpenGL 2.0; when hardware supports newer versions (3+), several optimizations are possible (e.g. geometry processing is done on the GPU rather than on CPU), possibly resulting in improved user experience.\nWidgets I worked on and are reasonably usable:\nScatterPlot3D Its GUI has the same options as the ordinary ScatterPlot (2D),with an additional dropdown for the third attribute (Z) and some new checkboxes (e.g. 2D/3D symbols). The data can be easily rotated, translated and scaled.Supports zoom levels and selections as well. VizRank works.Thanks to hardware acceleration, ScatterPlot3D is quite responsive even with largerdatasets (30k examples).\nLinProj3D LinProj3D is displayed using dark theme (themes are available in all 3D widgets).\nSphereviz3D Sphereviz3D has 2D symbols option enabled (also available in all 3D widgets). VizRank has been modified to work with three dimensions; PCA and SPCA options under FreeViz return first three most important components when used in these widgets.\nFuture Documentation for widgets and the library is still missing. Some additional widgets are being considered, such as NetExplorer3D.\nI wrote few technical details here.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Over the summer I worked (and still do) on several new 3D visualization widgets as well as a 3D plotting library they use, which will hopefully simplify making more widgets. The library is designed to be similar in terms of API to the new Qt plotting library Noughmad is working on.\nThe library uses OpenGL 2/3: since Khronos deprecated parts of the old OpenGL API (particularly immediate mode and fixed-function functionality) care has been taken to use only capabilities less likely to go away in the years to come." ,
	"author" : "BIOLAB",
	"summary" : "Over the summer I worked (and still do) on several new 3D visualization widgets as well as a 3D plotting library they use, which will hopefully simplify making more widgets. The library is designed to be similar in terms of API to the new Qt plotting library Noughmad is working on.\nThe library uses OpenGL 2/3: since Khronos deprecated parts of the old OpenGL API (particularly immediate mode and fixed-function functionality) care has been taken to use only capabilities less likely to go away in the years to come.",
	"date" : "Sep 7, 2011"
}

    
    , {
    "uri": "/blog/2011/09/04/orange-badges-are-here/",
	"title": "Orange badges are here!",
	"categories": ["orange3"],
	"description": "",
	"content": "Orange badges are here! They come in two flavors. Tasty!\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange badges are here! They come in two flavors. Tasty!" ,
	"author" : "BIOLAB",
	"summary" : "Orange badges are here! They come in two flavors. Tasty!",
	"date" : "Sep 4, 2011"
}

    
    , {
    "uri": "/blog/2011/09/03/gsoc-review-visualizations-with-qt/",
	"title": "GSoC Review: Visualizations with Qt",
	"categories": ["gsoc", "plot", "qt", "visualization"],
	"description": "",
	"content": "During the course of this summer, I created a new plotting library for Orange plot, replacing the use of PyQwt. I can say that I have succesfully completed my project, but the library (and especially the visualization widgets) could still use some more work. The new library supports a similar interface, so little change is needed to convert individual widgets, but it also has several advantages over the old implementation:\n Animations: When using a single curve to show all data points, data changes only move the points instead of replacing them. These moves are now animated, as are color and size changes. Multithreading: All position calculations are done in separate threads, so the interface remains responsive even when an long operation is running in the background. Speed: I removed several occurances of needlessly clearing and repopulating the graph. Simplicity: Because it was written with Orange in mind, the new library has functions that match Orange\u0026rsquo;s data structures. This leads to simpler code in widgets using the library, and less operations in Python. Appearance: The plot can use the system palette, or a custom color theme. In general, I think it looks much nicer that Qwt-based plots. Documentation: There is an extensive API documentation (will soon be available at Orange 2.5 documentation), as well as two widget examples.  However, there are also disadvantages to using the new library. They are not many, and I\u0026rsquo;ve been trying to keep them as few and as small as possible, but there still are some.\n Line rendering: For some reason, whenever lines are rendered on the plot, the whole widget starts acting very slow. The effect is even more noticeable when zooming. As far as I can tell, this happens in Qt\u0026rsquo;s drawing libraries, so there is not much I can do about it. Axis labels: With a large number of long axis labels, the formatting gets rather ugly. This is a minor inconvenience, but it does make the plots look unprofessional.  Fortunately, I have little school obligations this september, so I think I will be able to work on Orange some more, at least until school starts. I have already added gesture support and some minor improvements since the end of the program.\nFinally, I\u0026rsquo;d like to take this opportunity to thank the Orange team, especially my mentor Miha, for accepting me and helping me throughout the summer. It\u0026rsquo;s been an interesting project, and I\u0026rsquo;ll be happy to continue working with the same software and the same team.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "During the course of this summer, I created a new plotting library for Orange plot, replacing the use of PyQwt. I can say that I have succesfully completed my project, but the library (and especially the visualization widgets) could still use some more work. The new library supports a similar interface, so little change is needed to convert individual widgets, but it also has several advantages over the old implementation:" ,
	"author" : "BIOLAB",
	"summary" : "During the course of this summer, I created a new plotting library for Orange plot, replacing the use of PyQwt. I can say that I have succesfully completed my project, but the library (and especially the visualization widgets) could still use some more work. The new library supports a similar interface, so little change is needed to convert individual widgets, but it also has several advantages over the old implementation:",
	"date" : "Sep 3, 2011"
}

    
    , {
    "uri": "/blog/2011/09/02/gsoc-review-multi-label-classification-implementation/",
	"title": "GSoC Review: Multi-label Classification Implementation",
	"categories": ["classification", "gsoc", "multilabel"],
	"description": "",
	"content": "Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| \u0026gt; 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| \u0026gt; 2, then it is called a multi-class classification problem (Tsoumakas \u0026amp; Katakis, 2007).\nMulti-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification. However, currently, Orange can only handle single-label problems. As a result, the project Multi-label classification Implementation has been proposed to extend Orange to support multi-label.\nWe can group the existing methods for multi-label classification into two main categories: a) problem transformation method, and b) algorithm adaptation methods. In the former one, multi-label problems are converted to single-label, and then the traditional binary classification can apply; in the latter case, methods directly classify the multi-label data, instead.\nIn this project, two transformation methods and two algorithm adaptation methods are implemented. Along with the methods, their widgets are also added. As the evaluation metrics for multi-label data are different from the single-label ones, new evaluation measures are supported. The code is available in SVN branch.\nFortunately, benefiting from the Tab file format, the ExampleTable can store multi-label data without any modification. Now, we can add a special value – label into the attributes dictionary of the domain with value 1. In this way, if the attribute description has the keyword label, then it is a label; otherwise, it is a normal feature.\nWhat have been done in this project Transformation methods  br – Binary Relevance Learner (Tsoumakas \u0026amp; Katakis, 2007) lp – Label Powerset Classification (Tsoumakas \u0026amp; Katakis, 2007)  Algorithm Adaptation methods  mlknn – Multi-kNN Classification (Zhang \u0026amp; Zhou, 2007) brknn – BR-kNN Classification (Spyromitros et al. 2008)  Evaluation methods  mlc_hamming_loss – Example-based Hamming Loss (Schapire and Singer 2000) mlc_accuracy, mlc_precision, mlc_recall – Example-based accuracy, precision, recall (Godbole \u0026amp; Sarawagi, 2004)  Widgets  OWBR – Widget for Binary Relevance Learner OWLP – Widget for Label Powerset Classification OWMLkNN – Widget for Multi-kNN Classification OWBRkNN – Widget for BR-kNN Classification OWTestLearner – Widget for Evaluation  File Format Extension  extend the loadARFF function to support sparse Weka format new support mulan xml and arff format  Plan for the future  add more classification methods for multi-label, such as PT1 to PT6 add feature extraction method add ranking-based evaluation methods  How to use Basically, the way to use multi-label classification and evaluation is nearly the same as the single-label ones. The only difference between them is the different types of input data.\nExample for Classification import Orange data = Orange.data.Table(\u0026quot;emotions.tab\u0026quot;) classifier = Orange.multilabel.BinaryRelevanceLearner(data) for e in data: c,p = classifier(e,Orange.classification.Classifier.GetBoth) print c,p powerset_cliassifer = Orange.multilabel.LabelPowersetLearner(data) for e in data: c,p = powerset_cliassifer(e,Orange.classification.Classifier.GetBoth) print c,p mlknn_cliassifer = Orange.multilabel.MLkNNLearner(data,k=1) for e in data: c,p = mlknn_cliassifer(e,Orange.classification.Classifier.GetBoth) print c,p br_cliassifer = Orange.multilabel.BRkNNLearner(data,k=1) for e in data: c,p = br_cliassifer(e,Orange.classification.Classifier.GetBoth) print c,p  Example for Evaluation import Orange learners = [ Orange.multilabel.BinaryRelevanceLearner(name=\u0026quot;br\u0026quot;), Orange.multilabel.BinaryRelevanceLearner(name=\u0026quot;br\u0026quot;, \\ base_learner=Orange.classification.knn.kNNLearner), Orange.multilabel.LabelPowersetLearner(name=\u0026quot;lp\u0026quot;), Orange.multilabel.LabelPowersetLearner(name=\u0026quot;lp\u0026quot;, \\ base_learner=Orange.classification.knn.kNNLearner), Orange.multilabel.MLkNNLearner(name=\u0026quot;mlknn\u0026quot;,k=5), Orange.multilabel.BRkNNLearner(name=\u0026quot;brknn\u0026quot;,k=5), ] data = Orange.data.Table(\u0026quot;emotions.xml\u0026quot;) res = Orange.evaluation.testing.cross_validation(learners, data,2) loss = Orange.evaluation.scoring.mlc_hamming_loss(res) accuracy = Orange.evaluation.scoring.mlc_accuracy(res) precision = Orange.evaluation.scoring.mlc_precision(res) recall = Orange.evaluation.scoring.mlc_recall(res) print 'loss=', loss print 'accuracy=', accuracy print 'precision=', precision print 'recall=', recall  References  G. Tsoumakas and I. Katakis. Multi-label classification: An overview\u0026quot;. International Journal of Data Warehousing and Mining, 3(3):1-13, 2007. E. Spyromitros, G. Tsoumakas, and I. Vlahavas, An Empirical Study of Lazy Multilabel Classification Algorithms. Proc. 5th Hellenic Conference on Artificial Intelligence (SETN 2008), Springer, Syros, Greece, 2008. M. Zhang and Z. Zhou. ML-KNN: A lazy learning approach to multi-label learning. Pattern Recognition, 40, 7 (Jul. 2007), 2038-2048. S. Godbole and S. Sarawagi. Discriminative Methods for Multi-labeled Classification, Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2004. R. E. Schapire and Y. Singer. Boostexter: a bossting-based system for text categorization, Machine Learning, vol.39, no.2/3, 2000, pp:135-68.  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| \u0026gt; 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| \u0026gt; 2, then it is called a multi-class classification problem (Tsoumakas \u0026amp; Katakis, 2007).\nMulti-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification." ,
	"author" : "BIOLAB",
	"summary" : "Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| \u0026gt; 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| \u0026gt; 2, then it is called a multi-class classification problem (Tsoumakas \u0026amp; Katakis, 2007).\nMulti-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification.",
	"date" : "Sep 2, 2011"
}

    
    , {
    "uri": "/blog/2011/09/01/gsoc-review-mf-matrix-factorization-techniques-for-data-mining/",
	"title": "GSoC Review: MF - Matrix Factorization Techniques for Data Mining",
	"categories": ["gsoc", "matrixfactorization"],
	"description": "",
	"content": "MF - Matrix Factorization Techniques for Data Mining is a Python scripting library which includes a number of published matrix factorization algorithms, initialization methods, quality and performance measures and facilitates the combination of these to produce new strategies. The library represents a unified and efficient interface to matrix factorization algorithms and methods.\nThe MF works with numpy dense matrices and scipy sparse matrices (where this is possible to save on space). The library has support for multiple runs of the algorithms which can be used for some quality measures. By setting runtime specific options tracking the residuals error within one (or more) run or tracking fitted factorization model is possible. Extensive documentation with working examples which demonstrate real applications, commonly used benchmark data and visualization methods are provided to help with the interpretation and comprehension of the results.\nContent of Current Release Factorization Methods  BD - Bayesian nonnegative matrix factorization Gibbs sampler [Schmidt2009] BMF - Binary matrix factorization [Zhang2007] ICM - Iterated conditional modes nonnegative matrix factorization [Schmidt2009] LFNMF - Fisher nonnegative matrix factorization for learning local features [Wang2004], [Li2001] LSNMF - Alternative nonnegative least squares matrix factorization using projected gradient method for subproblems [Lin2007] NMF - Standard nonnegative matrix factorization with Euclidean / Kullback-Leibler update equations and Frobenius / divergence / connectivity cost functions [Lee2001], [Brunet2004] NSNMF - Nonsmooth nonnegative matrix factorization [Montano2006] PMF - Probabilistic nonnegative matrix factorization [Laurberg2008], [Hansen2008] PSMF - Probabilistic sparse matrix factorization [Dueck2005], [Dueck2004], [Srebro2001], [Li2007] SNMF - Sparse nonnegative matrix factorization based on alternating nonnegativity constrained least squares [Park2007] SNMNMF - Sparse network regularized multiple nonnegative matrix factorization [Zhang2011]  Initialization Methods  Random Fixed NNDSVD [Boutsidis2007] Random C [Albright2006] Random VCol [Albright2006]  Quality and Performance Measures  Distance Residuals Connectivity matrix Consensus matrix Entropy of the fitted NMF model [Park2007] Dominant basis components computation Explained variance Feature score computation representing its specificity to basis vectors [Park2007] Computation of most basis specific features for basis vectors [Park2007] Purity [Park2007] Residual sum of squares - can be used for rank estimate [Hutchins2008], [Frigyesi2008] Sparseness [Hoyer2004] Cophenetic correlation coefficient of consensus matrix - can be used for rank estimate [Brunet2004] Dispersion [Park2007] Factorization rank estimation Selected matrix factorization method specific  Plans for Future General plan for future releases of MF library is to alleviate the usage for non-technical users, increase library stability and provide comprehensive visualization methods. Specifically, in algorithm sense addition of the following could be provided.\n Extending Bayesian methods with variational BD and linearly constrained BD. Adaptation of the PMF model to interval-valued matrices. Nonnegative matrix approximation. Multiplicative iterative schema.  Usage # Import MF library entry point for factorization import mf from scipy.sparse import csr_matrix from scipy import array from numpy import dot # We will try to factorize sparse matrix. Construct sparse matrix in CSR format. V = csr_matrix((array([1,2,3,4,5,6]),array([0,2,2,0,1,2]),array([0,2,3,6])),shape=(3,3)) # Run Standard NMF rank 4 algorithm # Returned object is fitted factorization model. # Through it user can access quality and performance measures. fit = mf.mf(V,method = \u0026quot;nmf\u0026quot;,max_iter = 30,rank = 4,update = 'divergence',objective = 'div') # Basis matrix. It is sparse, as input V was sparse as well. W = fit.basis() print \u0026quot;Basis matrix\\n\u0026quot;, W.todense() # Mixture matrix. We print this tiny matrix in dense format. H = fit.coef() print \u0026quot;Coef\\n\u0026quot;, H.todense() # Return the loss function according to Kullback-Leibler divergence. print \u0026quot;Distance Kullback-Leibler\u0026quot;, fit.distance(metric = \u0026quot;kl\u0026quot;) # Compute generic set of measures to evaluate the quality of the factorization sm = fit.summary() # Print sparseness (Hoyer, 2004) of basis and mixture matrix print \u0026quot;Sparseness W: %5.3f H: %5.3f\u0026quot; % (sm['sparseness'][0], sm['sparseness'][1]) # Print actual number of iterations performed print \u0026quot;Iterations\u0026quot;, sm['n_iter'] # Print estimate of target matrix V print \u0026quot;Estimate\\n\u0026quot;, dot(W.todense(), H.todense())  Examples Examples with visualized results in bioinformatics, image processing, text analysis, recommendation systems are provided in Examples section of Documentation.\nFigure 1: Reordered consensus matrix generated for rank = 2 on Leukemia data set.\nFigure 2: Interpretation of NMF - Divergence basis vectors on Medlars data set. By considering the highest weighted terms in this vector, we can assign a label or topic to basis vector W1, a user might attach the label liver to basis vector W1.\nFigure 3: Basis images of LSNMF obtained after 500 iterations on original face images. The bases trained by LSNMF are additive but not spatially localized for representation of faces.\nRelevant Links  Extensive published documentation with examples Orange wiki MF Project page Github repository with source code Short presentation in pdf format of MF library  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "MF - Matrix Factorization Techniques for Data Mining is a Python scripting library which includes a number of published matrix factorization algorithms, initialization methods, quality and performance measures and facilitates the combination of these to produce new strategies. The library represents a unified and efficient interface to matrix factorization algorithms and methods.\nThe MF works with numpy dense matrices and scipy sparse matrices (where this is possible to save on space)." ,
	"author" : "BIOLAB",
	"summary" : "MF - Matrix Factorization Techniques for Data Mining is a Python scripting library which includes a number of published matrix factorization algorithms, initialization methods, quality and performance measures and facilitates the combination of these to produce new strategies. The library represents a unified and efficient interface to matrix factorization algorithms and methods.\nThe MF works with numpy dense matrices and scipy sparse matrices (where this is possible to save on space).",
	"date" : "Sep 1, 2011"
}

    
    , {
    "uri": "/blog/2011/08/24/faster-classification-and-regression-trees/",
	"title": "Faster classification and regression trees",
	"categories": ["classification", "regression", "tree"],
	"description": "",
	"content": "SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.\nThe motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values.\nComparison with TreeLearner The graph below shows SimpleTreeLearner construction times on datasets bundled with Orange normalized to TreeLearner. Smaller is better.\nThe harmonic mean (average speedup) on all the benchmarks is 11.4.\nUsage The user can set four parameters:\nmaxMajority\nMaximal proportion of majority class.\nminExamples\nMinimal number of examples in leaves.\nmaxDepth\nMaximal depth of tree.\nskipProb\nAt every split an attribute will be skipped with probability skipProb. This parameter is especially useful for building random forests.\nThe code snippet below demonstrates the basic usage of SimpleTreeLearner. It behaves much like any other Orange learner would.\nimport Orange data = Orange.data.Table(\u0026quot;iris\u0026quot;) # build classifier and classify train data classifier = Orange.classification.tree.SimpleTreeLearner(data, maxMajority=0.8) for ex in data: print classifier(ex) # estimate classification accuracy with cross-validation learner = Orange.classification.tree.SimpleTreeLearner(minExamples=2) result = Orange.evaluation.testing.cross_validation([learner], data) print 'CA:', Orange.evaluation.scoring.CA(result)[0]  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.\nThe motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values." ,
	"author" : "BIOLAB",
	"summary" : "SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.\nThe motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values.",
	"date" : "Aug 24, 2011"
}

    
    , {
    "uri": "/blog/2011/08/19/golden-sublime-triangles-in-orange/",
	"title": "Golden (sublime) triangles in Orange",
	"categories": ["visualization"],
	"description": "",
	"content": "Hand in hand with the development of the new visualization framework and the financial crisis we are putting some gold into Orange. The arrows at the ends of the axes are, as of today, small golden triangles. See the changes in owaxis.py!\n- path.moveTo(0, 3) - path.lineTo(0, -3) - path.lineTo(5, 0) + path.moveTo(0, 3.09) + path.lineTo(0, -3.09) + path.lineTo(9.51, 0)  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Hand in hand with the development of the new visualization framework and the financial crisis we are putting some gold into Orange. The arrows at the ends of the axes are, as of today, small golden triangles. See the changes in owaxis.py!\n- path.moveTo(0, 3) - path.lineTo(0, -3) - path.lineTo(5, 0) + path.moveTo(0, 3.09) + path.lineTo(0, -3.09) + path.lineTo(9.51, 0)  " ,
	"author" : "MARKO",
	"summary" : "Hand in hand with the development of the new visualization framework and the financial crisis we are putting some gold into Orange. The arrows at the ends of the axes are, as of today, small golden triangles. See the changes in owaxis.py!\n- path.moveTo(0, 3) - path.lineTo(0, -3) - path.lineTo(5, 0) + path.moveTo(0, 3.09) + path.lineTo(0, -3.09) + path.lineTo(9.51, 0)  ",
	"date" : "Aug 19, 2011"
}

    
    , {
    "uri": "/blog/2011/08/03/orange-at-ismbeccb-2011/",
	"title": "Orange at ISMB/ECCB 2011",
	"categories": ["bioinformatics", "bioorange", "conference"],
	"description": "",
	"content": "We presented the Orange Bioinformatics add-on at the ISMB/ECCB conference in Vienna, a joined event covering both 19th Annual International Conference on Intelligent Systems for Molecular Biology and 10th European Conference on Computational Biology.\nWe were giving out Orange stickers (with the URL) to the poster\u0026rsquo;s visitors. There was some interest; in the end we gave out about 10 of them, mostly to biologists, who were excited to perform some of the analysis themselves. Among the visitors was also a developer of a similar tool who seemed slightly surprised that something like this already exists, while another was disappointed because Orange only runs locally.\nSee the poster in action on the photo taken by Gregor Rot.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We presented the Orange Bioinformatics add-on at the ISMB/ECCB conference in Vienna, a joined event covering both 19th Annual International Conference on Intelligent Systems for Molecular Biology and 10th European Conference on Computational Biology.\nWe were giving out Orange stickers (with the URL) to the poster\u0026rsquo;s visitors. There was some interest; in the end we gave out about 10 of them, mostly to biologists, who were excited to perform some of the analysis themselves." ,
	"author" : "MARKO",
	"summary" : "We presented the Orange Bioinformatics add-on at the ISMB/ECCB conference in Vienna, a joined event covering both 19th Annual International Conference on Intelligent Systems for Molecular Biology and 10th European Conference on Computational Biology.\nWe were giving out Orange stickers (with the URL) to the poster\u0026rsquo;s visitors. There was some interest; in the end we gave out about 10 of them, mostly to biologists, who were excited to perform some of the analysis themselves.",
	"date" : "Aug 3, 2011"
}

    
    , {
    "uri": "/blog/2011/07/29/networkx-in-orange/",
	"title": "NetworkX in Orange",
	"categories": ["analysis", "network", "networkx", "visualization"],
	"description": "",
	"content": "NetworkX – a popular open-source python library for network analysis has finally found its way into Orange. It is now used as a base class for network representation in all Orange modules and widgets. By that, we offered to the widespread network community a fruitful and fun way to visualize and explore networks, using their existing NetworkX scripts. It has never been easier to combine network analysis and visualization with existing machine learning and data discovery methods.\nComplete documentation is available in the Orange network headquarters. For a brief overview, take a look at the following example. Let us suppose we would like to analyse the data about patients, having one of two types of leukemia. So, we have a data set with 72 patient, 4600+ gene expressions and a class variable. We also have a vast network of human genes, connected if they share a biological function. What we would like to examine is a sub-network with only several hundred most expressed genes from the data set. To show off a bit, we will also use the Orange Bioinformatics add-on. Here is how we do it:\nimport Orange import obiExpression # load leukemia data set table = Orange.data.Table(\u0026quot;/media/Ox/Projects_Archive/res/BIO/leukemia/leukemiaGSEA.tab\u0026quot;) useAttributeLabels = False ttest = obiExpression.ExpressionSignificance_TTest(table, useAttributeLabels) target = [table.domain.classVar(0), table.domain.classVar(1)] # test for significantly expressed genes score = ttest(target = target) # each gene is scored (t-test, p-value) print score[0] \u0026gt;\u0026gt;\u0026gt; (FloatVariable 'HIST1H4C', (1.8377179790830149, 0.07034778767062116)) # sort by p-value from operator import itemgetter score.sort(key=lambda s: s[1][1]) # select 200 genes with the lowest p-value important_genes = [gene_var.name for gene_var, s in score[:200]] # read the gene network (5000+ genes, dense network) G = Orange.network.readwrite.read('genes_biofunct.gpickle') items = G.items().filter_bool({'gene': important_genes}) indices = [i for i, present in enumerate(items) if present] # build a subraph of 200 most expressed genes G_sub = G.subgraph(indices)  In addition to the power of scripting environment, we also get the benefits of visual data exploration with Orange widgets. However, network widgets are currently under heavy development, so expect some bugs if you dare to try them. Coding should be finished in a month or two, check the blog for progress updates. Here is how to open the network in Nx Explorer widget:\nimport sys import PyQt4 # must have OWNxExplorer in python path! import OWNxExplorer app=PyQt4.QtGui.QApplication(sys.argv) ow=OWNxExplorer.OWNxExplorer() ow.show() # set the network ow.set_graph(G_sub) app.exec_()  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "NetworkX – a popular open-source python library for network analysis has finally found its way into Orange. It is now used as a base class for network representation in all Orange modules and widgets. By that, we offered to the widespread network community a fruitful and fun way to visualize and explore networks, using their existing NetworkX scripts. It has never been easier to combine network analysis and visualization with existing machine learning and data discovery methods." ,
	"author" : "BIOLAB",
	"summary" : "NetworkX – a popular open-source python library for network analysis has finally found its way into Orange. It is now used as a base class for network representation in all Orange modules and widgets. By that, we offered to the widespread network community a fruitful and fun way to visualize and explore networks, using their existing NetworkX scripts. It has never been easier to combine network analysis and visualization with existing machine learning and data discovery methods.",
	"date" : "Jul 29, 2011"
}

    
    , {
    "uri": "/blog/2011/07/20/orange-gsoc-multi-label-classification-implementation/",
	"title": "Orange GSoC: Multi-label Classification Implementation",
	"categories": ["gsoc", "multilabel"],
	"description": "",
	"content": "Multi-label classification is one of the three projects of Google Summer Code 2011 for Orange. The main goal is to extend the Orange to support multi-label, including dataset support, two basic multi-label classifications-problem-transformation methods \u0026amp; algorithm adaptation methods, evaluation measures, GUI support, documentation, testing, and so on.\nMy name is Wencan Luo, from China. I\u0026rsquo;m very happy to work with my mentor Matija. Until now, we have finished a framework for multi-label support for Orange.\nTo support multi-label data structure, a special value is added into their \u0026lsquo;attributes\u0026rsquo; dictionary. In this way, we can know whether the attribute is a type of class without altering the old Example Table class.\nMoreover, a transformation classification method to support multilabel is implemented, named Binary Relevance. All the codes are extended from the old Orange code using Python to be compatible with single-label classification methods.\nIn addition, the evaluator for multilalbel classification is also implemented based on the old single-label evaluator in Orange.evaluator.testing and Orange.evaluator.scoring modules.\nAt last, the widget for Binary Relevance method and Evaluator is implemented.\nMany work has to be done as following:\n one more transformation method two adaptive methods ranking-based evaluator widgets to support the above methods testing  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Multi-label classification is one of the three projects of Google Summer Code 2011 for Orange. The main goal is to extend the Orange to support multi-label, including dataset support, two basic multi-label classifications-problem-transformation methods \u0026amp; algorithm adaptation methods, evaluation measures, GUI support, documentation, testing, and so on.\nMy name is Wencan Luo, from China. I\u0026rsquo;m very happy to work with my mentor Matija. Until now, we have finished a framework for multi-label support for Orange." ,
	"author" : "BIOLAB",
	"summary" : "Multi-label classification is one of the three projects of Google Summer Code 2011 for Orange. The main goal is to extend the Orange to support multi-label, including dataset support, two basic multi-label classifications-problem-transformation methods \u0026amp; algorithm adaptation methods, evaluation measures, GUI support, documentation, testing, and so on.\nMy name is Wencan Luo, from China. I\u0026rsquo;m very happy to work with my mentor Matija. Until now, we have finished a framework for multi-label support for Orange.",
	"date" : "Jul 20, 2011"
}

    
    , {
    "uri": "/blog/2011/07/03/fink-packages-now-also-64-bit/",
	"title": "Fink packages now also 64-bit",
	"categories": ["distribution", "download", "packaging"],
	"description": "",
	"content": "Fink packages (we are using for system-wide Orange installations on Mac OS X) were updated to 64-bit. So if you were using 64-bit Fink installation you will be now able also to use Orange (and our binary Fink repository of already compiled packages). Just use this this installation script to configure your local Fink installation to use our binary Fink repository and add information about Orange packages (they are not available among official Fink packages).\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Fink packages (we are using for system-wide Orange installations on Mac OS X) were updated to 64-bit. So if you were using 64-bit Fink installation you will be now able also to use Orange (and our binary Fink repository of already compiled packages). Just use this this installation script to configure your local Fink installation to use our binary Fink repository and add information about Orange packages (they are not available among official Fink packages)." ,
	"author" : "BIOLAB",
	"summary" : "Fink packages (we are using for system-wide Orange installations on Mac OS X) were updated to 64-bit. So if you were using 64-bit Fink installation you will be now able also to use Orange (and our binary Fink repository of already compiled packages). Just use this this installation script to configure your local Fink installation to use our binary Fink repository and add information about Orange packages (they are not available among official Fink packages).",
	"date" : "Jul 3, 2011"
}

    
    , {
    "uri": "/blog/2011/06/30/orange-gsoc-visualizations-with-qt/",
	"title": "Orange GSoC: Visualizations with Qt",
	"categories": ["gsoc", "visualization"],
	"description": "",
	"content": "Hello, my name is Miha Čančula and this summer I\u0026rsquo;m working on Orange as part of Google\u0026rsquo;s Summer of Code program, mentored by Miha Štajdohar. My task is to replace the current visualization framework based on Qwt with a custom library, depending only on Qt. This library will better support Orange\u0026rsquo;s very specific visualizations and will replace the unmaintained PyQwt.\nI have a lot of experience with Qt and its graphics classes, both in C++ and Python, but I\u0026rsquo;m relatively now to Orange. As a physics student, especially now that I\u0026rsquo;m selecting a computational physics program, this a great learning opportunity for me.\nI think my work is progressing very well, because many visualizations already work with the new library with only minor modifications:\n Scatterplot Linear projections Discretize All the visualizations in VizRank and FreeViz dialogs  The library is written partially in C++, especially the performance-sensitive parts, and partially in Python. It uses the Qt Graphics View Framework, with several reimplemented or wrapped method to preserve the old behavior and API. I have tried to keep the necessary modification to the widgets themselves to a minimum, so the large majority of changes are in the OWGraph class which server as the base class for all graphs.\nGraphs made by Qwt are not very flexible, they only support graphs with cartesian axes. On the other hand, visualization Orange often use custom axes and transformations. That\u0026rsquo;s why I designed the new library with support for arbitrary axes, curves and other elements. All of these can be extendeng with classes written in Python, C++, or a combination thereof. The required changes to visualizations I already ported to the new OWGraph class were mostly simplifications, with very little new code added.\nFor example, zooming and selection is implemented completely in the new OWGraph class, with the same function and attribute names as before, so visualizations themselves didn\u0026rsquo;t need any changes at all.\nThe new framework is also able to produce much nicer graphs. I haven\u0026rsquo;t had the time to customize the looks much, but it\u0026rsquo;s possible to set colors, line widths, point sizes and symbols from Python. There are still some settings that have no UI configuration, but I will focus on that after making sure that visualization widgets work with the new library.\nCurrently I\u0026rsquo;m trying to change as many widgets as possible to the new classes. As I said above, I only completed a few of them, but I believe the others won\u0026rsquo;t require too much work.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Hello, my name is Miha Čančula and this summer I\u0026rsquo;m working on Orange as part of Google\u0026rsquo;s Summer of Code program, mentored by Miha Štajdohar. My task is to replace the current visualization framework based on Qwt with a custom library, depending only on Qt. This library will better support Orange\u0026rsquo;s very specific visualizations and will replace the unmaintained PyQwt.\nI have a lot of experience with Qt and its graphics classes, both in C++ and Python, but I\u0026rsquo;m relatively now to Orange." ,
	"author" : "BIOLAB",
	"summary" : "Hello, my name is Miha Čančula and this summer I\u0026rsquo;m working on Orange as part of Google\u0026rsquo;s Summer of Code program, mentored by Miha Štajdohar. My task is to replace the current visualization framework based on Qwt with a custom library, depending only on Qt. This library will better support Orange\u0026rsquo;s very specific visualizations and will replace the unmaintained PyQwt.\nI have a lot of experience with Qt and its graphics classes, both in C++ and Python, but I\u0026rsquo;m relatively now to Orange.",
	"date" : "Jun 30, 2011"
}

    
    , {
    "uri": "/blog/2011/06/30/debian-packages-for-squeeze/",
	"title": "Debian packages for Squeeze",
	"categories": ["debian", "distribution", "download", "packaging"],
	"description": "",
	"content": "We have updated our daily Debian packages to Squeeze (current Debian stable). You just have to reconfigure our package repository in your /etc/apt/sources.list to:\ndeb http://orange.biolab.si/debian squeeze main deb-src http://orange.biolab.si/debian squeeze main  Those packages are compiled for Python 2.6.\nYou can read more about Debian packages in our old blog post.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have updated our daily Debian packages to Squeeze (current Debian stable). You just have to reconfigure our package repository in your /etc/apt/sources.list to:\ndeb http://orange.biolab.si/debian squeeze main deb-src http://orange.biolab.si/debian squeeze main  Those packages are compiled for Python 2.6.\nYou can read more about Debian packages in our old blog post." ,
	"author" : "BIOLAB",
	"summary" : "We have updated our daily Debian packages to Squeeze (current Debian stable). You just have to reconfigure our package repository in your /etc/apt/sources.list to:\ndeb http://orange.biolab.si/debian squeeze main deb-src http://orange.biolab.si/debian squeeze main  Those packages are compiled for Python 2.6.\nYou can read more about Debian packages in our old blog post.",
	"date" : "Jun 30, 2011"
}

    
    , {
    "uri": "/blog/2011/06/24/orange-gsoc-mf-techniques-for-data-mining/",
	"title": "Orange GSoC: MF Techniques for Data Mining",
	"categories": ["gsoc", "matrixfactorization"],
	"description": "",
	"content": "I am one of three students who are working on GSoC projects for Orange this year. The objective of the project Matrix Factorization Techniques for Data Mining is to provide the Orange community with a unified and efficient interface to matrix factorization algorithms and methods.\nFor that purpose I have been developing a library which will include a number of published factorization algorithms and initialization methods and will facilitate combinations of these to produce new strategies. Extensive documentation with working examples that demonstrate applications, commonly used benchmark data and possibly some visualization methods will be provided to help with the interpretation and comprehension of the factorization results.\nMain factorization techniques and their variations included in the library are:\n family of nonnegative matrix factorization algorithms (NMF), including Brunet NMF, sparse NMF, non-smooth NMF, least-squares NMF, local Fisher NMF; probabilistic factorization (PMF) and its sparse variant (PSMF); Bayesian decomposition (BD); iterated conditional modes (ICM) algorithm.  Different multiplicative and update algorithms for NMF will be analyzed which minimize least-squares error or generalized Kullback-Leibler divergence.\nFor those interested some more information with details about algorithms is available at project home page.\nThere is still much work to do but I have been enjoying at it and I am looking forward to continuing with the project.\nThanks to the Orange team and mentor prof. dr. Blaz Zupan for support and advice.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "I am one of three students who are working on GSoC projects for Orange this year. The objective of the project Matrix Factorization Techniques for Data Mining is to provide the Orange community with a unified and efficient interface to matrix factorization algorithms and methods.\nFor that purpose I have been developing a library which will include a number of published factorization algorithms and initialization methods and will facilitate combinations of these to produce new strategies." ,
	"author" : "BIOLAB",
	"summary" : "I am one of three students who are working on GSoC projects for Orange this year. The objective of the project Matrix Factorization Techniques for Data Mining is to provide the Orange community with a unified and efficient interface to matrix factorization algorithms and methods.\nFor that purpose I have been developing a library which will include a number of published factorization algorithms and initialization methods and will facilitate combinations of these to produce new strategies.",
	"date" : "Jun 24, 2011"
}

    
    , {
    "uri": "/blog/2011/06/24/orange-t-shirts/",
	"title": "Orange T-shirts",
	"categories": ["orange3"],
	"description": "",
	"content": "If you maybe missed on our Facebook page: we have made our own fruity t-shirts. They are simply awesome and show to everybody around you that you have a taste! Just check the handsomeness:\nWe will be selling them on the website soon for $15 (shipping costs included), but if you want to have one (or more) in advance, drop us a line and we will see what we can do. We have them for all you brave girls and boys and in different sizes.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "If you maybe missed on our Facebook page: we have made our own fruity t-shirts. They are simply awesome and show to everybody around you that you have a taste! Just check the handsomeness:\nWe will be selling them on the website soon for $15 (shipping costs included), but if you want to have one (or more) in advance, drop us a line and we will see what we can do." ,
	"author" : "BIOLAB",
	"summary" : "If you maybe missed on our Facebook page: we have made our own fruity t-shirts. They are simply awesome and show to everybody around you that you have a taste! Just check the handsomeness:\nWe will be selling them on the website soon for $15 (shipping costs included), but if you want to have one (or more) in advance, drop us a line and we will see what we can do.",
	"date" : "Jun 24, 2011"
}

    
    , {
    "uri": "/blog/2011/06/14/contact-us/",
	"title": "Contact us!",
	"categories": ["website"],
	"description": "",
	"content": "We have added a simple contact form so that you can get in direct contact with us. Please do not misuse it (we will simply ignore you). Support and other general questions should be posted in our forum and for issues with Orange you should use our ticketing system to report them.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have added a simple contact form so that you can get in direct contact with us. Please do not misuse it (we will simply ignore you). Support and other general questions should be posted in our forum and for issues with Orange you should use our ticketing system to report them." ,
	"author" : "BIOLAB",
	"summary" : "We have added a simple contact form so that you can get in direct contact with us. Please do not misuse it (we will simply ignore you). Support and other general questions should be posted in our forum and for issues with Orange you should use our ticketing system to report them.",
	"date" : "Jun 14, 2011"
}

    
    , {
    "uri": "/blog/2011/05/20/orange-2-5-progress/",
	"title": "Orange 2.5 progress",
	"categories": ["orange25"],
	"description": "",
	"content": "We decided that we should reorganize Orange to provide more intuitive interface to the scripting interface. The next release, Orange 2.5 is getting better every day. But fear not, dear reader, we are working hard to ensure that your scripts will still work.\nIn the last morning of the camp in Bohinj we decided to use undercase_separated names instead of CamelCase. We have been steadily converting them and the deprecation utilities by Aleš help a lot. We just list the name changes for class attributes or arguments and their renaming is handled gracefully; they also remain accessible with the old names. Therefore, the code does not need to be duplicated to ensure backwards compatibility.\nA simple example from the documentation of bagging and boosting. The old version first:\nimport orange, orngEnsemble, orngTree import orngTest, orngStat tree = orngTree.TreeLearner(mForPruning=2, name=\u0026quot;tree\u0026quot;) bs = orngEnsemble.BoostedLearner(tree, name=\u0026quot;boosted tree\u0026quot;) bg = orngEnsemble.BaggedLearner(tree, name=\u0026quot;bagged tree\u0026quot;) data = orange.ExampleTable(\u0026quot;lymphography.tab\u0026quot;) learners = [tree, bs, bg] results = orngTest.crossValidation(learners, data, folds=3) print \u0026quot;Classification Accuracy:\u0026quot; for i in range(len(learners)): print (\u0026quot;%15s: %5.3f\u0026quot;) % (learners[i].name, orngStat.CA(results)[i])  Orange 2.5 version:\nimport Orange tree = Orange.classification.tree.TreeLearner(m_pruning=2, name=\u0026quot;tree\u0026quot;) bs = Orange.ensemble.boosting.BoostedLearner(tree, name=\u0026quot;boosted tree\u0026quot;) bg = Orange.ensemble.bagging.BaggedLearner(tree, name=\u0026quot;bagged tree\u0026quot;) table = Orange.data.Table(\u0026quot;lymphography.tab\u0026quot;) learners = [tree, bs, bg] results = Orange.evaluation.testing.cross_validation(learners, table, folds=3) print \u0026quot;Classification Accuracy:\u0026quot; for i in range(len(learners)): print (\u0026quot;%15s: %5.3f\u0026quot;) % (learners[i].name, Orange.evaluation.scoring.CA(results)[i])  In new Orange we only need to import a single module, Orange, the root of the new hierarchy.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We decided that we should reorganize Orange to provide more intuitive interface to the scripting interface. The next release, Orange 2.5 is getting better every day. But fear not, dear reader, we are working hard to ensure that your scripts will still work.\nIn the last morning of the camp in Bohinj we decided to use undercase_separated names instead of CamelCase. We have been steadily converting them and the deprecation utilities by Aleš help a lot." ,
	"author" : "MARKO",
	"summary" : "We decided that we should reorganize Orange to provide more intuitive interface to the scripting interface. The next release, Orange 2.5 is getting better every day. But fear not, dear reader, we are working hard to ensure that your scripts will still work.\nIn the last morning of the camp in Bohinj we decided to use undercase_separated names instead of CamelCase. We have been steadily converting them and the deprecation utilities by Aleš help a lot.",
	"date" : "May 20, 2011"
}

    
    , {
    "uri": "/blog/2011/04/25/accepted-gsoc-2011-students-announced/",
	"title": "Accepted GSoC 2011 students announced",
	"categories": ["gsoc"],
	"description": "",
	"content": "Accepted proposals/projects for Google Summer of Code 2011 have been announced. We got 3 students which will this year work on Orange:\n  Marinka Žitnik: Matrix Factorization Techniques for Data Mining\n  Miha Čančula: 2D visualization using PyQt\n  Wencan Luo: Multi-label classification\n  Congrats to all accepted students. We are looking forward working with you. And for all other students: please apply again next year. Your proposals were good, but we just could not accept everybody.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Accepted proposals/projects for Google Summer of Code 2011 have been announced. We got 3 students which will this year work on Orange:\n  Marinka Žitnik: Matrix Factorization Techniques for Data Mining\n  Miha Čančula: 2D visualization using PyQt\n  Wencan Luo: Multi-label classification\n  Congrats to all accepted students. We are looking forward working with you. And for all other students: please apply again next year. Your proposals were good, but we just could not accept everybody." ,
	"author" : "BIOLAB",
	"summary" : "Accepted proposals/projects for Google Summer of Code 2011 have been announced. We got 3 students which will this year work on Orange:\n  Marinka Žitnik: Matrix Factorization Techniques for Data Mining\n  Miha Čančula: 2D visualization using PyQt\n  Wencan Luo: Multi-label classification\n  Congrats to all accepted students. We are looking forward working with you. And for all other students: please apply again next year. Your proposals were good, but we just could not accept everybody.",
	"date" : "Apr 25, 2011"
}

    
    , {
    "uri": "/blog/2011/04/08/student-application-period-for-gsoc-2011-has-ended/",
	"title": "Student application period for GSoC 2011 has ended",
	"categories": ["gsoc"],
	"description": "",
	"content": "Student application period for Google Summer of Code 2011 has ended. We got 47 proposals from students all around the world. Now it is time for us to evaluate them and select the best proposals and the best students to work this year on Orange.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Student application period for Google Summer of Code 2011 has ended. We got 47 proposals from students all around the world. Now it is time for us to evaluate them and select the best proposals and the best students to work this year on Orange." ,
	"author" : "BIOLAB",
	"summary" : "Student application period for Google Summer of Code 2011 has ended. We got 47 proposals from students all around the world. Now it is time for us to evaluate them and select the best proposals and the best students to work this year on Orange.",
	"date" : "Apr 8, 2011"
}

    
    , {
    "uri": "/blog/2011/03/29/our-gsoc-2011-posters/",
	"title": "Our GSoC 2011 posters",
	"categories": ["gsoc"],
	"description": "",
	"content": "We have made our own recruitment posters for this year\u0026rsquo;s Google Summer of Code inviting students to participate.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have made our own recruitment posters for this year\u0026rsquo;s Google Summer of Code inviting students to participate." ,
	"author" : "BIOLAB",
	"summary" : "We have made our own recruitment posters for this year\u0026rsquo;s Google Summer of Code inviting students to participate.",
	"date" : "Mar 29, 2011"
}

    
    , {
    "uri": "/blog/2011/03/28/data-loading-speedups/",
	"title": "Data loading speedups",
	"categories": ["dataloading", "performance"],
	"description": "",
	"content": "Orange has been loading data faster since the end of February, especially if there are many attributes in the file.\nQuick comparisons between the old new versions, measured on my computer:\n adult.tab (32561 examples, 15 attributes): old version = 1.41s, new version = 0.86s. DLBCL.tab (77 examples, 7071 attributes): old version = 2.72s, new version = 0.93s. GDS1962.tab (104 examples, 31837 attributes): old version = 33.5s, new version = 6.6s.  The speedups were obtained with:\n reuse of a buffer for parsing, skipping type detection for attributes with known types, and by keeping attributes in a different data structure internally.  ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "Orange has been loading data faster since the end of February, especially if there are many attributes in the file.\nQuick comparisons between the old new versions, measured on my computer:\n adult.tab (32561 examples, 15 attributes): old version = 1.41s, new version = 0.86s. DLBCL.tab (77 examples, 7071 attributes): old version = 2.72s, new version = 0.93s. GDS1962.tab (104 examples, 31837 attributes): old version = 33.5s, new version = 6." ,
	"author" : "MARKO",
	"summary" : "Orange has been loading data faster since the end of February, especially if there are many attributes in the file.\nQuick comparisons between the old new versions, measured on my computer:\n adult.tab (32561 examples, 15 attributes): old version = 1.41s, new version = 0.86s. DLBCL.tab (77 examples, 7071 attributes): old version = 2.72s, new version = 0.93s. GDS1962.tab (104 examples, 31837 attributes): old version = 33.5s, new version = 6.",
	"date" : "Mar 28, 2011"
}

    
    , {
    "uri": "/blog/2011/03/18/orange-has-been-accepted-into-gsoc-2011/",
	"title": "Orange has been accepted into GSoC 2011",
	"categories": ["gsoc"],
	"description": "",
	"content": "This year Orange has been accepted into the Google summer of Code program as a mentoring organization. It is one of 175 open-source organizations/projects/groups which will this year mentor students while they will be working on those accepted open source projects.\nWe have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year. Check out also official project page for Orange.\nGoogle Summer of Code is a Google-sponsored program where Google stipends students working for a summer job on an open source projects from all around the world. Student is paid $5000 (and a t-shirt!) for approximately two months of work/contribution to the project. More about the program is available on its homepage.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "This year Orange has been accepted into the Google summer of Code program as a mentoring organization. It is one of 175 open-source organizations/projects/groups which will this year mentor students while they will be working on those accepted open source projects.\nWe have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year." ,
	"author" : "BIOLAB",
	"summary" : "This year Orange has been accepted into the Google summer of Code program as a mentoring organization. It is one of 175 open-source organizations/projects/groups which will this year mentor students while they will be working on those accepted open source projects.\nWe have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year.",
	"date" : "Mar 18, 2011"
}

    
    , {
    "uri": "/blog/2011/02/11/biolab-retreat-februar-2011/",
	"title": "Biolab retreat Februar 2011",
	"categories": ["bohinj", "orange25", "retreat"],
	"description": "",
	"content": "From Wednesday, 2nd February 2011, to Saturday, 5th February 2011, we have been on working retreat at Lake Bohinj. The whole Bioinformatics Laboratory of the Faculty of Computer and Information technology has temporary moved to a nice house just few meters from the lake, enjoing the nature and without any distractions. Plan: working on the next version of Orange, Orange 2.5 and documentation rewrite. Orange 2.5 will have a better and restructured Python scripting interface along with great and shinny documentation.\nOverall summary of the retreat: first commit (revision 9743) by Marko on Wednesday, last commit (revision 10181) by Matija at 1:26:49 on Saturday. This gives 439 revisions made during the retreat.\nSome photos to give you a taste how it was.\n                                                                       ",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "From Wednesday, 2nd February 2011, to Saturday, 5th February 2011, we have been on working retreat at Lake Bohinj. The whole Bioinformatics Laboratory of the Faculty of Computer and Information technology has temporary moved to a nice house just few meters from the lake, enjoing the nature and without any distractions. Plan: working on the next version of Orange, Orange 2.5 and documentation rewrite. Orange 2.5 will have a better and restructured Python scripting interface along with great and shinny documentation." ,
	"author" : "BIOLAB",
	"summary" : "From Wednesday, 2nd February 2011, to Saturday, 5th February 2011, we have been on working retreat at Lake Bohinj. The whole Bioinformatics Laboratory of the Faculty of Computer and Information technology has temporary moved to a nice house just few meters from the lake, enjoing the nature and without any distractions. Plan: working on the next version of Orange, Orange 2.5 and documentation rewrite. Orange 2.5 will have a better and restructured Python scripting interface along with great and shinny documentation.",
	"date" : "Feb 11, 2011"
}

    
    , {
    "uri": "/blog/2010/03/04/debian-repository-lives/",
	"title": "Debian repository lives!",
	"categories": ["debian", "distribution", "download", "packaging"],
	"description": "",
	"content": "We have made still-experimental-but-probably-working Debian repository with daily built Orange packages. Currently without add-ons.\nTo get access to those packages just add those two lines to your /etc/apt/sources.list (this file contains a list of repositories with packages):\ndeb http://orange.biolab.si/debian lenny main deb-src http://orange.biolab.si/debian lenny main  And then you can install Orange with this command:\naptitude update aptitude install orange-svn  Packages are not signed as they are made automatically so you will probably be warned about this.\nThose packages will probably work also on other Debian-based Linux distributions like Ubuntu, but have not yet been tested there. Please test it and report how it goes.\nYou can also get source of those packages with this command:\napt-get source python-orange-svn  And then build package by yourself in extracted source directory with:\ndpkg-buildpackage  For example this will be useful on amd64 platform for which we currently do not yet provide binary packages. (Edit: now we provide binary packages also for amd64 platform.) But we will once we see how well this system works.\n",
	"image" : "<no value>",
	"thumbImage" : "<no value>",
	"shortExcerpt" : "<no value>",
	"longExcerpt" :  "We have made still-experimental-but-probably-working Debian repository with daily built Orange packages. Currently without add-ons.\nTo get access to those packages just add those two lines to your /etc/apt/sources.list (this file contains a list of repositories with packages):\ndeb http://orange.biolab.si/debian lenny main deb-src http://orange.biolab.si/debian lenny main  And then you can install Orange with this command:\naptitude update aptitude install orange-svn  Packages are not signed as they are made automatically so you will probably be warned about this." ,
	"author" : "BIOLAB",
	"summary" : "We have made still-experimental-but-probably-working Debian repository with daily built Orange packages. Currently without add-ons.\nTo get access to those packages just add those two lines to your /etc/apt/sources.list (this file contains a list of repositories with packages):\ndeb http://orange.biolab.si/debian lenny main deb-src http://orange.biolab.si/debian lenny main  And then you can install Orange with this command:\naptitude update aptitude install orange-svn  Packages are not signed as they are made automatically so you will probably be warned about this.",
	"date" : "Mar 4, 2010"
}

    
]

