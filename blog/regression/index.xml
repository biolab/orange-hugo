<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>regression on Orange</title>
    <link>/blog/regression/</link>
    <description>Recent content in regression on Orange</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Nov 2017 12:40:06 +0000</lastBuildDate>
    
	<atom:link href="/blog/regression/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Neural Network is Back!</title>
      <link>/blog/2017/11/03/neural-network-is-back/</link>
      <pubDate>Fri, 03 Nov 2017 12:40:06 +0000</pubDate>
      
      <guid>/blog/2017/11/03/neural-network-is-back/</guid>
      <description>We know you&amp;rsquo;ve missed it. We&amp;rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.
Neural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.
Do we make each layer a widget and then stack them?</description>
    </item>
    
    <item>
      <title>Model replaces Classify and Regression</title>
      <link>/blog/2017/04/07/model-replaces-classify-and-regression/</link>
      <pubDate>Fri, 07 Apr 2017 14:27:44 +0000</pubDate>
      
      <guid>/blog/2017/04/07/model-replaces-classify-and-regression/</guid>
      <description>Did you recently wonder where did Classification Tree go? Or what happened to Majority?
Orange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.
This, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing.</description>
    </item>
    
    <item>
      <title>The Beauty of Random Forest</title>
      <link>/blog/2016/12/22/the-beauty-of-random-forest/</link>
      <pubDate>Thu, 22 Dec 2016 08:55:36 +0000</pubDate>
      
      <guid>/blog/2016/12/22/the-beauty-of-random-forest/</guid>
      <description>It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange.</description>
    </item>
    
    <item>
      <title>Overfitting and Regularization</title>
      <link>/blog/2016/03/12/overfitting-and-regularization/</link>
      <pubDate>Sat, 12 Mar 2016 16:48:38 +0000</pubDate>
      
      <guid>/blog/2016/03/12/overfitting-and-regularization/</guid>
      <description>A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit.</description>
    </item>
    
    <item>
      <title>Model-Based Feature Scoring</title>
      <link>/blog/2015/12/19/model-based-feature-scoring/</link>
      <pubDate>Sat, 19 Dec 2015 19:27:51 +0000</pubDate>
      
      <guid>/blog/2015/12/19/model-based-feature-scoring/</guid>
      <description>Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression.</description>
    </item>
    
    <item>
      <title>A visit from the Tilburg University</title>
      <link>/blog/2015/10/02/a-visit-from-the-tilburg-university/</link>
      <pubDate>Fri, 02 Oct 2015 12:52:27 +0000</pubDate>
      
      <guid>/blog/2015/10/02/a-visit-from-the-tilburg-university/</guid>
      <description>Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it.</description>
    </item>
    
    <item>
      <title>New in Orange: Partial least squares regression</title>
      <link>/blog/2012/02/02/new-in-orange-partial-least-squares-regression/</link>
      <pubDate>Thu, 02 Feb 2012 16:21:00 +0000</pubDate>
      
      <guid>/blog/2012/02/02/new-in-orange-partial-least-squares-regression/</guid>
      <description>Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!
You can use PLS regression model on single-target or multi-target data sets.</description>
    </item>
    
    <item>
      <title>Earth - Multivariate adaptive regression splines</title>
      <link>/blog/2011/12/20/earth-multivariate-adaptive-regression-splines/</link>
      <pubDate>Tue, 20 Dec 2011 12:22:00 +0000</pubDate>
      
      <guid>/blog/2011/12/20/earth-multivariate-adaptive-regression-splines/</guid>
      <description>There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.
So lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):
import Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange.</description>
    </item>
    
    <item>
      <title>Faster classification and regression trees</title>
      <link>/blog/2011/08/24/faster-classification-and-regression-trees/</link>
      <pubDate>Wed, 24 Aug 2011 22:26:00 +0000</pubDate>
      
      <guid>/blog/2011/08/24/faster-classification-and-regression-trees/</guid>
      <description>SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.
The motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values.</description>
    </item>
    
  </channel>
</rss>