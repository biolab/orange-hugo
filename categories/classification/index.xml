<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>classification on Orange</title>
    <link>/categories/classification/</link>
    <description>Recent content in classification on Orange</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Oct 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/classification/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How to identify fake news with document embeddings</title>
      <link>/blog/2020/2020-10-15-document-embedders/</link>
      <pubDate>Thu, 15 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/blog/2020/2020-10-15-document-embedders/</guid>
      <description>Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText. In this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.</description>
    </item>
    
    <item>
      <title>Explaining Models: Workshop in Belgrade</title>
      <link>/blog/2019/2019-11-20-belgrade-workshop/</link>
      <pubDate>Wed, 20 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/blog/2019/2019-11-20-belgrade-workshop/</guid>
      <description>On Monday, Blaž and I held a technical tutorial Data Mining through Visual Programming and Interactive Analytics in Orange at this year&amp;rsquo;s edition of Data Science Conference in Belgrade, Serbia. The tutorial explained how to quickly prototype standard data mining and machine learning workflows in Orange and how to interactively explore clustering and classification models. The final part raised an interesting question that we&amp;rsquo;re going to explore in continuation.</description>
    </item>
    
    <item>
      <title>Data Mining Course at Higher School of Economics, Moscow</title>
      <link>/blog/2018/05/03/data-mining-course-at-higher-school-of-economics-moscow/</link>
      <pubDate>Thu, 03 May 2018 11:37:28 +0000</pubDate>
      
      <guid>/blog/2018/05/03/data-mining-course-at-higher-school-of-economics-moscow/</guid>
      <description>Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master&amp;rsquo;s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.
It was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best.</description>
    </item>
    
    <item>
      <title>Stack Everything!</title>
      <link>/blog/2018/01/05/stack-everything/</link>
      <pubDate>Fri, 05 Jan 2018 15:37:34 +0000</pubDate>
      
      <guid>/blog/2018/01/05/stack-everything/</guid>
      <description>We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.
Stacking enables you to combine several trained models into one meta model and use it in Test&amp;amp;Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let&amp;rsquo;s see an example.</description>
    </item>
    
    <item>
      <title>How to Properly Test Models</title>
      <link>/blog/2017/11/29/how-to-properly-test-models/</link>
      <pubDate>Wed, 29 Nov 2017 12:26:44 +0000</pubDate>
      
      <guid>/blog/2017/11/29/how-to-properly-test-models/</guid>
      <description>On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!
One thing we discussed was how to properly test your model. Ok, we know never to test on the same data you&amp;rsquo;ve built your model with, but even training and testing on separate data is sometimes not enough.</description>
    </item>
    
    <item>
      <title>Neural Network is Back!</title>
      <link>/blog/2017/11/03/neural-network-is-back/</link>
      <pubDate>Fri, 03 Nov 2017 12:40:06 +0000</pubDate>
      
      <guid>/blog/2017/11/03/neural-network-is-back/</guid>
      <description>We know you&amp;rsquo;ve missed it. We&amp;rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.
Neural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.
Do we make each layer a widget and then stack them?</description>
    </item>
    
    <item>
      <title>It&#39;s Sailing Time (Again)</title>
      <link>/blog/2017/08/11/its-sailing-time-again/</link>
      <pubDate>Fri, 11 Aug 2017 08:59:54 +0000</pubDate>
      
      <guid>/blog/2017/08/11/its-sailing-time-again/</guid>
      <description>Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A.</description>
    </item>
    
    <item>
      <title>Text Analysis Workshop at Digital Humanities 2017</title>
      <link>/blog/2017/08/08/text-analysis-workshop-at-digital-humanities-2017/</link>
      <pubDate>Tue, 08 Aug 2017 14:56:43 +0000</pubDate>
      
      <guid>/blog/2017/08/08/text-analysis-workshop-at-digital-humanities-2017/</guid>
      <description>How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?
It seems so, especially when Orange is involved.
Yesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers.</description>
    </item>
    
    <item>
      <title>Nomogram</title>
      <link>/blog/2017/06/05/nomogram/</link>
      <pubDate>Mon, 05 Jun 2017 11:39:12 +0000</pubDate>
      
      <guid>/blog/2017/06/05/nomogram/</guid>
      <description>One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly.</description>
    </item>
    
    <item>
      <title>Model replaces Classify and Regression</title>
      <link>/blog/2017/04/07/model-replaces-classify-and-regression/</link>
      <pubDate>Fri, 07 Apr 2017 14:27:44 +0000</pubDate>
      
      <guid>/blog/2017/04/07/model-replaces-classify-and-regression/</guid>
      <description>Did you recently wonder where did Classification Tree go? Or what happened to Majority?
Orange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.
This, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing.</description>
    </item>
    
    <item>
      <title>The Beauty of Random Forest</title>
      <link>/blog/2016/12/22/the-beauty-of-random-forest/</link>
      <pubDate>Thu, 22 Dec 2016 08:55:36 +0000</pubDate>
      
      <guid>/blog/2016/12/22/the-beauty-of-random-forest/</guid>
      <description>It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange.</description>
    </item>
    
    <item>
      <title>Data Mining for Political Scientists</title>
      <link>/blog/2016/11/30/data-mining-for-political-scientists/</link>
      <pubDate>Wed, 30 Nov 2016 08:24:53 +0000</pubDate>
      
      <guid>/blog/2016/11/30/data-mining-for-political-scientists/</guid>
      <description>Being a political scientist, I did not even hear about data mining before I&amp;rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!
But hold on! There are still many social scientists out there who haven&amp;rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning.</description>
    </item>
    
    <item>
      <title>Visualization of Classification Probabilities</title>
      <link>/blog/2016/08/16/polynomial-classification/</link>
      <pubDate>Tue, 16 Aug 2016 17:32:20 +0000</pubDate>
      
      <guid>/blog/2016/08/16/polynomial-classification/</guid>
      <description>This is a guest blog from the Google Summer of Code project.
Polynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.
Here is an example workflow. The data comes from the File widget.</description>
    </item>
    
    <item>
      <title>Rule Induction (Part I - Scripting)</title>
      <link>/blog/2016/08/05/rule-induction-part-i-scripting/</link>
      <pubDate>Fri, 05 Aug 2016 11:52:39 +0000</pubDate>
      
      <guid>/blog/2016/08/05/rule-induction-part-i-scripting/</guid>
      <description>This is a guest blog from the Google Summer of Code project.
We’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3.</description>
    </item>
    
    <item>
      <title>Pythagorean Trees and Forests</title>
      <link>/blog/2016/07/29/pythagorean-trees-and-forests/</link>
      <pubDate>Fri, 29 Jul 2016 11:52:54 +0000</pubDate>
      
      <guid>/blog/2016/07/29/pythagorean-trees-and-forests/</guid>
      <description>Classification Trees are great, but how about when they overgrow even your 27&amp;rsquo;&amp;rsquo; screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.
Pythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square.</description>
    </item>
    
    <item>
      <title>Rehaul of Text Mining Add-On</title>
      <link>/blog/2016/07/05/rehaul-of-text-mining-add-on/</link>
      <pubDate>Tue, 05 Jul 2016 06:51:55 +0000</pubDate>
      
      <guid>/blog/2016/07/05/rehaul-of-text-mining-add-on/</guid>
      <description>Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we&amp;rsquo;re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We&amp;rsquo;ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content.</description>
    </item>
    
    <item>
      <title>All I See is Silhouette</title>
      <link>/blog/2016/03/23/all-i-see-is-silhouette/</link>
      <pubDate>Wed, 23 Mar 2016 09:09:26 +0000</pubDate>
      
      <guid>/blog/2016/03/23/all-i-see-is-silhouette/</guid>
      <description>Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn&amp;rsquo;t wait to get it into Orange3. And now we did.
What this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster.</description>
    </item>
    
    <item>
      <title>Model-Based Feature Scoring</title>
      <link>/blog/2015/12/19/model-based-feature-scoring/</link>
      <pubDate>Sat, 19 Dec 2015 19:27:51 +0000</pubDate>
      
      <guid>/blog/2015/12/19/model-based-feature-scoring/</guid>
      <description>Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression.</description>
    </item>
    
    <item>
      <title>Learners in Python</title>
      <link>/blog/2015/10/16/learners-in-python/</link>
      <pubDate>Fri, 16 Oct 2015 08:32:43 +0000</pubDate>
      
      <guid>/blog/2015/10/16/learners-in-python/</guid>
      <description>We&amp;rsquo;ve already written about classifying instances in Python. However, it&amp;rsquo;s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.
TRAINING THE CLASSIFIER
We start with simply importing Orange module into Python and loading our data set.
&amp;gt;&amp;gt;&amp;gt;&amp;gt; import Orange &amp;gt;&amp;gt;&amp;gt;&amp;gt; data = Orange.data.Table(&amp;quot;titanic&amp;quot;)  We are using &amp;lsquo;titanic.tab&amp;rsquo; data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression).</description>
    </item>
    
    <item>
      <title>Classifying instances with Orange in Python</title>
      <link>/blog/2015/08/14/classifying-instances-with-orange-in-python/</link>
      <pubDate>Fri, 14 Aug 2015 12:31:57 +0000</pubDate>
      
      <guid>/blog/2015/08/14/classifying-instances-with-orange-in-python/</guid>
      <description>Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.
First we’re going to create a new data table with 10 fruits as our instances.
import Orange from Orange.data import * color = DiscreteVariable(&amp;quot;color&amp;quot;, values=[&amp;quot;orange&amp;quot;, &amp;quot;green&amp;quot;, &amp;quot;yellow&amp;quot;])calories = ContinuousVariable(&amp;quot;calories&amp;quot;) fiber = ContinuousVariable(&amp;quot;fiber&amp;quot;) fruit = DiscreteVariable(&amp;quot;fruit&amp;quot;, values=[&amp;quot;orange&amp;quot;, &amp;quot;apple&amp;quot;, &amp;quot;peach&amp;quot;]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [&amp;lt;/span&amp;gt; [&amp;quot;green&amp;quot;, 4, 1.</description>
    </item>
    
    <item>
      <title>Visualizing Misclassifications</title>
      <link>/blog/2015/07/24/visualizing-misclassifications/</link>
      <pubDate>Fri, 24 Jul 2015 08:03:40 +0000</pubDate>
      
      <guid>/blog/2015/07/24/visualizing-misclassifications/</guid>
      <description>In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.
In Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification.</description>
    </item>
    
    <item>
      <title>Learn with Paint Data</title>
      <link>/blog/2015/07/10/learn-with-paint-data/</link>
      <pubDate>Fri, 10 Jul 2015 07:00:54 +0000</pubDate>
      
      <guid>/blog/2015/07/10/learn-with-paint-data/</guid>
      <description>Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).
The widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph.</description>
    </item>
    
    <item>
      <title>Support vectors output in SVM widget</title>
      <link>/blog/2015/07/03/support-vectors-output-in-svm-widget/</link>
      <pubDate>Fri, 03 Jul 2015 06:46:50 +0000</pubDate>
      
      <guid>/blog/2015/07/03/support-vectors-output-in-svm-widget/</guid>
      <description>Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.
Ideally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance.</description>
    </item>
    
    <item>
      <title>Orange GSoC: Multi-Target Learning for Orange</title>
      <link>/blog/2012/04/30/orange-gsoc-multi-target-learning-for-orange/</link>
      <pubDate>Mon, 30 Apr 2012 12:00:00 +0000</pubDate>
      
      <guid>/blog/2012/04/30/orange-gsoc-multi-target-learning-for-orange/</guid>
      <description>Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year&amp;rsquo;s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange&amp;rsquo;s SimpleTreeLearner and will be integrated with Orange 3.</description>
    </item>
    
    <item>
      <title>Multi-label classification (and Multi-target prediction) in Orange</title>
      <link>/blog/2012/01/09/multi-label-classification-and-multi-target-prediction-in-orange/</link>
      <pubDate>Mon, 09 Jan 2012 12:41:00 +0000</pubDate>
      
      <guid>/blog/2012/01/09/multi-label-classification-and-multi-target-prediction-in-orange/</guid>
      <description>The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been &amp;ldquo;hidden&amp;rdquo; in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet &amp;hellip;) and added a more general support for multi-target prediction.
You can load multi-label tab-delimited data (e.</description>
    </item>
    
    <item>
      <title>GSoC Review: Multi-label Classification Implementation</title>
      <link>/blog/2011/09/02/gsoc-review-multi-label-classification-implementation/</link>
      <pubDate>Fri, 02 Sep 2011 05:47:00 +0000</pubDate>
      
      <guid>/blog/2011/09/02/gsoc-review-multi-label-classification-implementation/</guid>
      <description>Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| &amp;gt; 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| &amp;gt; 2, then it is called a multi-class classification problem (Tsoumakas &amp;amp; Katakis, 2007).
Multi-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification.</description>
    </item>
    
    <item>
      <title>Faster classification and regression trees</title>
      <link>/blog/2011/08/24/faster-classification-and-regression-trees/</link>
      <pubDate>Wed, 24 Aug 2011 22:26:00 +0000</pubDate>
      
      <guid>/blog/2011/08/24/faster-classification-and-regression-trees/</guid>
      <description>SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.
The motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values.</description>
    </item>
    
  </channel>
</rss>