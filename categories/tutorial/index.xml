<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tutorial on Orange</title>
    <link>/categories/tutorial/</link>
    <description>Recent content in tutorial on Orange</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 May 2018 08:19:34 +0000</lastBuildDate>
    
	<atom:link href="/categories/tutorial/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Clustering of Monet and Manet</title>
      <link>/blog/2018/05/09/clustering-of-monet-and-manet/</link>
      <pubDate>Wed, 09 May 2018 08:19:34 +0000</pubDate>
      
      <guid>/blog/2018/05/09/clustering-of-monet-and-manet/</guid>
      <description>Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet&amp;rsquo;s masterpiece is indeed very similar to Manet&amp;rsquo;s! Use Image Analytics add-on and play with it. Here&amp;rsquo;s how:   </description>
    </item>
    
    <item>
      <title>k-Means and Silhouette Score</title>
      <link>/blog/2017/03/17/k-means-silhouette-score/</link>
      <pubDate>Fri, 17 Mar 2017 17:05:25 +0000</pubDate>
      
      <guid>/blog/2017/03/17/k-means-silhouette-score/</guid>
      <description>k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.
But&amp;hellip; have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers.</description>
    </item>
    
    <item>
      <title>BDTN 2016 Workshop: Introduction to Data Science</title>
      <link>/blog/2016/12/16/bdtn-2016-workshop-introduction-to-data-science/</link>
      <pubDate>Fri, 16 Dec 2016 15:57:11 +0000</pubDate>
      
      <guid>/blog/2016/12/16/bdtn-2016-workshop-introduction-to-data-science/</guid>
      <description>Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.
Related: Intro to Data Mining for Life Scientists
The task was to teach and explain basic data mining concepts and techniques in four hours.</description>
    </item>
    
    <item>
      <title>Data Mining for Political Scientists</title>
      <link>/blog/2016/11/30/data-mining-for-political-scientists/</link>
      <pubDate>Wed, 30 Nov 2016 08:24:53 +0000</pubDate>
      
      <guid>/blog/2016/11/30/data-mining-for-political-scientists/</guid>
      <description>Being a political scientist, I did not even hear about data mining before I&amp;rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!
But hold on! There are still many social scientists out there who haven&amp;rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning.</description>
    </item>
    
    <item>
      <title>Intro to Data Mining for Life Scientists</title>
      <link>/blog/2016/10/02/intro-to-data-mining-for-life-scientists/</link>
      <pubDate>Sun, 02 Oct 2016 11:21:20 +0000</pubDate>
      
      <guid>/blog/2016/10/02/intro-to-data-mining-for-life-scientists/</guid>
      <description>RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.
We have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding.</description>
    </item>
    
    <item>
      <title>Data Mining Course in Houston #2</title>
      <link>/blog/2016/09/15/data-mining-in-houston-2/</link>
      <pubDate>Thu, 15 Sep 2016 13:38:30 +0000</pubDate>
      
      <guid>/blog/2016/09/15/data-mining-in-houston-2/</guid>
      <description>This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.
The course was designed for students with little or no experience in data science.</description>
    </item>
    
    <item>
      <title>Overfitting and Regularization</title>
      <link>/blog/2016/03/12/overfitting-and-regularization/</link>
      <pubDate>Sat, 12 Mar 2016 16:48:38 +0000</pubDate>
      
      <guid>/blog/2016/03/12/overfitting-and-regularization/</guid>
      <description>A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit.</description>
    </item>
    
    <item>
      <title>Getting Started Series: Part Two</title>
      <link>/blog/2016/02/26/getting-started-series-pt2/</link>
      <pubDate>Fri, 26 Feb 2016 10:35:33 +0000</pubDate>
      
      <guid>/blog/2016/02/26/getting-started-series-pt2/</guid>
      <description>We&amp;rsquo;ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.
The first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:
  while the second video introduces classification trees and predictive modelling:
  The seventh video in the series will address how to score classification and regression models by different evaluation methods.</description>
    </item>
    
    <item>
      <title>Orange YouTube Tutorials</title>
      <link>/blog/2016/01/04/orange-youtube-tutorials/</link>
      <pubDate>Mon, 04 Jan 2016 08:09:24 +0000</pubDate>
      
      <guid>/blog/2016/01/04/orange-youtube-tutorials/</guid>
      <description>It&amp;rsquo;s been a long time coming, but finally we&amp;rsquo;ve created out our first set of YouTube tutorials. In a series &amp;lsquo;Getting Started with Orange&amp;rsquo; we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy.</description>
    </item>
    
    <item>
      <title>Orange workshops around the world</title>
      <link>/blog/2015/06/19/435/</link>
      <pubDate>Fri, 19 Jun 2015 10:40:43 +0000</pubDate>
      
      <guid>/blog/2015/06/19/435/</guid>
      <description>Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion.</description>
    </item>
    
    <item>
      <title>New scripting tutorial</title>
      <link>/blog/2013/01/06/new-scripting-tutorial/</link>
      <pubDate>Sun, 06 Jan 2013 19:30:00 +0000</pubDate>
      
      <guid>/blog/2013/01/06/new-scripting-tutorial/</guid>
      <description>Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:
class SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name=&#39;small&#39;, m=5): self.</description>
    </item>
    
  </channel>
</rss>