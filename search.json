[{
    "uri": "/docs/programming/",
	"title": "Visual Programming",
	"description": "",
	"content": "Getting started YouTube tutorials Loading your data Widget catalog\n",
	"image" : "/images/orange_docs_01_small.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Getting started YouTube tutorials Loading your data Widget catalog" ,
	"author" : "",
	"summary" : "Getting started YouTube tutorials Loading your data Widget catalog",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/orange_docs_01_small.png",
	"kind" : "page",
	"type" : "docs",
	"LinkTitle" : "Visual Programming",
	"icon" : ""
},
{
    "uri": "/docs/development/",
	"title": "Development",
	"description": "",
	"content": "Widget development Example addon\n",
	"image" : "/images/orange_docs_02_small.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Widget development Example addon" ,
	"author" : "",
	"summary" : "Widget development Example addon",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/orange_docs_02_small.png",
	"kind" : "page",
	"type" : "docs",
	"LinkTitle" : "Development",
	"icon" : ""
},
{
    "uri": "/docs/python_library/",
	"title": "Python Library",
	"description": "",
	"content": "Tutorial Reference Orange 2.7 documentation\n",
	"image" : "/images/orange_docs_03_small.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Tutorial Reference Orange 2.7 documentation" ,
	"author" : "",
	"summary" : "Tutorial Reference Orange 2.7 documentation",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/orange_docs_03_small.png",
	"kind" : "page",
	"type" : "docs",
	"LinkTitle" : "Python Library",
	"icon" : ""
},
{
    "uri": "/getting-started/download_install/",
	"title": "Download and install",
	"description": "",
	"content": "Download Orange distribution package and run the installation file on your local computer. Follow installation guides for your operating system.\n",
	"image" : "/images/interactive-viz1.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Download Orange distribution package and run the installation file on your local computer. Follow installation guides for your operating system." ,
	"author" : "",
	"summary" : "Download Orange distribution package and run the installation file on your local computer. Follow installation guides for your operating system.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/interactive-viz1.png",
	"kind" : "page",
	"type" : "getting-started",
	"LinkTitle" : "Download and install",
	"icon" : ""
},
{
    "uri": "/screenshots/load_and_edit_your_data/",
	"title": "Load and edit your data in the File widget.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/file.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/file.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Load and edit your data in the File widget.",
	"icon" : ""
},
{
    "uri": "/workflows/template/",
	"title": "Workflow template",
	"description": "",
	"content": "Load the data from scOrange single cell database, see the expression values in a spreadsheet, and use Louvain clustering to find cell groups. Visualize cell landscape in a t-SNE projection and inspect the results of clustering. Select a subset of cells in t-SNE to examine their type or gene expression statistics in the Box Plot. Find enriched Gene Ontology terms, which hint on cell types.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Load the data from scOrange single cell database, see the expression values in a spreadsheet, and use Louvain clustering to find cell groups. Visualize cell landscape in a t-SNE projection and inspect the results of clustering. Select a subset of cells in t-SNE to examine their type or gene expression statistics in the Box Plot. Find enriched Gene Ontology terms, which hint on cell types." ,
	"author" : "",
	"summary" : "Load the data from scOrange single cell database, see the expression values in a spreadsheet, and use Louvain clustering to find cell groups. Visualize cell landscape in a t-SNE projection and inspect the results of clustering. Select a subset of cells in t-SNE to examine their type or gene expression statistics in the Box Plot. Find enriched Gene Ontology terms, which hint on cell types.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/clustering-and-tsne.png"],
	"image" : "",
	"kind" : "page",
	"type" : "test",
	"LinkTitle" : "Workflow template",
	"icon" : ""
},
{
    "uri": "/home/interactive_data_visualization/",
	"title": "Interactive Data Visualization",
	"description": "",
	"content": "Orange is all about data visualizations that help to uncover hidden data patterns, provide intuition behind data analysis procedures or support communication between data scientists and domain experts. Visualization widgets include scatter plot, box plot and histogram, and model-specific visualizations like dendrogram, silhouette plot, and tree visualizations, just to mention a few. Many other visualizations are available in add-ons and include visualizations of networks, word clouds, geographical maps, and more.\nWe take care to make Orange visualizations interactive: you can select data points from a scatter plot, a node in the tree, a branch in the dendrogram. Any such interaction will instruct visualization to send out a data subset that corresponds to the selected part of visualization. Consider the combination of a scatter plot and classification tree below. Scatter plot shows all the data, but highlights the data subset that corresponds to the selected node in the classification tree.\n Great Visualizations Orange includes many standard visualizations. Scatter plot is great for visualizing correlations between pair of attributes, box plot for displaying basic statistics, heat map to provide an overview across entire data set, and projection plots like MDS for plotting the multinomial data in two dimensions.\n Besides visualizations one would expect in a data mining suite, Orange includes some great extras that you may not find in other packages. These include widgets for silhouette plot to analyze the results of clustering, mosaic and Sieve diagram to discover feature interactions, and Pythagorean tree visualization for classification trees and forests.\n Exploratory Data Analysis Interactive visualizations enable exploratory data analysis. One can select interesting data subsets directly from plots, graphs and data tables and mine them in them downstream widgets. For example, select a cluster from the dendrogram of hierarchical clustering and map it to a 2D data presentation in the MDS plot. Or check their values of in the data table. Or observe the spread of its feature values in a box plot. Open all these windows at once and see how the changes in your selection affect other widgets. Or, for another example, cross-validate logistic regression on a data set and map some of the misclassifications to the two-dimensional projection. It is easy to turn Orange into a tool where domain experts can explore their data even if they lack insights in underlying statistics or machine learning.\n Intelligent Visualizations Sometimes there are just too many choices. Say, when data has many features, which feature pair should we visualize in a scatter plot to provide most information? Intelligent visualization comes to the rescue! In Orange’s scatter plot, this is called Score Plots. When class information is provided, Score Plots finds projections with best class separation. Consider brown-selected data set (comes with Orange) and its 79 features. There are 3,081 (79*78/2) different features pairs, way too many to check them manually, but there are only a few feature combinations that yield a great scatter plot. Score Plots finds them all, and allows us to browse through them.\n Reporting Finally, we can include the most important visualizations, statistics and information about the models into the report with a single click. Orange includes clever reporting where you can access workflow history for every widget and visualization directly from the report.\n ",
	"image" : "/images/interactive_data_vis_01_scaled.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange is all about data visualizations that help to uncover hidden data patterns, provide intuition behind data analysis procedures or support communication between data scientists and domain experts. Visualization widgets include scatter plot, box plot and histogram, and model-specific visualizations like dendrogram, silhouette plot, and tree visualizations, just to mention a few. Many other visualizations are available in add-ons and include visualizations of networks, word clouds, geographical maps, and more." ,
	"author" : "",
	"summary" : "Orange is all about data visualizations that help to uncover hidden data patterns, provide intuition behind data analysis procedures or support communication between data scientists and domain experts. Visualization widgets include scatter plot, box plot and histogram, and model-specific visualizations like dendrogram, silhouette plot, and tree visualizations, just to mention a few. Many other visualizations are available in add-ons and include visualizations of networks, word clouds, geographical maps, and more.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/interactive_data_vis_01_scaled.png",
	"kind" : "page",
	"type" : "home",
	"LinkTitle" : "Interactive Data Visualization",
	"icon" : ""
},
{
    "uri": "/screenshots/paint_a_two_dimensional/",
	"title": "Paint a two-dimensional data set.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/paint-data.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/paint-data.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Paint a two-dimensional data set.",
	"icon" : ""
},
{
    "uri": "/getting-started/youtube_tutorials/",
	"title": "YouTube tutorials",
	"description": "",
	"content": "Introduction to the Orange data mining software. Learn about the development of Orange workflows, data loading, basic machine learning algorithms and interactive visualizations. Video tutorials are available in button below.\n",
	"image" : "/images/youtube_tutorials.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Introduction to the Orange data mining software. Learn about the development of Orange workflows, data loading, basic machine learning algorithms and interactive visualizations. Video tutorials are available in button below." ,
	"author" : "",
	"summary" : "Introduction to the Orange data mining software. Learn about the development of Orange workflows, data loading, basic machine learning algorithms and interactive visualizations. Video tutorials are available in button below.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/youtube_tutorials.png",
	"kind" : "page",
	"type" : "getting-started",
	"LinkTitle" : "YouTube tutorials",
	"icon" : ""
},
{
    "uri": "/screenshots/data_selection_in_scatter_plot/",
	"title": "Data selection in Scatter Plot is visualised in a Box Plot.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/explorative-analysis.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/explorative-analysis.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Data selection in Scatter Plot is visualised in a Box Plot.",
	"icon" : ""
},
{
    "uri": "/home/visual-_programming/",
	"title": "Visual Programming",
	"description": "",
	"content": "Orange is a great data mining tool for beginners as well as for expert data scientists. Thanks to its user interface users can focus on data analysis instead on laborious coding, making a construction of complex data analytics pipelines simple.\nComponent-Based Data Mining In Orange, data analysis is done by stacking components into workflows. Each component, called a widget, embeds some data retrieval, preprocessing, visualization, modeling or evaluation task. Combining different widgets in a workflow enables you to build comprehensive data analysis schemas as you go. With a large library of widgets you won’t be short for choice. Additional widgets are available through add-ons and allow for a more focused and topic-oriented research.\n Interactive Data Exploration Orange widgets communicate with each other. They receive data on the input and send out filtered or processed data, models, or anything the widget does on the output. Say, start with a File widget that reads the data and connect its output to another widget, say, a Data Table, and you have a functioning workflow. Alter any change in one widget, the changes are instantaneously propagated through the downstream workflow. Changing a data file in the File widget will trigger the response in all downstream widgets. This is especially fun if the widgets are open and when you can immediately see the results of any changes in that data, parameters of the methods or selections in interactive visualizations. For example, in a simple workflow below, where selection of the data in the spreadsheet propagates to a scatter plot, which marks the selected data instances. \\\n \\\nThrough the choice of the right widgets and their connections, it is easy to construct complex workflows for a broad variety of data analysis tasks.\nClever Workflow Design Interface Orange is easy to use even for complete novices. Start with the File widget and Orange will automatically suggest the next widgets that can be connected to it. For example, Orange knows you are likely to want Hierarchical Clustering after you’ve set up your Distances widget. All other defaults in the widgets are also set in a way that enables a simple analysis even without knowing a whole lot about statistics, machine learning, or exploratory data mining in general.\n ",
	"image" : "/images/visual_programming_01_scaled.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange is a great data mining tool for beginners as well as for expert data scientists. Thanks to its user interface users can focus on data analysis instead on laborious coding, making a construction of complex data analytics pipelines simple.\nComponent-Based Data Mining In Orange, data analysis is done by stacking components into workflows. Each component, called a widget, embeds some data retrieval, preprocessing, visualization, modeling or evaluation task. Combining different widgets in a workflow enables you to build comprehensive data analysis schemas as you go." ,
	"author" : "",
	"summary" : "Orange is a great data mining tool for beginners as well as for expert data scientists. Thanks to its user interface users can focus on data analysis instead on laborious coding, making a construction of complex data analytics pipelines simple.\nComponent-Based Data Mining In Orange, data analysis is done by stacking components into workflows. Each component, called a widget, embeds some data retrieval, preprocessing, visualization, modeling or evaluation task. Combining different widgets in a workflow enables you to build comprehensive data analysis schemas as you go.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/visual_programming_01_scaled.png",
	"kind" : "page",
	"type" : "home",
	"LinkTitle" : "Visual Programming",
	"icon" : ""
},
{
    "uri": "/getting-started/workflow_examples/",
	"title": "Workflow examples",
	"description": "",
	"content": "Software Orange includes a wide array of workflow templates designed to help you get familiar with the application. Pick Templates on the Welcome screen to explore.\n",
	"image" : "/images/template_workflow.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Software Orange includes a wide array of workflow templates designed to help you get familiar with the application. Pick Templates on the Welcome screen to explore." ,
	"author" : "",
	"summary" : "Software Orange includes a wide array of workflow templates designed to help you get familiar with the application. Pick Templates on the Welcome screen to explore.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/template_workflow.png",
	"kind" : "page",
	"type" : "getting-started",
	"LinkTitle" : "Workflow examples",
	"icon" : ""
},
{
    "uri": "/screenshots/orange_can_suggest_which/",
	"title": "Orange can suggest which widget to add to the workflow.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/suggestion.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/suggestion.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Orange can suggest which widget to add to the workflow.",
	"icon" : ""
},
{
    "uri": "/home/teachers_and_students_love_it/",
	"title": "Teachers and Students Love It",
	"description": "",
	"content": "Education in Data Science Orange is the perfect tool for hands-on training. Teachers enjoy the clear program design and the visual explorations of data and models. Students benefit from the flexibility of the tool and the power to invent new combinations of data mining methods. The educational strength of Orange comes from the combination of visual programming and interactive visualizations. We have also designed some educational widgets that have been explicitly created to support teaching.\nHere are a few example workflows that we have used recently in data mining training (yes, we do not only develop Orange, we teach with it as well).\nLinear Regression Wouldn’t be great if we could just paint data and with each new data point observe how linear regression fits the line? In Orange, there’s a widget for data painting and a polynomial regression widget (from the educational add-on) to display the fitted model.\n Overfitting Not everything is a line. We can use linear regression on augmented data input with added columns for powers of input features. This is called polynomial regression. It is guaranteed to surprise students. Using a linear model you can now discover non-linear functions. But you can also heavily overfit the training data. For say two, three … or ten input data points, what is the degree of polynomial expansion for the linear model to perfectly fit the data? When linear models overfit, model coefficients become very high. It is so easy to play with this in Orange: add some data here, raise or lower the degree of polynomial there…\n Regularization If overfitting leads to the explosion of the values of the coefficients, it could easily be prevented by requesting the optimization to keep these low. That is exactly the idea behind regularization. In the workflow below, Polynomial Regression was given a regularized model. No more overfitting! It is also great to explore how the strength of regularization smooths the resulting model and reduces the values of coefficients.\n Always Evaluate Models on the Test Data The following workflow is a bit more complex. We split the painted data to a training and test set. This time, as we use standard Orange widgets for learning, we need to declare that y is a class attribute. We do this in the Select Columns widget. We then evaluate the linear regression model with polynomial expansion on the training data and on the separate test data.\n Overfitted models have small errors on training data, but large errors on test data. To escape this, regularization helps. With it, the error on the test data is lower, while the error on the training data increases. Huh! The error on training data set is thus not a good indicator of the predictive power of the model. While this looks simple, everything in machine learning is about how to design models that will work well on the data they have not seen in training.\n When teaching, the workflow presented here needs quite some thought and time. It should come after we explain linear regression, polynomial expansion, overfitting and regularization. But it gives so much freedom for students to explore: consider the interplay of different complexity of (painted) data set, degrees of polynomial expansion, and the effects of regularization. Plus, it provides us (teachers and trainers) the opportunity to talk about regression scoring and nicely leads to the introduction of cross-validation. Oh, the richness and art of data mining…\nExperimenting with k-Means Clustering After the intro on k-means clustering algorithms (there is a widget from educational add on to support this), a great exercise for students is to check when the algorithm works and where it fails. Painting the data helps again! Say, for the smiley data set, k-means guided by silhouette scoring finds four clusters instead of three. Orange widgets can be set to automatic commit (Send Automatically) so that every time we change the input data, the signals propagate through the workflow for the users to immediately see the consequences of the changes. Can you paint a smiley data set where even clustering with k=3 would fail?\n Scoring of Clustering Models We did mention the clustering silhouette, right? It is the easiest approach to score the clustering. Silhouettes are estimated on data instances, and the silhouette of a clustering is the mean across data instance silhouettes. A high silhouette means that a data instance is surrounded by instances from the same cluster, while a low silhouette score indicates that data instances are close to another cluster. Orange has a widget that can plot the silhouette scores. And because Orange is all about interactive visualization, you can select silhouettes and check where their data instances are. Like in the workflow below, where we showcase that low silhouettes are assigned to borderline data instances. Silhouette Plot is great when explaining pros and cons of different clustering methods (yes, it works with any method, not just k-means).\n ",
	"image" : "/images/iadv_data_mining_02_scaled.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Education in Data Science Orange is the perfect tool for hands-on training. Teachers enjoy the clear program design and the visual explorations of data and models. Students benefit from the flexibility of the tool and the power to invent new combinations of data mining methods. The educational strength of Orange comes from the combination of visual programming and interactive visualizations. We have also designed some educational widgets that have been explicitly created to support teaching." ,
	"author" : "",
	"summary" : "Education in Data Science Orange is the perfect tool for hands-on training. Teachers enjoy the clear program design and the visual explorations of data and models. Students benefit from the flexibility of the tool and the power to invent new combinations of data mining methods. The educational strength of Orange comes from the combination of visual programming and interactive visualizations. We have also designed some educational widgets that have been explicitly created to support teaching.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/iadv_data_mining_02_scaled.png",
	"kind" : "page",
	"type" : "home",
	"LinkTitle" : "Teachers and Students Love It",
	"icon" : ""
},
{
    "uri": "/getting-started/widget_catalog/",
	"title": "Widget catalog",
	"description": "",
	"content": "Orange widgets are building blocks of data analysis workflows that are assembled in Orange’s visual programming environment. Widgets are grouped into classes according to their function. A typical workflow may mix widgets for data input and filtering, visualization, and predictive data mining. Here you can get list of all widgets available in Orange.\n",
	"image" : "/images/markers-tsne.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange widgets are building blocks of data analysis workflows that are assembled in Orange’s visual programming environment. Widgets are grouped into classes according to their function. A typical workflow may mix widgets for data input and filtering, visualization, and predictive data mining. Here you can get list of all widgets available in Orange." ,
	"author" : "",
	"summary" : "Orange widgets are building blocks of data analysis workflows that are assembled in Orange’s visual programming environment. Widgets are grouped into classes according to their function. A typical workflow may mix widgets for data input and filtering, visualization, and predictive data mining. Here you can get list of all widgets available in Orange.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/markers-tsne.png",
	"kind" : "page",
	"type" : "getting-started",
	"LinkTitle" : "Widget catalog",
	"icon" : ""
},
{
    "uri": "/home/add_ons_extend_functionality/",
	"title": "Add-ons Extend Functionality",
	"description": "",
	"content": "",
	"image" : "/images/add_ons_scaled.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/add_ons_scaled.png",
	"kind" : "page",
	"type" : "home",
	"LinkTitle" : "Add-ons Extend Functionality",
	"icon" : ""
},
{
    "uri": "/screenshots/join_two_data_sets/",
	"title": "Join two data sets.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/concatenate.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/concatenate.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Join two data sets.",
	"icon" : ""
},
{
    "uri": "/screenshots/box_plot_displays_basic_statistics/",
	"title": "Box plot displays basic statistics of attributes.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/box-plot.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/box-plot.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Box plot displays basic statistics of attributes.",
	"icon" : ""
},
{
    "uri": "/screenshots/sieve_diagram_on_titanic_data_set/",
	"title": "Sieve diagram on Titanic data set.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/sieve-diagram.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/sieve-diagram.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Sieve diagram on Titanic data set.",
	"icon" : ""
},
{
    "uri": "/screenshots/heatmap_visualisation/",
	"title": "Heatmap visualisation.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/heat-map.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/heat-map.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Heatmap visualisation.",
	"icon" : ""
},
{
    "uri": "/screenshots/explorative_analysis_with_classification/",
	"title": "Explorative analysis with classification trees.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/tree-explorative.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/tree-explorative.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Explorative analysis with classification trees.",
	"icon" : ""
},
{
    "uri": "/screenshots/data_can_contain_references_to_images/",
	"title": "Data can contain references to images.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/image-viewer.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/image-viewer.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Data can contain references to images.",
	"icon" : ""
},
{
    "uri": "/screenshots/hierarchial_clustering_supports_interactive/",
	"title": "Hierarchial clustering supports interactive cluster selection.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/hierarchical-clustering.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/hierarchical-clustering.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Hierarchial clustering supports interactive cluster selection.",
	"icon" : ""
},
{
    "uri": "/workflows/file-and-data-table-widget/",
	"title": "File and Data Table",
	"description": "",
	"content": "The basic data mining units in Orange are called widgets. In this workflow, the File widget reads the data. File widget communicates this data to Data Table widget that shows the data in a spreadsheet. The output of File is connected to the input of Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The basic data mining units in Orange are called widgets. In this workflow, the File widget reads the data. File widget communicates this data to Data Table widget that shows the data in a spreadsheet. The output of File is connected to the input of Data Table." ,
	"author" : "",
	"summary" : "The basic data mining units in Orange are called widgets. In this workflow, the File widget reads the data. File widget communicates this data to Data Table widget that shows the data in a spreadsheet. The output of File is connected to the input of Data Table.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/file-and-data-table-widget.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "File and Data Table",
	"icon" : ""
},
{
    "uri": "/workflows/scatterplot-data-table/",
	"title": "Interactive Visualizations",
	"description": "",
	"content": "Most visualizations in Orange are interactive. Scatter Plot for example. Double click its icon to open it and click-and-drag to select a few data points from the plot. Selected data will automatically propagate to Data Table. Double click it to check which data was selected. Change selection and observe the change in the Data Table. This works best if both widgets are open.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Most visualizations in Orange are interactive. Scatter Plot for example. Double click its icon to open it and click-and-drag to select a few data points from the plot. Selected data will automatically propagate to Data Table. Double click it to check which data was selected. Change selection and observe the change in the Data Table. This works best if both widgets are open." ,
	"author" : "",
	"summary" : "Most visualizations in Orange are interactive. Scatter Plot for example. Double click its icon to open it and click-and-drag to select a few data points from the plot. Selected data will automatically propagate to Data Table. Double click it to check which data was selected. Change selection and observe the change in the Data Table. This works best if both widgets are open.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/scatterplot-data-table.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Interactive Visualizations",
	"icon" : ""
},
{
    "uri": "/screenshots/playing_with_paint_data_and/",
	"title": "Playing with Paint Data and an automatic selection of clusters in k-Means.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/k-means.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/k-means.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Playing with Paint Data and an automatic selection of clusters in k-Means.",
	"icon" : ""
},
{
    "uri": "/screenshots/multidimensional_scaling_of_zoo_data_set_reveals/",
	"title": "Multidimensional scaling of Zoo data set reveals phylogeny groups.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/mds.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/mds.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Multidimensional scaling of Zoo data set reveals phylogeny groups.",
	"icon" : ""
},
{
    "uri": "/workflows/data-subsets/",
	"title": "Visalization of Data Subsets",
	"description": "",
	"content": "Some visualization widget, like Scatter Plot and several data projection widgets, can expose the data instances in the data subset. In this workflow, Scatter Plot visualizes the data from the input data file, but also marks the data points that have been selected in the Data Table (selected rows).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Some visualization widget, like Scatter Plot and several data projection widgets, can expose the data instances in the data subset. In this workflow, Scatter Plot visualizes the data from the input data file, but also marks the data points that have been selected in the Data Table (selected rows)." ,
	"author" : "",
	"summary" : "Some visualization widget, like Scatter Plot and several data projection widgets, can expose the data instances in the data subset. In this workflow, Scatter Plot visualizes the data from the input data file, but also marks the data points that have been selected in the Data Table (selected rows).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/scatterplot-visualize-subset.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Visalization of Data Subsets",
	"icon" : ""
},
{
    "uri": "/workflows/pivot-table/",
	"title": "Pivot Table",
	"description": "",
	"content": "Pivot Table can help us aggregate and transform the data. This workflow takes Kickstarter projects and aggregates them by month. We can inspect the frequency of the published projects per month and observe the difference between funded and non-funded projects. Try constructing several tables with pivot and experiment with different aggregation methods.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Pivot Table can help us aggregate and transform the data. This workflow takes Kickstarter projects and aggregates them by month. We can inspect the frequency of the published projects per month and observe the difference between funded and non-funded projects. Try constructing several tables with pivot and experiment with different aggregation methods." ,
	"author" : "",
	"summary" : "Pivot Table can help us aggregate and transform the data. This workflow takes Kickstarter projects and aggregates them by month. We can inspect the frequency of the published projects per month and observe the difference between funded and non-funded projects. Try constructing several tables with pivot and experiment with different aggregation methods.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/pivot-table.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Pivot Table",
	"icon" : ""
},
{
    "uri": "/screenshots/principal_component_analysis_with_scree_diagram/",
	"title": "Principal component analysis with scree diagram.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/pca.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/pca.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Principal component analysis with scree diagram.",
	"icon" : ""
},
{
    "uri": "/screenshots/roc/",
	"title": "Receiver operating characteristics (ROC) analysis.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/roc.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/roc.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Receiver operating characteristics (ROC) analysis.",
	"icon" : ""
},
{
    "uri": "/screenshots/cross_validated_calibration_plot/",
	"title": "Cross-validated calibration plot.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/calibration-plot.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/calibration-plot.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Cross-validated calibration plot.",
	"icon" : ""
},
{
    "uri": "/screenshots/data_preprocessing_embedded_within/",
	"title": "Data preprocessing embedded within a learning algorithm.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/pca-evaluation.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/pca-evaluation.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Data preprocessing embedded within a learning algorithm.",
	"icon" : ""
},
{
    "uri": "/screenshots/feature_scoring_for_finding_interesting/",
	"title": "Feature scoring for finding interesting data projections.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/rank.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/rank.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Feature scoring for finding interesting data projections.",
	"icon" : ""
},
{
    "uri": "/screenshots/model_based_feature_scoring/",
	"title": "Model-based feature scoring.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/model-based-scoring.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/model-based-scoring.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Model-based feature scoring.",
	"icon" : ""
},
{
    "uri": "/screenshots/cross_validation_and_scoring_of_classifiers/",
	"title": "Cross-validated calibration plot.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/evaluation.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/evaluation.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Cross-validated calibration plot.",
	"icon" : ""
},
{
    "uri": "/screenshots/visualizing_misclassifications/",
	"title": "Visualizing misclassifications.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/misclassifications.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/misclassifications.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Visualizing misclassifications.",
	"icon" : ""
},
{
    "uri": "/screenshots/finding_common_misclassifications_of_three/",
	"title": "Finding common misclassifications of three predictive models.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/venn-misclassified.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/venn-misclassified.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Finding common misclassifications of three predictive models.",
	"icon" : ""
},
{
    "uri": "/screenshots/model_testing_and_scoring_on_a_separate_test_data_set/",
	"title": "Model testing and scoring on a separate test data set.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/test-train.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/test-train.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Model testing and scoring on a separate test data set.",
	"icon" : ""
},
{
    "uri": "/screenshots/intersection_of_misclassified_data_and_data/",
	"title": "Intersection of misclassified data and data with low silhouette score.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/silhouette.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/silhouette.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Intersection of misclassified data and data with low silhouette score.",
	"icon" : ""
},
{
    "uri": "/workflows/tree-scatterplot/",
	"title": "Classification Tree",
	"description": "",
	"content": "This workflow combines the interface and visualization of classification trees with scatter plot. When both the tree viewer and the scatter plot are open, selection of any node of the tree sends the related data instances to scatter plot. In the workflow, the selected data is treated as a subset of the entire dataset and is highlighted in the scatter plot. With simple combination of widgets we have constructed an interactive classification tree browser.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This workflow combines the interface and visualization of classification trees with scatter plot. When both the tree viewer and the scatter plot are open, selection of any node of the tree sends the related data instances to scatter plot. In the workflow, the selected data is treated as a subset of the entire dataset and is highlighted in the scatter plot. With simple combination of widgets we have constructed an interactive classification tree browser." ,
	"author" : "",
	"summary" : "This workflow combines the interface and visualization of classification trees with scatter plot. When both the tree viewer and the scatter plot are open, selection of any node of the tree sends the related data instances to scatter plot. In the workflow, the selected data is treated as a subset of the entire dataset and is highlighted in the scatter plot. With simple combination of widgets we have constructed an interactive classification tree browser.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/tree-scatterplot.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Classification Tree",
	"icon" : ""
},
{
    "uri": "/screenshots/cn2_rule_induction/",
	"title": "CN2 rule induction.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/cn2.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/cn2.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "CN2 rule induction.",
	"icon" : ""
},
{
    "uri": "/workflows/outliers/",
	"title": "Inspecting Outliers with Silhouette",
	"description": "",
	"content": "Silhouette Plot shows how ‘well-centered’ each data instance is with respect to its cluster or class label. In this workflow we use iris’ class labels to observe which flowers are typical representatives of their class and which are the outliers. Select instances left of zero in the plot and observe which flowers are these. Try connecting the selection with the Scatter Plot to highlight the outliers.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Silhouette Plot shows how \u0026lsquo;well-centered\u0026rsquo; each data instance is with respect to its cluster or class label. In this workflow we use iris\u0026rsquo; class labels to observe which flowers are typical representatives of their class and which are the outliers. Select instances left of zero in the plot and observe which flowers are these. Try connecting the selection with the Scatter Plot to highlight the outliers." ,
	"author" : "",
	"summary" : "Silhouette Plot shows how \u0026lsquo;well-centered\u0026rsquo; each data instance is with respect to its cluster or class label. In this workflow we use iris\u0026rsquo; class labels to observe which flowers are typical representatives of their class and which are the outliers. Select instances left of zero in the plot and observe which flowers are these. Try connecting the selection with the Scatter Plot to highlight the outliers.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/silhouette.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Inspecting Outliers with Silhouette",
	"icon" : ""
},
{
    "uri": "/screenshots/showcase_for_approximation_by_regression_tree/",
	"title": "Showcase for approximation by regression tree.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/polynomial-regression.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/polynomial-regression.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Showcase for approximation by regression tree.",
	"icon" : ""
},
{
    "uri": "/screenshots/interactive_gradient_descent/",
	"title": "Interactive gradient descent.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/gradient-descent.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/gradient-descent.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Interactive gradient descent.",
	"icon" : ""
},
{
    "uri": "/screenshots/predicting_text_categories/",
	"title": "Interactive gradient descent.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/text-predictions.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/text-predictions.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Interactive gradient descent.",
	"icon" : ""
},
{
    "uri": "/screenshots/topic_modelling_of_recent_tweets/",
	"title": "Topic modelling of recent tweets.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/text-topics.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/text-topics.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Topic modelling of recent tweets.",
	"icon" : ""
},
{
    "uri": "/screenshots/image_analytics_with_deep_network_embedding/",
	"title": "Image analytics with deep-network embedding.",
	"description": "",
	"content": "",
	"image" : "/screenshots/images/image-analytics.png",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/screenshots/images/image-analytics.png",
	"kind" : "page",
	"type" : "screenshots",
	"LinkTitle" : "Image analytics with deep-network embedding.",
	"icon" : ""
},
{
    "uri": "/workflows/principal-component-analysis/",
	"title": "Principal Component Analysis",
	"description": "",
	"content": "PCA transforms the data into a dataset with uncorrelated variables, also called principal components. PCA widget displays a graph (scree diagram) showing a degree of explained variance by best principal components and allows to interactively set the number of components to be included in the output dataset. In this workflow, we can observe the transformation in the Data Table and in Scatter Plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "PCA transforms the data into a dataset with uncorrelated variables, also called principal components. PCA widget displays a graph (scree diagram) showing a degree of explained variance by best principal components and allows to interactively set the number of components to be included in the output dataset. In this workflow, we can observe the transformation in the Data Table and in Scatter Plot." ,
	"author" : "",
	"summary" : "PCA transforms the data into a dataset with uncorrelated variables, also called principal components. PCA widget displays a graph (scree diagram) showing a degree of explained variance by best principal components and allows to interactively set the number of components to be included in the output dataset. In this workflow, we can observe the transformation in the Data Table and in Scatter Plot.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/pca.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Principal Component Analysis",
	"icon" : ""
},
{
    "uri": "/workflows/hierarchical-clustering/",
	"title": "Hierarchical Clustering",
	"description": "",
	"content": "The workflow clusters the data items in iris dataset by first examining the distances between data instances. Distance matrix is passed to Hierarchical Clustering, which renders the dendrogram. Select different parts of the dendrogram to further analyze the corresponding data.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The workflow clusters the data items in iris dataset by first examining the distances between data instances. Distance matrix is passed to Hierarchical Clustering, which renders the dendrogram. Select different parts of the dendrogram to further analyze the corresponding data." ,
	"author" : "",
	"summary" : "The workflow clusters the data items in iris dataset by first examining the distances between data instances. Distance matrix is passed to Hierarchical Clustering, which renders the dendrogram. Select different parts of the dendrogram to further analyze the corresponding data.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/hierarchical-clustering.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Hierarchical Clustering",
	"icon" : ""
},
{
    "uri": "/workflows/cluster-inspection/",
	"title": "Cluster Inspection",
	"description": "",
	"content": "We use the zoo data set in combination with Hierarchical Clustering to discover groups of animals. Now that we have the clusters we want to find out what is significant for each cluster! Pass the clusters to Box Plot and use ‘Order by relevance’ to discover what defines a cluster. Seems like they are well-separated by the type, even though the clustering was unaware of the class label!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We use the zoo data set in combination with Hierarchical Clustering to discover groups of animals. Now that we have the clusters we want to find out what is significant for each cluster! Pass the clusters to Box Plot and use \u0026lsquo;Order by relevance\u0026rsquo; to discover what defines a cluster. Seems like they are well-separated by the type, even though the clustering was unaware of the class label!" ,
	"author" : "",
	"summary" : "We use the zoo data set in combination with Hierarchical Clustering to discover groups of animals. Now that we have the clusters we want to find out what is significant for each cluster! Pass the clusters to Box Plot and use \u0026lsquo;Order by relevance\u0026rsquo; to discover what defines a cluster. Seems like they are well-separated by the type, even though the clustering was unaware of the class label!",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/cluster-inspection.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Cluster Inspection",
	"icon" : ""
},
{
    "uri": "/workflows/feature-ranking/",
	"title": "Feature Ranking",
	"description": "",
	"content": "For supervised problems, where data instances are annotated with class labels, we would like to know which are the most informative features. Rank widget provides a table of features and their informativity scores, and supports manual feature selection. In the workflow, we used it to find the best two features (of initial 79 from brown-selected dataset) and display its scatter plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "For supervised problems, where data instances are annotated with class labels, we would like to know which are the most informative features. Rank widget provides a table of features and their informativity scores, and supports manual feature selection. In the workflow, we used it to find the best two features (of initial 79 from brown-selected dataset) and display its scatter plot." ,
	"author" : "",
	"summary" : "For supervised problems, where data instances are annotated with class labels, we would like to know which are the most informative features. Rank widget provides a table of features and their informativity scores, and supports manual feature selection. In the workflow, we used it to find the best two features (of initial 79 from brown-selected dataset) and display its scatter plot.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/feature-ranking.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Feature Ranking",
	"icon" : ""
},
{
    "uri": "/workflows/data-sampler/",
	"title": "Train and Test Data",
	"description": "",
	"content": "In building predictive models it is important to have a separate train and test data sets in order to avoid overfitting and to properly score the models. Here we use Data Sampler to split the data into training and test data, use training data for building a model and, finally, test on test data. Try several other classifiers to see how the scores change.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "In building predictive models it is important to have a separate train and test data sets in order to avoid overfitting and to properly score the models. Here we use Data Sampler to split the data into training and test data, use training data for building a model and, finally, test on test data. Try several other classifiers to see how the scores change." ,
	"author" : "",
	"summary" : "In building predictive models it is important to have a separate train and test data sets in order to avoid overfitting and to properly score the models. Here we use Data Sampler to split the data into training and test data, use training data for building a model and, finally, test on test data. Try several other classifiers to see how the scores change.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/data-sampler.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Train and Test Data",
	"icon" : ""
},
{
    "uri": "/workflows/cross-validation/",
	"title": "Cross Validation",
	"description": "",
	"content": "How good are supervised data mining methods on your classification dataset? Here’s a workflow that scores various classification techniques on a dataset from medicine. The central widget here is the one for testing and scoring, which is given the data and a set of learners, does cross-validation and scores predictive accuracy, and outputs the scores for further examination.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "How good are supervised data mining methods on your classification dataset? Here\u0026rsquo;s a workflow that scores various classification techniques on a dataset from medicine. The central widget here is the one for testing and scoring, which is given the data and a set of learners, does cross-validation and scores predictive accuracy, and outputs the scores for further examination." ,
	"author" : "",
	"summary" : "How good are supervised data mining methods on your classification dataset? Here\u0026rsquo;s a workflow that scores various classification techniques on a dataset from medicine. The central widget here is the one for testing and scoring, which is given the data and a set of learners, does cross-validation and scores predictive accuracy, and outputs the scores for further examination.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/cross-validation.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Cross Validation",
	"icon" : ""
},
{
    "uri": "/workflows/where-are-misclassifications/",
	"title": "Where Are Misclassifications",
	"description": "",
	"content": "Cross-validation of, say, logistic regression can expose the data instances which were misclassified. There are six such instances for iris dataset and ridge-regularized logistic regression. We can select different types of misclassification in Confusion Matrix and highlight them in the Scatter Plot. No surprise: the misclassified instances are close to the class-bordering regions in the scatter plot projection.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Cross-validation of, say, logistic regression can expose the data instances which were misclassified. There are six such instances for iris dataset and ridge-regularized logistic regression. We can select different types of misclassification in Confusion Matrix and highlight them in the Scatter Plot. No surprise: the misclassified instances are close to the class-bordering regions in the scatter plot projection." ,
	"author" : "",
	"summary" : "Cross-validation of, say, logistic regression can expose the data instances which were misclassified. There are six such instances for iris dataset and ridge-regularized logistic regression. We can select different types of misclassification in Confusion Matrix and highlight them in the Scatter Plot. No surprise: the misclassified instances are close to the class-bordering regions in the scatter plot projection.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/misclassifications.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Where Are Misclassifications",
	"icon" : ""
},
{
    "uri": "/workflows/text-preprocessing/",
	"title": "Text Preprocessing",
	"description": "",
	"content": "Text mining requires careful preprocessing. Here’s a workflow that uses simple preprocessing for creating tokens from documents. First, it applies lowercase, then splits text into words, and finally, it removes frequent stopwords. Preprocessing is language specific, so change the language to the language of texts where required. Results of preprocessing can be observe in a Word Cloud.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Text mining requires careful preprocessing. Here\u0026rsquo;s a workflow that uses simple preprocessing for creating tokens from documents. First, it applies lowercase, then splits text into words, and finally, it removes frequent stopwords. Preprocessing is language specific, so change the language to the language of texts where required. Results of preprocessing can be observe in a Word Cloud." ,
	"author" : "",
	"summary" : "Text mining requires careful preprocessing. Here\u0026rsquo;s a workflow that uses simple preprocessing for creating tokens from documents. First, it applies lowercase, then splits text into words, and finally, it removes frequent stopwords. Preprocessing is language specific, so change the language to the language of texts where required. Results of preprocessing can be observe in a Word Cloud.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/text-preprocessing.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Text Preprocessing",
	"icon" : ""
},
{
    "uri": "/workflows/text-clustering/",
	"title": "Text Clustering",
	"description": "",
	"content": "The workflow clusters Grimm’s tales corpus. We start by preprocessing the data and constructing the bag of words matrix. Then we compute cosine distances between documents and use Hierarchical Clustering, which displays the dendrogram. We observe how well the type of the tale corresponds to the cluster in the MDS.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The workflow clusters Grimm\u0026rsquo;s tales corpus. We start by preprocessing the data and constructing the bag of words matrix. Then we compute cosine distances between documents and use Hierarchical Clustering, which displays the dendrogram. We observe how well the type of the tale corresponds to the cluster in the MDS." ,
	"author" : "",
	"summary" : "The workflow clusters Grimm\u0026rsquo;s tales corpus. We start by preprocessing the data and constructing the bag of words matrix. Then we compute cosine distances between documents and use Hierarchical Clustering, which displays the dendrogram. We observe how well the type of the tale corresponds to the cluster in the MDS.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/text-clustering.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Text Clustering",
	"icon" : ""
},
{
    "uri": "/workflows/text-classification/",
	"title": "Text Classification",
	"description": "",
	"content": "We can use predictive models to classify documents by authorship, their type, sentiment and so on. In this workflow we classify documents by their Aarne-Thompshon-Uther index, that is the defining topic of the tale. We use two simple learners, Logistic Regression and Naive Bayes, both of which can be inspected in the Nomogram.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We can use predictive models to classify documents by authorship, their type, sentiment and so on. In this workflow we classify documents by their Aarne-Thompshon-Uther index, that is the defining topic of the tale. We use two simple learners, Logistic Regression and Naive Bayes, both of which can be inspected in the Nomogram." ,
	"author" : "",
	"summary" : "We can use predictive models to classify documents by authorship, their type, sentiment and so on. In this workflow we classify documents by their Aarne-Thompshon-Uther index, that is the defining topic of the tale. We use two simple learners, Logistic Regression and Naive Bayes, both of which can be inspected in the Nomogram.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/text-classification.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Text Classification",
	"icon" : ""
},
{
    "uri": "/workflows/twitter/",
	"title": "Twitter Data Analysis",
	"description": "",
	"content": "Tweets are a valuable source of information, for social scientists, marketing managers, linguists, economists, and so on. In this workflow we retrieve data from Twitter, preprocess it, and uncover latent topics with topic modeling. We observe the topics in a Heat Map.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Tweets are a valuable source of information, for social scientists, marketing managers, linguists, economists, and so on. In this workflow we retrieve data from Twitter, preprocess it, and uncover latent topics with topic modeling. We observe the topics in a Heat Map." ,
	"author" : "",
	"summary" : "Tweets are a valuable source of information, for social scientists, marketing managers, linguists, economists, and so on. In this workflow we retrieve data from Twitter, preprocess it, and uncover latent topics with topic modeling. We observe the topics in a Heat Map.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/twitter.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Twitter Data Analysis",
	"icon" : ""
},
{
    "uri": "/workflows/story-arcs/",
	"title": "Story Arcs",
	"description": "",
	"content": "In this workflow we explore story arcs in the Little Match Seller story. First we select the story from the corpus of Andersen tales. Then we create a table, where each sentence of the tale is a separate row. We use sentiment analysis to compute the sentiment of each sentence, then observe the emotional arcs through the story. We also inspect sentences with similar scores in the Heat Map and Corpus Viewer.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "In this workflow we explore story arcs in the Little Match Seller story. First we select the story from the corpus of Andersen tales. Then we create a table, where each sentence of the tale is a separate row. We use sentiment analysis to compute the sentiment of each sentence, then observe the emotional arcs through the story. We also inspect sentences with similar scores in the Heat Map and Corpus Viewer." ,
	"author" : "",
	"summary" : "In this workflow we explore story arcs in the Little Match Seller story. First we select the story from the corpus of Andersen tales. Then we create a table, where each sentence of the tale is a separate row. We use sentiment analysis to compute the sentiment of each sentence, then observe the emotional arcs through the story. We also inspect sentences with similar scores in the Heat Map and Corpus Viewer.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : ["/workflow_images/story-arcs.png"],
	"image" : "",
	"kind" : "page",
	"type" : "workflows",
	"LinkTitle" : "Story Arcs",
	"icon" : ""
},
{
    "uri": "/blog/",
	"title": "Blogs",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 5, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "blog",
	"LinkTitle" : "Blogs",
	"icon" : ""
},
{
    "uri": "/blog/education/",
	"title": "education",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 5, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "education",
	"icon" : ""
},
{
    "uri": "/blog/2021/2021-03-05-overfitting-course/",
	"title": "Hands-On Training About Overfitting",
	"description": "",
	"content": "PLOS Computation Biology has just published our paper on training about overfitting:\n Demšar J, Zupan B (2021) Hands-on training about overfitting. PLoS Comput Biol 17(3): e1008671.  Machine learning has recently propelled approaches for the analysis of data, but “for the uninitiated, the technology poses significant difficulties” (Deep learning for biology, Nature, Feb 22, 2018). One of the hard concepts for starters in machine learning is overfitting. Overfitting can lead to models that include patterns that do not generalize well and could be meaningless. It is thus vital to include teaching about overfitting in any data science course.\n For years, we have been developing Orange, a data science platform. Since we are also educators, we have designed Orange to support the teaching of concepts in machine learning. In the paper, we lay out a short course that uses Orange to teach about overfitting. The specific advantage of our proposed course is that it is entirely hands-on, can be carried out in few hours, does not require any prerequisites or much background knowledge, and is suitable for students of biomedicine or molecular biology that do not necessarily know how to code. The course layout we are proposing is practical; students learn by analyzing the data, making mistakes in the analysis that lead to overfitting, and correcting these by adjusting the workflows.\nIn the past several years, we have been giving and perfecting the lecture we are reporting in the paper. The lecture is carried out yearly at the University of Ljubljana, Slovenia, and at Baylor College of Medicine in Houston. The lecture was also included in over 50 short hands-on courses on machine learning we have been giving around the world. Our paper reports on the course structure, pedagogical principles we use in teaching, and a walk through the course that educators can use for teaching material and ideas within their lessons.\nOur other manuscripts, where we report on Orange as a tool for education in data science, include\n Stražar M, Žagar L, Kokošar J, Tanko V, Erjavec A, Poličar P, Starič A, Demšar J, Shaulsky G, Menon V, Lamire A, Parikh A, and Zupan B (2019) scOrange – A Tool for Hands-On Training of Concepts from Single Cell Data Analytics, Bioinformatics 35(14):i4-i12. Godec P, Pančur M, Ilenič N, Čopar A, Stražar M, Erjavec A, Pretnar A, Demšar J, Starič A, Toplak M, Žagar L, Hartman J, Wang H, Bellazzi R, Petrovič U, Garagna S, Zuccotti M, Park D, Shaulsky G, Zupan B (2019) Democratized image analytics by visual programming through integration of deep models and small-scale machine learning, Nature Communications 10(1):4551.  ",
	"image" : "",
	"thumbImage" : "/blog_img/2021/2021-01-11-orange-in-education-small.png",
	"shortExcerpt" : "We have designed a course on overfitting.",
	"longExcerpt" :  "PLOS Computation Biology has just published our paper on training about overfitting." ,
	"author" : "Blaž Zupan",
	"summary" : "PLOS Computation Biology has just published our paper on training about overfitting:\n Demšar J, Zupan B (2021) Hands-on training about overfitting. PLoS Comput Biol 17(3): e1008671.  Machine learning has recently propelled approaches for the analysis of data, but \u0026ldquo;for the uninitiated, the technology poses significant difficulties\u0026rdquo; (Deep learning for biology, Nature, Feb 22, 2018). One of the hard concepts for starters in machine learning is overfitting. Overfitting can lead to models that include patterns that do not generalize well and could be meaningless.",
	"date" : "Mar 5, 2021",
	"frontPageImage" :"/blog_img/2021/2021-03-05-overfitting-tree-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Hands-On Training About Overfitting",
	"icon" : ""
},
{
    "uri": "/blog/machine-learning/",
	"title": "machine learning",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 5, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "machine learning",
	"icon" : ""
},
{
    "uri": "/",
	"title": "Orange",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 5, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "home",
	"type" : "page",
	"LinkTitle" : "Orange",
	"icon" : ""
},
{
    "uri": "/blog/teaching/",
	"title": "teaching",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 5, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "teaching",
	"icon" : ""
},
{
    "uri": "/blog/explain/",
	"title": "explain",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 10, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "explain",
	"icon" : ""
},
{
    "uri": "/blog/explainable-AI/",
	"title": "explainable AI",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 10, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "explainable AI",
	"icon" : ""
},
{
    "uri": "/blog/2021/2021-02-10-explaining-models/",
	"title": "Explaining Predictive Models",
	"description": "",
	"content": "It is easy to build powerful predictive models in Orange. But how does the model “look like”? Which attributes and which values of those attributes are important? And when making predictions, which attributes contributed to the decision? Orange’s new Explain add-on helps you answer all those questions.\nRelated: Explaining Models\nGo to Options –\u003e Add-ons and install Explain add-on. Restart Orange for the add-on to appear. It only contains two widgets, but boy are they great!\nLet us start with the attrition data set from the Datasets widget. We will go with Attrition - Train, which a data set on which employees resigned from the company and which stayed. The target variable is called Attrition, where No means that the employee stayed and Yes that the employee resigned. The other attributes describe the employee - her position, education, department, years since promotion, and so on.\n Next, we will build a simple logistic regression predictive model. If inspecting the model in Test and Score, we learn that the model has an AUC of 0.788 and 86 % CA. But what kind of a model is this? How does it makes its decisions?\nLet us add Explain Model to Logistic Regression and add another connection passing the data. The workflow should look like this:\n Now open Explain Model. The widget lists top ranked variables, which means they contribute the most to the selected target variable. As we are trying to understand why people leave the company, we have set the target variable to Yes.\nThe highest ranked variable is OverTime - this is the variable with the highest impact on the prediction. Having a value Yes in the categorical attribute OverTime (red dots on the right) means the employee is likely to quit. Also, having low job satisfaction contributes to attrition (blue values on the right). The visualization shows the values which have a high impact on the prediction of the selected class on the right and those which vote against the selected class on the left. The color of dot represents the value of the attribute (red for higher values and blue for lower).\n Let us look at, for example, YearsAtCompany. How would we interpret this? The variable has more red dots on the left, which means high variable values contribute against the target value (against attrition, i.e. employees will stay). Red dots refer to the value of the attribute. So if the employee is with the company for a long time (high value == red dot), it means it is more likely she will stay (dots are on the left, while our target value is Yes).\nGreat, now we understand the model and we are ready to make some predictions. Let us load Attrition - Predict with another Dataset widget. We have three new employees, who are described with all the previous variables, but they are lacking Attrition - we do not know, who is more likely to leave.\n Now pass the logistic regression model and the train data set to Explain Predictions. Then select John from the table and pass the selection to Explain Predictions. The widget requires three inputs: the model, training data, and the instance we are predicting (John).\n Once again, we are interested in target value Yes. Variables in red increase the probability of the target value (conversely, blue decrease it). The size of the arrow corresponds to the SHAP value - in other words, the larger the arrow the larger the variable’s contribution to the target value. The model also predicted that John will leave the job with 77 % probability.\n As before, the most important variable for John is overtime. Him working overtime contributes a lot to the final prediction. Also, his job satisfaction is low (1 out of 5), making him likely to quit.\nThe results correspond very much to those of the model, but it might not always be the case. Some people might leave because they are very dissatisfied without working overtime. This would show in Explain Predictions. See how the results change for the other two employees, Rachel and Veronica. Or make up your own employee with Excel and see what would the prediction be.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2021/2021-02-10-explain-models-small.png",
	"shortExcerpt" : "New Orange add-on for explaining predictive models.",
	"longExcerpt" :  "New Orange Explain add-on for understanding predictions and predictive models." ,
	"author" : "Ajda Pretnar",
	"summary" : "It is easy to build powerful predictive models in Orange. But how does the model \u0026ldquo;look like\u0026rdquo;? Which attributes and which values of those attributes are important? And when making predictions, which attributes contributed to the decision? Orange\u0026rsquo;s new Explain add-on helps you answer all those questions.\nRelated: Explaining Models\nGo to Options \u0026ndash;\u0026gt; Add-ons and install Explain add-on. Restart Orange for the add-on to appear. It only contains two widgets, but boy are they great!",
	"date" : "Feb 10, 2021",
	"frontPageImage" :"/blog_img/2021/2021-02-10-explain-models-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Explaining Predictive Models",
	"icon" : ""
},
{
    "uri": "/blog/model-explanation/",
	"title": "model explanation",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 10, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "model explanation",
	"icon" : ""
},
{
    "uri": "/blog/predictive-modelling/",
	"title": "predictive modelling",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 10, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "predictive modelling",
	"icon" : ""
},
{
    "uri": "/blog/bar-plot/",
	"title": "bar plot",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 27, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "bar plot",
	"icon" : ""
},
{
    "uri": "/blog/2021/2021-01-27-word-distribution/",
	"title": "Observing Word Distribution",
	"description": "",
	"content": "In text mining, one of key tasks is understanding and inspecting the corpus. It makes it easier to determine the preprocessing techniques and downstream analysis (the selection of document frequency weights, topic modelling technique, lemmatization and so on).\nEven though Orange sometimes doesn’t have a widget for a specific task, the said task can be achieved with a combination of widgets and their outputs. Let us look at an example of word distribution. There is no such widget in Orange, but word distributions are generally available in Word Cloud. Word Cloud shows a list of most frequent words and their frequencies on the left and a cloud visualization on the right.\n This is a great start, but Word Cloud only shows the 100 words and the visualization doesn’t directly correspond to the word frequency (words are scaled so that very frequent words don’t overwhelm less frequent ones). Yet Word Cloud has an output called Word Counts, which outputs a table with words and their frequencies in columns. Just what we would like to see!\n Can we see these frequencies as distributions? Yes, we can. A general widget showing numeric values (such as word counts) is Bar Plot. We pass the Word Counts output of Word Cloud to Bar Plot. We can also label each bar by setting Annotations to Word.\n If we zoom in, we can see that “the”, “and”, “of”, “to”, “a” are by far the most frequent words. This calls for some preprocessing, particularly stopword removal. We place Preprocess Text between Corpus and Word Cloud and use default preprocessing. Our bar plot has changed. Now, the most frequent word is “said”, so perhaps another round of stopword removal is necessary.\n  Orange widgets are intended to be as general as possible, which it easy to stack them into custom workflows. Don’t forget to explore all the outputs different widgets offer, for example the All Topics output from Topic Modelling, Concordances from Concordance, or Grouped Data from Pivot Table.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2021/2021-01-27-word-distribution-small.png",
	"shortExcerpt" : "How to inspect word distribution in a corpus with in Orange.",
	"longExcerpt" :  "How to inspect word distribution in a corpus with a clever combination of widgets in Orange." ,
	"author" : "Ajda Pretnar",
	"summary" : "In text mining, one of key tasks is understanding and inspecting the corpus. It makes it easier to determine the preprocessing techniques and downstream analysis (the selection of document frequency weights, topic modelling technique, lemmatization and so on).\nEven though Orange sometimes doesn\u0026rsquo;t have a widget for a specific task, the said task can be achieved with a combination of widgets and their outputs. Let us look at an example of word distribution.",
	"date" : "Jan 27, 2021",
	"frontPageImage" :"/blog_img/2021/2021-01-27-word-distribution-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Observing Word Distribution",
	"icon" : ""
},
{
    "uri": "/blog/text-mining/",
	"title": "text mining",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 27, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "text mining",
	"icon" : ""
},
{
    "uri": "/blog/word-cloud/",
	"title": "word cloud",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 27, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "word cloud",
	"icon" : ""
},
{
    "uri": "/blog/word-distribution/",
	"title": "word distribution",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 27, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "word distribution",
	"icon" : ""
},
{
    "uri": "/blog/Orange/",
	"title": "Orange",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 11, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "Orange",
	"icon" : ""
},
{
    "uri": "/blog/2021/2021-01-11-orange-in-classroom/",
	"title": "Orange in Classroom",
	"description": "",
	"content": "Orange in Classroom About three weeks ago, we put out a short survey asking professors, teaching assistants, and students to tell us how they use Orange in class. We have gotten four-hundred responses, and it turns out that over two hundred universities from around the world actively use Orange in the classroom.\nWe sincerely thank everyone for the answers!\n Here is a a list of universities, in alphabetical order, we have compiled from the survey.\n Ahmad Dahlan University, Indonesia Aix-Marseille University, France Amirkabir University of Technology, Iran Andrés Bello Catholic University, Venezuela Anil Neerukonda Institute of Technology \u0026 Sciences, India Antenor Orrego Private University, Peru BSE Institute Ltd., India Babeș-Bolyai University, Romania Bandung Institute of Technology, Indonesia Bandırma Onyedi Eylül University, Turkey Baylor College of Medicine, USA Birla Institute of Technology and Science, India Bocconi University, Italy Bombay Stock Exchange Institute, India Budi Luhur University, Indonesia California Polytechnic State University, USA Carleton University, Canada Center for Applied Mathemathics, Mexico Center of Higher Education of Brasilia, Brazil Central Washington University, USA Cestar College of Business, Health and Technology Charles University, Faculty of Science Chiang Mai University, Thailand Complutense University of Madrid, Spain DNACapitals, Singapore Daegu Software High School, South Korea Darmstadt University of Applied Sciences, Germany Data Science Dojo, USA Delft University of Technology, Netherlands Des Moines Area Community College, USA Dian Nuswantoro University, Indonesia Duta Bangsa University, Indonesia Ecuador Technological University, Ecuador Eskişehir Technical University, Turkey Estio Training, UK European Academy of Neurology, Austria Faculty of Sciences of the University of Lisbon, Portugal Federal Institute of Bahia, Brazil Federal Institute of Education, Science and Technology of Tocantins Federal Institute of São Paulo, Brazil Federal University of Goiás, Brazil Federal University of Pelotas, Brazil Federal University of Rio Grande, Brazil Federal University of Rio Grande do Norte, Brazil Federal University of Santa Maria, Brazil Florida State University, USA Francisco José de Caldas District University, Colombia Giresun University, Turkey Gunadarma University, Indonesia Guru Gobind Singh Indraprastha University, India Hacettepe University, Turkey Hankuk University of Foreign Studies, South Korea Harrisburg University of Science \u0026 Technology, USA Holon Institute of Technology, Israel I.E.S. Juan Carlos I, Spain ICFAI Business School, India ICFAI Business School Hyderabad, India IPB University, Indonesia ISLA Santarém, Portugal ITB STIKOM Bali, Indonesia ITC Infotech, India ITESO, Universidad Jesuita de Guadalajara ITM Business School, India IULM University - Milan, Italy Indian Institute of Management, India Indian Institute of Management Sambalpur, India Indian Statistical Institute, India Informatics \u0026 Business Institute Darmajaya, Indonesia Institut catholique d’arts et métiers, France Institute of Technical Education and Research, India Instituto Potosino de Investigación Científica y Tecnológica, Mexico Instituto Superior de Engenharia de Lisboa, Portugal Instituto Tecnológico Superior de Xalapa, Mexico International Trademark Association, USA JK Faculty, Brazil Jakarta State Polytechnic, Indonesia KAIST, South Korea Kielce University of Technology, Poland King Mongkut’s Institute of Technology Ladkrabang, Thailand Laval University, Canada Linnaeus University, Sweden Liverpool John Moores University, UK Lodz University of Technology, Poland Lviv Polytechnic National University, Ukraine MGMU Institute of Biosciences \u0026 Technology, India Mahidol University, Thailand Mauricio de Nassau Faculty, Brazil Memorial University of Newfoundland, Canada Mercu Buana University, Indonesia Mexican Institute of Knowledge Management, Mexico Mexican Institute of Social Security, Mexico Mittelhessen University of Applied Sciences, Germany National Central University, Taiwan National Chung Hsing University, Taiwan National Conservatory of Arts and Crafts, France National Institute of Technology Kurukshetra, India National School of Computer Sciences, Tunisia National Service for Industrial Training, Brazil National University, USA National University of General San Martín, Argentina New Bulgarian University, Bulgaria Nigerian Defence Academy, Nigeria North American University, USA Northern Alberta Institute of Technology, Canada Ohio University, USA Pablo de Olavide University, Spain Padjadjaran University, Indonesia Palacký University Olomouc, Czechia Panamerican University, Mexico Panteion University of Social and Political Sciences, Greece Peking University, China Pennsylvania State University, USA Pirogov Russian National Research Medical University, Russia Plovdiv University “Paisii Hilendarski”, Bolgaria Politecnica Salesiana University, Ecuador Polytechnic Institute of Coimbra, Portugal Polytechnic University of Yucatan, Mexico Pontifical Catholic University of Peru, Peru Pontifical Catholic University of Rio de Janeiro, Brazil Poznań University of Economics and Business, Poland Prague University of Economics and Business, Czech Republic Praxis Business School, India Professional Institute Santo Tomas, Chile RWTH Aachen University, Germany Republic Polytechnic, Singapore Research Institute for Development, France Rheinische University of Applied Science, Germany Riga Technical University, Latvia Riphah International University, Pakistan Rochester Institute of Technology, USA Sabanci University, Turkey Sabancı University, Turkey Sakarya University, Turkey Santo Tomas, Spain School of Information Management and Computer IKMI, Indonesia School of Technology and Management of Oliveira do Hospital, Portugal Sepuluh Nopember Institute of Technology, Indonesia Shahroud University of Technology, Iran Silesian University of Technology, Poland Singapore Polytechnic, Singapore Sol Solution, France Soongsil University, South Korea South Ural State University, Russia St. Petersburg State University of Industrial Technologies and Design, Russia Sungkyunkwan University, South Korea Syracuse University School of Information Studies, USA Södertörn University, Sweden Tecnológico de Monterrey, Mexico Telkom Institute of Technology, Indonesia Telkom Institute of Technology Purwokerto, Indonesia Texas State University, USA The American University (Nicaragua), Nicaragua The Free University of Berlin, Germany The Hong Kong Polytechnic University, Hongkong The Institute of Bioengineering of Catalonia, ? The United Nations University, The Netherlands The University of Santa Cruz do Sul, Brazil The University of Utah, USA Tohoku University of Community Service and Science, Japan Tokyo University of Science, Japan Treptow-Köpenick University of Appplied Sciences, Germany UNICAMP Universidade Estadual de Campinas, Brazil Universidad Autónoma Latinoamericana, Colombia Universidad EAFIT, Colombia Universidad Internacional SEK, Ecuador Universidad Norbert Wiener, Peru Universidad Tecnológica de Pereira, Colombia Universidade Estácio de Sá, Brazil Universidade do Sul de Santa Catarina, Brazil Universitas Gadjah Mada, Indonesia Universitas Narotama, Indonesia Universiti Utara Malaysia, Malaysia University Putra Malaysia, Malaysia University of Algarve, Portugal University of Applied Sciences Mittelhessen, Germany University of Belgrade, Serbia University of Brasília, Brazil University of Essex, UK University of Guadalajara, Mexico University of Guelph, Canada University of Health Sciences, Turkey University of Houston, USA University of Indonesia, Indonesia University of La Laguna, Spain University of Ljubljana, Slovenia University of Malaya, Malaysia University of Milano-Bicocca, Italy University of Monterrey, Mexico University of Montreal, Canada University of Nebraska Kearney, USA University of Oviedo, Spain University of Paris, France University of Split, Croatia University of Stirling, UK University of Technology and Applied Sciences, Oman University of Tübingen, Germany University of the Basque Country, Basque Country University of the Republic, Uruguay Università di Foggia, Italy Utah Valley University, USA Weill Cornell Medicine, USA Yogyakarta State University, Indonesia Zwickau University of Applied Sciences, Germany  ",
	"image" : "",
	"thumbImage" : "/blog_img/2021/2021-01-11-orange-in-education-small.png",
	"shortExcerpt" : "Orange is used in over two hundred universities around the world.",
	"longExcerpt" :  "Orange is actively used in classrooms at over two hundred universities from around the world." ,
	"author" : "Ajda Pretnar",
	"summary" : "Orange in Classroom About three weeks ago, we put out a short survey asking professors, teaching assistants, and students to tell us how they use Orange in class. We have gotten four-hundred responses, and it turns out that over two hundred universities from around the world actively use Orange in the classroom.\nWe sincerely thank everyone for the answers!\n Here is a a list of universities, in alphabetical order, we have compiled from the survey.",
	"date" : "Jan 11, 2021",
	"frontPageImage" :"/blog_img/2021/2021-01-11-orange-in-education-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange in Classroom",
	"icon" : ""
},
{
    "uri": "/blog/university/",
	"title": "university",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 11, 2021",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "university",
	"icon" : ""
},
{
    "uri": "/blog/2020/",
	"title": "2020",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 21, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "2020",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-12-21-year-in-code/",
	"title": "2020 - Year in Code",
	"description": "",
	"content": "2020 - Year in Code 2020 is coming a to close. This year had its share of challenges, but we are among the lucky ones being able to work from home. Of course, some of us had to manage being a parent and a developer at the same time, but for the most part, we were successful. We’ve managed to write a couple of new widgets, solve issues, implement enhancement, wrote documentation, and tried to keep the Orange community alive and kicking.\nHere’s Orange’s year in code (and other stats).\nWe’ve made 1302 commits since Jan 1 2020, which amounts to more than 3 and a half per day! There were 20 contributors to the repository. The top contributor with 201 commits was @janezd, who wrote the first piece of Orange code back in 1996. A close second with 185 commits is @ales-erjavec, who is in charge of keeping Orange in top shape (has written and deleted the most lines of code too, +12,369 and -168,333).\n We’ve gone from version 3.24.0 to 3.27.1. There’s one new widget in core Orange (Bar Plot), 3 in Text mining (Statistics, Document Embedding, Corpus to Network), 1 in Network (Node Embedding), and one new add-on (Explain with 2 widgets).\nWe’ve had 651,479 views of YouTube videos and 6,700 new subscribers, which made us really happy! We produced 7 new videos, 3 about analyzing COVID-19 data and 4 about text mining. We’ve not been so good in terms of blog writing - we’ve only published 12 blogs. New Year’s resolution - write more blogs.\n Despite the pandemic, we’ve had three workshops, one summer school, and one online course. We absolutely hate holding lectures online, because we love to interact with our students and see in person how they use Orange. On the other hand, we’ve joined Discord, where users can ask and answer questions and we’ve trying Github Discussions as well. Our main goal for 2021 is to create an online community of Orange novices and experts, where the users will be able to exchange information, ideas, projects, data, code, and experiences (or just chat).\nWe wish you all a lovely holiday season and may 2021 treat us better. To a fruitful and fun new year!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-12-21-github-stats.png",
	"shortExcerpt" : "Statistics of Orange development in 2020.",
	"longExcerpt" :  "Statistical report on Orange software development and educational content for 2020." ,
	"author" : "Ajda Pretnar",
	"summary" : "2020 - Year in Code 2020 is coming a to close. This year had its share of challenges, but we are among the lucky ones being able to work from home. Of course, some of us had to manage being a parent and a developer at the same time, but for the most part, we were successful. We\u0026rsquo;ve managed to write a couple of new widgets, solve issues, implement enhancement, wrote documentation, and tried to keep the Orange community alive and kicking.",
	"date" : "Dec 21, 2020",
	"frontPageImage" :"/blog_img/2020/2020-12-21-github-stats.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "2020 - Year in Code",
	"icon" : ""
},
{
    "uri": "/blog/code/",
	"title": "code",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 21, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "code",
	"icon" : ""
},
{
    "uri": "/blog/Github/",
	"title": "Github",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 21, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "Github",
	"icon" : ""
},
{
    "uri": "/blog/overview/",
	"title": "overview",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 21, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "overview",
	"icon" : ""
},
{
    "uri": "/blog/classification/",
	"title": "classification",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 15, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "classification",
	"icon" : ""
},
{
    "uri": "/blog/corpus/",
	"title": "corpus",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 15, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "corpus",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-10-15-document-embedders/",
	"title": "How to identify fake news with document embeddings",
	"description": "",
	"content": "Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText. In this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.\nWord embedding and document embedding Before we understand document embeddings, we need to understand the concept of word embeddings. Word embedding is a representation of a word in multidimensional spaces such that words with similar meanings have similar embedding. It means that each word is mapped to the vector of real numbers that represents the word. Embedding models are mostly based on neural networks.\nDocument embedding is computed in two steps. First, each word is embedded with the word embedding then word embeddings are aggregated. The most common type of aggregation is the average over each dimension.\nWhy and when should we use embedders? Compared to bag-of-words, which counts the number of appearances of each token in the document, embeddings have two main advantages:\n They do not have a dimensionality problem The result of bag-of-words is a table which has the number of features equal to the number of unique tokens in all documents in a corpus. Large corpora with long texts result in a large number of unique tokens. It results in huge tables which can exceed memory in the computer. Huge tables also increase the learning and evaluation time of machine learning models. Embedders have constant dimensionality of the vector, which is 300 for fastText embeddings that Orange uses. Most of the preprocessing is not required In case of bag-of-words approach, we solve the dimensionality problem with the text preprocessing where we remove tokens (e.g. words) that seem to be less important for the analysis. It can also cause the removal of some important tokens. When using embedders, we do not need to remove tokens, so we are not losing the accuracy this way. Also most of the basic preprocessing can be omitted (such as normalization) in case of fastText embeddings. They can be pretrained Word embedding models can be pretrained on large corpora with billions of tokens. That way, they capture the significant characteristics of the language and produce the embeddings of high quality. Pretrained models are then used to obtain embeddings of smaller datasets. Our Document Embedding widget uses pretrained fastText models and is suitable for corpora of any size.  The shortcoming of the embedders is that they are difficult to understand. For example, when we use a bag-of-words, we can easily observe which tokens are important for classification with Nomogram widget since tokens themselves are features. In the case of document embeddings, features are numbers which are not understandable to human by themselves.\nDocument Embedding widget Orange now offers document embedders through Document Embedding widget. We decided to use fastText pretrained embedders, which support 157 languages. Orange’s Document Embedding widget currently supports 31 most common languages.\n In the widget, the user sets the language of documents and the aggregation method – it is how embeddings for each word in a document are aggregated into one document embedding.\nThe Fake News dataset For this tutorial, we use the sample of Fake News dataset. The dataset sample is available at Orange’s file repository. It is a zip archive containing two datasets: training set including 2725 text items and testing set with 275 items. Each item is an article which is labelled as a real or fake.\nFake news identification Here we present a fake news identification. First, we will load a training part of the dataset with the Corpus widget.\n After the dataset is loaded, we make sure that the text variable is selected in the Used text features field. It means that the text in this variable is used in the text analysis. When the dataset is loaded we connect the Corpus widget to the Document embedder widget which will compute text embeddings. Our workflow should look like this now:\n In the document embeddings widget, we check that language is set to English since texts in this dataset are English. We will use mean (average) aggregation in this experiment – it is the most standard one. After minute documents are embedded – embedding progress is shown with the bar around the widget.\nWhen embeddings are ready, we can train models. In this tutorial, we train two models – Logistic regression and Random forest. We will use default settings for both learners.\n When our models are trained, we prepare the testing data. To load testing data, we use another Corpus widget and connect it to the Document embedder widget. Settings are the same as before. The only difference is that this time we load testing part of the dataset in the second Corpus widget. To make predictions and inspect the prediction results on the testing dataset, we use the prediction widget.\n In the bottom part of the widget, we inspect the accuracies. In the column with name CA (classification accuracy), we can see that both models perform with around 80 % accuracy. In the table above, we can find cases where models made mistakes. If we select rows, we can check them in the Corpus Viewer widget which is connected to the Predictions widget. We have also connected the confusion matrix widget to our workflow, which shows the proportions between the predicted and actual classes.\n We can see that Logistic regression is slightly more accurate in cases of real news while Random forest model is better for predicting fake news.\nIt is just one example which shows how to use document embeddings. You can also use them for other tasks such as clustering, regression or other types of analysis.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-10-15-document-embedding-widget.png",
	"shortExcerpt" : "New Document embedder widget and its use for classification",
	"longExcerpt" :  "Presenting document embeddings widget and how to identify fake news." ,
	"author" : "Primož Godec and Nikola Đukić",
	"summary" : "Text is described by the sequence of character. Since every machine learning algorithm needs numbers, we need to transform it into vectors of real numbers before we can continue with the analysis. To do this, we can use various approaches. Orange currently offers bag-of-words approach and now also Document embedding by fastText. In this post, we explain what document embedding is, why it is useful, and show its usage on the classification example.",
	"date" : "Oct 15, 2020",
	"frontPageImage" :"/blog_img/2020/2020-10-15-document-embedding-title.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "How to identify fake news with document embeddings",
	"icon" : ""
},
{
    "uri": "/blog/embedding/",
	"title": "embedding",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "embedding",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-09-28-text-tutorials/",
	"title": "New Video Tutorials on Text Mining",
	"description": "",
	"content": "In July, we were pleasantly surprised to be awarded a NumFocus Small Development Grant, which is intended to support small tasks in open source projects they sponsor. We decided to extend our text mining tutorials with four new videos, which cover the recent additions to the Text Mining add-on. Our YouTube channel already has a playlist for getting started with Orange and several specialized playlists for learning spectroscopy, single-cell analysis, text mining and image analytics with Orange.\nRelated: Getting Started Series Part 2\nWhile Twitter widget is not a new addition to the Text add-on, it has been missing a tutorial all this time. In the video, we describe how to use the widget and how to perform topic modelling on tweets.\n The second video describes sentiment analysis on tweets for monitoring topic or brand sentiment.\n The third video shows the alternative to computing a bag-of-words matrix. Document embeddings are a popular alternative, which can increase model’s accuracy.\n Finally, we show how to compute a network from Twitter mentions. This tutorials also shows how to mix-and-match Orange components from different add-ons.\n Don’t forget to subscribe to our channel for more videos! And give us a thumbs up if you enjoyed the tutorials. :)\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-09-28-text-tutorials.png",
	"shortExcerpt" : "New video tutorials on text mining available on our YouTube channel.",
	"longExcerpt" :  "New video tutorials on text mining available on our YouTube channel." ,
	"author" : "Ajda Pretnar",
	"summary" : "In July, we were pleasantly surprised to be awarded a NumFocus Small Development Grant, which is intended to support small tasks in open source projects they sponsor. We decided to extend our text mining tutorials with four new videos, which cover the recent additions to the Text Mining add-on. Our YouTube channel already has a playlist for getting started with Orange and several specialized playlists for learning spectroscopy, single-cell analysis, text mining and image analytics with Orange.",
	"date" : "Sep 28, 2020",
	"frontPageImage" :"/blog_img/2020/2020-09-28-text-tutorials.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "New Video Tutorials on Text Mining",
	"icon" : ""
},
{
    "uri": "/blog/sentiment-analysis/",
	"title": "sentiment analysis",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "sentiment analysis",
	"icon" : ""
},
{
    "uri": "/blog/tutorial/",
	"title": "tutorial",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "tutorial",
	"icon" : ""
},
{
    "uri": "/blog/twitter/",
	"title": "twitter",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "twitter",
	"icon" : ""
},
{
    "uri": "/blog/video/",
	"title": "video",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "video",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-07-27-story-arcs/",
	"title": "Detecting Story Arcs with Orange",
	"description": "",
	"content": "Reading is fun because it takes you on a journey. Mostly, it is a journey of emotions as you live and breathe with the protagonist and her adventures. Today, we will have a look at how to detect sentiment in a story, plot story arcs and analyze the content of the key segments in a corpus.\nRelated: Text Workshops in Ljubljana\nWe will be using a corpus of Anderson’s tales, which is available in the Corpus widget (data set anderson.tab). Load it in the widget. Next, we will select a single tale which we will analyze, say, Little Match Seller. Connect Corpus to Data Table and select the tale. We all know the story of a little girl selling matches on a New Year’s Eve and freezing to death. It is one of the saddest stories ever told. One could almost forget there are positive parts, such as the girl’s visions in the moments before her death, which show a glimmer of hope, the only consolation the girl had in her life. Let us verify this in Orange.\n   With our story selected, we have to split it into sentences. At the moment, our story is a single row in the data, but we wish to have each sentence in its own row. We will use Preprocess Text and select Sentence tokenization. I have removed the redundant preprocessors and kept the only one we need.\n Some Python magic will help us create a new corpus from the existing tokens (sentences). Copy and paste the script below into the Python Script widget. Do not forget to press Run once you have pasted the script into the widget.\nimport numpy as np from Orange.data import Domain, StringVariable from orangecontrib.text.corpus import Corpus tokens = in_data.tokens new_domain = Domain(attributes=[], metas=[StringVariable('Sentences'), StringVariable('Title')]) titles = [] content = [] for i, doc in enumerate(tokens): for t in doc: titles.append(in_data[i]['Title'].value) content.append(t) metas = np.column_stack((content, titles)) out_data = Corpus.from_numpy(domain=new_domain, X=np.empty((len(content), 0)), metas=metas) out_data.set_text_features([StringVariable('Sentences')])   Perfect, our data is now ready for the final step. Add another Preprocess Text and keep the default preprocessors. They will lowercase our sentences, split by words and remove English stopwords.\n Finally, add the Sentiment Analysis widget. By default, the widget uses the Vader algorithm, which works quite well for English. Please note that it won’t work for other languages, as all of Orange’s sentiment models are language specific. You can use Multilingual sentiment for non-English texts.\n  At last, it is time to analyze the data. We will use Timeseries add-on to plot sequential data. First, we will pass the data to the As Timeseries widget and set the Sequence implied by the instance order option. This tells Orange that our data is already ordered by time - in our case, by the order in which each sentence appears in the story.\n Connect Line Chart to As Timeseries. In the widget, select Compound variable, which shows the total sentiment of each sentence. The peaks represent the parts of the story with positive emotions and the drops the parts with the negative ones.\n  To explore the data in depth, connect Heat Map to Sentiment Analysis. Heat Map will show all 4 sentiment attributes, namely positive (pos), negative (neg), neutral (neu) and compound sentiment. But our data is all over the place. Let us order it. Select Clustering (opt. ordering) under the Clustering - Rows option. This will cluster the sentences by their similarity, specifically by how similar their emotion is.\n Great! Now this looks like something useful. The blue sections represent the negative sentences, while the yellow and white sections represent the positive ones. Let us select the cluster with negative sentences and observe it in a Corpus Viewer.\n  I have set Sentences as the single Display variable and selected all the sentences to show them in a list. Unsurprisingly, here is the last sentence of the story, in which a girl if found dead in the street on a New Year’s Day.\nTo finish, let us explore the positive sentences, too. Select the positive section in the Heat Map and observe it in a Corpus Viewer. Now rethink the story, reread her visions in the last moments of her life and how happy she was when she died. Couldn’t we say that … the story has a happy ending?\n While the workflow is quite long, it is conceptually very simple. This is a quick and easy way to explore the story arcs and sentiment in a text. We imagine this to be a very useful tool for the teachers who wish to experiment a bit in their language classes and offer a fun and fruitful way of exploring literature.\n ",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-07-27-story-arcs-small.png",
	"shortExcerpt" : "How to detect and analyze story arcs in a corpus.",
	"longExcerpt" :  "How to detect sentiment, plot story arcs and analyze the key segments in a corpus." ,
	"author" : "Ajda Pretnar",
	"summary" : "Reading is fun because it takes you on a journey. Mostly, it is a journey of emotions as you live and breathe with the protagonist and her adventures. Today, we will have a look at how to detect sentiment in a story, plot story arcs and analyze the content of the key segments in a corpus.\nRelated: Text Workshops in Ljubljana\nWe will be using a corpus of Anderson\u0026rsquo;s tales, which is available in the Corpus widget (data set anderson.",
	"date" : "Jul 27, 2020",
	"frontPageImage" :"/blog_img/2020/2020-07-27-story-arcs-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Detecting Story Arcs with Orange",
	"icon" : ""
},
{
    "uri": "/blog/heat-map/",
	"title": "heat map",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 27, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "heat map",
	"icon" : ""
},
{
    "uri": "/blog/line-chart/",
	"title": "line chart",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 27, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "line chart",
	"icon" : ""
},
{
    "uri": "/blog/story-arc/",
	"title": "story arc",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 27, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "story arc",
	"icon" : ""
},
{
    "uri": "/blog/data/",
	"title": "data",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 19, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "data",
	"icon" : ""
},
{
    "uri": "/blog/domain/",
	"title": "domain",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 19, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "domain",
	"icon" : ""
},
{
    "uri": "/blog/edit/",
	"title": "edit",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 19, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "edit",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-06-19-edit-domain/",
	"title": "Managing Data with Edit Domain",
	"description": "",
	"content": "Importing data into Orange is easy. File, import, and voila, your data is here. But what about if you want to rename a variable, change it’s type or edit labels? Edit Domain to the rescue!\nFirst of all, what is ‘domain’. Domain is like a metadata of your data - it describes column names, column types (categorical, numeric, string, datetime), and values for categorical variables. You will come across domain everywhere in Orange, because Orange’s table (Orange.data.Table for programmers) is nothing without it.\nEdit Domain helps you organize the domain of your data. Let us use Datasets widget and load HDI, a dataset of human development index for most countries in the world. We have 188 rows (countries) and 66 features (index variables).\n Now, let us check the domain in Edit Domain. First of all, our variable names are long, so we can make them shorter with Edit Domain. Select, say, Gross National Income (GNI) per capita and rename it to GNI. Simple! If you are unhappy with the change, simply press Reset Selected at the bottom of the widget.\n Scroll down and check the rest of the variables. We have two categorical variables in the data, child labour and maternity leave. Let us check Child labour (% ages 5-14) 2009-2015 and rename it first to Child labour. You can see the variable has many values, from 1 to 47. How about we merge the less frequent values into one?\n Press M on the right side of the widget and a new window will pop up. There are several ways to group less frequent variables, but let’s go with merging all but top 10 most frequent values. Let us also change the label of these values from other to infrequent.\n Finally, we will select Country from the list and set it as a categorical variable. Click on Type drop down and select Categorical. This will reinterpret the variable according to the selected type. Now you can also see the values of the reinterpreted variable. Double-click Antigua and Barb. and rename it to Antigua and Barbuda. You can use up and down arrows to change the order of the variable. Let us push Antigua and Barbuda to the top. The order will be evident in, say, visualizations and their legends. If you connect Line Plot to Edit Domain and set Group by to Country, Antigua and Barbuda will have a long name and will be placed at the top.\n  Edit Domain is a great widget to organize your data. See documentation for other great widgets, such as Create Class or Feature Statistics.\n ",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-06-19-edit-domain-small.png",
	"shortExcerpt" : "Handle your data with Edit Domain.",
	"longExcerpt" :  "How to handle your data with Edit Domain - rename, change type, merge, sort..." ,
	"author" : "Ajda Pretnar",
	"summary" : "Importing data into Orange is easy. File, import, and voila, your data is here. But what about if you want to rename a variable, change it\u0026rsquo;s type or edit labels? Edit Domain to the rescue!\nFirst of all, what is \u0026lsquo;domain\u0026rsquo;. Domain is like a metadata of your data - it describes column names, column types (categorical, numeric, string, datetime), and values for categorical variables. You will come across domain everywhere in Orange, because Orange\u0026rsquo;s table (Orange.",
	"date" : "Jun 19, 2020",
	"frontPageImage" :"/blog_img/2020/2020-06-19-edit-domain-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Managing Data with Edit Domain",
	"icon" : ""
},
{
    "uri": "/blog/addons/",
	"title": "addons",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 20, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "addons",
	"icon" : ""
},
{
    "uri": "/blog/covid-19/",
	"title": "covid-19",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 20, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "covid-19",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-04-15-covid-19-part-3/",
	"title": "Data Mining COVID-19 Epidemics: Part 3",
	"description": "",
	"content": "So far, we’ve seen how to make basic visualizations related to the corona virus and how to look at the disease progression on the map. Be sure to check them out first, before delving into this one.\nWe are now heading towards somewhat more advanced visualizations that let us observe trends in the data. Just as a heads up: your results may be different, depending on the day you downloaded the data. We are working with confirmed cases up to April 13, on the previously mentioned data from the John Hopkins University.\nPrerequisite: Timeseries add-on The spread of the virus is influenced by many factors, time being one of them. Timeseries add-on specializes in manipulation of time-related data. If you followed our blogs, you should already have it installed. If you don’t, just take a peek in the second one.\nFormatting the data As Timeseries We’ll load our data as usual with the File widget. The data contains latitude and longitude columns, which are just getting in the way in our further analysis. We will use Select Columns to put them into meta columns and thus exclude them from any calculations. Then we’ll select some countries we are interested in with the help of the Data Table. You could also do it straight after inspecting the data in Geo Map.\n Now we just need to tell Orange, which variable contains time stamps. Our dates are represented as columns instead of as rows, so we’ll use Transpose widget to make each row represent a datum and then connect it to As Timeseries Widget. In Transpose, we set the variable whose values will be used to name the rows when they are turned into columns. We’ll name them by the column Country, as shown in the image, since the Province field is mostly empty. In As Timeseries, we set sequence as implied by instance order.\n Here is our chain of widgets. We should now be ready to start making some plots. Phew.\n Line Chart and log plot Make sure you’ve selected countries you are interested in the Data Table. I’ve chosen Italy, US, Iran, France and Chinese province Hubei. Then, connect As Timeseries widget and Line Chart and let’s get plotting. Once in Line Chart, we can select multiple countries by clicking Ctrl/Cmd and draw them on the same axis or we could compare multiple plots by clicking on Add plot.\n Here, we see how the virus started its path in China, where the number of cases quickly rose but has since then stayed level. Other countries were hit later on. The slopes of the curves tell us how fast the numbers are growing, but more on that later. One thing to notice though: due to the fast growth of the virus in some countries, some smaller slopes are almost invisible. We can remedy that by checking the Logarithmic axis box just like this:\n We can now see the first steps as well as the current situation.\nPython Script We can see from these plots, that different countries came into contact with the virus at different times. It seems it would be much easier to compare the curves if all of them started their climb at the same time. We can do that with just a few lines of code. Example snippet here aligns the curves with the moment where the country recorded 100 cases. You can set n to any other (positive) number if you will, though.\nimport numpy as np from copy import deepcopy n = 100 out_data = deepcopy(in_data) out_data.X[out_data.X \u003c n] = 0 ar = np.argwhere(out_data.X) cols, shifts = np.unique(ar[:, 1], return_counts=True) out_data.X = np.array([np.roll(out_data.X[:, col], shift) for col, shift in zip(cols, shifts)]).T out_data.X[out_data.X==0]= np.nan Copy these lines into Python Script widget, and plug it into our initial snake of widgets like so: \nLook at the aligned curves now, they are much easier to compare. \nAbsolute growth with derivative Let’s take a look at how fast the confirmed cases are spreading. Connect As Timeseries with Difference widget, choose Differencing and select countries of interest. Differencing order of 1 means we’ll be looking at derivative of first order, which just means daily change in our case. We’ll leave shift as is, on 1.\n Again, we’ll view the transformed curves in Line Chart. Notice how the difference (and every other) transform adds a new column? That means we can compare our curves in different states, so be sure to always check what is being shown. For starters, try looking at, for example, China and its transformed version. See how easy it is to spot the daily spikes that are perhaps out of the ordinary and need to be checked? The most prominent spike here is probably due to the change in counting.\n If we again compare multiple countries, we notice the US’s speedy climb and Iran seemingly being more successful in curbing the growth.\n The difference could be only due to the difference in population and what is an overwhelmingly large number of patients for one country might be almost business as usual for another one.\nScale free growth with quotient Difference widget also has the option to output the quotients. It will allow us to observe the relative growth and compare countries directly.\n Let’s look at, for example, China (Hubei province) vs France.\n It seems China is doing worse in the beginning and better later on, but it is impossible to tell at which point the trends start to shift. There are just too many jumps, due to noise in testing and reporting. So for our final step, let’s smooth things out.\nMoving transform and smoothing Smoothing is usually a part of preprocessing, so insert the Moving Transform widget between As Timeseries and Difference, and add some moving average transforms, like this:\n  Selecting now smoothened China and the France data in Difference widget, yields the following plot in Line Chart:\n The trends now seem clearer. The countries have somewhat similar trends up to 15th day, when China’s growth falls decisively below the French.\nQuotient plot obviously benefited from smoothing, as did the difference plot. For completeness, we added another Difference widget after Moving Transform, and plotted now smoothed difference between the two countries. Notice how the turning point is now seemingly further down the line. Orange prides itself on interactivity, so just by sliding the mouse over the plot, we get the exact information regarding the days and counts. The turning point seems to be on the 25th day.\n This blog concludes our Corona virus series for now. Until our next blog, stay safe.\n  Orange is a multi-platform open-source machine learning and data visualization tool for beginners and experts alike. Download Orange, and load and explore your own data sets!\nIn addition to a variety of learning materials posted online in the form of blog posts, tutorial videos, we’ve created a Discord server. Join the community, tell us what you think!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-04-15-thumb-log-growth.png",
	"shortExcerpt" : "Inspection of Covid-19 time trends.",
	"longExcerpt" :  "Inspecting and comparing Covid-19 time trends, absolute and relative growth." ,
	"author" : "Andreja Kovačič",
	"summary" : "So far, we\u0026rsquo;ve seen how to make basic visualizations related to the corona virus and how to look at the disease progression on the map. Be sure to check them out first, before delving into this one.\nWe are now heading towards somewhat more advanced visualizations that let us observe trends in the data. Just as a heads up: your results may be different, depending on the day you downloaded the data.",
	"date" : "Apr 20, 2020",
	"frontPageImage" :"/blog_img/2020/2020-04-15-thumb-log-growth.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining COVID-19 Epidemics: Part 3",
	"icon" : ""
},
{
    "uri": "/blog/time/",
	"title": "time",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 20, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "time",
	"icon" : ""
},
{
    "uri": "/blog/trends/",
	"title": "trends",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 20, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "trends",
	"icon" : ""
},
{
    "uri": "/blog/visualization/",
	"title": "visualization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 20, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "visualization",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-04-09-covid-19-part-2/",
	"title": "Data Mining COVID-19 Epidemics: Part 2",
	"description": "",
	"content": "Previously on Data Mining COVID-19 Epidemics: Part 1 we fired up a COVID-19 epidemics data set and looked at some basic visualizations. If you haven’t got your hands dirty with it yet, check that out first.\nIn this post, we’ll try putting the data on a map. We’ll also expand on the “data mining is interactive” mantra by creating some animations showing the epidemics spread throughout the world.\nPrerequisite: installing add-ons Orange already comes with a great selection of widgets that work with all sorts of data. But sometimes we’re faced with a particular data set, such as textual data, time-series, or geographical data, which requires widgets with specific functionality – add-ons to the rescue! Add-ons are packages of widgets specially designed to handle specific data types.\nWe’ll be working with the Geo and Timeseries add-ons. To install them, open Options in the menu bar and select Add-ons. A new window will open, showing a list of officially supported add-ons. Select Orange3-Geo and Orange3-TimeSeries and click OK. After installation, restart Orange. Check out the new sections in the widget toolbox; you should see some cool new widgets.\n Note: the two add-ons have been revamped very recently. If you already have them, use the Add-ons dialog to upgrade them to the latest version.\nFrom Scatter Plot to Geo Map You should already be familiar with the Scatter Plot widget. In the previous blog post, we used it as an impromptu map by putting latitude and longitude on the axes.\n A better solution is to use Geo Map from the Orange3-Geo add-on. It provides a variety of maps as backgrounds. With them, it’s much easier to associate points with locations. To start, first load the COVID-19 data set using the File widget, as shown in the previous post. Then, add and connect a Geo Map widget.\n Latitude and longitude are automatically selected. Below, you can define similar point properties, as in the scatter plot. The size of a point can intuitively represent the number of cases, so let’s select the latest date in the Size dropdown. If you’re feeling adventurous, try changing the map type to satellite or topological.\n Just like in Scatter Plot, you can scroll to zoom in and out, and click/drag to select data points. As opposed to using Scatter Plot, showing the points over a map of Europe makes it easier to select the countries we want to inspect.\n The chloro… chorophl… chloropet… map with areas colored in The map is nice and informative, and it’s easy to distinguish larger epidemics from smaller ones. Still, those similarly-sized points on the map represent entire countries from big Russia to small Slovenia. We could, alternatively, represent countries by coloring their surface area. This is a job for Choropleth Map. Let’s add it and connect it.\n This widget is similar to Geo Map, but instead of displaying data as points, it aggregates them into regions (e.g., Country, State, Province), and displays those instead. Under Attribute, select the latest date and under Agg., select sum. The colored regions now show the sum of confirmed COVID-19 cases for each country.\nTry choosing different dates. Notice how China initially experiences rapid growth, followed by a period of stagnation, and how in the past week, the USA overtook all other countries’ confirmed COVID-19 case count.\n A warning that 7 points are not in any region refers to some small islands and cruise ships included in the data.\nIf a picture paints a thousand words, an animation paints a million When working with time-series data, the coolest insights arise from observing trends – how data changes depending on time. Let’s animate our maps!\nWe’ll be using a special widget from Orange3-Timeseries: Time Slice, which lets you select data in a time window. It also features a playback function that slowly moves the time window, outputting corresponding data as it goes.\nTranspose: part I Time Slice requires time instances in the form of rows, while our data specifies them in columns. Let’s start again with the File widget that loads the data. Move Lat and Long to meta attributes by double-clicking feature in the Role column and selecting meta. This will tell Orange to exclude them from data manipulations.\n To flip rows and columns, we use the Transpose widget. Connect it to the File widget.\n Set From variable to Country/Region. Ignore the warning that indices were added to variables with the same name; it won’t hurt us.\n To better understand what just happened, connect a Data Table to Transpose and see the result: rows and columns were swapped. Columns are named after countries (because we chose Country/Region in the Transpose widget), while a new column, Feature name, contains the original names of the columns.\n Right now, Orange is treating values in Feature name as strings. We need to tell Orange that the feature contains timestamps. Let’s connect an Edit Domain widget.\n Scroll to the bottom of Variables and select Feature name. In Edit, change its type to Time. While you’re at it, change its name to something meaningful, like date. Finally, click Apply\n The data is now ready to be processed by Time Slice.\n  The top indicates the currently selected slice of data. You can change the interval by interacting with the histogram, or by adjusting the start and end dates directly below. Let’s select a day’s slice between the 25th and 26th January 2020.\nWith a click of the play button, the slice moves, and the data output updates accordingly. To verify that it works, connect a Data Table, and see it in action.\nTranspose: part II Make sure to stop the Time Slice playback before proceeding. We’re almost at the finish line; we just need to transform the data back into its original form. To do so, connect another Transpose widget.\n Under Future names select Generic, which will name the new column Feature 1.\n Let’s connect a Data Table for a final check of our data before putting it on a map.\n Looking good!\nThe final step entails connecting the Choropleth Map, selecting Feature 1 as Attribute, and Sum as the Agg.\n To observe the animation, open up both Choropleth and Time Slice at the same time.\nProtip: pressing Ctrl/CMD+Up reveals the workflow, Ctrl/CMD+Down reveals widget windows.\nAll that’s left is to hit play.\n Now that we’re familiar with Time Slice, let’s use it with Geo Map too. For bonus points, let’s also remove countries with zero confirmed cases. That way, the number of points on the map will increase as the virus spreads throughout the world. For this, we’ll use a Select Rows widget.\n Under Conditions, select Feature 1, select is greater than, and type in 0.\n Add and connect a Geo Map widget. Zoom out, so you see the whole world, and on the left side, check Freeze map. This will prevented the map from jumping around as new points are added.\n Press play once again – let’s see what we’ve built!\n Where from here? In the blog, we explained how to turn Orange into an interactive data exploration tool for COVID-19 data! With a few widgets we were able to determine the change in time for the confirmed cases and plot the results on a map.\nTime Slice isn’t the only widget in the Orange3-Timeseries add-on. Stay tuned for next week’s blog, where we’ll delve deeper into the patterns emerging from time-series data.\n Orange is a multi-platform open-source machine learning and data visualization tool for beginners and experts alike. Download Orange, and load and explore your own data sets!\nIn addition to a variety of learning materials posted online in the form of blog posts, tutorial videos, we’ve created a Discord server. Join the community, tell us what you think!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-04-09-choropleth.png",
	"shortExcerpt" : "Visualizing COVID-19 data using maps.",
	"longExcerpt" :  "Visualizing COVID-19 data using area and point maps and interactive timeseries." ,
	"author" : "Robert Cvitkovič",
	"summary" : "Previously on Data Mining COVID-19 Epidemics: Part 1 we fired up a COVID-19 epidemics data set and looked at some basic visualizations. If you haven\u0026rsquo;t got your hands dirty with it yet, check that out first.\nIn this post, we\u0026rsquo;ll try putting the data on a map. We\u0026rsquo;ll also expand on the \u0026ldquo;data mining is interactive\u0026rdquo; mantra by creating some animations showing the epidemics spread throughout the world.\nPrerequisite: installing add-ons Orange already comes with a great selection of widgets that work with all sorts of data.",
	"date" : "Apr 13, 2020",
	"frontPageImage" :"/blog_img/2020/2020-04-09-choropleth.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining COVID-19 Epidemics: Part 2",
	"icon" : ""
},
{
    "uri": "/blog/geo/",
	"title": "geo",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 13, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "geo",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-04-02-covid-19-basic/",
	"title": "Data Mining COVID-19 Epidemics: Part 1",
	"description": "",
	"content": "These days we are all following the statistics of COVID-19, looking at how our own country is faring and how it’s comparing with other countries. Luckily, only a few have a statistically meaningful number of deaths (which solemnly reminds us of the difference between statistical and practical significance!), so we concentrate on the number of confirmed cases.\nYou’re reading the first and most basic blog post from a series in which we will investigate this data using Orange. Most people are capable of doing something in Excel(-like programs), and some can do everything in Python with pandas and jupyter. I’ll show you how many people can do many things in Orange.\nToday, we will see how to get this data into Orange, draw some basic curves, and relate it to other data sources. Don’t expect anything dramatic; this will be more about showing some creative ways of connecting a few widgets, and starting to explore the data about COVID-19 epidemics.\nGetting the data John Hopkins University collated some COVID-19 information in a machine-readable format and published it on Github. We will examine the table with confirmed cases by regions and countries.\nTo get the numbers behind the above table, click “Raw”. Or this link: https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv. Save the page and you’ll have a file to play with.\n Easier still, Orange’s File widget can load the data directly from the web, and handles a basic .csv file (for more fine-grained options, use CSV File Import). \nSo, for starters, add the File widget to the canvas and copy the above link to the URL field. You can then connect it to a Data Table widget and check that everything’s loaded OK. You should see a table with rows corresponding to regions and countries, and columns corresponding to dates, with two additional columns detailing region locations (latitude, longitude).\nNote that this is live data, so your results will differ from those we show here.\nPlot the usual plots  Connect the File to a Line Plot widget to see the graph. By default, Line Plot shows means and ranges, while we are interested in raw lines. Let’s click the checkboxes accordingly.\n Pretty boring. Anybody can do this in Excel. Orange allows us to play with these curves, though.\n The curve that grows rapidly but then flattens out – is probably China, right? To check this, select it. It may be hard to click the curve, so select it by dragging a line across it.\n Then, connect the Line Plot widget to Data Table, and it will show the data for this curve. You’ll learn that this is, of course, the Hubei province – where it all started.\n What are the fast-rising curves? Select them by dragging a line across them, and check the Data Table widget: these are the US, Iran, Italy, Spain, Germany and France.\n We can do it the other way around: connect File to Data Table and Data Table to Line Plot, such that the Data Table sends its selected subset of data.\n In the Data Table, you can now find and click your favourite country. Line Plot will now highlight the curve for Slovenia (or whichever country you chose). You can also select multiple countries, like all the Chinese provinces. Or all European countries.\nWell, the latter is a bit difficult; there is no data about the continent. There’s a trick, though.\n Replace the Data Table widget with a Scatter Plot widget, and choose Long and Lat for x and y respectively. Zoom in on Europe and select the points by dragging across them. Line Plot will highlight the curves belonging to the countries chosen in the Scatter Plot. \nIn the case of Slovenia (and possibly your choice as well), we couldn’t really see its curve because it’s negligent in comparison with others. The only thing we’ve learned is that the 327-million-man US have more cases of COVID-19 than a 2-million-man Slovenia. The same goes for Chinese provinces: the only one that sticks out is Hubei. Also, the “map” we improvised in Scatter Plot was a poor-man’s map. Orange has proper geographical support (Orange3-Geo add-on), we’ll look at it in-depth in a follow-up post.\nThe message here regards the essential difference between plotting curves and real data mining: static curves are dead; data mining is interactive.\nConnecting data sets Let’s solve the problem of negligible curves for small countries. This is not just about populations: the biggest problem is that confirmed cases are the result of testing, and testing strategies vary by country. Icelanders and Koreans test like crazy, while the number of tests in the US was (initially) so small it was comparable to that of Slovenia (and with it, the number of confirmed cases). The number of tests could thus normalize the number of confirmed cases, but this wouldn’t do either; the testing isn’t equivalent – some countries test more at random, while others save tests for groups at greater risk. And regardless, exact data about the number of tests is not readily available.\nSo, let’s fallback to the number of cases per million inhabitants. Neither this nor the population of countries are present in the data provided by John Hopkins University. We do, however, have another data set at arm’s reach: the population of countries (for year 2015) appears in the Worldbank’s Human development index (HDI) data.\n To load it, use the Datasets widget, find and double-click the HDI data set. \nNow we’re working with two data sets.\n Let’s merge them using the Merge Data widget: connect both the File widget (with the John Hopkins data), and the Datasets widget (with the HDI data) to Merge Data. Make sure to match the “Country/Region” feature in COVID-19 data with the “Country” feature in HDI. It is also important to connect the widgets in this order, so File provides the main data and the Datasets widget augments it with additional annotation columns. If you do it incorrectly, double-click the connections to fix it.\nThe match is imperfect: some countries appear with different names, for instance, “Russia” from John Hopkins doesn’t match the “Russian Federation” from HDI, and the John Hopkin’s “US” and doesn’t match HDI’s “United States”.\nThe population of Liechtenstein in millions with one decimal is 0.0; similar for Palau.\n To remove countries with unknown or zero population, we continue with Select Rows, where we set the condition “Total Population (millions) 2015 is greater than 0”. \nNow for some constructive business.\n To get the number of cases on some particular day, we use the Feature Constructor widget, where we can input a formula to compute new data columns. In “New” we select Numeric, type cases per million as the name of the new column, Select Feature “3_29_20” (or whichever date we’d like), add /, and then select “Total_Population__millions__2015”. As a shortcut, you can just copy _3_29_20 / Total_Population__millions__2015 into the “Expression…” line. (Note that an underscore precedes 3_29_20: this way Orange knows that this is not a number but a name of a column.)\n Things got crowded, so perhaps we can remove some columns.\n We add Select Columns, open it, and remove all columns we don’t need right now. We can just select all features (Ctrl-A or Cmd-A) and drag them to the left, and then drag “cases per million” back to features. \nFinally, add and connect a Data Table.\n Sort by the last column and, voilá, you’ve ranked countries the number of cases per a million inhabitants.\n Missing in action: China. The data for China is split into provinces, yet every row is divided by the country’s entire population. France seems to have a similar problem: it is given as a single country, but followed by external territories whose numbers are divided by the entire population. Also missing in action: Russia / Russian Federation, and US / United States, as we’ve previously explained.\nFixing these glitches will require some manual work.\nEditing the data To fix the non-unique country names, we could open one of the two files in Excel and fix discrepancies, but there’s no need for it: this is easier done in Orange, which will help us identify the missing countries. They are removed in the Select Rows widget. We hence connect a Data Table to Select Rows, double click the connection and do some editing: click the line between Matching Data and Data to remove it, and drag a line from Unmatched Data to Data.\n This way, we can see which countries were filtered out. They seem to be absent due to either not being present in HDI, or having too small of a population.\nLet’s insert an Edit Domain widget between the Datasets widget and Merge Data.\n We find and select column Country. Now change its type from Text to Categorical. Note, this is conceptually wrong: “Country” is a text variable, it contains a name and not a “category” of country, like the continent or the direction of writing. However, Edit Domain only lets us map its values if we change it to Categorical. And having it as categorical won’t hurt.\n After changing the variable type, the widget will show another box, Values, with a mapping of values. Let’s look at the countries in the new excluded rows Data Table widget. We see, for instance, “Bosnia and Herzegovina”. We need to match to how John Hopkins data refers to countries, so go back to Edit Domain, and find “Bosnia and Herz.”. Double click it and change it to “Bosnia and Herzegovina”.\nSimilarly, Czech Rep. should become Czechia, Macedonia is now officially North Macedonia, Russian Federation is Russia, United States is US and Viet Nam is Vietnam. John Hopkins has “Korea, South” and HDI includes just Korea. Based on its high human development index and GDP, we can safely assume that this is the northern one. (Kidding, map this Korea to Korea, South, of course. There’s no virus in the other Korea because Kim anticipated it before it even appeared and already invented a cure long ago.)\nNow that I’ve taught you how to fish, I give you a fish for today: after editing these and all the other countries whose matches I found, I saved it to a file you can download and use. Replace the Datasets widget with a File, and load it.\nAnalysis beyond population Comparing this data with the World Bank’s data is great because it allows us to relate the epidemiological data to data about particular countries.\n In Select Columns, we removed most of the features. Bring back all those that come from HDI. Now connect, say, a Scatter Plot and observe the relation between the number of cases per million and the number of physicians per ten thousand, which is the closest approximation to the capabilities of health systems.\n The conclusion here would be that you may want to find yourself in Qatar, Cuba or Greece right now. People are often surprised to see Cuba in such contexts, though Cuba has an excellent public health system and pretty good (though perhaps not entirely unbiased) free education. Seeing this graph, it shouldn’t come as a surprise that Cuban doctors are heading to Italy and other countries to help fight the epidemics.\nA seemingly related measure, Public health expenditure doesn’t work here. It is expressed as a percentage of GDP, so it is adjusted for physicians’ salaries, but not necessarily for the prices of (imported) equipment. Furthermore, higher spending may reflect a more expensive but not necessarily a more efficient health system.\nAnother potentially interesting factor to follow will be the age of the population. HDI tells us the number of people older than 65 years per 100 people between 15 and 64.\n I should have used future tense; the population age will be interesting to observe with respect to mortality rate, but not enough countries have a sufficient number of deaths. Let’s hope it stays this way.\nWe will eventually be able to (or – let’s hope we won’t) explore the relations between epidemics and other sociodemographic data. Generally, the problem with analyzing this data per country is currently that the disease has mostly spread (or been detected?) in developed countries, resulting in many correlations like the speed of spreading and percentage of urban populations, which may or may not reflect actual causation.\nBack to curves What about curves showing the number of confirmed cases per million? We computed the cases per million for a single day, not for all days. While Orange is a general-purpose data mining tool, some operations are too specific to be implemented in widgets. For such cases, we can program some processing in Python, using the Python Script widget.\n The start of the sequence is the same as before: we merge the two data sources and remove countries with unknown or zero population.\nIn Select Columns, we move all variables except daily data from features to metas.\nIn Python Script, we type the following program and run it:\nimport numpy as np import Orange normalized = in_data.X / in_data[:, \"Total Population (millions) 2015\"].metas out_data = Orange.data.Table.from_numpy( in_data.domain, normalized, in_data.Y, in_data.metas)  With in_data[:, \"Total Population (millions) 2015\"] we take the table corresponding to the column with the population. Since population is now a meta attribute, we take .metas. We divide in_data.X with this column. Finally, we construct a new table named out_data, with the same domain (that is, the same variables), the normalized data, and the original data for target variable(s) and metas.\nWe get a similar line plot as before, just with proper scale. Observe it. There are two countries with 3000-4000 confirmed cases per million?! Which? Check yourself!\n Would you like to see a plot with a logarithmic axis? And Line Plot doesn’t support it? (Yet?) The beauty of using the Python script widget is that we can transform the data any way we want. Change the line that computes the normalized data to\nnormalized = np.log10(in_data.X / in_data[:, \"Total Population (millions) 2015\"].metas) And we have a logarithmic axis (just with wrong labels).\nWhere from here? This data is related to countries. Hence it would be nice to put it on a map. It also deals with time, and core Orange is not well-equipped for it. In the two follow-up posts, we will explore the add-ons for geographical data and for time series.\n Orange is a multi-platform open-source machine learning and data visualization tool for beginners and experts alike. Download Orange, and load and explore your own data sets!\nIn addition to a variety of learning materials posted online in the form of blog posts, tutorial videos, we’ve created a Discord server. Join the community, tell us what you think!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-04-02-europe-scatter-line-plot.png",
	"shortExcerpt" : "Basic tricks for loading and analysing COVID-19 data.",
	"longExcerpt" :  "Basic tricks for loading and analysing COVID-19 data." ,
	"author" : "Janez Demšar",
	"summary" : "These days we are all following the statistics of COVID-19, looking at how our own country is faring and how it\u0026rsquo;s comparing with other countries. Luckily, only a few have a statistically meaningful number of deaths (which solemnly reminds us of the difference between statistical and practical significance!), so we concentrate on the number of confirmed cases.\nYou\u0026rsquo;re reading the first and most basic blog post from a series in which we will investigate this data using Orange.",
	"date" : "Apr 2, 2020",
	"frontPageImage" :"/blog_img/2020/2020-04-02-thumb.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining COVID-19 Epidemics: Part 1",
	"icon" : ""
},
{
    "uri": "/blog/feature-construction/",
	"title": "feature construction",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 2, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "feature construction",
	"icon" : ""
},
{
    "uri": "/blog/line-plot/",
	"title": "line plot",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 2, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "line plot",
	"icon" : ""
},
{
    "uri": "/blog/anaconda/",
	"title": "anaconda",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 24, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "anaconda",
	"icon" : ""
},
{
    "uri": "/blog/installation/",
	"title": "installation",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 24, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "installation",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-02-24-anaconda-navigator/",
	"title": "Installing with Anaconda Navigator",
	"description": "",
	"content": "We are fortunate enough to be featured on the front page of Anaconda Navigator, a graphical user interface for conda package management. Orange has been a conda package for some time now, since this is the easiest way to provide pre-compiled packages for Windows. And since most of our user base uses Windows, this was the way to go.\nIf you are an avid Anaconda user and you wish to install Orange with Anaconda Navigator, there are some steps you need to take to ensure everything works correctly. First, install Orange in the home screen. Once Orange is installed, it will appear at the top.\n Next, go to the Environments pane. You likely see only base (root) environment. Environments in Python are special ‘containers’ that isolate all your dependencies for different project. You can create a new environment called ‘Orange’ to keep everything Orange-related separate from your base environment. Click Create to make a new environment and follow instructions.\n Here, we will use base, but the procedure is the same for any other environment. Select the environment you wish to set. In the upper right, select Channels. You should see defaults in your options.\n In the upper right, select Add…, then type conda-forge. Conda-forge channel is where the most recent versions of Orange and its add-ons live. Click Update channels once you have added the conda-forge channel.\n That’s it. Your channel is set. Now you can update Orange to the latest version and use add-on that require pre-compiled packages, such as Text, Network, and so on.\n Make sure to regularly update Orange to get the latest bug fixes and features.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-02-24-navigator-small.png",
	"shortExcerpt" : "Essential information for installing Orange via Anaconda Navigator.",
	"longExcerpt" :  "Essential information for installing Orange via Anaconda Navigator." ,
	"author" : "Ajda Pretnar",
	"summary" : "We are fortunate enough to be featured on the front page of Anaconda Navigator, a graphical user interface for conda package management. Orange has been a conda package for some time now, since this is the easiest way to provide pre-compiled packages for Windows. And since most of our user base uses Windows, this was the way to go.\nIf you are an avid Anaconda user and you wish to install Orange with Anaconda Navigator, there are some steps you need to take to ensure everything works correctly.",
	"date" : "Feb 24, 2020",
	"frontPageImage" :"/blog_img/2020/2020-02-24-navigator-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Installing with Anaconda Navigator",
	"icon" : ""
},
{
    "uri": "/blog/navigator/",
	"title": "navigator",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 24, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "navigator",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-02-08-lecture-notes/",
	"title": "Orange Lecture Notes",
	"description": "",
	"content": "In the past, we, the developers of Orange at the University of Ljubljana, have carried out over fifty hands-on workshops. We carried out the workshops for students in secondary schools, universities, Ph.D. programs, employees of scientific institutes, companies, and government institutions. We carried out the courses around the world and lectured in places like Houston, Pavia, Hanover, Moscow, Verona, Montreal, Luxemburg, Kolkata, Liverpool, Bari, Ashburn, Sao Paolo, Trieste, Bled, Lisbon, Konstanz, Oslo, Belgrade, Ostrava, Melbourne, Ås, Bochum, and Ljubljana. The lectures comprised an introduction to machine learning and data visualization and sometimes focused on more specific topics, like text or image mining, mining of spectral data, or even data mining in molecular biology.\nAll of our workshops are hands-on. We start with the data and a problem and then dive into solving a problem with Orange. No PowerPoint slides, no dull and detached lectures. In the workshop, Orange allows us to go straight into data analysis. Orange is different from other workflow-based tools since we designed it for teaching. Our workshops are, therefore, a perfect testbed for Orange. Through their design and execution, we learn about possible improvements and functionalities that we then add to Orange and try out at a forthcoming workshop.\nWe plan our Orange workshops by crafting a workshop program that we assemble into lecture notes. These notes are a refreshment material for the students and help workshop organizers who follow the lectures in the notes more or less strictly, depending on the audience, their engagements, and questions they may have during the workshop.\nWe are aware that our lecture notes can be of assistance to other lecturers and workshop organizers. We are releasing them here for reading, browsing, or reuse. Please find them on our server, or download them from the following list of selected workshops:\n Data Mining, a sixteen-hour two-week Ph.D. course at Baylor College of Medicine (Houston, 2019) Introduction to Data Mining, a three-hour workshop for Ph.D. students (Ljubljana, 2017) A Gentle Introduction to Data Science, a two-hour workshop at GIS Ostrava (Ostrava, 2019) Single-Cell Gene Expression Analysis, a three-hour workshop at Baylor College of Medicine (Houston, 2019)  The lecture notes teach concepts in machine learning and data science. For example, here is a snapshot of one of the pages from our lecture notes.\n We tend to explain this through examples, and most often avoid any heavy mathematics. Our aim is the democratization of data science and would like to see Orange as an enabling tool that reaches a broad audience and complements toolboxes in R and Python that are intended for more tech savvy.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-02-08-lecture-notes-small.png",
	"shortExcerpt" : "Lecture notes for Orange workshops on machine learning and data science are now available online.",
	"longExcerpt" :  "Lecture notes for Orange workshops on machine learning and data science are now available online." ,
	"author" : "Blaž Zupan",
	"summary" : "In the past, we, the developers of Orange at the University of Ljubljana, have carried out over fifty hands-on workshops. We carried out the workshops for students in secondary schools, universities, Ph.D. programs, employees of scientific institutes, companies, and government institutions. We carried out the courses around the world and lectured in places like Houston, Pavia, Hanover, Moscow, Verona, Montreal, Luxemburg, Kolkata, Liverpool, Bari, Ashburn, Sao Paolo, Trieste, Bled, Lisbon, Konstanz, Oslo, Belgrade, Ostrava, Melbourne, Ås, Bochum, and Ljubljana.",
	"date" : "Feb 8, 2020",
	"frontPageImage" :"/blog_img/2020/2020-02-08-lecture-notes-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange Lecture Notes",
	"icon" : ""
},
{
    "uri": "/blog/workshop/",
	"title": "workshop",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 8, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "workshop",
	"icon" : ""
},
{
    "uri": "/blog/anthropology/",
	"title": "anthropology",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 29, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "anthropology",
	"icon" : ""
},
{
    "uri": "/blog/big-data/",
	"title": "big data",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 29, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "big data",
	"icon" : ""
},
{
    "uri": "/blog/machine/",
	"title": "machine",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 29, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "machine",
	"icon" : ""
},
{
    "uri": "/blog/pivot-table/",
	"title": "pivot table",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 29, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "pivot table",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-01-29-machine-anthropology/",
	"title": "What is Machine Anthropology?",
	"description": "",
	"content": "For those unfamiliar with the field, cultural anthropology is the study of human cultures, practices and habits in a holistic and comparative manner. Its core method is ethnographic fieldwork, which means researchers spend a long time in the field with their subjects, live with them, talk with them, socialize with them, and observe relationships and behaviours. But recently, anthropology has begun to use also machine learning and data mining as a part of its method. The subdiscipline is called computational anthropology (combining ethnographic fieldwork with big data) or machine anthropology (ethnography as big data).\nRelated: Data Mining for Anthropologists\n Copenhagen Center for Social Data Science (SODAS) held a conference on machine anthropology just a few days ago. It was inspiring to see how the humanities benefit from computer sciences and vice versa! There, I presented my PhD research of sensor data from a smart building and used Orange to show how to detect patterns of behaviour in such data.\nHere I will show a different example by using a publicly available Philadelphia Crime data set that can be loaded in the Datasets widget. This data set reports crimes in the city of Philadelphia from 2006 to 2016. We will have to create two new features, one for hour of the day and one for month. In the Feature Constructor we will write two Python expressions that parse month and hour from the datetime feature.\n How does our data look like now?\n Perfect. Now say we would like to establish the frequency of crimes by the hour. We will use Pivot Table to count the occurrences of each type of crime. We set rows to type, because we are interested in the differences between different types of crimes. Then we set hour as columns. This will create a new timeseries-like data table, where each column will represent a single hour of the day. Values can be set to anything, because we will not use any special type of aggregation, but simply count occurrences of crimes.\n We can observe the results of the pivot table in a Line Plot. We group by type and see how crime frequencies change with respect to the hour of the day. Unsurprisingly, the liquor law violations and prostitution go up late at night. Prostitution in particular, is a very frequent crime in Philadelphia (or at least was in the time of reporting).\n If we standardize the data with Preprocess (default normalization option), we see a more balanced picture, where homicides are relatively much more frequent early in the morning than at any other time of the day. Apparently, murderers are early birds.\n There you are, a workflow for observing simple timeseries patterns in the data. Of course, you can create much more complicated workflows in Orange or even write a custom Python script. If you have you own examples of anthropological, sociological, or any kind of socially-oriented data analysis in Orange, we’d love to hear about it!\n ",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-01-29-MA-small.png",
	"shortExcerpt" : "At the recent Machine Anthropology workshop we used Orange to explore anthropological data.",
	"longExcerpt" :  "At the recent Machine Anthropology workshop we used Orange to explore anthropological data." ,
	"author" : "Ajda Pretnar",
	"summary" : "For those unfamiliar with the field, cultural anthropology is the study of human cultures, practices and habits in a holistic and comparative manner. Its core method is ethnographic fieldwork, which means researchers spend a long time in the field with their subjects, live with them, talk with them, socialize with them, and observe relationships and behaviours. But recently, anthropology has begun to use also machine learning and data mining as a part of its method.",
	"date" : "Jan 29, 2020",
	"frontPageImage" :"/blog_img/2020/2020-01-29-MA-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "What is Machine Anthropology?",
	"icon" : ""
},
{
    "uri": "/blog/cloud/",
	"title": "cloud",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 16, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "cloud",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-01-16-orange-cloud/",
	"title": "Orange in the Cloud",
	"description": "",
	"content": "Many problems are too big and require too much processing power to be efficiently processed on your laptop or PC. In such cases, the data is usually transferred to a remote server and processed using custom code, which is often time consuming. Now, there is a way to run Orange on a remote server so that you can keep using its interactive graphical interface. We will show you how to run Orange on the remote server so that you can use it through your web browser.\n \\\nIn a nutshell, you browser opens a remote desktop connection to an Orange instance that runs inside a docker container on a remote server. Now let us go into more details.\nIn our configuration we used several different technologies such as Docker and Apache Guacamole, which are shown in the diagram. First, you need to set up Nginx (or alternatively Apache web server), which is used to ensure that all communication to the server is SSL encrypted. This is very important, because remote desktop protocols such as Remote Desktop protocol (RDP) and Virtual network computing (VNC) are not encrypted. Failure to do so will expose your data to anyone listening on the network.\n \\\nNginx redirects you to the Apache Guacamole web application. In Guacamole you can manage multiple users and specify which of your Orange instances each user has access to. Guacamole then connects you to a selected Orange-docker container through an RDP or VNC connection. Once it is connected, you can see the remote desktop in your browser. You can use Orange just like on a local computer (see the image above), although you may need a few minutes to get used to the Linux environment.\nDockers allow you to run many lightweight isolated Linux containers on the same machine. We prepared a docker image that comes pre-configured with graphical desktop environment, a remote desktop server, Orange application and a few convenience applications (Libre Office, web browsers). You can run many isolated Orange-docker containers, so each user can work on a different project. When users collaborate on a project, they can connect to the same instance and share the same screen (read-only or full access). You can upload and download your data to and from the remote server using drag \u0026 drop feature or the side menu. Alternatively you can transfer the data with one of the web browsers that are provided in the container.\nThere are many other benefits to running Orange on the server infrastructure. First, Orange can stay open and continue to process the data even when you are offline. Second, you can access the same workflow from any computer. Third, multiple users can interactively collaborate on the same workflow. Finally, you do not have to keep the data on a local computer and you do not need to install Orange on a local computer. Note that this is a self-hosted solution, which means that all your data remains on the servers under your control.\nFor a complete installation guide and more details see orange-docker Github repository.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-01-16-orange-cloud.png",
	"shortExcerpt" : "Use Orange remotely by running it on a remote server as a docker container.",
	"longExcerpt" :  "Use Orange remotely by running it on a remote server as a docker container." ,
	"author" : "Andrej Čopar",
	"summary" : "Many problems are too big and require too much processing power to be efficiently processed on your laptop or PC. In such cases, the data is usually transferred to a remote server and processed using custom code, which is often time consuming. Now, there is a way to run Orange on a remote server so that you can keep using its interactive graphical interface. We will show you how to run Orange on the remote server so that you can use it through your web browser.",
	"date" : "Jan 16, 2020",
	"frontPageImage" :"/blog_img/2020/2020-01-16-orange-cloud.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange in the Cloud",
	"icon" : ""
},
{
    "uri": "/blog/remote/",
	"title": "remote",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 16, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "remote",
	"icon" : ""
},
{
    "uri": "/blog/server/",
	"title": "server",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 16, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "server",
	"icon" : ""
},
{
    "uri": "/blog/images/",
	"title": "images",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 8, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "images",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-01-08-neighbors-images/",
	"title": "Look-alike Images",
	"description": "",
	"content": "There is a cool and perhaps not often used widget in Orange called Neighbors. The widget accepts the data and a reference data item and outputs the nearest neighbors of that item.\nRelated: Image Analytics Workshop at AIUCD 2018\nHere I will show how to use it to display a set of images most similar to a selected reference image. I will use the following workflow:\n \\\nWe may use any collection of images, and for this blog, I have decided to pull out some image sets available from Datasets widget. To use your own collection of images, check out our YouTube video on image clustering to see how to prepare it. In the Dataset widget, filter the data sets to list only those that include images:\n \\\nI will use Bone Healing image set from our recent Nature Communications paper. In the workflow, we first embed the images in the vector space. We send all the embedded images to the Neighbors widget through its input data channel and display them in Image Viewer. In the viewer, we can select an image. Image Viewer sends its output – the selected image – to the reference channel of the Neighbors widget. I have instructed this widget to send out three data items that are the most similar to the item in the reference, and on the output excluded the reference item:\n \\\nImage Viewer (1) at the end of the pipeline displays three images that are most like the chosen image:\n \\\nAny other set of images works as well. Here is our image look-alike for traffic signs:\n \\\nWe skipped any details on image embedding, measuring distances and so on. For more on these, check out our recent paper in Nature Communications or see our set of image analytics videos on YouTube.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2020/2020-01-08-neighbors-images-small.png",
	"shortExcerpt" : "We show how to use Neighbors widget on image embedding space to find image look-alikes.",
	"longExcerpt" :  "We show how to use Neighbors widget on image embedding space to find image look-alikes." ,
	"author" : "Blaž Zupan",
	"summary" : "There is a cool and perhaps not often used widget in Orange called Neighbors. The widget accepts the data and a reference data item and outputs the nearest neighbors of that item.\nRelated: Image Analytics Workshop at AIUCD 2018\nHere I will show how to use it to display a set of images most similar to a selected reference image. I will use the following workflow:\n \\\nWe may use any collection of images, and for this blog, I have decided to pull out some image sets available from Datasets widget.",
	"date" : "Jan 8, 2020",
	"frontPageImage" :"/blog_img/2020/2020-01-08-neighbors-images-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Look-alike Images",
	"icon" : ""
},
{
    "uri": "/blog/neighbors/",
	"title": "neighbors",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 8, 2020",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "neighbors",
	"icon" : ""
},
{
    "uri": "/blog/belgrade/",
	"title": "belgrade",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 20, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "belgrade",
	"icon" : ""
},
{
    "uri": "/blog/decision-tree/",
	"title": "decision tree",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 20, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "decision tree",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-11-20-belgrade-workshop/",
	"title": "Explaining Models: Workshop in Belgrade",
	"description": "",
	"content": "On Monday, Blaž and I held a technical tutorial Data Mining through Visual Programming and Interactive Analytics in Orange at this year’s edition of Data Science Conference in Belgrade, Serbia. The tutorial explained how to quickly prototype standard data mining and machine learning workflows in Orange and how to interactively explore clustering and classification models. The final part raised an interesting question that we’re going to explore in continuation.\n \n\\\nFirst, let us load the titanic data with the Datasets widget. The data is a simplified version of Kaggle’s Titanic data set. Instead of 10 variables, we kept just 4 - gender (male or female), age (adult or child), status (first, second, third or crew) and the survival class label (yes or no).\n \n\\\nLet us build a simple decision tree model with a Tree widget and observe it in a Tree Viewer. I have hidden the bottom branches for simplicity. Let us observe, what gives me the highest probability of survival. Seems like it is best if I travel as female in the first or second class or as a member of the crew. This would give me 92% chance of surviving the trip.\n \n\\\nOk, what about another classifier. Let’s say Naive Bayes. Connect Naive Bayes to Datasets and then Nomogram to Naive Bayes. It seems like I have the best change of surviving the trip if I travel as female in the first class - the chance of surviving is 90%. But why does Naive Bayes say it is bad to travel as a female crew member, while Tree says it is ok?\n \n\\\nFirst, Naive Bayes does not say it is bad to travel as a female crew member. Naive Bayes assumes attribute independence, so it does not claim anything about being a female AND a member of the crew. It just says it is better to travel as a female and not so good if you are a member of the crew.\nSecond, if we observe the parameters of the Tree classifier, we see that we asked for a binary tree. So inevitably the variables will fall into two categories - third class (which has a low chance of survival) and everything else. If we uncheck this box, we see that the probability of surviving for female crew is slightly lower - 87%. That is still not too bad, but note that we only have 23 instances for this branch! There were not many female crew members on Titanic, so a sample is fairly small compared to other status values.\n \n\\\nThe key message of this part of the tutorial was that different models work differently and we have to understand what they are telling us and how they were constructed. Luckily, Orange enables us to explore certain models, so we can inspect them and draw conclusions from the best ranked attributes.\n ",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-11-20_workshop_small.jpg",
	"shortExcerpt" : "We explained how different models mean different things and how to interpret them at a recent tutorial in Belgrade.",
	"longExcerpt" :  "We explained how different models mean different things and how to interpret them at a recent tutorial in Belgrade." ,
	"author" : "Ajda Pretnar",
	"summary" : "On Monday, Blaž and I held a technical tutorial Data Mining through Visual Programming and Interactive Analytics in Orange at this year\u0026rsquo;s edition of Data Science Conference in Belgrade, Serbia. The tutorial explained how to quickly prototype standard data mining and machine learning workflows in Orange and how to interactively explore clustering and classification models. The final part raised an interesting question that we\u0026rsquo;re going to explore in continuation.",
	"date" : "Nov 20, 2019",
	"frontPageImage" :"/blog_img/2019/2019-11-20_workshop_small.jpg",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Explaining Models: Workshop in Belgrade",
	"icon" : ""
},
{
    "uri": "/blog/naive-bayes/",
	"title": "naive bayes",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 20, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "naive bayes",
	"icon" : ""
},
{
    "uri": "/blog/nomogram/",
	"title": "nomogram",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 20, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "nomogram",
	"icon" : ""
},
{
    "uri": "/blog/clustering/",
	"title": "clustering",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 15, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "clustering",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-11-15-telekom-workshop/",
	"title": "Explaining Customer Segments for Business",
	"description": "",
	"content": "Last month we held a workshop for a large Slovenian Telco company. Their two key questions were - what machine learning techniques can we use on our data and how do we explain the models afterwards. So the workshop focused on three use cases - product segmentation, modeling churn, and clustering customers. With any kind of models, especially unsupervised ones, we often get the question - but how can we explain the clusters? What do these clusters tell us about the customers?\n \n\\\nLet us see how we do this in Orange. We will use Telecom customer churn data set, which you can load with the Datasets widget. But we will not be interested in predicting churn, but rather how to segment customers. Orange already ignores target variable for clustering, but we can remove it with Select Columns to make the example clearer.\n \n\\\nNext, let us design a standard hierarchical clustering workflow. Pass the data to Distances and then to Hierarchical Clustering. Our data set is quite big and 20 variables is not negligible, but for the sake of simplicity we can use Euclidean distance in Distances and Average linkage in Hierarchical Clustering. Finally, let us select three clusters. You can of course try different parameters - say cosine distance and Ward linkage.\n \n\\\nNow for the fun part. Connect another Select Columns to Hierarchical Clustering and put the Cluster variable into the target variable section. Then connect Naive Bayes to Select Columns and Nomogram to Naive Bayes. Like so:\n \n\\\nBasically, we will use cluster labels as target variable and try to predict clusters with Naive Bayes. Nomogram then helps us explain the model. Remember, Naive Bayes assumes independence between variables, so it doesn’t take correlation into account. Logistic Regression does, so feel free to use it instead of Naive Bayes. Anyhow, Nomogram lists the top 10 variables that are important for defining the selected cluster. It seems that cluster C1 does not use internet (or at least does not buy internet from us). C3 on the other hand normally has the whole package.\n \n\n\n\\\nWhat does this mean from a business perspective? Well, that you should focus your marketing on cluster C1 and offer discounted internet packages. Marketing to C3 would be essentially useless. This is how Orange can help you identify business opportunities and understand you customer base better.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-11-15_ws-title.JPG",
	"shortExcerpt" : "Explaining customer base enables businesses to make informed decisions. We present the case for Telco companies.",
	"longExcerpt" :  "Explaining customer base for businesses to make informed decisions. We present the case for Telco companies." ,
	"author" : "Ajda Pretnar",
	"summary" : "Last month we held a workshop for a large Slovenian Telco company. Their two key questions were - what machine learning techniques can we use on our data and how do we explain the models afterwards. So the workshop focused on three use cases - product segmentation, modeling churn, and clustering customers. With any kind of models, especially unsupervised ones, we often get the question - but how can we explain the clusters?",
	"date" : "Nov 15, 2019",
	"frontPageImage" :"/blog_img/2019/2019-11-15_ws-title.JPG",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Explaining Customer Segments for Business",
	"icon" : ""
},
{
    "uri": "/blog/telco/",
	"title": "telco",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 15, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "telco",
	"icon" : ""
},
{
    "uri": "/blog/hypothesis-testing/",
	"title": "hypothesis testing",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 29, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "hypothesis testing",
	"icon" : ""
},
{
    "uri": "/blog/multiple-hypothesis-testing/",
	"title": "multiple hypothesis testing",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 29, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "multiple hypothesis testing",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-09-29-of-carrots-horses-and-fear-of-heights/",
	"title": "Of Carrots and Horses and the Fear of Heights",
	"description": "",
	"content": "A word of warning. When teaching in the US, I am careful enough to not say things like “consider a binary variable, for instance, gender”. But I’m writing this from Russia where considering gender as a binary variable is not only an acceptable but rather a desired, official position. In Russia you can even get away with some mild swearing. While I never use strong curse words, neither in the classroom nor in private, I occasionally say “shit”, which is, as I have been warned, not acceptable in US classrooms (while on corridors you can f… all you want). This blog entry targets Russian audience.\n* * * A(n imaginary) friend of mine has this hobby of searching for all kinds of weird relations. Basically to annoy people, and me in particular. Like “you know, I discovered that people who wear glasses tend to like horses”. Yes, sure they do. “No, seriously, I collected some data. One hundred people, actually. See? It’s totally scientific.” Like hell it is. “Prove me wrong then!” he said. Pathetic.\nOne day I’ve had enough and built a BS-detector, short for “bullshit detector”. I invented a formula that computes the similarity between two columns of 0’s and 1’s. It’s normalized so that it gives me a 1 if the columns are the same, a -1 if they are exactly opposite, and 0 if one tells us nothing about the other, that is, they are the same in exactly half of cases. The score can also be any other number in between, of course - the more different from 0, the stronger the relation. At first I called the formula “Demšar’s bullshit score”, but the next day it sounded too much like being about “Demšar’s bullshit” rather than “bullshit score”. So I made up a better name: “Pearson correlation coefficient”. After watching all that Star Trek series featuring all kinds of “rotating field harmonics”, I became rather proficient at inventing fancy names myself. Pearson correlation coefficient sounds really convincing.\nWhen the imaginary friend (children need them for company and professors need them for making up stories) brought me his next relation, I computed the so-called “Pearson correlation coefficient”, got a result of 0.012 and explained what it meant. In his concrete example it meant that drinking apple juice has nothing to do with fear of heights.\nOf course he complained that 0.012 is not zero, but I was prepared for this. I already realized that the numbers I’ll get for his relations won’t be neither 0 nor 1. So I needed to get some intuition into what kind of numbers to expect from his random (to my belief) relations. I produced some random data myself. I pretended to have a sample of one hundred people for which I recorded two properties. Both binary. One may be a tendency to scratch head, and the other binary variable may be, for instance, gender. Except that I of course had no actual data and just made up two random columns of zeros and ones. And computed my formula.\nTo do so, I opened Orange and created the following workflow. Widget Random Data from add-on “Orange3-Educational” provides random data drawn from different distributions. I set it up to prepare two variables drawn from Bernoulli distribution, which is a fancy name for a random column of 0’s and 1’s. The two variables are discrete, so I attached its output to Continuize to make them numeric. Widget Correlations computes the Demšar’s … I mean “Pearson correlation coefficient”.\n  I checked the coefficient for my pair of random variables. It gave me 0.084, which is more than the score of his – to his claim related – pair, 0.012. I went back to Random Data, generated a different sample, and checked the correlation. And repeated it again. And again. Always larger than his 0.012. I told him that I can run this for 1000 times, but then I saw it’s more practical to just construct 45 variables instead of 2 by changing parameter Variables in Random data to 45.\nThis gave me 990 pairs. A large majority of them was larger than 0.012 that he presented. Actually, I told him (after scrolling way down through the widget) that apple juice and fear of heights were so unrelated that 914 out of (almost) 1000 random relations were stronger than this.\nI opened a beer about two minutes later, after closing the doors behind him. I earned it.\nNext day he came again. And the next. And the following day, too. He just wouldn’t give up. And I kept opening beers until his fifth day’s relation indeed had a BS score of 0.22 (and luckily made some sense – I indeed believe that women can distinguish between more colors than men, who recognize about six or seven). I argued that 0.22 is still low, but he forced me to look into my correlations and, damn, only 32 out of 1000 random relations were so strongly correlated. I had to yield this time: I confessed that random, uncorrelated variables were unlikely to give such strong BS scores.\nWe agreed that even some of 1000 random (non-)relations are bound to be related by chance, so I couldn’t request that he beats all my random pairs. We decided that we will consider a relation significant, if its Pearson correlation coefficient is so large that only 5 % of random relations exceed it. With his samples of size 100, my experiments showed that this value was 0.204. So 5 % it was. I thought it safe enough, he thought it fair enough.\nWe now had a perfect test. One that we would both trust.\nNothing happened for a few days. I believed I essentially won. He knew that we had a method for detecting his bullshit so he won’t bother me again (so soon and so often).\nOr so I thought. Several days later some devil brought him again, with some more relations to check. Total rubbish, but some were really strong. Then he confessed he made up the data for those. It made no sense why he would do this – and then even confess. But when I looked into the data I realized he was trying to guess my secret formula! By the looks of it, he actually succeeded. I made a stand: I made him swear he would never ever cheated by falsifed data. Ever. Again. Or it was the last time I talk with him.\nA day passed. He came grinning so I knew something was up. Eating carrots helps against missing a bus. It has beaten all except the top 2% of my random relations.\nSo I opened a beer – with him. After the second one he opened up. He actually thanked me for the formula! He said it makes his life so much easier. I realized that I haven’t won but shot myself in the foot. He now uses my formula methodically. Yes, he actually calls it a method. He even gave it a name, he calls it “null-hypothesis significance testing”. I didn’t know he was a fellow Star Trek fan.\nHe collected a huge number of his stupid measurements over the years. All on the same group of one hundred individuals. He still believes that some variables are related. And I gave him a method for checking them. We agreed that any pair that beats at least 95% of my random pairs (for which it needs a score of 0.204) is truly correlated. So his method now is to simply compute my formula (which he knows under the name Pearson correlation coefficient) on all pairs of features. He picked one out of many pairs with correlation above 0.204 and brought it to me for testing. Hence the grin.\nThe worst thing was that he wasn’t doing this (only) to tease me. He sincerely believed in the method. We agreed that this would be the way to test his relations and he was serious about it. If my formula gives the number above 0.204 for some relation, the relation is true.\nHe thanked for the beer and promised to come again tomorrow. And the day after, too. Now that we are collaborators.\nЧёрт! (Another confession: I’m writing this at Patriarch ponds, sitting on a bench with my face towards the pond and my back towards Malaya bronnaya.)\nI had some serious thinking to do.\nNext day he came as promised. He told me about the latest relation he found. I forgot what it was; I didn’t pay attention. Neither did I bother to check it – because I knew he has. I instead asked him how many variables he has. Like, for every person he knows whether she or he smokes, wears socks at work, speaks any Hungarian, reads Russian grotesque novels … how many different things? He said 154.\n“Oh, this is quite a collection. Good work,” I kept chatting while typing something into a calculator. “So you must have identified around 590 ‘statistically significant’ relations in your list, haven’t you?” He was significantly less happy after hearing this. “Actually 592,” he said. “But – how did you know?” “People with strange beards like mine tend to have telepathic abilities,” I said seriosly. “What?” “Не шалю, никого не трогаю, починяю примус.”\n* * * I hate spoilers and told him nothing. But I’ll tell you. It’s easy. We can rather safely assume that all his correlations are random. So we can replicate his data without actually collecting it. Open the Random data widget and generate 154 variables instead of 45, so that you simulate his data. Go to Correlations and check how many pairs of random variables have a correlation above 0.204. You’ll see there are around 590. Quite close to the number he got.\nBut I wrote that I typed the numbers into a calculator. Sure. I don’t need Orange for this. With 154 variables, there are 154 * 153 / 2 = 11781 pairs. We agreed that he can show me any relation that beats all but top 5 % of random relations. His relations are random so there are 11781 * 0.05 = 590 relations in the top 5 %.\n* * * The story of course follows the dogma of hypothesis testing. We test a relation by computing some statistics, like Pearson correlation coefficient, and compare it with the value of this statistics that we would get on random data of this kind. This can be done like here. We generate a great number of random cases and compute the statistics for each. If the tested hypothesis beats all except 3 % of random ones, we say p = 0.03, meaning that “there is only 3% probability that a randomly generated hypothesis would reach such a high score”.\nThere is of course a faster way to test the significance of the Pearson correlation coefficient. Pearson was a smart guy (and my calling his correlation coefficient a “bullshit score” shouldn’t be taken out of the context of the story). Distribution of Pearson correlations for random data is known, so critical values (like 0.204 in our case) can be computed analytically, without simulation. We nevertheless used the simulation in this story to make it more evident what significance testing actually does.\nThe story is not without flaws. The 11891 hypotheses are not unrelated. More importantly, when I wrote that the guy went home unhappy (though grudging), I implied that I proved the lack of correlation. With the common null-hypotheses testing you cannot prove the null-hypothesis (in my case: the hypothesis that there is no correlation). Applied properly, he wins if my test proves the correlation, or we call it a draw if it doesn’t. I can’t win.\nThe moral of the story is that statistical tests should not be used in the way we use them here. They are not a magic bullet for testing the correctness of hypotheses. There is not just one but two problems here.\nIf the test requires beating 95 % of my random relations, it will incorrectly recognize 5 % of relations as significant. This is pretty obvious: the top 5 % of my relations beat the remaining 95 %. His random relations will be equally successful, by pure chance. I knew the test was not failsafe. But he cheated: he checked all his relations and brought me those that passed the test. This is the skill of the Texas sharpshooter: shooting first, and drawing targets around holes next. He just found the relations that were successful by chance. For this reason, statistical hypothesis testing requires that you form the hypothesis first, and only then test it on the data - instead of forming them form the data, like he did.\nBut even if he wouldn’t cheat and he would pick pairs at random, with 5 % threshold, 1 of 20 hypothesis would still be successful by chance, so he would be happy once every full moon, assuming he’d take weekends off. In statistics, this is called multiple hypotheses testing. You have to account for randomly “successful” hypotheses by using various corrections. Let us mention just the Bonferroni correction as the simplest example.\n* * * This being an Orange blog, let us conclude by showing a problem with (careless use of) Orange. Connect Distributions widget to Random data. A dialog will pop-up so you can decide which Correlation’s output to use. Connect output “Correlations” to input “Data”. Correlations now shows the distribution of Pearson correlation coefficients on this data. Correlation coefficient is significant if it’s in the 5 % tail of this distribution. You can fit some curve to this if you wish.\n \\\nNow connect Sieve to Random data. Open it and click Score Combinations. It will order the pairs according to their significance. For every pair we select, Sieve will also show its chi-square and the corresponding p-value (at the bottom left). Predictably, the first 50 pairs have p-values below 0.05. By using Score Combinations and picking the top ones, we are making the same mistake as my imaginary friend. (Sieve does not compute Pearson coefficients but chi squares, yet in this context the two statistics are completely related. You can check that Correlations and VizRank show the same order of pairs.)\nSo using Score Combinations (or equivalent buttons in other widgets) and then claiming that you found and proved some relation, is exactly what my friend was doing.\nDoes this mean that we shouldn’t use Score Combinations? Why does Orange have such buttons then? They are safe to use for as long as we do not consider relation that we found in this way as “proven”. Automated tools for forming hypotheses can, well, form hypotheses. To prove them, you need to check them on another data (and still risk a 5 % chance of being successful by pure luck, if you use statistical tests and require p \u003c 0.05) or, preferably, you should find the underlying reason for the correlation.\nEating a lot of carrots decreases the chances of missing a bus because being more like a rabbit helps you run faster.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-09-29-horses.jpg",
	"shortExcerpt" : "A cautionary tale of imaginary friend who made too many hypotheses to test - and how Orange is no different",
	"longExcerpt" :  "A cautionary tale of imaginary friend who made too many hypotheses to test - and how Orange is no different" ,
	"author" : "Janez Demšar",
	"summary" : "A word of warning. When teaching in the US, I am careful enough to not say things like \u0026ldquo;consider a binary variable, for instance, gender\u0026rdquo;. But I\u0026rsquo;m writing this from Russia where considering gender as a binary variable is not only an acceptable but rather a desired, official position. In Russia you can even get away with some mild swearing. While I never use strong curse words, neither in the classroom nor in private, I occasionally say \u0026ldquo;shit\u0026rdquo;, which is, as I have been warned, not acceptable in US classrooms (while on corridors you can f\u0026hellip; all you want).",
	"date" : "Sep 29, 2019",
	"frontPageImage" :"/blog_img/2019/2019-09-29-horses.jpg",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Of Carrots and Horses and the Fear of Heights",
	"icon" : ""
},
{
    "uri": "/blog/p-value/",
	"title": "p-value",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 29, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "p-value",
	"icon" : ""
},
{
    "uri": "/blog/statistical-significance/",
	"title": "statistical significance",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 29, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "statistical significance",
	"icon" : ""
},
{
    "uri": "/blog/confusion-matrix/",
	"title": "confusion matrix",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "confusion matrix",
	"icon" : ""
},
{
    "uri": "/blog/model-performance/",
	"title": "model performance",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "model performance",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-09-28-on-expected-vomiting-time/",
	"title": "On Expected Vomiting Time",
	"description": "",
	"content": "We just finished a lecture and a student came with a question. The lecture was, in short, about how predictive models compute probabilities; to use a model for making decisions, we must find a threshold that will minimize the cost, maximize the profit, strike a good balance between sensitivity and specificity, or increase the general human well-being in some other way. So this student came with a question about something I didn’t explain well, and she gave me one last chance.\nI had some data that looked like this.\n   ID predicted prob.  аctual class     1 0.984 p   2 0.907 p   3 0.881 n   4 0.865 n   5 0.815 p   6 0.741 p   7 0.735 p   8 0.635 n   9 0.582 n   10 0.480 p   11 0.413 n   12 0.317 n   13 0.287 n   14 0.225 n   15 0.216 p   16 0.183 n     The data is made up, so I had to make up its meaning to form an example. In the rush of a moment, I succumbed to classics: “For each of those 16 people you predicted the probability for being sick. Whom are you going to give the cure?” Her answer was in line with the best manners of modern medical practice (who cares about bacteria becoming resistant to antibiotics?): “To all!” I didn’t expect this, but anyway planned to continue with emphasizing that this medicine is not harmless: “People who take it will vomit for one week.” She turned by 180 degrees: “Oh, no. To nobody!” I didn’t expect this either, but it still led nicely to my next condition: “But if a person is indeed sick and you don’t give him the drug, he’ll vomit for four weeks.”\nDecision making becomes much more interesting when both options have consequences.\nSo, to restate:\n if you give the medicine to anybody, sick or healthy, he vomits for a week; if you don’t give the cure to a sick person, he vomits for four weeks – and you can no longer help him after he starts.  Optimal threshold To solve the problem, we had to compute the expected vomiting time. This being a fairly new medical term, we need to define it. For instance, what would be the expected vomiting time for a person who is sick with a probability of 0.865? If we have 1000 people with such probability of being sick, 865 will vomit (for four weeks) and 135 won’t.\n If we don’t give these 1000 people the medicine, 865 will vomit for 4 weeks, hence the total vomiting time will be 4 x 865 = 3460 weeks, or 3.460 weeks per person, where the average includes those who are not vomiting. This is of course the same as if we just multiplied 4 x 0.865, without multiplying and then dividing by 1000. If we give these 1000 people the medicine, all 1000 will vomit for a week, thus totaling 1000 weeks of vomiting or, obviously, 1 week per person.  The latter option is better. Administering the medicine will reduce the expected vomiting time (per person) from 3.460 weeks to 1 week. Well done.\nWhat about people with 0.216 probability of being sick? Expected vomiting time of such persons is 4 x 0.216 = 0.864, which is less than a week. No drug for them. (The poor bloke with the 0.216 probability is actually sick. But we don’t know this yet. S..t happens. I mean, vomit happens.)\nObviously, the optimal threshold here is 0.250, which gives the expected vomiting time of 1 week, with or without the drug. We should administer the drug to all except the last three people in the list.\nWe could have computed this threshold even before having the model. It is a property of the disease and the drug, not the model.\nFor an extra task, compute the expected vomiting time with a threshold of 0.25. Note that you can’t assume that 25 % of untreated people will vomit for 4 weeks: some of those below the 0.25 threshold may also have just 12 % or 3 % chance of being sick. Assume a uniform probability distribution.\nOptimal threshold for the model We have so far used just probabilities and ignored the information whether somebody is sick (p) or not (n), which is given in the third column. We also assumed that predicted probabilities were correct because we had nothing else to go by.\nNow let us take the true status into account. It will allow us to compute the actual vomiting times when using this concrete model at particular thresholds.\nIf we indeed used the threshold of 0.25 and administered the drug to the first 13 people, we’d have 13 people vomiting for 1 week, and the poor guy #15 suffering for 4, which gives us (well, which gives them) the total vomiting time of 13 + 4 = 17 weeks, or 17/16 = 1.0625 per person.\nIf we instead used a threshold of 0.2, 15 people would vomit for 1 week and nobody for 4 weeks. This reduces total vomiting time to 15 weeks.\n(The actual story we developed with the student was slightly different, but roughly at this point, a student who was doing something else and has just started listening to our debate about expected and total vomiting times, interrupted us with: “Who are all these people and why are they vomiting?” We had to explain that nobody is vomiting and that we (that is: I) just have a sick sense for examples.)\nBut there’s an even better threshold. If we give the drug only to people whose probability is at least 0.48, 10 people will vomit for one week and the poor guy #15 for four weeks, giving us a total of 14 weeks of vomiting. Or 14/16 = 0.875 weeks per person.\nWe shouldn’t go any higher. Every sick person whom we don’t give a drug needs to be counterbalanced by at least three healthy people being freed from a one-week ordeal, and there are not enough healthy persons left above 0.48.\nIn this way, we simulated what would happen if we used this model on the population represented by this sample of people. So if we use it in practice and our goal is to reduce the total (or average) vomiting time, we know to administer the drug to everybody with p ≥ 0.48.\nWhy the discrepancy? Why isn’t the threshold that we computed above, 0.250, also the optimal threshold? 0.250 would be the optimal threshold if predicted probabilities were true probabilities. Model’s may be badly calibrated. While there exist methods for fixing their calibration (though not with so little data), we here took a more direct approach and computed the threshold for probabilities as predicted by our model.\nPedagogical Moral The nice thing about the story is that we didn’t care about classification accuracy. Oh, yes, after fixing the threshold to 0.48, we can compute it (it’s 11/16 – just count the p’s above and n’s below the threshold). We also see that the probability of administering a drug to a healthy person (and making him needlessly hug the toilet for seven days) is 40 %, because in our sample, we had 4 such cases out of 10 whom we gave the drug. We call such poor victims false positives, and 40 % is the false discovery rate. The miserables whom we don’t give the drug though we should, are false negatives. The false omission rate (being sick if you’re not given a drug) is 1/6. The probability of not being given the drug if you’re sick (miss rate or false negative rate) is 1/7. I’m of course just copying this from Wikipedia. Nobody knows the entire list of names.\nAs a lecturer, I believe that emphasizing this list of names too much may do more harm than good. I usually show them the list just to say that all these things have names, but then try to compute something meaningful and not fancy-named. Try forcing them to learn the list and then give them a task like above. They’ll likely spend the entire exam guessing whether you want them to compute specificity or critical success index – instead of simply computing the expected vomiting time. Which is not even on Wikipedia.\nYet.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-09-28-vomiting-pumpkin.jpg",
	"shortExcerpt" : "Computing meaningful performance scores of models should be creative",
	"longExcerpt" :  "A report of an interesting ending to a lecture about setting probability thresholds for predictive models" ,
	"author" : "Janez Demšar",
	"summary" : "We just finished a lecture and a student came with a question. The lecture was, in short, about how predictive models compute probabilities; to use a model for making decisions, we must find a threshold that will minimize the cost, maximize the profit, strike a good balance between sensitivity and specificity, or increase the general human well-being in some other way. So this student came with a question about something I didn\u0026rsquo;t explain well, and she gave me one last chance.",
	"date" : "Sep 28, 2019",
	"frontPageImage" :"/blog_img/2019/2019-09-28-vomiting-pumpkin.jpg",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "On Expected Vomiting Time",
	"icon" : ""
},
{
    "uri": "/blog/aggregate/",
	"title": "aggregate",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 27, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "aggregate",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-08-27-pivot-table/",
	"title": "Aggregate, Group By and Pivot with... Pivot Table!",
	"description": "",
	"content": "Orange recently welcomed its new Pivot Table widget, which offers functionalities for data aggregation, grouping and, well, pivot tables. The widget is a one-stop-shop for pandas’ aggregate, groupby and pivot_table functions.\n \\\nLet us see how to achieve these tasks in Orange. For all of the below examples we will be using the heart_disease.tab data.\npandas.DataFrame.agg The first task is computing a simple mean for the column age.\nIn pandas:\n\u003e\u003e\u003edf['age'].agg('mean') 54.43894389438944 \\\nIn Orange:\nIn Pivot Table we set Values to age and set aggregations to mean. There is no way to turn off splitting by rows in Pivot Table, but the values in Total report the mean value for the chosen attribute.\n \\\nAn even better way to observe simple statistics is in Feature Statistics widget. The widget also reports on the mean value of each attribute with handy distributions plots included.\n \\\nYet another way to observe a mean value is in a Box Plot.\n \n\\\npandas.DataFrame.groupby The second task is grouping the data by a discrete column. Say we want to group by gender and report the mean value for each column.\nIn pandas:\n\u003e\u003e\u003e df.groupby(['gender']).mean() age rest SBP ... ST by exer. major ves. col. diameter narrowing gender female 55.721649 133.340206 ... 0.867010 0.546392 0.257732 male 53.834951 130.912621 ... 1.120874 0.732673 0.553398 [2 rows x 9 columns] In Orange:\nIn Pivot Table set Rows to gender and aggregation method to mean. The Values option in this example has no effect. Now, connect Data Table to Pivot Table. Finally, reset the connections. Pivot Table outputs three types of data - Pivot Table, Filtered Data, and Grouped Data. The output we are looking for here is Grouped Data.\n \\\n \\\nThis is our data table. Exactly what we were looking for.\n \n\\\npandas.DataFrame.pivot_table The third, final task is constructing a pivot table where rows are values of diameter narrowing, columns are values of gender and the values in cells is the mean of age for each subgroup. In other words, we want to see the average age of females with diameter narrowing, males with diameter narrowing, females without diameter narrowing and males without diameter narrowing.\nIn pandas:\n\u003e\u003e\u003e pd.pivot_table(df, values='age', index=['diameter narrowing'], columns=['gender'], aggfunc=np.mean) gender female male diameter narrowing 0 54.555556 51.043478 1 59.080000 56.087719 In Orange:\nIn Pivot Table set Rows to diameter narrowing, Columns to gender, Values to age and aggregation method to mean. The widget already offers a view of the final data table, but we can also output it and use it in other Orange widgets.\n \n\\\nPivot Table widget really versatile - like a Swiss knife for data transformation.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-08-27_pivot-preview.png",
	"shortExcerpt" : "Orange&#39;s brand new Pivot Table widget with many aggregation and grouping functionalities.",
	"longExcerpt" :  "Orange has a brand new Pivot Table widget with many aggregation and grouping functionalities. It can be used to transform the data on-the-fly and use the output for downstream analysis." ,
	"author" : "Ajda Pretnar",
	"summary" : "Orange recently welcomed its new Pivot Table widget, which offers functionalities for data aggregation, grouping and, well, pivot tables. The widget is a one-stop-shop for pandas\u0026rsquo; aggregate, groupby and pivot_table functions.\n \\\nLet us see how to achieve these tasks in Orange. For all of the below examples we will be using the heart_disease.tab data.\npandas.DataFrame.agg The first task is computing a simple mean for the column age.\nIn pandas:",
	"date" : "Aug 27, 2019",
	"frontPageImage" :"/blog_img/2019/2019-08-27_pivot-preview.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Aggregate, Group By and Pivot with... Pivot Table!",
	"icon" : ""
},
{
    "uri": "/blog/groupby/",
	"title": "groupby",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 27, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "groupby",
	"icon" : ""
},
{
    "uri": "/blog/data-science/",
	"title": "data science",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 26, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "data science",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-07-26-doctoral_summer_school/",
	"title": "Doctoral Summer School",
	"description": "",
	"content": "For the second year in a row, the Orange team was a part of the Ljubljana Doctoral Summer School, which is organized by the School of Economics and Business, University of Ljubljana. Our course, called Pratical Introduction to Machine Learning and Data Analytics, was aimed at presenting the nuts and bolts of data science methods and concepts with the help of visual programming. In Orange, of course.\n \n\\\nOne important lesson is comparing model accuracy. Let us load the heart_disease data with the File widget. This is the data on the presence of diameter narrowing (a sign of heart disease) in 303 patients. Now let us sample the data into the training and testing data. We will use Data Sampler for this.\n \\\nNow let’s check model accuracy on, say, Tree and Naive Bayes, one of the first two classifiers that we introduce. Connect both outputs of Data Sampler to Test \u0026 Score, the first as Data and the second as Test Data.\n \n\\\nFirst, check the model accuracy with the Test on train data option. Great scores! Seems like Tree performs almost flawlessly. It must be the better classifier of the two!\n \n\\\nOk, just to be sure, let us check the scores on a separate test data set by selecting Test on test data. Oh. This doesn’t look good. Tree now performs much worse than Naive Bayes. Why is that?\n  \n\\\nWe won’t answer this directly, but this is one of the homeworks we give the students in order to introduce cross-validation. This is such an important concept that we make sure everyone in the class really understand why we need separate training and testing data.\nFeel free to use this exercise in any of your future classes!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-07-26_ekonomska_small.jpg",
	"shortExcerpt" : "This year&#39;s Data Science course at the Doctoral Summer School.",
	"longExcerpt" :  "For the second year in a row we took part in the Ljubljana Doctoral Summer School, organized by the School of Economics and Business." ,
	"author" : "Ajda Pretnar",
	"summary" : "For the second year in a row, the Orange team was a part of the Ljubljana Doctoral Summer School, which is organized by the School of Economics and Business, University of Ljubljana. Our course, called Pratical Introduction to Machine Learning and Data Analytics, was aimed at presenting the nuts and bolts of data science methods and concepts with the help of visual programming. In Orange, of course.\n \n\\",
	"date" : "Jul 26, 2019",
	"frontPageImage" :"/blog_img/2019/2019-07-26_ekonomska_small.jpg",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Doctoral Summer School",
	"icon" : ""
},
{
    "uri": "/blog/summer-school/",
	"title": "summer school",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 26, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "summer school",
	"icon" : ""
},
{
    "uri": "/blog/bioinformatics/",
	"title": "bioinformatics",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 25, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "bioinformatics",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-07-25-scorange/",
	"title": "Orange at ISMB/ECCB 2019",
	"description": "",
	"content": "Our entry to this year’s largest bioinformatics conference was on the training of single-cell data analytics. We claim that with Orange and its new single-cell RNA analysis add-on, one can assemble a workshop to teach essential concepts from single-cell analytics in a single day.\n \\\nSingle-cell genomics is driven on revolutionary technology from molecular biology that allows us to peek into inner workings of the individual cell. Single-cell datasets feature thousands of genes and cells and benefit from the analysis that integrates this data with other knowledge sources, like those on the classification of genes, lists of pathways, and sets of cell-type markers. To support such analysis, we have been developing a single-cell add-on for Orange. We are also experimenting with packing this add-on, the add-on for bioinformatics, and the rest of Orange within a stand-alone application called scOrange. Installation of scOrange comes with a preinstalled single-cell add-on and workflows and videos that are specific to this area of research. Our single-cell RNA analysis tool still uses Orange’s widgets at the core, but additionally packs components for reading single-cell data, filtering cells and genes, preprocessing the data and handling peculiarities of single-cell analysis like batch effects and cell-type annotation.\nISMB/ECCB 2019 is the largest and most prestigious bioinformatics conference. It joins the meetings of International and European and Society for Computational Biology. Martin Stražar, a post-doc at our Bioinformatics Lab in Ljubljana and responsible for much of developments in scOrange, presented scOrange in light of its support for case-based teaching and hands-on workshops. During the talk, Martin showcased several scOrange’s workflows. The one I liked best includes automatic annotation of cell-types in point-based visualizations.\n Martin’s presentation was based on our ISMB/ECCB paper just published in Bioinformatics:\nMartin Stražar, Lan Žagar, Jaka Kokošar, Vesna Tanko, Aleš Erjavec, Pavlin G Poličar, Anže Starič, Janez Demšar, Gad Shaulsky, Vilas Menon, Andrew Lemire, Anup Parikh, Blaž Zupan (2019) scOrange—a tool for hands-on training of concepts from single-cell data analytics, Bioinformatics 35(14):i4–i12.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/scorange-annotation-thumb.png",
	"shortExcerpt" : "One of Orange&#39;s latest features is its add-on for single-cell data analytics.",
	"longExcerpt" :  "Our entry to this year\u0026rsquo;s largest bioinformatics conference was on the training of single-cell data analytics. We claim that with Orange and its new single-cell RNA analysis add-on, one can assemble a workshop to teach essential concepts from single-cell analytics in a single day.\n \\\nSingle-cell genomics is driven on revolutionary technology from molecular biology that allows us to peek into inner workings of the individual cell. Single-cell datasets feature thousands of genes and cells and benefit from the analysis that integrates this data with other knowledge sources, like those on the classification of genes, lists of pathways, and sets of cell-type markers." ,
	"author" : "Blaž Zupan",
	"summary" : "Our entry to this year\u0026rsquo;s largest bioinformatics conference was on the training of single-cell data analytics. We claim that with Orange and its new single-cell RNA analysis add-on, one can assemble a workshop to teach essential concepts from single-cell analytics in a single day.\n \\\nSingle-cell genomics is driven on revolutionary technology from molecular biology that allows us to peek into inner workings of the individual cell. Single-cell datasets feature thousands of genes and cells and benefit from the analysis that integrates this data with other knowledge sources, like those on the classification of genes, lists of pathways, and sets of cell-type markers.",
	"date" : "Jul 25, 2019",
	"frontPageImage" :"/blog_img/2019/scorange-annotation-thumb.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at ISMB/ECCB 2019",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-07-02-data-science-made-easy/",
	"title": "Data Science Made Easy: How To Identify Hate Comments with AI",
	"description": "",
	"content": "The IdeenExpo is a biennial participatory event for children, adolescents and young adults taking place in Hanover, Germany. Companies, research organizations, schools and universities participate to show young people the possibilities of the modern working world and gain their interest in technologies and natural sciences. As a part of one of the biggest research-computing-centers in North Germany the GWDG (Gesellschaft für wissenschaftliche Datenverarbeitung mbh Göttingen) took a part in that event to present the possibilities of Data Science and how its methods can be used in different areas.\nRelated: Text Workshops in Ljubljana\nOur goal was to give the 9th grade students a 60-minute hands-on introduction to some possible real-life use cases. As we were working with Orange3 now for some time, we decided to use it in our workshop, because it has the great benefit of being able to do data analysis without the need to write code, which wouldn’t have worked in a 60 minute workshop.\n \\\nWe started with some introductory talks and discussions about the presence of Big Data in our daily life to get some insights into the existing knowledge of the students. A quick question about who is active in social media made it clear that everyone in that workshop has one or more accounts on social media platforms. Next we asked about hate comments on social media and everyone was aware about that topic as well.\nAfter displaying some hate comments and discussing about how we as humans identify them, we split the group in two parts. One group had then to write hate comments and insults, whereas the other group wrote neutral comments concerning a certain topic we decided on.\nWe then collected that comments in a GoogleSheets Table, labeled them and every student opened Orange3 on his or her laptop. The students were asked to explain in their own words how they would train an “AI” to learn to distinguish between hate and neutral comments and we showed them how this can be translated in an Orange workflow. This way the students already learned that we have to do some preprocessing to filter out uninformative words for example.\n \\\nWith just the tweets we made up in our session we gained a precision value of 0.66 in the first session and after we appended the tweets from the second group, we already gained a value of 0.76. Afterwards the students were asked to made up 4 other tweets the model was not trained on and used the Predictions widget to see how well our model performed. Well, we just got the results we would have thought of, if we would have had to classify them on our own!\nOrange3 made it possible to develop a model for detecting hate comments in just 60 minutes with students, who had no programming skills. Thanks to the Orange Team!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/twitter-police-workshop-picture.jpeg",
	"shortExcerpt" : "How to teach text mining and data science to the 9th grade students in 60 minutes?",
	"longExcerpt" :  "How to teach text mining and data science to the 9th grade students in 60 minutes? With Orange and the analysis of hate speech on social media, of course!" ,
	"author" : "Dr. Sven Bingert &amp; Steffen Rörtgen",
	"summary" : "The IdeenExpo is a biennial participatory event for children, adolescents and young adults taking place in Hanover, Germany. Companies, research organizations, schools and universities participate to show young people the possibilities of the modern working world and gain their interest in technologies and natural sciences. As a part of one of the biggest research-computing-centers in North Germany the GWDG (Gesellschaft für wissenschaftliche Datenverarbeitung mbh Göttingen) took a part in that event to present the possibilities of Data Science and how its methods can be used in different areas.",
	"date" : "Jul 2, 2019",
	"frontPageImage" :"/blog_img/2019/twitter-police-workshop-picture.jpeg",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Science Made Easy: How To Identify Hate Comments with AI",
	"icon" : ""
},
{
    "uri": "/blog/2019/2019-06-28-bled-econference/",
	"title": "Orange at 32nd Bled eConference",
	"description": "",
	"content": "At the invitation of dr. Mirjana Kljajić, we participated in the 32nd Bled eConference. The conference is one of the most important in the region on trends and technologies for electronic communication and this year a short workshop on Data Science with Orange was included in its programme.\n \\\nWe covered many topics, including data loading, visualization, construction of a predictive modeling workflow, exploration of decision trees, overfitting, model scoring and of course finally predicting with cross-validated model on a new data set. One of the classic exercises for such courses includes using the Attrition - Train and Attrition - Predict data sets from the Datasets widget and answering the following questions:\n Find one or two interesting variables in a Box Plot and describe what they show. Use all known models in Test and Score. Which one has the highest classification accuracy? Which three variables are the most important predictors for Logistic Regression? On a new data set, identify the person that is most likely to quit. What would be your recommendations to the HR department, considering the Nomogram?  \n\n\\\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/2019-06-28_bled-small.png",
	"shortExcerpt" : "We held a short workshop for the participants of the 32nd Bled eConference.",
	"longExcerpt" :  "We held a short workshop covering the basics of Data Science for the participants of the 32nd Bled eConference. We focused on predictive modeling and explorations of predictions." ,
	"author" : "Ajda Pretnar",
	"summary" : "At the invitation of dr. Mirjana Kljajić, we participated in the 32nd Bled eConference. The conference is one of the most important in the region on trends and technologies for electronic communication and this year a short workshop on Data Science with Orange was included in its programme.\n \\\nWe covered many topics, including data loading, visualization, construction of a predictive modeling workflow, exploration of decision trees, overfitting, model scoring and of course finally predicting with cross-validated model on a new data set.",
	"date" : "Jun 28, 2019",
	"frontPageImage" :"/blog_img/2019/2019-06-28_bled-small.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at 32nd Bled eConference",
	"icon" : ""
},
{
    "uri": "/blog/gene-expression/",
	"title": "gene expression",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 3, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "gene expression",
	"icon" : ""
},
{
    "uri": "/blog/2019/6/gene-expression-profiles-with-line-plot/",
	"title": "Gene Expression Profiles with Line Plot",
	"description": "",
	"content": "Line Plot is one of our recent additions to the visualization widgets. It shows data profiles, meaning it plots values for all features in the data set. Each data instance in a line plot is a line or a ‘profile’.\nThe widget can show four types of information – individual data profiles (lines), data range, mean profile and error bars. It has the same cool features of other Orange visualizations – it is interactive, meaning you can select a subset of data instances from the plot, it allows grouping by a discrete variable, and it highlights an incoming data subset.\nRelated: Scatter Plot: The Tour\nLet us check a simple example. We will use brown-selected data, which is a data on gene expression of baker’s yeast. To observe gene expression profiles, we will use Line Plot.\n\n\\\n \n\\\nSince the data has class, which represents a function of the gene, Line Plot will automatically group by class variable. It seems like protease, respiratory and ribosome genes have quite distinctive profiles! Let us select the most interesting region in the plot by selecting the zoom tool and dragging across the area of interest. \\\n \n\\\nWe see that spo-mid feature distinguishes really well between protease and two other gene types and that values of protease are normally high for spo-mid.\nAnother thing we can do is select a subset from the plot. If we press the ‘rectangle’ icon on the left, our plot will be automatically resized to the original size. Then we press the ‘arrow’ icon, which will put us back to the selecting mode. Now let us select Lines instead of Range and Mean for display. This will show individual expression profiles. \\\n \n\\\nIf we click and drag across an area of interest, instances under the thick black line will be selected. We can connect, say a Box Plot to the Line Plot and observe the distribution of the selected subset. Unsurprisingly, the genes we have selected are mostly protease. \\\n \n\\\n \n\\\nThis is it. Line Plot is really simple to use and can reveal many interesting things not only for biologists, but for any kind of data analyst. Next week we will talk about how to work with timeseries data in combination with the Line Plot.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/6/3/gene-blog.png",
	"shortExcerpt" : "We show how to explore gene expression profiles with the new Line Plot widget.",
	"longExcerpt" :  "Line Plot shows profiles of data instances – each instance is a line in the plot and its profile are values across all variables in the data. We show how to explore gene expression profiles." ,
	"author" : "Ajda Pretnar",
	"summary" : "Line Plot is one of our recent additions to the visualization widgets. It shows data profiles, meaning it plots values for all features in the data set. Each data instance in a line plot is a line or a \u0026lsquo;profile\u0026rsquo;.\nThe widget can show four types of information – individual data profiles (lines), data range, mean profile and error bars. It has the same cool features of other Orange visualizations – it is interactive, meaning you can select a subset of data instances from the plot, it allows grouping by a discrete variable, and it highlights an incoming data subset.",
	"date" : "Jun 3, 2019",
	"frontPageImage" :"/blog_img/2019/6/3/gene-blog.png",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Gene Expression Profiles with Line Plot",
	"icon" : ""
},
{
    "uri": "/blog/2019/5/18/business-case-studies-with-orange/",
	"title": "Business Case Studies with Orange",
	"description": "",
	"content": "Previous week Blaž, Robert and I visited Wärtsilä in the lovely Dolina near Trieste, Italy. Wärtsilä is one of the leading designers of lifecycle power solutions for the global marine and energy markets and its subsidiary in Trieste is one of the largest Wärtsilä Group engine production plants. We were there to hold a one-day workshop on data mining and machine learning with the aim to identify relevant use cases in business and show how to address them.\n\\\n \n\\\nRelated: Data Mining for Business and Public Administration\nOne such important use case is employee attrition. It is vital for any company to retain its most valuable workers, so they must learn how to identify dissatisfied employees and provide incentive from the to stay. It is easy to construct a workflow in Orange that helps us with this.\nFirst, let us load Attrition – Train data set from the Datasets widget. This is a synthetic data set from IBM Watson that has 1470 instances (employees) and 18 features describing them. Our target variable is Attrition, where Yes means the person left the company and No means it stayed.\n\\\n \n\\\nNow our goal is to construct a predictive model that will successfully predict the likelihood of a person leaving. Let us connect a couple of classifiers and the data set to Test and Score and see which model performs best.\n\\\n \nSeems like Logistic Regression is the winner here, since its AUC score it the highest of the three.\n\\\n \nA great thing about Logistic Regression is that it is interpretable. We can connect the data from Datasets to Logistic Regression and the resulting model from LR to Nomogram. Nomogram shows the top ten features, ranked by their contribution to the final probability of a class.\n\\\n \n\\\nThe length of a line corresponds to the relative importance of the attribute. Seems like recently hired employees are more likely to leave (YearsAtCompany goes towards 0). We also should consider promoting those that haven’t been promoted in a while (YearsSinceLastPromotion goes towards 15) and cut the overtime (OverTime is Yes). Model inspection helps us identify relevant attributes and interpret their values. So useful for HR departments!\n\\\n \n\\\nFinally, we can take new data and predict the likelihood for leaving. Put another Datasets on the canvas and load Attrition – Predict data. This one contains only three instances – say the data for three employees we have forgotten to consider in our training data.\n\\\n \n\\\nSo who is more likely to leave? We obviously cannot afford to promote everyone, because this costs money. We need to optimize our decisions so that we both increase the satisfaction of employees while keep our costs low. This is where we can use predictive modeling. Connect Logistic Regression to Predictions widget. Then connect the second Datasets widget with the new data to Predictions as well.\n\\\n \nSeems like John is most likely to leave. He has been at the company for only a year and he works overtime.\n\\\n \n\\\nThis is something HR department can work with to design proper policies and keep best talent. The same workflow can be used for churn prediction, process optimization and predicting success of a new product.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/5/18/wartsila-blog.jpg",
	"shortExcerpt" : "At the latest workshop we demonstrated how to predict, which employees are most likely to resign in the future.",
	"longExcerpt" :  "At the latest workshop in Italy we taught the participants how to identify relevant business use cases and how to predict, which employees are most likely to resign in the future." ,
	"author" : "Ajda Pretnar",
	"summary" : "Previous week Blaž, Robert and I visited Wärtsilä in the lovely Dolina near Trieste, Italy. Wärtsilä is one of the leading designers of lifecycle power solutions for the global marine and energy markets and its subsidiary in Trieste is one of the largest Wärtsilä Group engine production plants. We were there to hold a one-day workshop on data mining and machine learning with the aim to identify relevant use cases in business and show how to address them.",
	"date" : "May 18, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Business Case Studies with Orange",
	"icon" : ""
},
{
    "uri": "/blog/business-intelligence/",
	"title": "business intelligence",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 18, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "business intelligence",
	"icon" : ""
},
{
    "uri": "/blog/HR/",
	"title": "HR",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 18, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "HR",
	"icon" : ""
},
{
    "uri": "/blog/logistic-regression/",
	"title": "logistic regression",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 18, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "logistic regression",
	"icon" : ""
},
{
    "uri": "/blog/predictive-models/",
	"title": "predictive models",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 18, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "predictive models",
	"icon" : ""
},
{
    "uri": "/blog/GIS/",
	"title": "GIS",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 24, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "GIS",
	"icon" : ""
},
{
    "uri": "/blog/hierarchical-clustering/",
	"title": "hierarchical clustering",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 24, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "hierarchical clustering",
	"icon" : ""
},
{
    "uri": "/blog/2019/4/24/orange-at-gis-ostrava/",
	"title": "Orange at GIS Ostrava",
	"description": "",
	"content": "Ostrava is a city in the north-east of the Czech Republic and is the capital of the Moravian-Silesian Region. GIS Ostrava is a yearly conference organized by Jiří Horák and his team at the Technical University of Ostrava. University has a nice campus with a number of new developments. I have learned that this is the largest university campus in central and eastern Europe, as most of the universities, like mine, are city universities with buildings dispersed around the city.\nDuring the conference, I gave an invited talk on “Data Science for Everyone” and showed how Orange can be used to teach basic data science concepts in a few hours so that the trainee can gain some intuition about what data science is and then, preferably, use the software on their own data. To prove this concept, I gave an example workshop during the next day of the conference. The workshop was also attended by several teachers that are thinking of incorporating Orange within their data science curricula.\n\\\n \n\\\nAdmittedly, there was not much GIS in my presentations, as I – as planned – focused more on data science. But I did include an example of how to project the data in Orange to geographical maps. The example involved the analysis of Human Development Index data and clustering. When projected to the map, the results of clustering could be unexpected if we select only the features that address quality of life: check out the map below and try to figure out what is wrong.\n\\\n \n\\\nHere, I would like to thank Igor Ivan and Jiří Horák for the invitation, and their group and specifically Michal Kacmarik for the hospitality.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/4/24/ostrava-workshop.jpg",
	"shortExcerpt" : "In late March 2019, we have been invited to present Orange during a GIS Ostrava conference.",
	"longExcerpt" :  "In late March 2019, we have been invited to present Orange during a GIS Ostrava conference. We have shown how to work with geospatial data in Orange." ,
	"author" : "Blaz Zupan",
	"summary" : "Ostrava is a city in the north-east of the Czech Republic and is the capital of the Moravian-Silesian Region. GIS Ostrava is a yearly conference organized by Jiří Horák and his team at the Technical University of Ostrava. University has a nice campus with a number of new developments. I have learned that this is the largest university campus in central and eastern Europe, as most of the universities, like mine, are city universities with buildings dispersed around the city.",
	"date" : "Apr 24, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at GIS Ostrava",
	"icon" : ""
},
{
    "uri": "/blog/release/",
	"title": "release",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 8, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "release",
	"icon" : ""
},
{
    "uri": "/blog/Status-Bar/",
	"title": "Status Bar",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 8, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "Status Bar",
	"icon" : ""
},
{
    "uri": "/blog/2019/3/8/the-changing-status-bar/",
	"title": "The Changing Status Bar",
	"description": "",
	"content": "Every week on Friday, when the core team of Orange developers meets, we are designing new improvements of Orange’s graphical interface. This time, it was the status bar. Well, actually, it was the status bar quite a while ago and required the change of the core widget library, but it is materializing these days and you will see the changes in the next release.\nConsider the Neighbors widget. The widget considers the input data and reference data items, and outputs instance form input data that are most similar to the references. Like, if the dolphin is a reference, we would like to know which are the three most similar animals. But this is not what want I wanted to write about. I would only like to say that we are making a slight change in the user interface. Below is the Neighbors widget in the current release of Orange, and the upcoming one.\n\\\n \n\\\nSee the difference? We are getting rid of the infobox on the top of the control tab, and moving it to the status bar. In the infobox widgets typically display what is in their input and what is on the output after the data has been processed. Moving this information to the status bar will make widgets more compact and less cluttered. We will similarly change the infoboxes in this way in all of the widgets.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/3/8/changing-blog.png",
	"shortExcerpt" : "We are constantly optimizing Orange&#39;s look-and-feel. New features in the status bar will simplify the user interface.",
	"longExcerpt" :  "We are constantly optimizing Orange's look-and-feel. New features in the status bar will simplify the user interface. We are getting rid of the infobox on the top of the control tab, and moving it to the status bar." ,
	"author" : "Blaz Zupan",
	"summary" : "Every week on Friday, when the core team of Orange developers meets, we are designing new improvements of Orange\u0026rsquo;s graphical interface. This time, it was the status bar. Well, actually, it was the status bar quite a while ago and required the change of the core widget library, but it is materializing these days and you will see the changes in the next release.\nConsider the Neighbors widget. The widget considers the input data and reference data items, and outputs instance form input data that are most similar to the references.",
	"date" : "Mar 8, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "The Changing Status Bar",
	"icon" : ""
},
{
    "uri": "/blog/gene-ontology/",
	"title": "gene ontology",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 1, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "gene ontology",
	"icon" : ""
},
{
    "uri": "/blog/genomics/",
	"title": "genomics",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 1, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "genomics",
	"icon" : ""
},
{
    "uri": "/blog/RNA-seq/",
	"title": "RNA-seq",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 1, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "RNA-seq",
	"icon" : ""
},
{
    "uri": "/blog/scOrange/",
	"title": "scOrange",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 1, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "scOrange",
	"icon" : ""
},
{
    "uri": "/blog/single-cell/",
	"title": "single cell",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 1, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "single cell",
	"icon" : ""
},
{
    "uri": "/blog/2019/3/1/single-cell-data-science-for-everyone/",
	"title": "Single-Cell Data Science for Everyone",
	"description": "",
	"content": "Molecular biologists have in the past twenty years invented technologies that can collect abundant experimental data. One such technique is single-cell RNA-seq, which, very simplified, can measure the activity of genes in possibly large collections of cells. The interpretation of such data can tell us about the heterogeneity of cells, cell types, or provide information on their development.\nTypical analysis toolboxes for single-cell data are available in R and Python and, most notably, include Seurat and scanpy, but they lack interactive visualizations and simplicity of Orange. Since the fall of 2017, we have been developing an extension of Orange, which is now (almost) ready. It has even been packed into its own installer. The first real test of the software was in early 2018 through a one day workshop at Janelia Research Campus. On March 6, and with a much more refined version of the software, we have now repeated the hands-on workshop at the University of Pavia.\n\\\n \n\\\nThe five-hour workshop covered both the essentials of data analysis and single cell analytics. The topics included data preprocessing, clustering, and two-dimensional embedding, as well as working with marker genes, differential expression analysis, and interpretation of clusters through gene ontology analysis.\nI want to thank Prof. Dr. Riccardo Bellazzi and his team for the organization, and Erasmus program for financial support. I have been a frequent guest to Pavia, and learn something new every time I am there. Besides meeting new students and colleagues that attended the workshop and hearing about their data analysis challenges, this time I have also learned about a dish I had never had before in all my Italian travels. For one of the dinners (thank you, Michela) we had Pizzoccheri. Simply great!\n\\\n \n\\\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/3/1/2019-pavia-blog.jpg",
	"shortExcerpt" : "We have been in Pavia, Italy, to carry out an introductory workshop on single-cell data science.",
	"longExcerpt" :  "We have been in Pavia, Italy, to carry out a five-hour workshop covered both the essentials of data analysis and single cell analytics. The topics included working with marker genes, differential expression analysis, and interpretation of clusters through gene ontology analysis." ,
	"author" : "Blaz Zupan",
	"summary" : "Molecular biologists have in the past twenty years invented technologies that can collect abundant experimental data. One such technique is single-cell RNA-seq, which, very simplified, can measure the activity of genes in possibly large collections of cells. The interpretation of such data can tell us about the heterogeneity of cells, cell types, or provide information on their development.\nTypical analysis toolboxes for single-cell data are available in R and Python and, most notably, include Seurat and scanpy, but they lack interactive visualizations and simplicity of Orange.",
	"date" : "Mar 1, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Single-Cell Data Science for Everyone",
	"icon" : ""
},
{
    "uri": "/blog/cross-validation/",
	"title": "cross validation",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "cross validation",
	"icon" : ""
},
{
    "uri": "/blog/leave-one-out/",
	"title": "leave one out",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "leave one out",
	"icon" : ""
},
{
    "uri": "/blog/LOO/",
	"title": "LOO",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "LOO",
	"icon" : ""
},
{
    "uri": "/blog/test-and-score/",
	"title": "test and score",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "test and score",
	"icon" : ""
},
{
    "uri": "/blog/2019/1/28/the-mystery-of-test-and-score/",
	"title": "The Mystery of Test &amp;amp; Score",
	"description": "",
	"content": "Test \u0026 Score is surely one the most used widgets in Orange. Fun fact: it is the fourth in popularity, right after Data Table, File and Scatter Plot. So let us dive into the nuts and bolts of the Test \u0026 Score widget.\nThe widget generally accepts two inputs – Data and Learner. Data is the data set that we will be using for modeling, say, iris.tab that is already pre-loaded in the File widget. Learner is any kind of learning algorithm, for example, Logistic Regression. You can only use those learners that support your type of task. If you wish to do classification, you cannot use Linear Regression and for regression you cannot use Logistic Regression. Most other learners support both tasks. You can connect more than one learner to Test \u0026 Score.\n\\\n \n\\\nTest \u0026 Score will now use each connected Learner and the Data to build a predictive model. Models can be built in different ways. The most typical procedure is cross validation, which splits the data into k folds and uses k – 1 folds for training and the remaining fold for testing. This procedure is repeated, so that each fold has been used for testing exactly once. Test \u0026 Score will then report on the average accuracy of the model.\nYou can also use Random Sampling, which will split the data into two sets with predefined proportions (e.g. 66% : 34%), build a model on the first set and test it on the second. This is similar to CV, except that each data instance can be used more than once for testing.\nLeave one out is again very similar to the above two methods, but it only takes one data instance for testing each time. If you have a 100 data instances, then 99 will be used for training and 1 for testing, and the procedure will be repeated a 100 times until every data instance was used once for testing. As you can imagine, this is a very time-intensive procedure and it is recommended for smaller data sets only.\nTest on train data uses the whole data set for training and again the same data for testing. Because of overfitting, this will usually overestimate the performance! Test on test data requires an additional data input (Test Data) and allows the user to control both data sets (training and testing) used for evaluation.\nFinally, you can also use cross validation by feature. Sometimes, you would have pre-defined folds for a procedure, that you wish to replicate. Then you can use Cross validation by feature to ensure data instances are split into the same folds every time. Just make sure the feature you are using for defining folds is a categorical variable and located in meta attributes.\nAnother scenario is when you have several examples from the same object, for example several measurements of the same patient or several images of the same plant. Then you absolutely want to make sure that all data instances for a particular object are in the same fold. Otherwise, your model would probably report severely overfitted scores.\n\\\n \n\\\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/1/28/ts-blog.png",
	"shortExcerpt" : "Test &amp; Score widget is used for evaluating model performance, but what do the methods do? We explain each of them in a few lines.",
	"longExcerpt" :  "Test \u0026 Score widget is used for evaluating model performance, but what do the methods do? We explain cross validation, random sampling, leave one out and cross validation by feature in a few lines." ,
	"author" : "Ajda Pretnar",
	"summary" : "Test \u0026amp; Score is surely one the most used widgets in Orange. Fun fact: it is the fourth in popularity, right after Data Table, File and Scatter Plot. So let us dive into the nuts and bolts of the Test \u0026amp; Score widget.\nThe widget generally accepts two inputs – Data and Learner. Data is the data set that we will be using for modeling, say, iris.tab that is already pre-loaded in the File widget.",
	"date" : "Jan 28, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "The Mystery of Test &amp; Score",
	"icon" : ""
},
{
    "uri": "/blog/correlations/",
	"title": "correlations",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 4, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "correlations",
	"icon" : ""
},
{
    "uri": "/blog/2019/1/4/how-to-abuse-p-values-in-correlations/",
	"title": "How to Abuse p-Values in Correlations",
	"description": "",
	"content": "In a parallel universe, not so far from ours, Orange’s Correlation widget looks like this.\n\\\n \n\\\nQuite similar to ours, except that this one shows p-values instead of correlation coefficients. Which is actually better, isn’t it? I mean, we have all attended Statistics 101, and we know that you can never trust correlation coefficients without looking at p-values to check that these correlations are real, right? So why on Earth doesn’t Orange show them?\nFirst a side note. It was Christmas not long ago. Let’s call a ceasefire on the frequentist vs. Bayesian war. Let us, for Christ’s sake, pretend, pardon, agree that null-hypothesis testing is not wrong per se.\nThe mantra of null-hypothesis significance testing goes like this:\n Form hypothesis. Collect data. Test hypothesis.  In contrast, the parallel-universe Correlation widget is (ab)used like this:\n Collect data. Test all possible hypotheses. Cherry pick those that are confirmed.  This is like the Texas sharpshooter who fires first and then draws targets around the shots. You should never formulate hypothesis based on some data and then use this same data to prove it. Because it usually (surprise!) works.\n\\\n \n\\\nBack to the above snapshot. It shows correlations between 100 vegetables based on 100 different measurements (Ca and Mg content, their consumption in Finland, number of mentions in Star Trek DS9 series, likelihood of finding it on the Mars, and so forth). In other words, it’s all made it up. Just a 100×100 matrix of random numbers with column labels from the simple Wikipedia list of vegetables. Yet the similarity between mung bean and sunchokes surely cannot be dismissed (p \u003c 0.001). Those who like bell pepper should try cilantro, too, because it’s basically one and the same thing (p = 0.001). And I honestly can’t tell black bean from wasabi (p = 0.001).\nHere are the p-values for the top 100 most correlated pairs.\nimport numpy as np import scipy as sp a = np.random.random((100, 100)) sorted(stats.pearsonr(a[i], a[j])[1] for i in range(100) for j in range(i))[:100] [0.0002774329730584203, 0.0004158786523819104, 0.0005008536192579852, 0.0007211022164265075, 0.0008268675086438253, 0.0010265740674904762, (...91 values omitted to reduce the nonsense) 0.01844720610938738, 0.018465602922746942, 0.018662079618069056] First 100 correlations are highly significant.  To learn a lesson we may have failed to grasp at the NHST 101 class, consider that there are 100 * 99 / 2 pairs. What is the significance of the pair at 5-th percentile?\ncorrelations = sorted(stats.pearsonr(a[i], a[j])[1] for i in range(100) for j in range(i)) npairs = 100 * 99 / 2 print(correlations[int(pairs * 0.05)] 0.0496868751692227  Roughly 0.05. This is exactly what should have happened, because:\ncorrelations[int(npairs * 0.10)] 0.10004180592217532 correlations[int(npairs * 0.15)] 0.15236602574520097 correlations[int(npairs * 0.30)] 0.3026816170584785  This proves only that p-values for the Pearson correlation coefficient are well calibrated (and that Mersenne twister that is used to generate random numbers in numpy works well). In theory, the p-value for a certain statistics (like Pearson’s r) is the probability of getting such or even more extreme value if the null-hypothesis (of no correlation, in our case) is actually true. 5 % of random hypotheses should have a p-value below 0.05, 10 % a value below 10, and 23 % a value below 23.\nImagine what they can do with the Correlations widget in the parallel universe! They compute correlations between all pairs, print out the first 5 % of them and start writing a paper without bothering to look at p-values at all. They know they should be statistically significant even if the data is random.\nWhich is precisely the reason why our widget must not compute p-values: because people would use it for Texas sharpshooting. P-values make sense only in the context of the proper NHST procedure (still pretending for the sake of Christmas ceasefire). They cannot be computed using the data on which they were found.\nIf so, why do we have the Correlation widget at all if it’s results are unpublishable? We can use it to find highly correlated pairs in a data sample. But we can’t just attach p-values to them and publish them. By finding these pairs (with assistance of Correlation widget) we just formulate hypotheses. This is only step 1 of the enshrined NHST procedure. We can’t skip the other two: the next step is to collect some new data (existing data won’t do!) and then use it to test the hypotheses (step 3).\nFollowing this procedure doesn’t save us from data dredging. There are still plenty of ways to cheat. It is the most tempting to select the first 100 most correlated pairs (or, actually, any 100 pairs), (re)compute correlations on some new data and publish the top 5 % of these pairs. The official solution for this is a patchwork of various corrections for multiple hypotheses testing, but… Well, they don’t work, but we should say no more here. You know, Christmas ceasefire.\n",
	"image" : "",
	"thumbImage" : "/blog_img/2019/1/4/correlations.png",
	"shortExcerpt" : "Why doesn&#39;t Orange show p-values for correlations coefficients? To save you from data dredging and Texas sharpshooter fallacy...",
	"longExcerpt" :  "We have all attended Statistics 101, and we know that you can never trust correlation coefficients without looking at p-values to check that these correlations are real, right? So why on Earth doesn’t Orange show them?" ,
	"author" : "Ajda Pretnar",
	"summary" : "In a parallel universe, not so far from ours, Orange’s Correlation widget looks like this.\n\\\n \n\\\nQuite similar to ours, except that this one shows p-values instead of correlation coefficients. Which is actually better, isn’t it? I mean, we have all attended Statistics 101, and we know that you can never trust correlation coefficients without looking at p-values to check that these correlations are real, right? So why on Earth doesn’t Orange show them?",
	"date" : "Jan 4, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "How to Abuse p-Values in Correlations",
	"icon" : ""
},
{
    "uri": "/blog/NHTS/",
	"title": "NHTS",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 4, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "NHTS",
	"icon" : ""
},
{
    "uri": "/blog/null-hypothesis/",
	"title": "null hypothesis",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 4, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "null hypothesis",
	"icon" : ""
},
{
    "uri": "/blog/statistics/",
	"title": "statistics",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 4, 2019",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "statistics",
	"icon" : ""
},
{
    "uri": "/blog/interactive-visualization/",
	"title": "interactive visualization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "interactive visualization",
	"icon" : ""
},
{
    "uri": "/blog/scatter-plot/",
	"title": "scatter plot",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "scatter plot",
	"icon" : ""
},
{
    "uri": "/blog/2018/12/21/scatter-plots-the-tour/",
	"title": "Scatter Plots: the Tour",
	"description": "",
	"content": "Scatter plots are surely one of the best loved visualizations in Orange. Very often, when we teach, people go back to scatter plots over and over again to see their data. We took people’s love for scatter plots to the heart and we redesigned them a bit to make them even more friendly.\nOur favorite still remains the Informative Projections button. This button helps you find interesting visualizations from all the combinations of your data variables. But what does interesting mean? Well, let us look at an example. Which of the two visualizations tells you more about the data?\n\\\n    \u003c/td\u003e \u003ctd style=\"padding:5px\"\u003e \u003ca href=\"/blog_img/2018/12/21/iris2.png\" class=\"blog-image_\"\u003e \u003cimg src=\"/blog_img/2018/12/21/iris2.png\" /\u003e    \u003c/td\u003e \u003c/tr\u003e \u003c/table\u003e   \\ We’d say it is the right one. Why? Because now we know that the combination of petal length and petal width nicely separates the classes!\nOf course, Informative Projections button will only work when you have set a class (target) variable.\nIn scatter plot, you can set also the color of the data points (class is selected by default), the size of the points and the shape. This means you can add three new layers of information to your data, but we warn you not to overuse them. This usually looks very incomprehensible, even though it packs a lot of information.\n\\\n \n\\\nYou might notice, that in the current version of Orange, you can no longer select discrete attributes in Scatter Plot. This is entirely intentional. Scatter plots are best at showing the relationship between two numeric variables, such as in the two examples above. Categorical variables are much better represented with Box Plots, histograms (in Distributions) or in Mosaic Display.\n\\\n \n\\\nAbove, we have presented the same information for titanic data set in different visualizations, that are particularly suitable for categorical variables.\nScatter plot also enables so cool tricks. Just like in most visualizations in Orange, I can select a part of the data and observe the subset downstream. Or the other way around. I have a particular subset I wish to observe and I can pass it to Scatter Plot widget, which will highlight selected data instances.\n\\\n \n\\\nThis is also true for all other point-based visualizations in Orange, namely t-SNE, MDS, Radviz, Freeviz, and Linear Projection.\nYou can see there are many great thing you can do with Scatter Plot. Finally, we have added a nice touch to the visualization. \\\n \n\\\nYes, setting the size of the attribute is now animated! 🙂\nHappy holidays, everyone!\n",
	"image" : "",
	"thumbImage" : "/blog_img/2018/12/21/scatter-blog.png",
	"shortExcerpt" : "Scatter Plot has recently been renovated and it is time to present some essential tricks for working with the widget!",
	"longExcerpt" :  "Scatter Plot has recently been renovated (under the hood and in GUI), so now it is time to present some essential tricks for working with the cool visualization!" ,
	"author" : "Ajda Pretnar",
	"summary" : "Scatter plots are surely one of the best loved visualizations in Orange. Very often, when we teach, people go back to scatter plots over and over again to see their data. We took people’s love for scatter plots to the heart and we redesigned them a bit to make them even more friendly.\nOur favorite still remains the Informative Projections button. This button helps you find interesting visualizations from all the combinations of your data variables.",
	"date" : "Dec 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Scatter Plots: the Tour",
	"icon" : ""
},
{
    "uri": "/blog/analysis/",
	"title": "analysis",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 22, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "analysis",
	"icon" : ""
},
{
    "uri": "/blog/interface/",
	"title": "interface",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 22, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "interface",
	"icon" : ""
},
{
    "uri": "/blog/2018/11/22/orange-is-getting-smarter/",
	"title": "Orange is Getting Smarter",
	"description": "",
	"content": "In the past few months, Orange has been getting smarter and sleeker.\nSince version 3.15.0, Orange remembers which distinct widgets users like to connect, adjusting the sorting on the widget search menu accordingly. Additionally, there is a new look for the Edit Links window coming soon.\nOrange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.\nWord cloud of widget popularity in Orange.\nThe information is collected anonymously for the users that opted-in. We will use this data to improve the widget suggestion system. Furthermore, the data provides us the first insight into how users interact with Orange. Let’s see what we’ve found out from the data recorded in the past few weeks.\nThere are four different ways of adding a widget to the canvas,\n clicking it in the sidebar, dragging it from the sidebar, searching for it by right-clicking on canvas, extending the workflow by dragging the communication channel from a widget.  A workflow extend action.\nAmong Orange users, the most popular way of adding a new widget is by dragging the communication line from the output widget – we think this is the most efficient way of using Orange too. However, the patterns vary among different widgets.\nHow users add widgets to canvas, from 20,775 add widget events.\nUsers tend to add root nodes such as File via a click or drag from the sidebar, while adding leaf nodes such as Data Table via extension from another widget.\nHow users add File to canvas.\n How users add Data Table to canvas.\n    The widget popularity contest goes to: Data Table! Rightfully so, one should always check their data with Data Table.\nWidget popularity visualization in Box Plot.\n52% of sessions tracked consisted of no widgets being added (the application just being opened and closed). While some people might really like watching the loading screen, most of these are likely due to the fact that usage is not tracked until the user explicitly opts in.\nEach bit of collected data comes at a cost to the privacy of the user. Care was put into minimizing the intrusiveness of data collection methods, while maximizing the usefulness of the collected data.\nInitially, widget addition events were planned to include a ‘time since application start’ value, in order to be able to plot a user’s actions as a function of time. While this would be cool, it was ultimately decided that its usefulness is outweighed by the privacy cost to users.\nFor the keen, data is gathered per canvas session, in the following structure:\n  Date\n  Orange version\n  Operating system\n  Widget addition events, each entailing:\n Widget name Type of addition (Click, Drag, Search or Extend) (Other widget name), if type is Extend (Query), if type is Search or Extend    ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "Orange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.",
	"longExcerpt" :  "In the past few months, Orange has been getting smarter and sleeker.\nSince version 3.15.0, Orange remembers which distinct widgets users like to connect, adjusting the sorting on the widget search menu accordingly. Additionally, there is a new look for the Edit Links window coming soon.\nOrange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.\nWord cloud of widget popularity in Orange." ,
	"author" : "IRGOLIC",
	"summary" : "In the past few months, Orange has been getting smarter and sleeker.\nSince version 3.15.0, Orange remembers which distinct widgets users like to connect, adjusting the sorting on the widget search menu accordingly. Additionally, there is a new look for the Edit Links window coming soon.\nOrange recently implemented a basic form of opt-in usage tracking, specifically targeting how users add widgets to the canvas.\nWord cloud of widget popularity in Orange.",
	"date" : "Nov 22, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange is Getting Smarter",
	"icon" : ""
},
{
    "uri": "/blog/orange3/",
	"title": "orange3",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 22, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "orange3",
	"icon" : ""
},
{
    "uri": "/blog/2018/11/06/data-mining-for-anthropologists/",
	"title": "Data Mining for Anthropologists?",
	"description": "",
	"content": "This weekend we were in Lisbon, Portugal, at the Why the World Needs Anthropologists conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry. I was there to hold a workshop on Data Ethnography, an approach that tries to combine methods from data science and anthropology into a fruitful interdisciplinary mix!\nData Ethnography workshop at this year’s Why the World Needs Anthropologists conference.\nData ethnography is a novel methodological approach that tries to view social phenomena from two different points of view - qualitative and quantitative. The quantitative approach is using data mining and machine learning methods on anthropological data (say from sensors, wearables, social media, online fora, field notes and so on) trying to find interesting patterns and novel information. The qualitative approach uses ethnography to substantiate the analytical findings with context, motivations, values, and other external data to provide a complete account of the studied phenomenon.\nAt the workshop, I presented a couple of approaches I use in my own research, namely text mining, clustering, visualization of patterns, image analytics, and predictive modeling. Data ethnography can be used, not only in its native field of computational anthropology, but also in museology, digital anthropology, medical anthropology, and folkloristics (the list is probably not exhaustive). There are so many options just waiting for the researchers to dig in!\nRelated: Text Analysis Workshop at Digital Humanities 2017\nHowever, having data- and tech-savvy anthropologists does not only benefit the research, but opens a platform for discussing the ethics of data science, human relationships with technology, and overcoming model bias. Hopefully, the workshop inspired some of the participants to join me on a journey through the amazing expanses of data science.\nTo get you inspired, here are two contributions that present some option for computational anthropological research: Data Mining Workspace Sensors: A New Approach to Anthropology and Power of Algorithms for Cultural Heritage Classification: The Case of Slovenian Hayracks.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "This weekend we were in Lisbon, Portugal, at the [Why the World Needs Anthropologists](https://www.applied-anthropology.com/) conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry.",
	"longExcerpt" :  "This weekend we were in Lisbon, Portugal, at the Why the World Needs Anthropologists conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry. I was there to hold a workshop on Data Ethnography, an approach that tries to combine methods from data science and anthropology into a fruitful interdisciplinary mix!\nData Ethnography workshop at this year\u0026rsquo;s Why the World Needs Anthropologists conference." ,
	"author" : "AJDA",
	"summary" : "This weekend we were in Lisbon, Portugal, at the Why the World Needs Anthropologists conference, an event that focuses on applied anthropology, design, and how soft skills can greatly benefit the industry. I was there to hold a workshop on Data Ethnography, an approach that tries to combine methods from data science and anthropology into a fruitful interdisciplinary mix!\nData Ethnography workshop at this year\u0026rsquo;s Why the World Needs Anthropologists conference.",
	"date" : "Nov 6, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining for Anthropologists?",
	"icon" : ""
},
{
    "uri": "/blog/2018/10/05/orange-now-speaks-50-languages/",
	"title": "Orange Now Speaks 50 Languages",
	"description": "",
	"content": "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian… Language support was most evident in the list of stopwords, normalization and POS tagging.\nRelated: Text Workshops in Ljubljana\nStopwords come from NLTK library, so we can only offer whatever is available there. However, TF-IDF already implicitly considers stopwords, so the functionality is already implemented. For POS tagging, we would rely on Stanford POS tagger, that already has pre-trained models available.\nThe main issue was with normalization. While English can do without lemmatization and stemming for simple tasks, morphologically rich languages, such as Slovenian, perform poorly on un-normalized tokens. Cases and declensions present a problem for natural language processing, so we wanted to provide a tool for normalization in many different languages. Luckily, we found UDPipe, a Czech initiative that offers trained lemmatization models for 50 languages. UDPipe is actually a preprocessing pipeline and we are already thinking about how to bring all of its functionality to Orange, but let us talk a bit about the recent improvements for normalization.\nLet us load a simple corpus from Corpus widget, say grimm-tales-selected.tab that contain 44 tales from the Grimm Brothers. Now, pass them through Preprocess Text and keep just the defaults, namely lowercase transformation, tokenization by words, and removal of stopwords. Here we see that we have came as quite a frequent word and come as a bit less frequent. But semantically, they are the same word from the verb to come. Shouldn’t we consider them as one word?\nWe can. This is what normalization does - it transforms all words into their lemmas or basic grammatical form. Came and come will become come, sons and son will become son, pretty and prettier will become pretty. This will result in less tokens that capture the text better, semantically speaking.\nWe can see that came became come with 435 counts. Went became go. Said became say. And so on. As we said, this doesn’t work only on verbs, but on all word forms.\nOne thing to note here. UDPipe has an internal tokenizer, that works with sentences instead of tokens. You can enable it by selecting UDPipe tokenizer option. What is the difference? A quicker version would be to tokenize all the words and just look up their lemma. But sometimes this can be wrong. Consider the sentence:\nI am wearing a tie to work.\nNow the word tie is obviously a piece of clothing, which is indicated by the word wearing before it. But tie alone can also be the verb to tie. So the UDPipe tokenizer will consider the entire sentence and correctly lemmatize this word, while lemmatization on regular tokens might not. While UDPipe works better, it is also slower, so you might want to work with regular tokenization to speed up the analysis.\nIn Preprocess Text, you turn on the Normalization button on the right, then select UDPipe Lemmatizer and select the language you wish to use. Finally, if you wish to go with the better albeit slower UDPipe tokenizer, tick the UDPipe tokenizer box.\nFinally, UDPipe does not remove punctuation, so you might end up with words like rose. and away., with the full stop at the end. This you can fix with using regular tokenization and also by select the Regex option in Filtering, which will remove pure punctuation.\nFinal workflow, where we compared the results of no normalization and UDPipe normalization in a word cloud.\nThis is it. UDPipe contains lemmatization models for 50 languages and only when you click on a particular language in the Language option, will the resource be loaded, so your computer won’t be flooded with models for languages you won’t ever use. The installation of UDPipe could also be a little tricky, but after some initial obstacles, we have managed to prepare packages for both pip (OSX and Linux) and conda (Windows).\nWe hope you enjoy the new possibilities of a freshly multilingual Orange!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian... ",
	"longExcerpt" :  "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian\u0026hellip; Language support was most evident in the list of stopwords, normalization and POS tagging.\nRelated: Text Workshops in Ljubljana\nStopwords come from NLTK library, so we can only offer whatever is available there." ,
	"author" : "AJDA",
	"summary" : "In the past couple of weeks we have been working hard on introducing a better language support for the Text add-on. Until recently, Orange supported only a limited number of languages, mostly English and some bigger languages, such as Spanish, German, Arabic, Russian\u0026hellip; Language support was most evident in the list of stopwords, normalization and POS tagging.\nRelated: Text Workshops in Ljubljana\nStopwords come from NLTK library, so we can only offer whatever is available there.",
	"date" : "Oct 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange Now Speaks 50 Languages",
	"icon" : ""
},
{
    "uri": "/blog/preprocessing/",
	"title": "preprocessing",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "preprocessing",
	"icon" : ""
},
{
    "uri": "/blog/infraorange/",
	"title": "infraorange",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "infraorange",
	"icon" : ""
},
{
    "uri": "/blog/infrared-spectra/",
	"title": "infrared spectra",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "infrared spectra",
	"icon" : ""
},
{
    "uri": "/blog/2018/09/21/orange-in-space/",
	"title": "Orange in Space",
	"description": "",
	"content": "Did you know that Orange has already been to space? Rosario Brunetto (IAS-Orsay, France) has been working on the analysis of infrared images of asteroid Ryugu as a member of the JAXA Hayabusa2 team. The Hayabusa2 asteroid sample-return mission aims to retrieve data and samples from the near-Earth Ryugu asteroid and analyze its composition. Hayabusa2 arrived at Ryugu on June 27 and while the spacecraft will return to Earth with a sample only in late 2020, the mission already started collecting and sending back the data. And of course, a part of the analysis of Hayabusa’s space data has been done in Orange!\nAn image of the asteroid Ryugu acquired by the Hayabusa2 (©JAXA).\nWithin the Hayabusa2 project, near-infrared spectral data will be collected in three series: the first part is the macro data from remote sensing measurements that are being collected at different altitudes from the asteroid by the Japanese spectrometer NIRS3 (©JAXA). The second part is surface infrared imaging at the micron scale that will soon be performed (October 2018) by the French MicrOmega instrument on the lander MASCOT (DLR-CNES). The third part are the samples that will be analyzed upon return. Among the techniques that will be used in different laboratories around the world in 2021 to analyze the returned samples are the hyperspectral imaging and micro-tomography with an infrared imaging FPA microscope, that will be performed by the IAS team at SMIS-SOLEIL. This means the data will contain satellite spectral images as well as microscope measurements.\nDr. Brunetto is currently working with the first part of the data, namely the macro hyperspectral images of the asteroid. Several tens of thousands of spectra over 70 spectral channels have already been acquired. The main goal of this initial exploration was to constrain the surface composition.\nOnce the data was preprocessed and cleaned in Python, separate surface regions were extracted in Orange with k-Means and PCA and plotted with the HyperSpectra widget, which comes as a part of the Spectroscopy package. So why was Orange chosen over other tools? Dr. Brunetto says Orange is an easy and friendly tool for complicated things, such as exploring the compositional diversity of the asteroid at the different scales. There are many clustering techniques he can use in Orange and he likes how he can interactively change the number of clusters and the changes immediately show in the plot. This enables the researchers to determine the level of granularity of the analysis, while they can also immediately inspect how each cluster looks like in a hyperspectra plot.\nMoreover, one can quickly test methods and visualize the effects and at the same time have a good overview of the workflow. Workflows can also be reused once the new data comes in or, if the pipeline is standard, used on a completely different data set!\nA simple workflow for the analysis of spectral data. 😁 A great thing about Orange is that you can label parts of the workflow and explore a different aspect of the data in each branch!\nWe would of course love to show you the results of the asteroid analysis, but as the project is still ongoing, the data is not yet available to the public. Instead, we asked Zélia Dionnet, dr. Brunetto’s PhD student, to share the results of her work on the organic and mineralogic heterogeneity of the Paris meteorite, which were already published.\nShe analyzed the composition of the Paris meteorite, which was discovered in 2008 in a statue. The story of how the meteorite was found is quite interesting in itself, but we wanted to know more on how the sample was analyzed in Orange. Dionnet had a slightly larger data set, with 16,000 spectra and 1600 wavenumbers. Just like dr. Brunetto, she used k-Means to discover interesting regions in the sample and Hyperspectra widget to plot the results.\nk-Means clusters plotted in the HyperSpectra widget.\nAt the top, you can see a 2D map of the meteorite sample showing the distribution of the clusters that were identified with k-Means. At the bottom, you see cluster averages for the spectra. The green region is the most interesting one and it shows crystalline minerals, which formed billions of years ago as the hydrothermal processes in the asteroid parent body of the meteorite turned amorphous silicates into phyllosilicates. The purple, on the contrary, shows different micro-sized minerals.\nThis is how to easily identify the compositional structure of samples with just a couple of widgets. Orange seems to love going to space and can’t wait to get its hands dirty with more astro-data!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Did you know that Orange has already been to space? Rosario Brunetto (IAS-Orsay, France) has been working on the analysis of infrared images of asteroid Ryugu as a member of the JAXA Hayabusa2 team. The Hayabusa2 asteroid sample-return mission aims to retrieve data and samples from the near-Earth Ryugu asteroid and analyze its composition. Hayabusa2 arrived at Ryugu on June 27 and while the spacecraft will return to Earth with a sample only in late 2020, the mission already started collecting and sending back the data." ,
	"author" : "AJDA",
	"summary" : "Did you know that Orange has already been to space? Rosario Brunetto (IAS-Orsay, France) has been working on the analysis of infrared images of asteroid Ryugu as a member of the JAXA Hayabusa2 team. The Hayabusa2 asteroid sample-return mission aims to retrieve data and samples from the near-Earth Ryugu asteroid and analyze its composition. Hayabusa2 arrived at Ryugu on June 27 and while the spacecraft will return to Earth with a sample only in late 2020, the mission already started collecting and sending back the data.",
	"date" : "Sep 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange in Space",
	"icon" : ""
},
{
    "uri": "/blog/python/",
	"title": "python",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "python",
	"icon" : ""
},
{
    "uri": "/blog/spectroscopy/",
	"title": "spectroscopy",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "spectroscopy",
	"icon" : ""
},
{
    "uri": "/blog/2018/09/11/text-workshops-in-ljubljana/",
	"title": "Text Workshops in Ljubljana",
	"description": "",
	"content": "In the past month, we had two workshops that focused on text mining. The first one, Faksi v praksi, was organized by the University of Ljubljana Career Centers, where high school students learned about what we do at the Faculty of Computer and Information Science. We taught them what text mining is and how to group a collection of documents in Orange. The second one took on a more serious note, as the public sector employees joined us for the third set of workshops from the Ministry of Public Affairs. This time, we did not only cluster documents, but also built predictive models, explored predictions in nomogram, plotted documents on a map and discovered how to find the emotion in a tweet.\nThese workshops gave us a lot of incentive to improve the Text add-on. We really wanted to support more languages and add extra functionalities to widgets. In the upcoming week, we will release the 0.5.0 version, which introduces support for Slovenian in Sentiment Analysis widget, adds concordance output option to Concordances and, most importantly, implements UDPipe lemmatization, which means Orange will now support about 50 languages! Well, at least for normalization. 😇\nToday, we will briefly introduce sentiment analysis for Slovenian. We have added the KKS 1.001 opinion corpus of Slovene web commentaries, which is a part of the CLARIN infrastructure. You can access it in the Corpus widget. Go to Browse documentation corpora and look for slo-opinion-corpus.tab. Let’s have a quick view in a Corpus Viewer.\nThe data comes from comment sections of Slovenian online media and contains a fairly expressive language. Let us observe, whether a post is negative or positive. We will use Sentiment Analysis widget and select the Liu Hu method for Slovenian. This is a dictionary based method, where the algorithm sums the positive words and subtracts the sum of negative words. This gives a final score of the post.\nWe will have to adjust the attributes for a nicer view in a Select Columns widget. Remove all attributes other than sentiment.\nFinally, we can observe the results in a Heat Map. The blue lines are the negative posts, while the yellow ones are positive. Let us select the most positive tweets and see, what they are about.\nLooks like Slovenians are happy, when petrol gets cheaper and sports(wo)men are winning. We can relate.\nOf course, there are some drawbacks of lexicon-based methods. Namely, they don’t work well with phrases, they often don’t consider modern language (see ‘Jupiiiiiii’ or ‘Hooooooraaaaay!’, where the more the letters, the more expressive the word is) and they fail with sarcasm. Nevertheless, even such crude methods give us a nice glimpse into the corpus and enable us to extract interesting documents.\nStay tuned for the information on the release date and the upcoming post on UDPipe infrastructure!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "In the past month, we had two workshops that focused on text mining. The first one, Faksi v praksi, was organized by the University of Ljubljana Career Centers, where high school students learned about what we do at the Faculty of Computer and Information Science. We taught them what text mining is and how to group a collection of documents in Orange. The second one took on a more serious note, as the public sector employees joined us for the third set of workshops from the Ministry of Public Affairs." ,
	"author" : "AJDA",
	"summary" : "In the past month, we had two workshops that focused on text mining. The first one, Faksi v praksi, was organized by the University of Ljubljana Career Centers, where high school students learned about what we do at the Faculty of Computer and Information Science. We taught them what text mining is and how to group a collection of documents in Orange. The second one took on a more serious note, as the public sector employees joined us for the third set of workshops from the Ministry of Public Affairs.",
	"date" : "Sep 11, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Text Workshops in Ljubljana",
	"icon" : ""
},
{
    "uri": "/blog/update/",
	"title": "update",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 11, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "update",
	"icon" : ""
},
{
    "uri": "/blog/2018/08/27/explaining-kickstarter-success/",
	"title": "Explaining Kickstarter Success",
	"description": "",
	"content": "On Kickstarter most app ideas don’t get funded. But why is that? When we are looking for possible explanations, it is easy to ascribe the failure to the type of the idea.\nBut what about those rare cases, where an app idea gets funded? Can we figure out why a particular idea succeeded? Our new widget Explain Predictions can do just that - explain why they will succeed. Or at least, explain why the classifier thinks they will.\nFirst, let us load the Kickstarter data from the Datasets widget and inspect it in a Data Table.\nSelect the data instance you wish to explore in a Data Table.\nNow, let’s see why the app Create Games \u0026 Apps Without Any Coding got funded.\nExplain Predictions needs 3 inputs. Our data set, a classifier and a data sample that we wish to inspect. Connect File widget with Explain Predictions. Then add the classifier, say, Logistic Regression. Finally, select Create Games \u0026 Apps Without Any Coding in the Data Table and connect it to the widget.\nExplain Predictions needs three inputs.\nThe highest ranking attributes are those that contributed the most (high Score value). The fact that there were 11 pledge levels, 13 images, many connections to other projects and the length of the project description - all of these attributes add something positive to the funding. On the other side, we see how the duration of the project, description length, maximal pledge tiers and the type of the idea work against the decision to fund the project. Lastly, not having a Facebook page or a video amounts to almost nothing in the making of the final prediction.\nHigh score means the attribute contributed positively to the the final decision (Funded: yes), while low scores contributed negatively.\nWhen explaining the decision of the classifier, we look at the values of the attributes for our sample and how they interact. We do that by approximating Shapely value, since calculating it exactly would sometimes take more then a lifetime. That means customized explanations for every individual case, while treating classifier like a black box. You could do the same for any model the Orange offers, including Neural Networks!\nAnd there you have it, an easy way to know what makes your Kickstarter campaign succeed, cell be classified as healthy, or a bank loan approved.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "On Kickstarter most app ideas don\u0026rsquo;t get funded. But why is that? When we are looking for possible explanations, it is easy to ascribe the failure to the type of the idea.\nBut what about those rare cases, where an app idea gets funded? Can we figure out why a particular idea succeeded? Our new widget Explain Predictions can do just that - explain why they will succeed. Or at least, explain why the classifier thinks they will." ,
	"author" : "ANDREJA",
	"summary" : "On Kickstarter most app ideas don\u0026rsquo;t get funded. But why is that? When we are looking for possible explanations, it is easy to ascribe the failure to the type of the idea.\nBut what about those rare cases, where an app idea gets funded? Can we figure out why a particular idea succeeded? Our new widget Explain Predictions can do just that - explain why they will succeed. Or at least, explain why the classifier thinks they will.",
	"date" : "Aug 27, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Explaining Kickstarter Success",
	"icon" : ""
},
{
    "uri": "/blog/prediction/",
	"title": "prediction",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 27, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "prediction",
	"icon" : ""
},
{
    "uri": "/blog/widget/",
	"title": "widget",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 27, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "widget",
	"icon" : ""
},
{
    "uri": "/blog/2018/07/17/data-mining-and-machine-learning-for-economists/",
	"title": "Data Mining and Machine Learning for Economists",
	"description": "",
	"content": "Last week Blaž, Marko and I held a week long introductory Data Mining and Machine Learning course at the Ljubljana Doctoral Summer School 2018. We got a room full of dedicated students and we embarked on a journey through standard and advanced machine learning techniques, all presented of course in Orange. We have covered a wide array of topics, from different clustering techniques (hierarchical clustering, k-means) to predictive models (logistic regression, naive Bayes, decision trees, random forests), regression and regularization, projections, text mining and image analytics.\nRelated: Data Mining for Business and Public Administration\nDefinitely the biggest crowd-pleaser was the Geo add-on in combination with the HDI data set. First, we got the HDI data from Datasets. A quick glimpse into a data table to check the output. We have information on some key performance indicators gathered by the United Nations for 188 countries. Now we would like to know which countries are similar based on the reported indicators. We will use Distances with Euclidean distance and use Ward linkage in Hierarchical Clustering.\nIn Datasets widget we have selected the HDI data set.\nThe HDI data set contains information on 188 countries, which are described with 66 features. The data set can be used for regression, but we will perform clustering to discover countries, similar by the proposed parameters.\nWe got our results in a dendrogram. Interestingly, the United States seems similar to Cuba. Let us select this cluster and inspect what the most significant feature for this cluster. We will use the Data output of Hierarchical Clustering which append a column indicating whether the data instances was selected or not. Then we will use Box Plot, group by Selected and check Order by relevance. It seems like these countries have the longest life expectancy at age 59. Go ahead and inspect other clusters by yourself!\nSelect an interesting cluster in Hierarchical Clustering.\nAnd inspect the results in a box plot. Seems like the selected cluster stands out from the other countries by high life expectancy.\nOf course, when we are talking about countries one naturally wants to see them on a map! That is easy. We will use the Geo add-on. First, we need to convert all the country names to geographical coordinates. We will do this with Geocoding, where we will encode column Country to latitude and longitude. Remember to use the same output as before, that is Data to Data.\n \\\nUse Encode to convert a column with region identifiers (in our case Country) to latitude/longitude pairs.\nNow, let us display these countries on a map with Choropleth widget. Beautiful. It is so easy to explore country data, when you see it on a map. You can try coloring also by HDI or any other feature.\nChoropleth shows us which countries were in the selected cluster (red). We used Selected as attribute and colored by Mode.\nThe final workflow:\nWe always try to keep our workshops fresh and interesting and visualizations are the best way to achieve this. Till the next workshop!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Last week Blaž, Marko and I held a week long introductory Data Mining and Machine Learning course at the Ljubljana Doctoral Summer School 2018. We got a room full of dedicated students and we embarked on a journey through standard and advanced machine learning techniques, all presented of course in Orange. We have covered a wide array of topics, from different clustering techniques (hierarchical clustering, k-means) to predictive models (logistic regression, naive Bayes, decision trees, random forests), regression and regularization, projections, text mining and image analytics." ,
	"author" : "AJDA",
	"summary" : "Last week Blaž, Marko and I held a week long introductory Data Mining and Machine Learning course at the Ljubljana Doctoral Summer School 2018. We got a room full of dedicated students and we embarked on a journey through standard and advanced machine learning techniques, all presented of course in Orange. We have covered a wide array of topics, from different clustering techniques (hierarchical clustering, k-means) to predictive models (logistic regression, naive Bayes, decision trees, random forests), regression and regularization, projections, text mining and image analytics.",
	"date" : "Jul 17, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining and Machine Learning for Economists",
	"icon" : ""
},
{
    "uri": "/blog/2018/06/21/girls-go-data-mining/",
	"title": "Girls Go Data Mining",
	"description": "",
	"content": "This week we held our first Girls Go Data Mining workshop. The workshop brought together curious women and intuitively introduced them to essential data mining and machine learning concepts. Of course, we used Orange to explore visualizations, build predictive models, perform clustering and dive into text analysis. The workshop was supported by NumFocus through their small development grant initiative and we hope to repeat it next year with even more ladies attending!\nRelated: Text Analysis for Social Scientists\nIn two days, we covered many topics. On day one, we got to know Orange and the concept of visual programming, where the user construct analytical workflow by stacking visual components. Then we got to know several useful visualizations, such as box plot, scatter plot, distributions, and mosaic display, which give us an initial overview of the data and the potentially interesting patterns. Finally, we got our hands dirty with predictive modeling. We learnt about decision trees, logistic regression, and naive Bayes classifiers, and observed the models in tree viewer and nomogram. It is great having interpretable models and we had great fun exploring what is in the model!\nOn the second day, we tried to uncover groups in our data with clustering. First, we tried hierarchical clustering and explored the discovered clusters with box plot. Then we also tried k-means and learnt, why this method is better than hierarchical clustering. In the final part, we talked about the methods for text mining, how to do preprocessing, construct a bag of words and perform the machine learning on corpora. We used both clustering and classification and tried to find interesting information about Grimm tales.\nOne of our workflows, where we explored the data in many different ways, including inspecting misclassifications in a scatter plot!\nOne thing that always comes up as really useful in our workshops is Orange’s ability to output different types of data. For example, in Hierarchical Clustering, we can select the similarity cutoff at the top and output clusters. Our data table will have an additional column Cluster, with cluster labels for each data instance.\nHierarchial Clustering outputs data with an additional Cluster column.\nWe can explore clusters by connecting a Box Plot to Hierarchical Clustering, selecting Cluster in Subgroups and using Order by relevance option. This sorts the variables in Box Plot by how well they separate between clusters or, in other words, what is typical of each cluster.\nWe have selected Cluster in Subgroups section and ticked ‘Order by relevance’ to sort the variables. Variables at the top are the most interesting ones. Looks like giving milk is an exclusive property of cluster C1.\nWe used zoo.tab and made the cutoff at three clusters. It looks like the first cluster gives milk. Could these be a cluster of mammals?\nWe said giving milk is a property of cluster C1. By selecting type as our variable, we can see that C1 is a cluster of mammals.\nIndeed it is!\nAnother option is to select a specific cluster in the dendrogram. Then, we have to rewire the connection between Hierarchical Clustering and Box Plot by setting it to Data. Data option will output the entire data set, with an extra column showing whether the data instance was selected or not. In our case, there would be a Yes if the instance is in the selected cluster and No if it is not.\nTo rewire the connection, double-click on it and drag a line from Data to Data.\nWe have selected one cluster in the dendrogram, rewired the connection to transmit Data (instead of Selected Data) and observed the results in a Data Table. We see an additional Selected column, which shows whether a data instance was selected in the visualization or not.\nThen we can use Box Plot to observe what is particular for our selected cluster.\nIn this Box Plot we have used Selected in the Subgroups section and kept ‘Order by relevance’ on. The suggested distinctive feature of our selected cluster is having feathers.\nIt looks like animals from our selected cluster have feathers. Probably, this is a cluster of birds. We can check this with the same procedure as above.\nIn summary, most Orange visualizations have two outputs - Selected Data and Data. Selected Data will output a subset of data instances selected in the visualization (or selected clusters in the case of hierarchical clustering), while Data will output the entire data table with a column defining whether a data instance was selected or not. This is very useful if we want to inspect what is typical of an interesting group in our data, inspect clusters or even manually define groups.\nOverall, this was another interesting workshop and we hope to continue our fruitful partnership with NumFocus and keep offering free educational events for beginners and experts alike!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This week we held our first Girls Go Data Mining workshop. The workshop brought together curious women and intuitively introduced them to essential data mining and machine learning concepts. Of course, we used Orange to explore visualizations, build predictive models, perform clustering and dive into text analysis. The workshop was supported by NumFocus through their small development grant initiative and we hope to repeat it next year with even more ladies attending!" ,
	"author" : "AJDA",
	"summary" : "This week we held our first Girls Go Data Mining workshop. The workshop brought together curious women and intuitively introduced them to essential data mining and machine learning concepts. Of course, we used Orange to explore visualizations, build predictive models, perform clustering and dive into text analysis. The workshop was supported by NumFocus through their small development grant initiative and we hope to repeat it next year with even more ladies attending!",
	"date" : "Jun 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Girls Go Data Mining",
	"icon" : ""
},
{
    "uri": "/blog/interactive-data-visualization/",
	"title": "interactive data visualization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 21, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "interactive data visualization",
	"icon" : ""
},
{
    "uri": "/blog/dataloading/",
	"title": "dataloading",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 12, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "dataloading",
	"icon" : ""
},
{
    "uri": "/blog/2018/06/12/from-surveys-to-orange/",
	"title": "From Surveys to Orange",
	"description": "",
	"content": "Today we have finished a series of workshops for the Ministry of Public Affairs. This was a year-long cooperation and we had many students asking many different questions. There was however one that we talked about a lot. If I have a survey, how do I get it into Orange?\nRelated: Analyzing Surveys\nWe are using EnKlik Anketa service, which is a great Slovenian product offering a wide array of options for the creation of surveys. We have created one such simple survey to use as a test. I am now inside EnKlik Anketa online service and I can see my survey has been successfully filled out.\nNow I have to create a public link to my survey in order to access the data in Orange. I have to click on an icon in the top right part and select ‘Public link’.\nA new window opens, where I select ‘Add new public link’. This will generate a public connection to my survey results. But be careful, the type of the connection needs to be Data, not Analysis! Orange can’t read already analyzed data, it needs raw data from Data pane.\nNow, all I have to do is open Orange, place EnKlik Anketa widget from the Prototypes add-on onto the canvas, enter the public link into the ‘Public link URL’ fields and press Enter. If your data has loaded successfully, the widget will display available variables and information in the Info pane.\nFrom here on you can continue your analysis just like you would with any other data source!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Today we have finished a series of workshops for the Ministry of Public Affairs. This was a year-long cooperation and we had many students asking many different questions. There was however one that we talked about a lot. If I have a survey, how do I get it into Orange?\nRelated: Analyzing Surveys\nWe are using EnKlik Anketa service, which is a great Slovenian product offering a wide array of options for the creation of surveys." ,
	"author" : "AJDA",
	"summary" : "Today we have finished a series of workshops for the Ministry of Public Affairs. This was a year-long cooperation and we had many students asking many different questions. There was however one that we talked about a lot. If I have a survey, how do I get it into Orange?\nRelated: Analyzing Surveys\nWe are using EnKlik Anketa service, which is a great Slovenian product offering a wide array of options for the creation of surveys.",
	"date" : "Jun 12, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "From Surveys to Orange",
	"icon" : ""
},
{
    "uri": "/blog/2018/05/30/spectroscopy-workshop-at-biospec-and-how-to-merge-data/",
	"title": "Spectroscopy Workshop at BioSpec and How to Merge Data",
	"description": "",
	"content": "Last week Marko and I visited the land of the midnight sun - Norway! We held a two-day workshop on spectroscopy data analysis in Orange at the Norwegian University of Life Sciences. The students from BioSpec lab were yet again incredible and we really dug deep into Orange.\nRelated: Orange with Spectroscopy Add-on\nA class full of dedicated scientists.\nOne thing we did was see how to join data from two different sources. It would often happen that you have measurements in one file and the labels in the other. Or in our case, we wanted to add images to our zoo.tab data. First, find the zoo.tab in the File widget under Browse documentation datasets. Observe the data in the Data Table.\nOriginal zoo data set.\nThis data contains 101 animal described with 16 different features (hair, aquatic, eggs, etc.), a name and a type. Now we will manually create the second table in Excel. The first column will contain the names of the animals as they appear in the original file. The second column will contain links to images of animals. Open your favorite browser and find a couple of images corresponding to selected animals. Then add links to images below the image column. Just like that:\nExtra data that we want to add to the original data.\nRemember, you need a three-row header to define the column that contains images. Under the image column add string in the second and type=image in the third row. This will tell Orange where to look for images. Now, we can check our animals in Image Viewer.\nA quick glance at an Image Viewer will tell us whether our images got loaded correctly.\nFinally, it is time to bring in the images to the existing zoo data set. Connect the original File to Merge Data. Then add the second file with animal images to Merge Data. The default merging method will take the first data input as original data and the second data as extra data. The column to match by is defined in the widget. In our case, it is the name column. This means Orange will look at the first name column and find matching instances in the second name column.\nA quick look at the merged data shows us an additional image column that we appended to the original file.\nMerged data with a new column.\nThis is the final workflow. Merge Data now contains a single data table on the output and you can continue your analysis from there.\nFind out more about spectroscopy for Orange on our YouTube channel or contribute to the project on Github.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Last week Marko and I visited the land of the midnight sun - Norway! We held a two-day workshop on spectroscopy data analysis in Orange at the Norwegian University of Life Sciences. The students from BioSpec lab were yet again incredible and we really dug deep into Orange.\nRelated: Orange with Spectroscopy Add-on\nA class full of dedicated scientists.\nOne thing we did was see how to join data from two different sources." ,
	"author" : "AJDA",
	"summary" : "Last week Marko and I visited the land of the midnight sun - Norway! We held a two-day workshop on spectroscopy data analysis in Orange at the Norwegian University of Life Sciences. The students from BioSpec lab were yet again incredible and we really dug deep into Orange.\nRelated: Orange with Spectroscopy Add-on\nA class full of dedicated scientists.\nOne thing we did was see how to join data from two different sources.",
	"date" : "May 30, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Spectroscopy Workshop at BioSpec and How to Merge Data",
	"icon" : ""
},
{
    "uri": "/blog/2018/05/15/python-script-managing-data-on-the-fly/",
	"title": "&amp;#39;Python Script: Managing Data on the Fly&amp;#39;",
	"description": "",
	"content": "Python Script is this mysterious widget most people don’t know how to use, even those versed in Python. Python Script is the widget that supplements Orange functionalities with (almost) everything that Python can offer. And it’s time we unveil some of its functionalities with a simple example.\nExample: Batch Transform the Data There might be a time when you need to apply a function to all your attributes. Say you wish to log-transform their values, as it is common in gene expression data. In theory, you could do this with Feature Constructor, where you would log-transform every attribute individually. Sounds laborious? It’s because it is. Why else we have computers if not to reduce manual labor for certain tasks? Let’s do it the fast way - with Python Script.\nFirst, open File widget and load geo-gds360.tab from Browse documentation data sets. This data set has 9485 features, so imagine having to transform each feature individually.\nInstead, we will connect Python Script to File and use a simple script to apply the same transformation to all attributes.\n\u003ccode\u003eimport numpy as np from Orange.data import Table new_X = np.log(in_data.X) out_data = Table(in_data.domain, new_X, in_data.Y, in_data.metas) \u003c/code\u003e  This is really simple. Use in_data.X, which accesses all features in the data set, to transform the data with np.log (or any other numpy function). Set out_data to new_X and, voila, the transformed data is on the output. In a few lines we have instantly handled all 9485 features.\nYou can inspect the data before and after transformation in a Data Table widget.\nOriginal data.\nLog-transformed data.\nThis is it. Now we can do our standard analysis on the transformed data. Even better! We can save our script and use it in Python Script widget any time we want.\nFor your convenience I have already added the [download id=\"2083”], so you can download and use it instantly!\nHave a more interesting example with Python Script? We’d love to hear about it!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Python Script is this mysterious widget most people don\u0026rsquo;t know how to use, even those versed in Python. Python Script is the widget that supplements Orange functionalities with (almost) everything that Python can offer. And it\u0026rsquo;s time we unveil some of its functionalities with a simple example.\nExample: Batch Transform the Data There might be a time when you need to apply a function to all your attributes. Say you wish to log-transform their values, as it is common in gene expression data." ,
	"author" : "AJDA",
	"summary" : "Python Script is this mysterious widget most people don\u0026rsquo;t know how to use, even those versed in Python. Python Script is the widget that supplements Orange functionalities with (almost) everything that Python can offer. And it\u0026rsquo;s time we unveil some of its functionalities with a simple example.\nExample: Batch Transform the Data There might be a time when you need to apply a function to all your attributes. Say you wish to log-transform their values, as it is common in gene expression data.",
	"date" : "May 15, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "&#39;Python Script: Managing Data on the Fly&#39;",
	"icon" : ""
},
{
    "uri": "/blog/scripting/",
	"title": "scripting",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 15, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "scripting",
	"icon" : ""
},
{
    "uri": "/blog/2018/05/09/clustering-of-monet-and-manet/",
	"title": "Clustering of Monet and Manet",
	"description": "",
	"content": "Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet’s masterpiece is indeed very similar to Manet’s! Use Image Analytics add-on and play with it. Here’s how:   ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet\u0026rsquo;s masterpiece is indeed very similar to Manet\u0026rsquo;s! Use Image Analytics add-on and play with it. Here\u0026rsquo;s how:   " ,
	"author" : "AJDA",
	"summary" : "Ever had a hard time telling the difference between Claude Monet and Édouard Manet? Orange can help you cluster these two authors and even more, discover which of Monet\u0026rsquo;s masterpiece is indeed very similar to Manet\u0026rsquo;s! Use Image Analytics add-on and play with it. Here\u0026rsquo;s how:   ",
	"date" : "May 9, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Clustering of Monet and Manet",
	"icon" : ""
},
{
    "uri": "/blog/image-analytics/",
	"title": "image analytics",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 9, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "image analytics",
	"icon" : ""
},
{
    "uri": "/blog/youtube/",
	"title": "youtube",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 9, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "youtube",
	"icon" : ""
},
{
    "uri": "/blog/2018/05/03/data-mining-course-at-higher-school-of-economics-moscow/",
	"title": "Data Mining Course at Higher School of Economics, Moscow",
	"description": "",
	"content": "Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master’s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.\nIt was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best.\nOne of the things we did was compute minimum cost of misclassifications. The story goes like this. Sara is a doctor and has data on 303 patients with heart disease (Orange’s heart-disease.tab data set). She used some classifiers and now has to decide how many patients to send for further tests. Naive Bayes classifier, for example, returned probabilities of a patient being sick (column Naive Bayes 1). For each threshold in probabilites, she will compute how many false positives (patients declared sick when healthy) and how many false negatives (patients declared healthy when sick) a classifiers returns. Each mistake is associated with a cost. Now she wants to find out, how many patients to send for tests (what probability threshold to choose) so that her cost is the lowest.\nFirst, import all the libraries we will need:\nimport matplotlib.pyplot as plt import numpy as np from Orange.data import Table from Orange.classification import NaiveBayesLearner, TreeLearner from Orange.evaluation import CrossValidation  Then load heart disease data (and print a sample).\nheart = Table(\"heart_disease\") print(heart[:5])  Now, train classifiers and select probabilities of Naive Bayes for a patient being sick.\nscores = CrossValidation(heart, [NaiveBayesLearner(), TreeLearner()]) #take probabilites of class 1 (sick) of NaiveBayesLearner p1 = scores.probabilities[0][:, 1] #take actual class values y = scores.actual #cost of false positive (patient classified as sick when healthy) fp_cost = 500 #cost of false negative (patient classified as healthy when sick) fn_cost = 800  Set counts, where we declare 0 patients being sick (threshold \u003e1).\nfp = 0 #start with threshold above 1 (no one is sick) fn = np.sum(y)  For each threshold, compute the cost associated with each type of mistake.\nps = [] costs = [] #compute costs of classifying i patients as sick for i in np.argsort(p1)[::-1]: if y[i] == 0: fp += 1 else: fn -= 1 ps.append(p1[i]) costs.append(fp * fp_cost + fn * fn_cost)  In the end, we get a list of probability thresholds and associated costs. Now let us find the minimum cost and its probability of a patient being sick.\ncosts = np.array(costs) #find probability of a patient being sick at lowest cost print(ps[costs.argmin()])  This means the threshold that minimizes our cost for a given classifier is 0.620655. Sara would send all the patients with a probability of being sick higher or equal than 0.620655 for further tests.\nAt the end, we can also plot the cost to patients sent curve.\nfig, ax = plt.subplots() plt.plot(ps, costs) ax.set_xlabel('Patients sent') ax.set_ylabel('Cost')  You can download the IPython Notebook here: [download id=\"2053”].\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master\u0026rsquo;s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.\nIt was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best." ,
	"author" : "AJDA",
	"summary" : "Janez and I have recently returned from a two-week stay in Moscow, Russian Federation, where we were teaching data mining to MA students of Applied Statistics. This is a new Master\u0026rsquo;s course that attracts the best students from different backgrounds and teaches them statistical methods for work in the industry.\nIt was a real pleasure working at HSE. The students were proactive by asking questions and really challenged us to do our best.",
	"date" : "May 3, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining Course at Higher School of Economics, Moscow",
	"icon" : ""
},
{
    "uri": "/blog/examples/",
	"title": "examples",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 3, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "examples",
	"icon" : ""
},
{
    "uri": "/blog/download/",
	"title": "download",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 23, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "download",
	"icon" : ""
},
{
    "uri": "/blog/2018/04/23/installing-add-ons-works-again/",
	"title": "Installing Add-ons Works Again",
	"description": "",
	"content": "Dear Orange users,\nSome of you might have an issue installing add-ons with the following issue popping up:\nxmlrpc.client.Fault: \u003cFault -32601: 'server error; requested method not found'\u003e\nThis is the result of the migration to a new infrastructure at PyPi, which provides the installation of add-ons. Our team has rallied to adjust the add-on installer so it works with the new and improved service.\nIn order to make the add-on installer work (again), please download the latest version of Orange (3.13.0).\nWe apologize for any inconvenience and wish you a fruitful data analysis in the future.\nYours truly,\nOrange team\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Dear Orange users,\nSome of you might have an issue installing add-ons with the following issue popping up:\nxmlrpc.client.Fault: \u0026lt;Fault -32601: 'server error; requested method not found'\u0026gt;\nThis is the result of the migration to a new infrastructure at PyPi, which provides the installation of add-ons. Our team has rallied to adjust the add-on installer so it works with the new and improved service.\nIn order to make the add-on installer work (again), please download the latest version of Orange (3." ,
	"author" : "AJDA",
	"summary" : "Dear Orange users,\nSome of you might have an issue installing add-ons with the following issue popping up:\nxmlrpc.client.Fault: \u0026lt;Fault -32601: 'server error; requested method not found'\u0026gt;\nThis is the result of the migration to a new infrastructure at PyPi, which provides the installation of add-ons. Our team has rallied to adjust the add-on installer so it works with the new and improved service.\nIn order to make the add-on installer work (again), please download the latest version of Orange (3.",
	"date" : "Apr 23, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Installing Add-ons Works Again",
	"icon" : ""
},
{
    "uri": "/blog/pypi/",
	"title": "pypi",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 23, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "pypi",
	"icon" : ""
},
{
    "uri": "/blog/neuralnetwork/",
	"title": "neuralnetwork",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "neuralnetwork",
	"icon" : ""
},
{
    "uri": "/blog/parallelization/",
	"title": "parallelization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "parallelization",
	"icon" : ""
},
{
    "uri": "/blog/performance/",
	"title": "performance",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "performance",
	"icon" : ""
},
{
    "uri": "/blog/programming/",
	"title": "programming",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "programming",
	"icon" : ""
},
{
    "uri": "/blog/qt/",
	"title": "qt",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "qt",
	"icon" : ""
},
{
    "uri": "/blog/2018/04/05/unfreezing-orange/",
	"title": "Unfreezing Orange",
	"description": "",
	"content": "Have you ever tried Orange with data big enough that some widgets ran for more than a second? Then you have seen it: Orange froze. While the widget was processing, the interface would not respond to any inputs, and there was no way to stop that widget.\nNot all the widgets freeze, though! Some widgets, like Test \u0026 Score, k-Means, or Image Embedding, do not block. While they are working, we are free to build other parts of the workflow, and these widgets also show their progress. Some, like Image Embedding, which work with lots of images, even allow interruptions.\nWhy does Orange freeze? Most widgets process users’ actions directly: after an event (click, pressed key, new input data) some code starts running: until it finishes, the interface can not respond to any new events. This is a reasonable approach for short tasks, such as making a selection in a Scatter Plot. But with longer tasks, such as building a Support Vector Model on big data, Orange gets unresponsive.\nTo make Orange responsive while it is processing, we need to start the task in a new thread. As programmers we have to consider the following:\n Starting the task. We have to make sure that other (older) tasks are not running. Showing results when the task has finished. Periodic communication between the task and the interface for status reports (progress bars) and task stopping.  Starting the task and showing the results are straightforward and well documented in a tutorial for writing widgets. Periodic communication with stopping is harder: it is completely task-dependent and can be either trivial, hard, or even impossible. Periodic communication is, in principle, unessential for responsiveness, but if we do not implement it, we will be unable to stop the running task and progress bars would not work either.\nTaking care of periodic communication was the hardest part of making the Neural Network widget responsive. It would have been easy, had we implemented neural networks ourselves. But we use the scikit-learn implementation, which does not expose an option to make additional function calls while fitting the network (we need to run code that communicates with the interface). We had to resort to a trick: we modified fitting so that a change to an attribute called n_iters_ called a function (see pull request). Not the cleanest solution, but it seems to work.\nFor now, only a few widgets work so that the interface remains responsive. We are still searching for the best way to make existing widgets behave nicely, but responsiveness is now one of our priorities.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Have you ever tried Orange with data big enough that some widgets ran for more than a second? Then you have seen it: Orange froze. While the widget was processing, the interface would not respond to any inputs, and there was no way to stop that widget.\nNot all the widgets freeze, though! Some widgets, like Test \u0026amp; Score, k-Means, or Image Embedding, do not block. While they are working, we are free to build other parts of the workflow, and these widgets also show their progress." ,
	"author" : "MARKO",
	"summary" : "Have you ever tried Orange with data big enough that some widgets ran for more than a second? Then you have seen it: Orange froze. While the widget was processing, the interface would not respond to any inputs, and there was no way to stop that widget.\nNot all the widgets freeze, though! Some widgets, like Test \u0026amp; Score, k-Means, or Image Embedding, do not block. While they are working, we are free to build other parts of the workflow, and these widgets also show their progress.",
	"date" : "Apr 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Unfreezing Orange",
	"icon" : ""
},
{
    "uri": "/blog/2018/03/28/orange-with-spectroscopy-add-on-workshop/",
	"title": "Orange with Spectroscopy Add-on Workshop",
	"description": "",
	"content": "We have just concluded our enhanced Introduction to Data Science workshop, which included several workflows for spectroscopy analysis. Spectroscopy add-on is intended for the analysis of spectral data and it is just as fun as our other add-ons (if not more!).\nWe will prove it with a simple classification workflow. First, install Spectroscopy add-on from Options - Add-ons menu in Orange. Restart Orange for the add-on to appear. Great, you are ready for some spectral analysis!\nUse Datasets widget and load Collagen spectroscopy data. This data contains cells measured with FTIR and annotated with the major chemical compound at the imaged part of a cell. A quick glance in a Data Table will give us an idea how the data looks like. Seems like a very standard spectral data set.\nCollagen data set from Datasets widget.\nNow we want to determine, whether we can classify cells by type based on their spectral profiles. First, connect Datasets to Test \u0026 Score. We will use 10-fold cross-validation to score the performance of our model. Next, we will add Logistic Regression to model the data. One final thing. Spectral data often needs some preprocessing. Let us perform a simple preprocessing step by applying Cut (keep) filter and retaining only the wave numbers between 1500 and 1800. When we connect it to Test \u0026 Score, we need to keep in mind to connect the Preprocessor output of Preprocess Spectra.\nPreprocessor that keeps a part of the spectra cut between 1500 and 1800. No data is shown here, since we are using only the preprocessing procedure as the input for Test \u0026 Score.\nLet us see how well our model performs. Not bad. A 0.99 AUC score. Seems like it is almost perfect. But is it really so?\n10-fold cross-validation on spectral data. Our AUC and CA scores are quite impressive.\nConfusion Matrix gives us a detailed picture. Our model fails almost exclusively on DNA cell type. Interesting.\nConfusion Matrix shows DNA is most often misclassified. By selecting the misclassified instances in the matrix, we can inspect why Logistic Regression couldn’t model these spectra\nWe will select the misclassified DNA cells and feed them to Spectra to inspect what went wrong. Instead of coloring by type, we will color by prediction from Logistic Regression. Can you find out why these spectra were classified incorrectly?\nMisclassified DNA spectra colored by the prediction made by Logistic Regression.\nThis is one of the simplest examples with spectral data. It is basically the same procedure as with standard data - data is fed as data, learner (LR) as learner and preprocessor as preprocessor directly to Test \u0026 Score to avoid overfitting. Play around with Spectroscopy add-on and let us know what you think! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have just concluded our enhanced Introduction to Data Science workshop, which included several workflows for spectroscopy analysis. Spectroscopy add-on is intended for the analysis of spectral data and it is just as fun as our other add-ons (if not more!).\nWe will prove it with a simple classification workflow. First, install Spectroscopy add-on from Options - Add-ons menu in Orange. Restart Orange for the add-on to appear. Great, you are ready for some spectral analysis!" ,
	"author" : "AJDA",
	"summary" : "We have just concluded our enhanced Introduction to Data Science workshop, which included several workflows for spectroscopy analysis. Spectroscopy add-on is intended for the analysis of spectral data and it is just as fun as our other add-ons (if not more!).\nWe will prove it with a simple classification workflow. First, install Spectroscopy add-on from Options - Add-ons menu in Orange. Restart Orange for the add-on to appear. Great, you are ready for some spectral analysis!",
	"date" : "Mar 28, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange with Spectroscopy Add-on Workshop",
	"icon" : ""
},
{
    "uri": "/blog/2018/03/05/single-cell-analytics-workshop-at-hhmi-janelia/",
	"title": "Single cell analytics workshop at HHMI | Janelia",
	"description": "",
	"content": "HHMI | Janelia is one of the prettiest researcher campuses I have ever visited. Located in Ashburn, VA, about 20 minutes from Washington Dulles airport, it is conveniently located yet, in a way, secluded from the buzz of the capital. We adored the guest house with a view of the lake, tasty Janelia-style breakfast (hash-browns with two eggs and sausage, plus a bagel with cream cheese) in the on-campus pub, beautifully-designed interiors to foster collaborations and interactions, and late-evening discussions in the in-house pub.\nAll these thanks to the invitation of Andrew Lemire, a manager of a shared high-throughput genomics resource, and Dr. Vilas Menon, a mathematician specializing in quantitative genomics. With Andy and Vilas, we have been collaborating in the past few months on trying to devise a simple and intuitive tool for analysis of single-cell gene expression data. Single cell high-throughput technology is one of the latest approaches that allow us to see what is happening within a single cell, and it does that by simultaneously scanning through potentially thousands of cells. That generates loads of data, and apparently, we have been trying to fit Orange for single-cell data analysis task.\nNamely, in the past half a year, we have been perfecting an add-on for Orange with components for single-cell analysis. This endeavor became so vital that we have even designed a new installation of Orange, called scOrange. With everything still in prototype stage, we had enough courage to present the tool at Janelia, first through a seminar, and the next day within a five-hour lecture that I gave together with Martin Strazar, a PhD student and bioinformatics expert from my lab. Many labs are embarking on single cell technology at Janelia, and by the crowd that gathered at both events, it looks like that everyone was there.\nOrange, or rather, scOrange, worked as expected, and hands-on workshop was smooth, despite testing the software on some rather large data sets. Our Orange add-on for single-cell analytics is still in early stage of development, but already has some advanced features like biomarker discovery and tools for characterization of cell clusters that may help in revealing hidden relations between genes and phenotypes. Thanks to Andy and Vilas, Janelia proved an excellent proving ground for scOrange, and we are looking forward to our next hands-on single-cell analytics workshop in Houston.\nRelated: Hands-On Data Mining Course in Houston\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "HHMI | Janelia is one of the prettiest researcher campuses I have ever visited. Located in Ashburn, VA, about 20 minutes from Washington Dulles airport, it is conveniently located yet, in a way, secluded from the buzz of the capital. We adored the guest house with a view of the lake, tasty Janelia-style breakfast (hash-browns with two eggs and sausage, plus a bagel with cream cheese) in the on-campus pub, beautifully-designed interiors to foster collaborations and interactions, and late-evening discussions in the in-house pub." ,
	"author" : "BLAZ",
	"summary" : "HHMI | Janelia is one of the prettiest researcher campuses I have ever visited. Located in Ashburn, VA, about 20 minutes from Washington Dulles airport, it is conveniently located yet, in a way, secluded from the buzz of the capital. We adored the guest house with a view of the lake, tasty Janelia-style breakfast (hash-browns with two eggs and sausage, plus a bagel with cream cheese) in the on-campus pub, beautifully-designed interiors to foster collaborations and interactions, and late-evening discussions in the in-house pub.",
	"date" : "Mar 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Single cell analytics workshop at HHMI | Janelia",
	"icon" : ""
},
{
    "uri": "/blog/2018/02/16/how-to-enable-sql-widget-in-orange/",
	"title": "How to enable SQL widget in Orange",
	"description": "",
	"content": "A lot of you have been interested in enabling SQL widget in Orange, especially regarding the installation of a psycopg backend that makes the widget actually work. This post will be slightly more technical, but I will try to keep it to a minimum. Scroll to the bottom for installation instructions.\nRelated: SQL for Orange\n Why won’t Orange recognize psycopg? The main issue for some people was that despite having installed the psycopg module in their console, the SQL widget still didn’t work. This is because Orange uses a separate virtual environment and most of you installed psycopg in the default (system) Python environment. For psycopg to be recognized in Orange, it needs to be installed in the same virtual environment, which is normally located in C:\\Users\\\u003cusr\u003e\\Anaconda3\\envs\\orange3 (on Windows). For the installation to work, you’d have to run it with the proper pip, namely:\nC:\\Users\\\u003cusr\u003e\\Anaconda3\\envs\\orange3\\Scripts\\pip.exe install psycopg2\n Installation instructions But there is a much easier way to do it. Head over to psycopg’s pip website and download the latest wheel for your platform. Py version has to be cp34 or higher (latest Orange from Anaconda comes with Python 3.6, so look for cp36).\nFor OSX, you would for example need: psycopg2-2.7.4-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\nFor 64-bit Windows: psycopg2-2.7.4-cp36-cp36m-win_amd64.whl\nAnd for Linux: psycopg2-2.7.4-cp36-cp36m-manylinux1_x86_64.whl\nThen open the add-on dialog in Orange (Options –\u003e Add-ons) and drag and drop the downloaded wheel into the add-on list. At the bottom, you will see psycopg2 with the tick next to it.\nClick OK to run the installation. Then re-start Orange and connect to your database with SQL widget. If you have any questions, drop them in the comment section!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "A lot of you have been interested in enabling SQL widget in Orange, especially regarding the installation of a psycopg backend that makes the widget actually work. This post will be slightly more technical, but I will try to keep it to a minimum. Scroll to the bottom for installation instructions.\nRelated: SQL for Orange\n Why won\u0026rsquo;t Orange recognize psycopg? The main issue for some people was that despite having installed the psycopg module in their console, the SQL widget still didn\u0026rsquo;t work." ,
	"author" : "AJDA",
	"summary" : "A lot of you have been interested in enabling SQL widget in Orange, especially regarding the installation of a psycopg backend that makes the widget actually work. This post will be slightly more technical, but I will try to keep it to a minimum. Scroll to the bottom for installation instructions.\nRelated: SQL for Orange\n Why won\u0026rsquo;t Orange recognize psycopg? The main issue for some people was that despite having installed the psycopg module in their console, the SQL widget still didn\u0026rsquo;t work.",
	"date" : "Feb 16, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "How to enable SQL widget in Orange",
	"icon" : ""
},
{
    "uri": "/blog/sql/",
	"title": "sql",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 16, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "sql",
	"icon" : ""
},
{
    "uri": "/blog/conference/",
	"title": "conference",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 2, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "conference",
	"icon" : ""
},
{
    "uri": "/blog/2018/02/02/image-analytics-workshop-at-aiucd-2018/",
	"title": "Image Analytics Workshop at AIUCD 2018",
	"description": "",
	"content": "This week, Primož and I flew to the south of Italy to hold a workshop on Image Analytics through Data Mining at AIUCD 2018 conference. The workshop was intended to familiarize digital humanities researchers with options that visual programming environments offer for image analysis.\nIn about 5 hours we discussed image embedding, clustering, finding closest neighbors and classification of images. While it is often a challenge to explain complex concepts in such a short time, it is much easier when working with Orange.\nRelated: Image Analytics: Clustering\nOne of the workflows we learned at the workshop was the one for finding the most similar image in a set of images. This is better explained with an example.\nWe had 15 paintings from different authors. Two of them were painted by Claude Monet, a famous French impressionist painter. Our task was, given a reference image of Monet, to find his other painting in a collection.\nA collection of images. It includes two Monet paintings.\nFirst, we loaded our data set with Import Images. Then we sent our images to Image Embedding. We selected Painters embedder since it was specifically trained to recognize authors of paintings.\nWe used Painters embedder here.\nOnce we have described our paintings with vectors (embeddings), we can compare them by similarity. To find the second Monet in a data set, we will have to compute the similarity of paintings and find the one most similar one to our reference painting.\nRelated: Video on image clustering\nLet us connect Image Embedding to Neighbors from Prototypes add-on. Neighbors widget is specifically intended to find a number of closest neighbors given a reference data point.\nWe will need to adjust the widget a bit. First, we will need cosine distance, since we will be comparing images by the content, not the magnitude of features. Next, we will tick off Exclude reference, in order to receive the reference image on the output. We do this just for visualization purposes. Finally, we set the number of neighbors to 2. Again, this is just for a nicer visualization, since we know there are only two Monet’s paintings in the data set.\nNeighbors was set to provide a nice visualization. Hence we ticked off Exclude references and set Neighbors to 2.\nThen we need to give Neighbors a reference image, for which we want to retrieve the neighbors. We do this by adding Data Table to Image Embedding, selecting one of Monet’s paintings in the spreadsheet and then connecting the Data Table to Neighbors. The widget will automatically consider the second input as a reference.\nMonet.jpg is our reference painting. We select it in Data Table.\nNow, all we need to do is to visualize the output. Connect Image Viewer to Neighbors and open it.\nVoila! The widget has indeed found the second Monet’s painting. So useful when you have thousands of images in your archive!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This week, Primož and I flew to the south of Italy to hold a workshop on Image Analytics through Data Mining at AIUCD 2018 conference. The workshop was intended to familiarize digital humanities researchers with options that visual programming environments offer for image analysis.\nIn about 5 hours we discussed image embedding, clustering, finding closest neighbors and classification of images. While it is often a challenge to explain complex concepts in such a short time, it is much easier when working with Orange." ,
	"author" : "AJDA",
	"summary" : "This week, Primož and I flew to the south of Italy to hold a workshop on Image Analytics through Data Mining at AIUCD 2018 conference. The workshop was intended to familiarize digital humanities researchers with options that visual programming environments offer for image analysis.\nIn about 5 hours we discussed image embedding, clustering, finding closest neighbors and classification of images. While it is often a challenge to explain complex concepts in such a short time, it is much easier when working with Orange.",
	"date" : "Feb 2, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Image Analytics Workshop at AIUCD 2018",
	"icon" : ""
},
{
    "uri": "/blog/features/",
	"title": "features",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 26, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "features",
	"icon" : ""
},
{
    "uri": "/blog/2018/01/26/visualizing-multiple-variables-freeviz/",
	"title": "Visualizing multiple variables: FreeViz",
	"description": "",
	"content": "Scatter plots are great! But sometimes, we need to plot more than two variables to truly understand the data. How can we achieve this, knowing humans can only grasp up to three dimensions? With an optimization of linear projection, of course!\nOrange recently re-introduced FreeViz, an interactive visualization for plotting multiple variables on a 2-D plane.\nLet’s load zoo.tab data with File widget and connect FreeViz to it. Zoo data has 16 features describing animals of different types - mammals, amphibians, insects and so on. We would like to use FreeViz to show us informative features and create a visualization that separates well between animal types.\nFreeViz with initial, un-optimized plot.\nWe start with un-optimized projection, where data points are scattered around features axes. Once we click Optimize, we can observe optimization process in real-time and at the end see the optimized projection.\nFreeViz with optimized projection.\nThis projection is much more informative. Mammals are nicely grouped together within a pink cluster that is characterized by hair, milk, and toothed features. Conversely, birds are charaterized by eggs, feathers and airborne, while fish are aquatic. Results are as expected, which means optimization indeed found informative features for each class value.\nFreeViz with Show class density option.\nSince we are working with categorical class values, we can tick Show class density to color the plot by majority class values. We can also move anchors around to see how data points change in relation to a selected anchor.\nFinally, as in most Orange visualizations, we can select a subset of data points and explore them further. For example, let us observe which amphibians are characterized by being aquatic in a Data Table. A newt, a toad and two types of frogs, one venomous and one not.\nData exploration is always much easier with clever visualizations!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Scatter plots are great! But sometimes, we need to plot more than two variables to truly understand the data. How can we achieve this, knowing humans can only grasp up to three dimensions? With an optimization of linear projection, of course!\nOrange recently re-introduced FreeViz, an interactive visualization for plotting multiple variables on a 2-D plane.\nLet\u0026rsquo;s load zoo.tab data with File widget and connect FreeViz to it. Zoo data has 16 features describing animals of different types - mammals, amphibians, insects and so on." ,
	"author" : "AJDA",
	"summary" : "Scatter plots are great! But sometimes, we need to plot more than two variables to truly understand the data. How can we achieve this, knowing humans can only grasp up to three dimensions? With an optimization of linear projection, of course!\nOrange recently re-introduced FreeViz, an interactive visualization for plotting multiple variables on a 2-D plane.\nLet\u0026rsquo;s load zoo.tab data with File widget and connect FreeViz to it. Zoo data has 16 features describing animals of different types - mammals, amphibians, insects and so on.",
	"date" : "Jan 26, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Visualizing multiple variables: FreeViz",
	"icon" : ""
},
{
    "uri": "/blog/2018/01/05/stack-everything/",
	"title": "Stack Everything!",
	"description": "",
	"content": "We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.\nStacking enables you to combine several trained models into one meta model and use it in Test\u0026Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let’s see an example.\nWe start with something as complex as this. We used Paint Data to create a complex data set, where classes somewhat overlap. This is naturally an artificial example, but you can try the same on your own, real life data.\nWe used 4 classes and painted a complex, 2-dimensional data set.\nThen we add several kNN models with different parameters, say 5, 10 and 15 neighbors. We connect them to Test\u0026Score and use cross validation to evaluate their performance. Not bad, but can we do even better?\nScores without staking, using only 3 different kNN classifiers.\nLet us try stacking. We will connect all three classifiers to the Stacking widget and use Logistic Regression as an aggregate, a method that aggregates the three models into a single meta model. Then we connect connect the stacked model into Test\u0026Score and see whether our scores improved.\nScores with stacking. Stack reports on improved performance.\nAnd indeed they have. It might not be anything dramatic, but in real life, say medical context, even small improvements count. Now go and try the procedure on your own data. In Orange, this requires only a couple of minutes.\nFinal workflow with channel names. Notice that Logistic Regression is used as Aggregate, not a Learner.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.\nStacking enables you to combine several trained models into one meta model and use it in Test\u0026amp;Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let\u0026rsquo;s see an example." ,
	"author" : "AJDA",
	"summary" : "We all know that sometimes many is better than few. Therefore we are happy to introduce the Stack widget. It is available in Prototypes add-on for now.\nStacking enables you to combine several trained models into one meta model and use it in Test\u0026amp;Score just like any other model. This comes in handy with complex problems, where one classifier might fail, but many could come up with something that works. Let\u0026rsquo;s see an example.",
	"date" : "Jan 5, 2018",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Stack Everything!",
	"icon" : ""
},
{
    "uri": "/blog/network/",
	"title": "network",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 23, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "network",
	"icon" : ""
},
{
    "uri": "/blog/2017/12/23/speeding-up-network-visualization/",
	"title": "Speeding Up Network Visualization",
	"description": "",
	"content": "The Orange3 Network add-on contains a convenient Network Explorer widget for network visualization. Orange uses an iterative force-directed method (a variation of the Fruchterman-Reingold Algorithm) to layout the nodes on the 2D plane.\nThe goal of force-directed methods is to draw connected nodes close to each other as if the edges that connect the nodes were acting as springs. We also don’t want all nodes crowded in a single point, but would rather have them spaced evenly. This is achieved by simulating a repulsive force, which decreases with the distance between nodes.\nThere are two types of forces acting on each node:\n the attractive force towards connected adjacent nodes, the repulsive force that is directed away from all other nodes.  We could say that such network visualization as a whole is rather repulsive. Let’s take for example the lastfm.net network that comes with Orange’s network add-on and which has around 1.000 nodes and 4.000 edges. In every iteration, we have to consider 4.000 attractive forces and 1.000.000 repulsive forces for every of 1.000 times 1.000 edges. It takes about 100 iterations to get a decent network layout. That’s a lot of repulsions, and you’ll have to wait a while before you get the final layout.\nFortunately, we found a simple hack to speed things up. When computing the repulsive force acting on some node, we only consider a 10% sample of other nodes to obtain an estimate. We multiply the result by 10 and hope it’s not off by too much. By choosing a different sample in every iteration we also avoid favoring some set of nodes.\nThe left layout is obtained without sampling while the right one uses a 10% sampling. The results are pretty similar, but the sampling method is 10 times faster!\nNow that the computation is fast enough, it is time to also speed-up the drawing. But that’s a task for 2018.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The Orange3 Network add-on contains a convenient Network Explorer widget for network visualization. Orange uses an iterative force-directed method (a variation of the Fruchterman-Reingold Algorithm) to layout the nodes on the 2D plane.\nThe goal of force-directed methods is to draw connected nodes close to each other as if the edges that connect the nodes were acting as springs. We also don\u0026rsquo;t want all nodes crowded in a single point, but would rather have them spaced evenly." ,
	"author" : "THOCEVAR",
	"summary" : "The Orange3 Network add-on contains a convenient Network Explorer widget for network visualization. Orange uses an iterative force-directed method (a variation of the Fruchterman-Reingold Algorithm) to layout the nodes on the 2D plane.\nThe goal of force-directed methods is to draw connected nodes close to each other as if the edges that connect the nodes were acting as springs. We also don\u0026rsquo;t want all nodes crowded in a single point, but would rather have them spaced evenly.",
	"date" : "Dec 23, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Speeding Up Network Visualization",
	"icon" : ""
},
{
    "uri": "/blog/2017/11/29/how-to-properly-test-models/",
	"title": "How to Properly Test Models",
	"description": "",
	"content": "On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\nOne thing we discussed was how to properly test your model. Ok, we know never to test on the same data you’ve built your model with, but even training and testing on separate data is sometimes not enough. Say I’ve tested Naive Bayes, Logistic Regression and Tree. Sure, I can select the one that gives the best performance, but we could potentially (over)fit our model, too.\nTo account for this, we would normally split the data to 3 parts:\n training data for building a model validation data for testing which parameters and which model to use test data for estmating the accurracy of the model  Let us try this in Orange. Load heart-disease.tab data set from Browse documentation data sets in File widget. We have 303 patients diagnosed with blood vessel narrowing (1) or diagnosed as healthy (0).\nNow, we will split the data into two parts, 85% of data for training and 15% for testing. We will send the first 85% onwards to build a model.\nWe sampled by a fixed proportion of data and went with 85%, which is 258 out of 303 patients.\nWe will use Naive Bayes, Logistic Regression and Tree, but you can try other models, too. This is also a place and time to try different parameters. Now we will send the models to Test \u0026 Score. We used cross-validation and discovered Logistic Regression scores the highest AUC. Say this is the model and parameters we want to go with.\nNow it is time to bring in our test data (the remaining 15%) for testing. Connect Data Sampler to Test \u0026 Score once again and set the connection Remaining Data - Test Data.\nTest \u0026 Score will warn us we have test data present, but unused. Select Test on test data option and observe the results. These are now the proper scores for our models.\nSeems like LogReg still performs well. Such procedure would normally be useful when testing a lot of models with different parameters (say +100), which you would not normally do in Orange. But it’s good to know how to do the scoring properly. Now we’re off to report on the results in Nature… ;)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\nOne thing we discussed was how to properly test your model. Ok, we know never to test on the same data you\u0026rsquo;ve built your model with, but even training and testing on separate data is sometimes not enough." ,
	"author" : "AJDA",
	"summary" : "On Monday we finished the second part of the workshop for the Statistical Office of Republic of Slovenia. The crowd was tough - these guys knew their numbers and asked many challenging questions. And we loved it!\nOne thing we discussed was how to properly test your model. Ok, we know never to test on the same data you\u0026rsquo;ve built your model with, but even training and testing on separate data is sometimes not enough.",
	"date" : "Nov 29, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "How to Properly Test Models",
	"icon" : ""
},
{
    "uri": "/blog/overfitting/",
	"title": "overfitting",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 29, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "overfitting",
	"icon" : ""
},
{
    "uri": "/blog/predictive-analytics/",
	"title": "predictive analytics",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 29, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "predictive analytics",
	"icon" : ""
},
{
    "uri": "/blog/scoring/",
	"title": "scoring",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 29, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "scoring",
	"icon" : ""
},
{
    "uri": "/blog/2017/11/17/data-mining-business-public-administration/",
	"title": "Data Mining for Business and Public Administration",
	"description": "",
	"content": "We’ve been having a blast with recent Orange workshops. While Blaž was getting tanned in India, Anže and I went to the charming Liverpool to hold a session for business school professors on how to teach business with Orange.\nRelated: Orange in Kolkata, India\nObviously, when we say teach business, we mean how to do data mining for business, say predict churn or employee attrition, segment customers, find which items to recommend in an online store and track brand sentiment with text analysis.\nFor this purpose, we have made some updates to our Associate add-on and added a new data set to Data Sets widget which can be used for customer segmentation and discovering which item groups are frequently bought together. Like this:\n We load the Online Retail data set.\nSince we have transactions in rows and items in columns, we have to transpose the data table in order to compute distances between items (rows). We could also simply ask Distances widget to compute distances between columns instead of rows. Then we send the transposed data table to Distances and compute cosine distance between items (cosine distance will only tell us, which items are purchased together, disregarding the amount of items purchased).\n Finally, we observe the discovered clusters in Hierarchical Clustering. Seems like mugs and decorative signs are frequently bought together. Why so? Select the group in Hierarchical Clustering and observe the cluster in a Data Table. Consider this an exercise in data exploration. :)\nThe second workshop was our standard Introduction to Data Mining for Ministry of Public Affairs.\nRelated: Analyzing Surveys\nThis group, similar to the one from India, was a pack of curious individuals who asked many interesting questions and were not shy to challenge us. How does a Tree know which attribute to split by? Is Tree better than Naive Bayes? Or is perhaps Logistic Regression better? How do we know which model works best? And finally, what is the mean of sauerkraut and beans? It has to be jota!\nWorkshops are always fun, when you have a curious set of individuals who demand answers! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We\u0026rsquo;ve been having a blast with recent Orange workshops. While Blaž was getting tanned in India, Anže and I went to the charming Liverpool to hold a session for business school professors on how to teach business with Orange.\nRelated: Orange in Kolkata, India\nObviously, when we say teach business, we mean how to do data mining for business, say predict churn or employee attrition, segment customers, find which items to recommend in an online store and track brand sentiment with text analysis." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve been having a blast with recent Orange workshops. While Blaž was getting tanned in India, Anže and I went to the charming Liverpool to hold a session for business school professors on how to teach business with Orange.\nRelated: Orange in Kolkata, India\nObviously, when we say teach business, we mean how to do data mining for business, say predict churn or employee attrition, segment customers, find which items to recommend in an online store and track brand sentiment with text analysis.",
	"date" : "Nov 17, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining for Business and Public Administration",
	"icon" : ""
},
{
    "uri": "/blog/2017/11/08/orange-in-kolkata-india/",
	"title": "Orange in Kolkata, India",
	"description": "",
	"content": "We have just completed the hands-on course on data science at one the most famous Indian educational institutions, Indian Statistical Institute. A one week course was invited by Institute’s director Prof. Dr. Sanghamitra Bandyopadhyay, and financially supported by the founding of India’s Global Initiative of Academic Networks.\nIndian Statistical Institute lies in the hearth of old Kolkata. A peaceful oasis of picturesque campus with mango orchards and waterlily lakes was founded by Prof. Prasanta Chandra Mahalanobis, one of the giants of statistics. Today, the Institute researches statistics and computational approaches to data analysis and runs a grad school, where a rather small number of students are hand-picked from tens of thousands of applicants.\nThe course was hands-on. The number of participants was limited to forty, the limitation posed by the number of the computers in Institute’s largest computer lab. Half of the students came from Institute’s grad school, and another half from other universities around Kolkata or even other schools around India, including a few participants from another famous institution, India Institutes of Technology. While the lecture included some writing on the white-board to explain machine learning, the majority of the course was about exploring example data sets, building workflows for data analysis, and using Orange on practical cases.\nThe course was not one of the lightest for the lecturer (Blaž Zupan). About five full hours each day for five days in a row, extremely motivated students with questions filling all of the coffee breaks, the need for deeper dive into some of the methods after questions in the classroom, and much need for improvisation to adapt our standard data science course to possibly the brightest pack of data science students we have seen so far. We have covered almost a full spectrum of data science topics: from data visualization to supervised learning (classification and regression, regularization), model exploration and estimation of quality. Plus computation of distances, unsupervised learning, outlier detection, data projection, and methods for parameter estimation. We have applied these to data from health care, business (which proposal on Kickstarter will succeed?), and images. Again, just like in our other data science courses, the use of Orange’s educational widgets, such as Paint Data, Interactive k-Means, and Polynomial Regression helped us in intuitive understanding of the machine learning techniques.\nThe course was beautifully organized by Prof. Dr. Saurabh Das with the help of Prof. Dr. Shubhra Sankar Ray and we would like to thank them for their devotion and excellent organization skills. And of course, many thanks to participating students: for an educator, it is always a great pleasure to lecture and work with highly motivated and curious colleagues that made our trip to Kolkata fruitful and fun.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have just completed the hands-on course on data science at one the most famous Indian educational institutions, Indian Statistical Institute. A one week course was invited by Institute\u0026rsquo;s director Prof. Dr. Sanghamitra Bandyopadhyay, and financially supported by the founding of India\u0026rsquo;s Global Initiative of Academic Networks.\nIndian Statistical Institute lies in the hearth of old Kolkata. A peaceful oasis of picturesque campus with mango orchards and waterlily lakes was founded by Prof." ,
	"author" : "BLAZ",
	"summary" : "We have just completed the hands-on course on data science at one the most famous Indian educational institutions, Indian Statistical Institute. A one week course was invited by Institute\u0026rsquo;s director Prof. Dr. Sanghamitra Bandyopadhyay, and financially supported by the founding of India\u0026rsquo;s Global Initiative of Academic Networks.\nIndian Statistical Institute lies in the hearth of old Kolkata. A peaceful oasis of picturesque campus with mango orchards and waterlily lakes was founded by Prof.",
	"date" : "Nov 8, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange in Kolkata, India",
	"icon" : ""
},
{
    "uri": "/blog/2017/11/03/neural-network-is-back/",
	"title": "Neural Network is Back!",
	"description": "",
	"content": "We know you’ve missed it. We’ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.\nNeural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.\nDo we make each layer a widget and then stack them? Do we use parallel processing or try to do something server-side? Theano or Keras? Tensorflow perhaps?\nWe were so determined to do things properly, that after the n-th iteration we still had no clue what to actually do.\nThen one day a silly novice programmer (a.k.a. me) had enough and just threw scikit-learn’s Multi-layer Perceptron model into a widget and called it a day. There you go. A Neural Network widget just like it was in Orange2 - a wrapper for a scikit’s function that works out-of-the-box. Nothing fancy, nothing powerful, but it does its job. It models things and it predicts things.\nJust like that:\nHave fun with the new widget!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We know you\u0026rsquo;ve missed it. We\u0026rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.\nNeural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.\nDo we make each layer a widget and then stack them?" ,
	"author" : "AJDA",
	"summary" : "We know you\u0026rsquo;ve missed it. We\u0026rsquo;ve been getting many requests to bring back Neural Network widget, but we also had many reservations about it.\nNeural networks are powerful and great, but to do them right is not straight-forward. And to do them right in the context of a GUI-based visual programming tool like Orange is a twisted double helix of a roller coaster.\nDo we make each layer a widget and then stack them?",
	"date" : "Nov 3, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Neural Network is Back!",
	"icon" : ""
},
{
    "uri": "/blog/regression/",
	"title": "regression",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 3, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "regression",
	"icon" : ""
},
{
    "uri": "/blog/2017/10/26/analyzing-surveys/",
	"title": "Analyzing Surveys",
	"description": "",
	"content": "Our streak of workshops continues. This time we taught professionals from public administration how they can leverage data analytics and machine learning to retrieve interesting information from surveys. Thanks to the Ministry of Public Administration, this is only the first in a line of workshops on data science we are preparing for public sector employees.\nFor this purpose, we have designed EnKlik Anketa widget, which you can find in Prototypes add-on. The widget reads data from a Slovenian online survey service OneClick Survey and imports the results directly into Orange.\nWe have prepared a test survey, which you can import by entering a public link to data into the widget. Here’s the link: https://www.1ka.si/podatki/141025/72F5B3CC/ . Copy it into the Public link URL line in the widget. Once you press Enter, the widget loads the data and displays retrieved features, just like the File widget.\nEnKlik Anketa widget is similar to the File widget. It also enables changing the attribute type and role.\nThe survey is in Slovenian, but we can use Edit Domain to turn feature names into English equivalent.\nWe renamed attributes in order as they appear in the survey. If you load the survey yourself, you can rename them just like you see here.\nAs always, we can check the data in a Data Table. We have 41 respondents and 7 questions. Each respondent chose a nickname, which makes it easier to browse the data.\nNow we can perform familiar clustering to uncover interesting groups in our data. Connect Distances to Edit Domain and Hierarchical Clustering to Distances.\nDistance from Pipi and Chad to other respondents is very high, which makes them complete outliers.\nWe have two outliers, Pipi and Chad. One is an excessive sportsman (100 h of sport per week) and the other terminally ill (general health -1). Or perhaps they both simply didn’t fill out the survey correctly. If we use the Data Table to filter out Pipi and Chad, we get a fairly good clustering.\nWe can use Box Plot, to observe what makes each cluster special. Connect Box Plot to Hierarchical Clustering (with the two groups selected), select grouping by_ Cluster_ and tick _Order by relevance_.\nBox Plot separates distributions by Cluster and orders attributes by how well they split selected subgroups.\nThe final workflow.\nSeems like our second cluster (C2) is the sporty one. If we are serving in the public administration, perhaps we can design initiatives targeting cluster C1 to do more sports. It is so easy to analyze the data in Orange!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Our streak of workshops continues. This time we taught professionals from public administration how they can leverage data analytics and machine learning to retrieve interesting information from surveys. Thanks to the Ministry of Public Administration, this is only the first in a line of workshops on data science we are preparing for public sector employees.\nFor this purpose, we have designed EnKlik Anketa widget, which you can find in Prototypes add-on." ,
	"author" : "AJDA",
	"summary" : "Our streak of workshops continues. This time we taught professionals from public administration how they can leverage data analytics and machine learning to retrieve interesting information from surveys. Thanks to the Ministry of Public Administration, this is only the first in a line of workshops on data science we are preparing for public sector employees.\nFor this purpose, we have designed EnKlik Anketa widget, which you can find in Prototypes add-on.",
	"date" : "Oct 26, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Analyzing Surveys",
	"icon" : ""
},
{
    "uri": "/blog/2017/10/13/diving-into-car-registration-data/",
	"title": "Diving Into Car Registration Data",
	"description": "",
	"content": "Last week, we presented Orange at the Festival of Open Data, a mini-conference organized by the Slovenian government, dedicated to the promotion of transparent access to government data. In a 10 minute presentation, we showed how Orange can be used to visualize and explore what kinds of vehicles were registered for the first time in Slovenia in 2017.\nThe original dataset is available at theOPSI portal and it consists of 73 files, one for each month since January 2012. For the presentation, we focused on the 2017 data. If you want to follow along, you can download the merged dataset (first 9 months of 2017 as a single file). The workflow I used to prepare the data is also available.\nWhen exploring the data, the first thing we do is take a look at distributions. If we observe the distribution of new and used cars bought by the gender of the buyer, we can see that men prefer used cars while women more often opt for a new car. Or we can observe the distribution by age to see that older people tend to buy newer cars.\nBut the true power of Orange can be seen if we visualize the data on a map. In order to do this, we need to first use Geocoding to map municipality names to regions which can be shown on a map by choosing the column that contains municipality name (C1.3-Obcina uporabnika) and clicking apply. Since municipalities in Slovenia are created all the time, not all of them can be matched. The right part of the widget allows us to map these small municipalities to the nearest region. Or we can just ignore them.\nThe geocoded data can be displayed with Choropleth. If we select attribute D.1-Znamka and aggregation by mode, we get a visualization showing the most frequently bought mode for each region. Care to guess which manufacturer corresponds to the pink(-ish) color? It’s Volkswagen, in some regions with Golf and in other regions with Passat. But the visualization gives us just the most frequent value for each municipality. What if we would like to know more? As is the case with all visualizations you can click on a specific region on a map to select it and get the corresponding data on the output. We can then use Purge Domain to ignore the models that were not sold in the selected region and Box Plot to visualize the distribution by the model or by the manufacturer.\nIn Box Plot, select D.1 Znamka as both the variable and Subgroup and you get an overview of the distribution of cars by manufacturers in the selected region. But that is just the first step. We can also take a look at the distribution of Fiat cars by adding another boxplot. Now you can select the manufacturer and get a detailed distribution of specific car models sold. If you take some care in positioning the windows, you can create an interactive explorer, where you click on regions and instantly see the detailed distributions in the connected boxplots.\nThe final workflow should look like this:\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Last week, we presented Orange at the Festival of Open Data, a mini-conference organized by the Slovenian government, dedicated to the promotion of transparent access to government data. In a 10 minute presentation, we showed how Orange can be used to visualize and explore what kinds of vehicles were registered for the first time in Slovenia in 2017.\nThe original dataset is available at theOPSI portal and it consists of 73 files, one for each month since January 2012." ,
	"author" : "ASTARIC",
	"summary" : "Last week, we presented Orange at the Festival of Open Data, a mini-conference organized by the Slovenian government, dedicated to the promotion of transparent access to government data. In a 10 minute presentation, we showed how Orange can be used to visualize and explore what kinds of vehicles were registered for the first time in Slovenia in 2017.\nThe original dataset is available at theOPSI portal and it consists of 73 files, one for each month since January 2012.",
	"date" : "Oct 13, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Diving Into Car Registration Data",
	"icon" : ""
},
{
    "uri": "/blog/2017/09/22/understanding-voting-patterns-at-akos-workshop/",
	"title": "Understanding Voting Patterns  at AKOS Workshop",
	"description": "",
	"content": "Two days ago we held another Introduction to Data Mining workshop at our faculty. This time the target audience was a group of public sector professionals and our challenge was finding the right data set to explain key data mining concepts. Iris is fun, but not everyone is a biologist, right? Fortunately, we found this really nice data set with ballot counts from the Slovenian National Assembly (thanks to Parlameter).\nRelated: Intro to Data Mining for Life Scientists\nWorkshop for the Agency for Communication Networks and Services (AKOS).\nThe data contains ballot counts, statistics, and description for 84 members of the parliament (MPs). First, we inspected the data in a Data Table. Each MP is described with 14 meta features and has 18 ballot counts recorded.\nOut data has 84 instances, 18 features (ballot counts) and 14 meta features (MP description).\nWe have some numerical features, which means we can also inspect the data in Scatter Plot. We will plot MPs’ attendance vs. the number of their initiatives. Quite interesting! There is a big group of MPs who regularly attend the sessions, but rarely propose changes. Could this be the coalition?\nScatter plot of MPs’ session attendance (in percentage) and the number of initiatives. Already an interesting pattern emerges.\nThe next question that springs to our mind is - can we discover interesting voting patterns from our data? Let us see. We first explored the data in Hierarchical Clustering. Looks like there are some nice clusters in our data! The blue cluster is the coalition, red the SDS party and green the rest (both from the opposition).\nRelated: Hierarchical Clustering: A Simple Explanation\nHierarchical Clustering visualizes a hierarchy of clusters. But it is hard to observe similarity of pairs of data instances. How similar are Luka Mesec and Branko Grims? It is hard to tell…\nBut it is hard to inspect so many data instances in a dendrogram. For example, we have no idea how similar are the voting records of Eva Irgl and Alenka Bratušek. Surely, there must be a better way to explore similarities and perhaps verify that voting patterns exist at even a party-level… Let us try MDS. MDS transforms multidimensional data into a 2D projection so that similar data instances lie close to each other.\nMDS can plot a multidimensional data in 2D so that similar data points lie close to each other. But sometimes this optimization is hard. This is why we have grey lines connecting the dots - the dots connected are similar at the selected cut-off level (Show similar pairs slider).\nAh, this is nice! We even colored data points by the party. MDS beautifully shows the coalition (blue dots) and the opposition (all other colors). Even parties are clustered together. But there are some outliers. Let us inspect Matej Tonin, who is quite far away from his orange group. Seems like he was missing at the last two sessions and did not vote. Hence his voting is treated differently.\nData Table is a handy tool for instant data inspection. It is always great to check, what is on the output of each widget.\nIt is always great to inspect discovered groups and outliers. This way an expert can interpret the clusters and also explain, what outliers mean. Sometimes it is simply a matter of data (missing values), but sometimes we could find shifting alliances. Perhaps an outlier could be an MP about to switch to another party.\nThe final workflow.\nYou can have fun with these data, too. Let us know if you discover something interesting!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Two days ago we held another Introduction to Data Mining workshop at our faculty. This time the target audience was a group of public sector professionals and our challenge was finding the right data set to explain key data mining concepts. Iris is fun, but not everyone is a biologist, right? Fortunately, we found this really nice data set with ballot counts from the Slovenian National Assembly (thanks to Parlameter)." ,
	"author" : "AJDA",
	"summary" : "Two days ago we held another Introduction to Data Mining workshop at our faculty. This time the target audience was a group of public sector professionals and our challenge was finding the right data set to explain key data mining concepts. Iris is fun, but not everyone is a biologist, right? Fortunately, we found this really nice data set with ballot counts from the Slovenian National Assembly (thanks to Parlameter).",
	"date" : "Sep 22, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Understanding Voting Patterns  at AKOS Workshop",
	"icon" : ""
},
{
    "uri": "/blog/unsupervised/",
	"title": "unsupervised",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 22, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "unsupervised",
	"icon" : ""
},
{
    "uri": "/blog/2017/09/15/orange-at-station-houston/",
	"title": "Orange at Station Houston",
	"description": "",
	"content": "With over 262 member companies, Station Houston is the largest hub for tech startups in Houston.\nOne of its members is also Genialis, a life science data exploration company that emerged from our lab and is now delivering pipelines and user-friendly apps for analytics in systems biology.\nThanks to the invitation by the director of operations Alex de la Fuente, we gave a seminar on Data Science for Everyone. We spoke about how Orange can support anyone to learn about data science and then use machine learning on their own data.\nWe pushed on this last point: say you walk in downtown Houston, pick first three passersby, take them to the workshop and train them in machine learning. To the point where they could walk out from the training and use some machine learning at home. Say, cluster their family photos, or figure out what Kickstarter project features to optimize to get the funding.\nHow long would such workshop take? Our informed guess: three hours. And of course, we illustrated this point to seminar attendees by giving a demo of the clustering of images in Orange and showcasing Kickstarter data analysis.\nRelated: Image Analytics: Clustering\nSeminars at Station Houston need to finish with a homework. So we delivered one. Here it is:\n Open your browser. Find some images of your interest (mountains, cities, cars, fish, dogs, faces, whatever). Place images in a folder (Mac: just drag the thumbnails, Win: right click and Save Image). Download \u0026 install Orange. From Orange, install Image Analytics add-on (Options, Add-Ons). Use Orange to cluster images. Does clustering make sense?  Data science and startups aside: there are some beautiful views from Station Houston. From the kitchen, there is a straight sight to Houston’s medical center looming about 4 miles away.\nAnd on the other side, there is a great view of the downtown.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "With over 262 member companies, Station Houston is the largest hub for tech startups in Houston.\nOne of its members is also Genialis, a life science data exploration company that emerged from our lab and is now delivering pipelines and user-friendly apps for analytics in systems biology.\nThanks to the invitation by the director of operations Alex de la Fuente, we gave a seminar on Data Science for Everyone. We spoke about how Orange can support anyone to learn about data science and then use machine learning on their own data." ,
	"author" : "BLAZ",
	"summary" : "With over 262 member companies, Station Houston is the largest hub for tech startups in Houston.\nOne of its members is also Genialis, a life science data exploration company that emerged from our lab and is now delivering pipelines and user-friendly apps for analytics in systems biology.\nThanks to the invitation by the director of operations Alex de la Fuente, we gave a seminar on Data Science for Everyone. We spoke about how Orange can support anyone to learn about data science and then use machine learning on their own data.",
	"date" : "Sep 15, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at Station Houston",
	"icon" : ""
},
{
    "uri": "/blog/2017/08/28/can-we-download-orange-faster/",
	"title": "Can We Download Orange Faster?",
	"description": "",
	"content": "One day Blaž and Janez came to us and started complaining how slow Orange download is in the US. Since they hold a large course at Baylor College of Medicine every year, this causes some frustration.\nRelated: Introduction to Data Mining Course in Houston\nBut we have the data and we’ve promptly tried to confirm their complaints by analyzing them… well, in Orange!\nFirst, let us observe the data. We have 4887 recorded download sessions with one meta feature reporting on the country of the download and four features with time, size, speed in bytes and speed in gigabytes of the download.\nData of Orange download statistics. We get reports on the country of download, the size and the time of the download. We have constructed speed and size in gigabytes ourselves with simple formulae.\nNow let us check the validity of Blaž’s and Janez’s complaint. We will use orange3-geo add-on for plotting geolocated data. For any geoplotting, we need coordinates - latitude and longitude. To retrieve them automatically, we will use Geocoding widget.\nWe instruct Geocoding to retrieve coordinates from our Country feature. Identifier type tells the widget in what format the region name appears.\nWe told the widget to use the ISO-compliant country code from Country attribute and encode it into coordinates. If we check the new data in a Data Table, we see our data is enhanced with new features.\nEnhanced data table. Besides latitude and longitude, Geocoding can also append country-level data (economy, continent, region…).\nNow that we have coordinates, we can plot these data regionally - in Choropleth widget! This widget plots data on three levels - country, state/region and county/municipality. Levels correspond to the administrative division of each country.\nChoropleth widget offers 3 aggregation levels. We chose country (e.g. administrative level 0), but with a more detailed data one could also plot by state/county/municipality. Administrative levels are different for each country (e.g. Bundesländer for Germany, states for the US, provinces for Canada…).\nIn the plot above, we have simply displayed the amount of people (Count) that downloaded Orange in the past couple of months. Seems like we indeed have most users in the US, so it might make sense to solve installation issues for this region first.\nNow let us check the speed of the download - it is really so slow in the US? If we take the mean, we can see that Slovenia is far ahead of the rest as far as download speed is concerned. No wonder - we are downloading via the local network. Scandinavia, Central Europe and a part of the Balkans seem to do quite ok as well.\nAggregation by mean.\nBut mean sometimes doesn’t show the right picture - it is sensitive to outliers, which would be the case of Slovenia here. Let us try median instead. Looks like 50% of American download at speed lower than 1.5MB/s. Quite average, but it could be better.\nAggregation by median.\nAnd the longest time someone was prepared to wait for the download? Over 3 hours. Kudos, mate! We appreciate it! 🙌\nThis simple workflow is all it took to do our analysis.\nSo how is your download speed for Orange compared to other things you are downloading? Better, worse? We’re keen to hear it! 👂\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One day Blaž and Janez came to us and started complaining how slow Orange download is in the US. Since they hold a large course at Baylor College of Medicine every year, this causes some frustration.\nRelated: Introduction to Data Mining Course in Houston\nBut we have the data and we\u0026rsquo;ve promptly tried to confirm their complaints by analyzing them\u0026hellip; well, in Orange!\nFirst, let us observe the data. We have 4887 recorded download sessions with one meta feature reporting on the country of the download and four features with time, size, speed in bytes and speed in gigabytes of the download." ,
	"author" : "AJDA",
	"summary" : "One day Blaž and Janez came to us and started complaining how slow Orange download is in the US. Since they hold a large course at Baylor College of Medicine every year, this causes some frustration.\nRelated: Introduction to Data Mining Course in Houston\nBut we have the data and we\u0026rsquo;ve promptly tried to confirm their complaints by analyzing them\u0026hellip; well, in Orange!\nFirst, let us observe the data. We have 4887 recorded download sessions with one meta feature reporting on the country of the download and four features with time, size, speed in bytes and speed in gigabytes of the download.",
	"date" : "Aug 28, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Can We Download Orange Faster?",
	"icon" : ""
},
{
    "uri": "/blog/2017/08/11/its-sailing-time-again/",
	"title": "It&amp;#39;s Sailing Time (Again)",
	"description": "",
	"content": "Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A. Olshen). And finally, because they make the base of random forests, one of the most accurate machine learning models for smaller and mid-size data sets.\nRelated: Introduction to Data Mining Course in Houston\nLecture on classification trees has to start with the data. Years back I have crafted a data set on sailing. Every data set has to have a story. Here is one:\nSara likes weekend sailing. Though, not under any condition. Past twenty Wednesdays I have asked her if she will have any company, what kind of boat she can rent, and I have checked the weather forecast. Then, on Saturday, I wrote down if she actually went to the Sea. Data on Sara’s sailing contains three attributes (Outlook, Company, Sailboat) and a class (Sail).\nThe data comes with Orange and you can get them from Data Sets widget (currently in Prototypes Add-On, but soon to be moved to core Orange). It takes time, usually two lecture hours, to go through probabilities, entropy and information gain, but at the end, the data analysis workflow we develop with students looks something like this:\nAnd here is the classification tree:\nTurns out that Sara is a social person. When the company is big, she goes sailing no matter what. When the company is smaller, she would not go sailing if the weather is bad. But when it is sunny, sailing is fun, even when being alone.\nRelated: Pythagorean Trees and Forests\nClassification trees are not very stable classifiers. Even with small changes in the data, the trees can change substantially. This is an important concept that leads to the use of ensembles like random forests. It is also here, during my lecture, that I need to demonstrate this instability. I use Data Sampler and show a classification tree under the current sampling. Pressing on Sample Data button the tree changes every time. The workflow I use is below, but if you really want to see this in action, well, try it in Orange.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A." ,
	"author" : "BLAZ",
	"summary" : "Every fall I teach a course on Introduction to Data Mining. And while the course is really on statistical learning and its applications, I also venture into classification trees. For several reasons. First, I can introduce information gain and with it feature scoring and ranking. Second, classification trees are one of the first machine learning approaches co-invented by engineers (Ross Quinlan) and statisticians (Leo Breiman, Jerome Friedman, Charles J. Stone, Richard A.",
	"date" : "Aug 11, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "It&#39;s Sailing Time (Again)",
	"icon" : ""
},
{
    "uri": "/blog/tree/",
	"title": "tree",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 11, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "tree",
	"icon" : ""
},
{
    "uri": "/blog/2017/08/08/text-analysis-workshop-at-digital-humanities-2017/",
	"title": "Text Analysis Workshop at Digital Humanities 2017",
	"description": "",
	"content": "How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?\nIt seems so, especially when Orange is involved.\nYesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers. But at the same time, everyone was also excited. The audience about the possibilities Orange offers for their future projects and the lecturers about the fantastic participants who even during the workshop were already experimenting with their own data.\nThe biggest challenge was presenting the inner workings of algorithms to a predominantly non-computer science crowd. Luckily, we had Tree Viewer and Nomogram to help us explain Classification Tree and Logistic Regression! Everything is much easier with vizualizations.\nClassification Tree splits first by the word ‘came’, since it results in the purest split. Next it splits by ‘strange’. Since we still don’t have pure nodes, it continues to ‘bench’, which gives a satisfying result. Trees are easy to explain, but can quickly overfit the data.\nLogistic Regression transforms word counts to points. The sum of points directly corresponds to class probability. Here, if you see 29 foxes in a text, you get a high probability of Animal Tales. If you don’t see any, then you get a high probability of the opposite class.\nAt the end, we were experimenting with explorative data analysis, where we had Hierarchical Clustering, Corpus Viewer, Image Viewer and Geo Map opened at the same time. This is how a researcher can interactively explore the dendrogram, read the documents from selected clusters, observe the corresponding images and locate them on a map.\nHierarchical Clustering, Image Viewer, Geo Map and Corpus Viewer opened at the same time create an interactive data browser.\nThe workshop was a nice kick-off to an exciting week full of interesting lectures and presentations at Digital Humanities 2017 conference. So much to learn and see!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?\nIt seems so, especially when Orange is involved.\nYesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers." ,
	"author" : "AJDA",
	"summary" : "How do you explain text mining in 3 hours? Is it even possible? Can someone be ready to build predictive models and perform clustering in a single afternoon?\nIt seems so, especially when Orange is involved.\nYesterday, on August 7, we held a 3-hour workshop on text mining and text analysis for a large crowd of esteemed researchers at Digital Humanities 2017 in Montreal, Canada. Surely, after 3 hours everyone was exhausted, both the audience and the lecturers.",
	"date" : "Aug 8, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Text Analysis Workshop at Digital Humanities 2017",
	"icon" : ""
},
{
    "uri": "/blog/2017/08/04/text-analysis-new-features/",
	"title": "Text Analysis: New Features",
	"description": "",
	"content": "As always, we’ve been working hard to bring you new functionalities and improvements. Recently, we’ve released Orange version 3.4.5 and Orange3-Text version 0.2.5. We focused on the Text add-on since we are lately holding a lot of text mining workshops. The next one will be at Digital Humanities 2017 in Montreal, QC, Canada in a couple of days and we simply could not resist introducing some sexy new features_._\nRelated: Text Preprocessing\nRelated: Rehaul of Text Mining Add-On\nFirst, Orange 3.4.5 offers better support for Text add-on. What do we mean by this? Now, every core Orange widget works with Text smoothly so you can mix-and-match the widgets as you like. Before, one could not pass the output of Select Columns (data table) to Preprocess Text (corpus), but now this is no longer a problem.\nOf course, one still needs to keep in mind that Corpus is a sparse data format, which does not work with some widgets by design. For example, Manifold Learning supports only t-SNE projection.\nSecond, we’ve introduced two new widgets, which have been long overdue. One is Sentiment Analysis, which enables basic sentiment analysis of corpora. So far it works for English and uses two nltk-supported techniques - Liu Hu and Vader. Both techniques are lexicon-based. Liu Hu computes a single normalized score of sentiment in the text (negative score for negative sentiment, positive for positive, 0 is neutral), while Vader outputs scores for each category (positive, negative, neutral) and appends a total sentiment score called a compound.\nLiu Hu score.\nVader scores.\nTry it with Heat Map to visualize the scores.\nYellow represent a high, positive score, while blue represent a low, negative score. Seems like Animal Tales are generally much more negative than Tales of Magic.\nThe second widget we’ve introduced is Import Documents. This widget enables you to import your own documents into Orange and outputs a corpus on which you can perform the analysis. The widget supports .txt, .docx, .odt, .pdf and .xml files and loads an entire folder. If the folder contains subfolders, they will be considered as class values. Here’s an example.\nThis is the structure of my Kennedy folder. I will load the folder with Import Documents. Observe, how Orange creates a class variable category with post-1962 and pre-1962 as class values.\nSubfolders are considered as class in the category column.\nNow you can perform your analysis as usual.\nFinally, some widgets have cool new updates. Topic Modelling, for example, colors words by their weights - positive weights are colored green and negative red. Coloring only works with LSI, since it’s the only method that outputs both positive and negative weights.\nIf there are many kings in the text and no birds, then the text belongs to Topic 2. If there are many children and no foxes, then it belongs to Topic 3.\nTake some time, explore these improvements and let us know if you are happy with the changes! You can also submit new feature requests to our issue tracker.\nThank you for working with Orange! 🍊\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "As always, we\u0026rsquo;ve been working hard to bring you new functionalities and improvements. Recently, we\u0026rsquo;ve released Orange version 3.4.5 and Orange3-Text version 0.2.5. We focused on the Text add-on since we are lately holding a lot of text mining workshops. The next one will be at Digital Humanities 2017 in Montreal, QC, Canada in a couple of days and we simply could not resist introducing some sexy new features_._\nRelated: Text Preprocessing" ,
	"author" : "AJDA",
	"summary" : "As always, we\u0026rsquo;ve been working hard to bring you new functionalities and improvements. Recently, we\u0026rsquo;ve released Orange version 3.4.5 and Orange3-Text version 0.2.5. We focused on the Text add-on since we are lately holding a lot of text mining workshops. The next one will be at Digital Humanities 2017 in Montreal, QC, Canada in a couple of days and we simply could not resist introducing some sexy new features_._\nRelated: Text Preprocessing",
	"date" : "Aug 4, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Text Analysis: New Features",
	"icon" : ""
},
{
    "uri": "/blog/version/",
	"title": "version",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 4, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "version",
	"icon" : ""
},
{
    "uri": "/blog/2017/07/28/support-orange-developers/",
	"title": "Support Orange Developers",
	"description": "",
	"content": "Do you love Orange? Do you think it is the best thing since sliced bread? Want to thank all the developers for their hard work?\nNothing says thank you like a fresh supply of ice cream and now you can help us stock our fridge with your generous donations. 🍦🍦🍦\nDonate\nSupport open source software and the team behind Orange. We promise to squander all your contributions purely on ice cream. Can’t have a development sprint without proper refreshments! ;)\nThank you in advance for all the contributions, encouragement and support! It wouldn’t be worth it without you.\n🍊Orange team🍊\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Do you love Orange? Do you think it is the best thing since sliced bread? Want to thank all the developers for their hard work?\nNothing says thank you like a fresh supply of ice cream and now you can help us stock our fridge with your generous donations. 🍦🍦🍦\nDonate\nSupport open source software and the team behind Orange. We promise to squander all your contributions purely on ice cream." ,
	"author" : "AJDA",
	"summary" : "Do you love Orange? Do you think it is the best thing since sliced bread? Want to thank all the developers for their hard work?\nNothing says thank you like a fresh supply of ice cream and now you can help us stock our fridge with your generous donations. 🍦🍦🍦\nDonate\nSupport open source software and the team behind Orange. We promise to squander all your contributions purely on ice cream.",
	"date" : "Jul 28, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Support Orange Developers",
	"icon" : ""
},
{
    "uri": "/blog/distribution/",
	"title": "distribution",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 14, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "distribution",
	"icon" : ""
},
{
    "uri": "/blog/2017/07/14/miniconda-installer/",
	"title": "Miniconda Installer",
	"description": "",
	"content": "Orange has a new friend! It’s Miniconda, Anaconda’s little sister.\nFor a long time, the idea was to utilize the friendly nature of Miniconda to install Orange dependencies, which often misbehaved on some platforms. Miniconda provides Orange with Python 3.6 and conda installer, which is then used to handle everything Orange needs for proper functioning. So sssssss-mooth!\nMiniconda Installer\nPlease know that our Miniconda installer is in a beta state, but we are inviting adventurous testers to try it and report any bugs they find to our issue tracker [there won’t be any of course! ;) ].\nHappy testing! 🐍|🍊\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange has a new friend! It\u0026rsquo;s Miniconda, Anaconda\u0026rsquo;s little sister.\nFor a long time, the idea was to utilize the friendly nature of Miniconda to install Orange dependencies, which often misbehaved on some platforms. Miniconda provides Orange with Python 3.6 and conda installer, which is then used to handle everything Orange needs for proper functioning. So sssssss-mooth!\nMiniconda Installer\nPlease know that our Miniconda installer is in a beta state, but we are inviting adventurous testers to try it and report any bugs they find to our issue tracker [there won\u0026rsquo;t be any of course!" ,
	"author" : "AJDA",
	"summary" : "Orange has a new friend! It\u0026rsquo;s Miniconda, Anaconda\u0026rsquo;s little sister.\nFor a long time, the idea was to utilize the friendly nature of Miniconda to install Orange dependencies, which often misbehaved on some platforms. Miniconda provides Orange with Python 3.6 and conda installer, which is then used to handle everything Orange needs for proper functioning. So sssssss-mooth!\nMiniconda Installer\nPlease know that our Miniconda installer is in a beta state, but we are inviting adventurous testers to try it and report any bugs they find to our issue tracker [there won\u0026rsquo;t be any of course!",
	"date" : "Jul 14, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Miniconda Installer",
	"icon" : ""
},
{
    "uri": "/blog/2017/06/19/text-preprocessing/",
	"title": "Text Preprocessing",
	"description": "",
	"content": "In data mining, preprocessing is key. And in text mining, it is the key and the door. In other words, it’s the most vital step in the analysis.\nRelated: Text Mining add-on\nSo what does preprocessing do? Let’s have a look at an example. Place Corpus widget from Text add-on on the canvas. Open it and load Grimm-tales-selected. As always, first have a quick glance of the data in Corpus Viewer. This data set contains 44 selected Grimms’ tales.\nNow, let us see the most frequent words of this corpus in a Word Cloud.\nUgh, what a mess! The most frequent words in these texts are conjunctions (‘and’, ‘or’) and prepositions (‘in’, ‘of’), but so they are in almost every English text in the world. We need to remove these frequent and uninteresting words to get to the interesting part. We remove the punctuation by defining our tokens. Regexp \\w+ will keep full words and omit everything else. Next, we filter out the uninteresting words with a list of stopwords. The list is pre-set by nltk package and contains frequently occurring conjunctions, prepositions, pronouns, adverbs and so on.\nOk, we did some essential preprocessing. Now let us observe the results.\nThis does look much better than before! Still, we could be a bit more precise. How about removing the words could, would, should and perhaps even said, since it doesn’t say much about the content of the tale? A custom list of stopwords would come in handy!\nOpen a plain text editor, such as Notepad++ or Sublime, and place each word you wish to filter on a separate line.\nSave the file and load it next to the pre-set stopword list.\nOne final check in the Word Cloud should reveal we did a nice job preparing our data. We can now see the tales talk about kings, mothers, fathers, foxes and something that is little. Much more informative!\nRelated: Workshop: Text Analysis for Social Scientists\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "In data mining, preprocessing is key. And in text mining, it is the key and the door. In other words, it\u0026rsquo;s the most vital step in the analysis.\nRelated: Text Mining add-on\nSo what does preprocessing do? Let\u0026rsquo;s have a look at an example. Place Corpus widget from Text add-on on the canvas. Open it and load Grimm-tales-selected. As always, first have a quick glance of the data in Corpus Viewer." ,
	"author" : "AJDA",
	"summary" : "In data mining, preprocessing is key. And in text mining, it is the key and the door. In other words, it\u0026rsquo;s the most vital step in the analysis.\nRelated: Text Mining add-on\nSo what does preprocessing do? Let\u0026rsquo;s have a look at an example. Place Corpus widget from Text add-on on the canvas. Open it and load Grimm-tales-selected. As always, first have a quick glance of the data in Corpus Viewer.",
	"date" : "Jun 19, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Text Preprocessing",
	"icon" : ""
},
{
    "uri": "/blog/2017/06/09/workshop-text-analysis-for-social-scientists/",
	"title": "Workshop: Text Analysis for Social Scientists",
	"description": "",
	"content": "Yesterday was no ordinary day at the Faculty of Computer and Information Science, University of Ljubljana - there was an unusually high proportion of Social Sciences students, researchers and other professionals in our classrooms. It was all because of a Text Analysis for Social Scientists workshop.\nRelated: Data Mining for Political Scientists\nText mining is becoming a popular method across sciences and it was time to showcase what it (and Orange) can do. In this 5-hour hands-on workshop we explained text preprocessing, clustering, and predictive models, and applied them in the analysis of selected Grimm’s Tales. We discovered that predictive models can nicely distinguish between animal tales and tales of magic and that foxes and kings play a particularly important role in separating between the two types.\nNomogram displays 6 most important words (attributes) as defined by Logistic Regression. Seems like the occurrence of the word ‘fox’ can tell us a lot about whether the text is an animal tale or a tale of magic.\nRelated: Nomogram\nThe second part of the workshop was dedicated to the analysis of tweets - we learned how to work with thousands of tweets on a personal computer, we plotted them on a map by geolocation, and used Instagram images for image clustering.\nRelated: Image Analytics: Clustering\nFive hours was very little time to cover all the interesting topics in text analytics. But Orange came to the rescue once again. Interactive visualization and the possibility of close reading in Corpus Viewer were such a great help! Instead of reading 6400 tweets ‘by hand’, now the workshop participants can cluster them in interesting groups, find important words in each cluster and plot them in a 2D visualization.\nParticipants at work.\nHere, we’d like to thank NumFocus for providing financial support for the course. This enabled us to bring in students from a wide variety of fields (linguists, geographers, marketers) and prove (once again) that you don’t have to be a computer scientists to do machine learning!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Yesterday was no ordinary day at the Faculty of Computer and Information Science, University of Ljubljana - there was an unusually high proportion of Social Sciences students, researchers and other professionals in our classrooms. It was all because of a Text Analysis for Social Scientists workshop.\nRelated: Data Mining for Political Scientists\nText mining is becoming a popular method across sciences and it was time to showcase what it (and Orange) can do." ,
	"author" : "AJDA",
	"summary" : "Yesterday was no ordinary day at the Faculty of Computer and Information Science, University of Ljubljana - there was an unusually high proportion of Social Sciences students, researchers and other professionals in our classrooms. It was all because of a Text Analysis for Social Scientists workshop.\nRelated: Data Mining for Political Scientists\nText mining is becoming a popular method across sciences and it was time to showcase what it (and Orange) can do.",
	"date" : "Jun 9, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Workshop: Text Analysis for Social Scientists",
	"icon" : ""
},
{
    "uri": "/blog/2017/06/05/nomogram/",
	"title": "Nomogram",
	"description": "",
	"content": "One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly.\nSo, how does it work? First, feed the Nomogram a classification model, say, Logistic Regression. We will use the Titanic survival data that comes with Orange for this example (in File widget, choose “Browse documentation datasets”).\nIn the nomogram, we see the top ranked attributes and how much they contribute to the target class. Seems like a male third class adult had a much lower survival rate than did female first class child.\nThe first box show the target class, in our case survived=no. The second box shows the most important attribute, sex, and its contribution to the probability of the target class (more for male, almost 0 for female). The final box shows the total probability of the target class for the selected values of attributes (blue dots).\nThe most important attribute, however, seems to be ‘sex’, where the chance for survival (target class = no) is lower for males than it is for females. How do I know? Grab the blue dot over the attribute and drag it from ‘male’ to ‘female’. The total probability for dying on Titanic (survived=no) drops from 89% to 43%.\nThe same goes for all the other attributes - you can interactively explore how much a certain value contributes to the probability of a selected target class.\nBut it gets even better! Instead of dragging the blue dots in the nomogram, you can feed it the data. In the workflow below, we pass the data through the Data Table widget and then feed the selected data instance to the Nomogram. The Nomogram would then show what is the probability of the target class for this particular instance, and it would “explain” what are the magnitudes of contributions of individual attribute values.\nThis makes Nomogram a great widget for understanding the model and for interactive data exploration.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly." ,
	"author" : "AJDA",
	"summary" : "One more exciting visualization has been introduced to Orange - a Nomogram. In general, nomograms are graphical devices that can approximate the calculation of some function. A Nomogram widget in Orange visualizes Logistic Regression and Naive Bayes classification models, and compute the class probabilities given a set of attributes values. In the nomogram, we can check how changing of the attribute values affect the class probabilities, and since the widget (like widgets in Orange) is interactive, we can do this on the fly.",
	"date" : "Jun 5, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Nomogram",
	"icon" : ""
},
{
    "uri": "/blog/2017/04/25/outliers-in-traffic-signs/",
	"title": "Outliers in Traffic Signs",
	"description": "",
	"content": "Say I am given a collection of images of traffic signs, and would like to find which signs stick out. That is, which traffic signs look substantially different from the others. I would assume that the traffic signs are not equally important and that some were designed to be noted before the others.\nI have assembled a small set of regulatory and warning traffic signs and stored the references to their images in a traffic-signs-w.tab data set.\nRelated: Viewing images\nRelated: Video on image clustering\nRelated: Video on image classification\nThe easiest way to display the images is by loading this data file with File widget and then passing the data to the Image Viewer,\nOpening the Image Viewer allows me to see the images:\nNote that initially the data table we have loaded contains no valuable features on which we can do any machine learning. It includes just a category of traffic sign, its name, and the link to its image.\nWe will use deep-network embedding to turn these images into numbers to describe them with 2048 real-valued features. Then, we will use Silhouette Plot to find which traffic signs are outliers in their own group. We would like to select these and visualize them in the Image Viewer.\nRelated: All I see is silhouette\nOur final workflow, with selection of three biggest outliers (we used shift-click to select its corresponding silhouettes in the Silhouette Plot), is:\nIsn’t this great? Turns out that traffic signs were carefully designed, such that the three outliers are indeed the signs we should never miss. It is great that we can now reconfirm this design choice by deep learning-based embedding and by using some straightforward machine learning tricks such as Silhouette Plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Say I am given a collection of images of traffic signs, and would like to find which signs stick out. That is, which traffic signs look substantially different from the others. I would assume that the traffic signs are not equally important and that some were designed to be noted before the others.\nI have assembled a small set of regulatory and warning traffic signs and stored the references to their images in a traffic-signs-w." ,
	"author" : "BLAZ",
	"summary" : "Say I am given a collection of images of traffic signs, and would like to find which signs stick out. That is, which traffic signs look substantially different from the others. I would assume that the traffic signs are not equally important and that some were designed to be noted before the others.\nI have assembled a small set of regulatory and warning traffic signs and stored the references to their images in a traffic-signs-w.",
	"date" : "Apr 25, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Outliers in Traffic Signs",
	"icon" : ""
},
{
    "uri": "/blog/2017/04/07/model-replaces-classify-and-regression/",
	"title": "Model replaces Classify and Regression",
	"description": "",
	"content": "Did you recently wonder where did Classification Tree go? Or what happened to Majority?\nOrange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.\nThis, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing.tab and iris.tab!\nLeading up to this change, many algorithms were refactored so that regression and classification versions of the same method were merged into a single widget (and class in the underlying python API). For example, Classification Tree and Regression Tree have become simply Tree, which is capable of modelling categorical or numeric target variables. And similarly for SVM, kNN, Random Forest, …\nHave you ever searched for a widget by typing its name and were confused by multiple options appearing in the search box? Now you do not need to decide if you need Classification SVM or Regression SVM, you can just select SVM and enjoy the rest of the time doing actual data analysis!\nHere is a quick wrap-up:\n Majority and Mean became Constant. Classification Tree and Regression Tree became Tree. In the same manner, Random Forest and Regression Forest became Random Forest. SVM, SGD, AdaBoost and kNN now work for both classification and regression tasks. Linear Regression only works for regression. Logistic Regression, Naive Bayes and CN2 Rule Induction only work for classification.  Sorry about the last part, we really couldn’t do anything about the very nature of these algorithms! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Did you recently wonder where did Classification Tree go? Or what happened to Majority?\nOrange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.\nThis, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing." ,
	"author" : "AJDA",
	"summary" : "Did you recently wonder where did Classification Tree go? Or what happened to Majority?\nOrange 3.4.0 introduced a new widget category, Model, which now contains all supervised learning algorithms in one place and replaces the separate Classify and Regression categories.\nThis, however, was not a mere cosmetic change to the widget hierarchy. We wanted to simplify the interface for new users and make finding an appropriate learning algorithm easier. Moreover, now you can reuse some workflows on different data sets, say housing.",
	"date" : "Apr 7, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Model replaces Classify and Regression",
	"icon" : ""
},
{
    "uri": "/blog/toolbox/",
	"title": "toolbox",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 7, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "toolbox",
	"icon" : ""
},
{
    "uri": "/blog/2017/04/03/image-analytics-clustering/",
	"title": "Image Analytics: Clustering",
	"description": "",
	"content": "Data does not always come in a nice tabular form. It can also be a collection of text, audio recordings, video materials or even images. However, computers can only work with numbers, so for any data mining, we need to transform such unstructured data into a vector representation.\nFor retrieving numbers from unstructured data, Orange can use deep network embedders. We have just started to include various embedders in Orange, and for now, they are available for text and images.\nRelated: Video on image clustering\nHere, we give an example of image embedding and show how easy is to use it in Orange. Technically, Orange would send the image to the server, where the server would push an image through a pre-trained deep neural network, like Google’s Inception v3. Deep networks were most often trained with some special purpose in mind. Inception v3, for instance, can classify images into any of 1000 image classes. We can disregard the classification, consider instead the penultimate layer of the network with 2048 nodes (numbers) and use that for image’s vector-based representation.\nLet’s see this on an example.\nHere we have 19 images of domestic animals. First, download the images and unzip them. Then use Import Images widget from Orange’s Image Analytics add-on and open the directory containing the images.\nWe can visualize images in Image Viewer widget. Here is our workflow so far, with images shown in Image Viewer:\nBut what do we see in a data table? Only some useless description of images (file name, the location of the file, its size, and the image width and height).\nThis cannot help us with machine learning. As I said before, we need numbers. To acquire numerical representation of these images, we will send the images to Image Embedding widget.\nGreat! Now we have the numbers we wanted. There are 2048 of them (columns n0 to n2047). From now on, we can apply all the standard machine learning techniques, say, clustering.\nLet us measure the distance between these images and see which are the most similar. We used Distances widget to measure the distance. Normally, cosine distance works best for images, but you can experiment on your own. Then we passed the distance matrix to Hierarchical Clustering to visualize similar pairs in a dendrogram.\nThis looks very promising! All the right animals are grouped together. But I can’t see the results so well in the dendrogram. I want to see the images - with Image Viewer!\nSo cool! All the cow family is grouped together! Now we can click on different branches of the dendrogram and observe which animals belong to which group.\nBut I know what you are going to say. You are going to say I am cheating. That I intentionally selected similar images to trick you.\nI will prove you wrong. I will take a new cow, say, the most famous cow in Europe - Milka cow.\nThis image is quite different from the other images - it doesn’t have a white background, it’s a real (yet photoshopped) photo and the cow is facing us. Will the Image Embedding find the right numerical representation for this cow?\nIndeed it has. Milka is nicely put together with all the other cows.\nImage analytics is such an exciting field in machine learning and now Orange is a part of it too! You need to install the Image Analytics add on and you are all set for your research!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Data does not always come in a nice tabular form. It can also be a collection of text, audio recordings, video materials or even images. However, computers can only work with numbers, so for any data mining, we need to transform such unstructured data into a vector representation.\nFor retrieving numbers from unstructured data, Orange can use deep network embedders. We have just started to include various embedders in Orange, and for now, they are available for text and images." ,
	"author" : "AJDA",
	"summary" : "Data does not always come in a nice tabular form. It can also be a collection of text, audio recordings, video materials or even images. However, computers can only work with numbers, so for any data mining, we need to transform such unstructured data into a vector representation.\nFor retrieving numbers from unstructured data, Orange can use deep network embedders. We have just started to include various embedders in Orange, and for now, they are available for text and images.",
	"date" : "Apr 3, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Image Analytics: Clustering",
	"icon" : ""
},
{
    "uri": "/blog/2017/03/17/k-means-silhouette-score/",
	"title": "k-Means and Silhouette Score",
	"description": "",
	"content": "k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.\nBut… have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers.\n#1 Constructing workflow with k-means\n  #2 How k-means works [interactive visualization]\n  #3 How silhouette score works and why it is useful\n  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.\nBut\u0026hellip; have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers." ,
	"author" : "AJDA",
	"summary" : "k-Means is one of the most popular unsupervised learning algorithms for finding interesting groups in our data. It can be useful in customer segmentation, finding gene families, determining document types, improving human resource management and so on.\nBut\u0026hellip; have you ever wondered how k-means works? In the following three videos we explain how to construct a data analysis workflow using k-means, how k-means works, how to find a good k value and how silhouette score can help us find the inliers and the outliers.",
	"date" : "Mar 17, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "k-Means and Silhouette Score",
	"icon" : ""
},
{
    "uri": "/blog/2017/03/09/why-orange/",
	"title": "Why Orange?",
	"description": "",
	"content": "Why is Orange so great? Because it helps people solve problems quickly and efficiently.\nSašo Jakljevič, a former student of the Faculty of Computer and Information Science at University of Ljubljana, created the following motivational videos for his graduation thesis. He used two belowed datasets, iris and zoo, to showcase how to tackle real-life problems with Orange.\n    ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Why is Orange so great? Because it helps people solve problems quickly and efficiently.\nSašo Jakljevič, a former student of the Faculty of Computer and Information Science at University of Ljubljana, created the following motivational videos for his graduation thesis. He used two belowed datasets, iris and zoo, to showcase how to tackle real-life problems with Orange.\n    " ,
	"author" : "AJDA",
	"summary" : "Why is Orange so great? Because it helps people solve problems quickly and efficiently.\nSašo Jakljevič, a former student of the Faculty of Computer and Information Science at University of Ljubljana, created the following motivational videos for his graduation thesis. He used two belowed datasets, iris and zoo, to showcase how to tackle real-life problems with Orange.\n    ",
	"date" : "Mar 9, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Why Orange?",
	"icon" : ""
},
{
    "uri": "/blog/2017/03/08/workshop-on-infraorange/",
	"title": "Workshop on InfraOrange",
	"description": "",
	"content": "Thanks to the collaboration with synchrotrons Elettra (Trieste) and Soleil (Paris), Orange is getting an add-on InfraOrange, with widgets for analysis of infrared spectra. Its primary users obviously come from these two institutions, hence we organized the first workshop for InfraOrange at one of them.\nSome 20 participants spent the first day of the workshop in Trieste learning the basics of Orange and its use for data mining. With Janez at the helm and Marko assisting in the back, we traversed the standard list of visual and statistical techniques and a bit of unsupervised and supervised learning. The workshop was perhaps a bit unusual as most attendees were already quite familiar with these methods, but most haven’t yet used them in such an interactive fashion.\nMarko explaining how to analyze spectral data with Orange.\nOn the second day Marko and Andrej took over and focused on the analysis of spectral data. We demonstrated the use of widgets specifically developed for infrared data and used them with data mining techniques we covered on the first day. After lunch the attendees tried to work on their own data sets, which was a real test for InfraOrange.\nOrange for spectral data.\nGroup photo!\nWe now have a lot of realistic feedback on what to improve. There is a lot of work to do still, but a week after the workshop the most often occurring bugs have already been fixed.\nThe future of InfraOrange looks bright and…. khm… well, colorful! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Thanks to the collaboration with synchrotrons Elettra (Trieste) and Soleil (Paris), Orange is getting an add-on InfraOrange, with widgets for analysis of infrared spectra. Its primary users obviously come from these two institutions, hence we organized the first workshop for InfraOrange at one of them.\nSome 20 participants spent the first day of the workshop in Trieste learning the basics of Orange and its use for data mining. With Janez at the helm and Marko assisting in the back, we traversed the standard list of visual and statistical techniques and a bit of unsupervised and supervised learning." ,
	"author" : "AJDA",
	"summary" : "Thanks to the collaboration with synchrotrons Elettra (Trieste) and Soleil (Paris), Orange is getting an add-on InfraOrange, with widgets for analysis of infrared spectra. Its primary users obviously come from these two institutions, hence we organized the first workshop for InfraOrange at one of them.\nSome 20 participants spent the first day of the workshop in Trieste learning the basics of Orange and its use for data mining. With Janez at the helm and Marko assisting in the back, we traversed the standard list of visual and statistical techniques and a bit of unsupervised and supervised learning.",
	"date" : "Mar 8, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Workshop on InfraOrange",
	"icon" : ""
},
{
    "uri": "/blog/2017/03/06/luxembourg-pavia-ljubljana/",
	"title": "Orange Workshops: Luxembourg, Pavia, Ljubljana",
	"description": "",
	"content": "February was a month of Orange workshops.\nLjubljana: Biologists We (Tomaž, Martin and I) have started in Ljubljana with a hands-on course for the COST Action FA1405 Systems Biology Training School. This was a four hour workshop with an introduction to classification and clustering, and then with application of machine learning to analysis of gene expression data on a plant called Arabidopsis. The organization of this course has even inspired us for a creation of a new widget GOMapMan Ontology that was added to Bioinformatics add-on. We have also experimented with workflows that combine gene expressions and images of mutant. The idea was to find genes with similar expression profile, and then show images of the plants for which these genes have stood out.\nLuxembourg: Statisticians This workshop took place at STATEC, Luxembourgh’s National Institute of Statistics and Economic Studies. We (Anže and I) got invited by Nico Weydert, STATEC’s deputy director, and gave a two day lecture on machine learning and data mining to a room full of experienced statisticians. While the purpose was to showcase Orange as a tool for machine learning, we have learned a lot from participants of the course: the focus of machine learning is still different from that of classical statistics.\nStatisticians at STATEC, like all other statisticians, I guess, value, above all, understanding of the data, where accuracy of the models does not count if it cannot be explained. Machine learning often sacrifices understanding for accuracy. With focus on data and model visualization, Orange positions itself somewhere in between, but after our Luxembourg visit we are already planning on new widgets for explanation of predictions.\nPavia: Engineers About fifty engineers of all kinds at University of Pavia. Few undergrads, then mostly graduate students, some postdocs and even quite a few of the faculty staff have joined this two day course. It was a bit lighter that the one in Luxembourg, but also covered essentials of machine learning: data management, visualization and classification with quite some emphasis on overfitting on the first day, and then clustering and data projection on the second day. We finished with a showcase on image embedding and analysis. I have in particular enjoyed this last part of the workshop, where attendees were asked to grab a set of images and use Orange to find if they can cluster or classify them correctly. They were all kinds of images that they have gathered, like flowers, racing cars, guitars, photos from nature, you name it, and it was great to find that deep learning networks can be such good embedders, as most students found that machine learning on their image sets works surprisingly well.\nRelated: BDTN 2016 Workshop on introduction to data science\nRelated: Data mining course at Baylor College of Medicine\nWe thank Riccardo Bellazzi, an organizer of Pavia course, for inviting us. Oh, yeah, the pizza at Rossopommodoro was great as always, though Michella’s pasta al pesto e piselli back at Riccardo’s home was even better.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "February was a month of Orange workshops.\nLjubljana: Biologists We (Tomaž, Martin and I) have started in Ljubljana with a hands-on course for the COST Action FA1405 Systems Biology Training School. This was a four hour workshop with an introduction to classification and clustering, and then with application of machine learning to analysis of gene expression data on a plant called Arabidopsis. The organization of this course has even inspired us for a creation of a new widget GOMapMan Ontology that was added to Bioinformatics add-on." ,
	"author" : "BLAZ",
	"summary" : "February was a month of Orange workshops.\nLjubljana: Biologists We (Tomaž, Martin and I) have started in Ljubljana with a hands-on course for the COST Action FA1405 Systems Biology Training School. This was a four hour workshop with an introduction to classification and clustering, and then with application of machine learning to analysis of gene expression data on a plant called Arabidopsis. The organization of this course has even inspired us for a creation of a new widget GOMapMan Ontology that was added to Bioinformatics add-on.",
	"date" : "Mar 6, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange Workshops: Luxembourg, Pavia, Ljubljana",
	"icon" : ""
},
{
    "uri": "/blog/2017/02/23/my-first-orange-widget/",
	"title": "My First Orange Widget",
	"description": "",
	"content": "Recently, I took on a daunting task - programming my first widget. I’m not a programmer or a computer science grad, but I’ve been looking at Orange code for almost two years now and I thought I could handle it.\nI set to create a simple Concordance widget that displays word contexts in a corpus (the widget will be available in the future release). The widget turned out to be a little more complicated than I originally anticipated, but it was a great exercise in programming.\nToday, I’ll explain how I got started with my widget development. We will create a very basic Word Finder widget, that just goes through the corpus (data) and tells you whether a word occurs in a corpus or not. This particular widget is meant to be a part of the Orange3-Text add-on (so you need the add-on installed to try it), but the basic structure is the same for all widgets.\nFirst, I have to set the basic widget class.\nclass OWWordFinder(OWWidget): name = \"Word Finder\" description = \"Display whether a word is in a text or not.\" icon = \"icons/WordFinder.svg\" priority = 1 inputs = [('Corpus', Table, 'set_data')] # This widget will have no output, but in case you want one, you define it as below. # outputs = [('Output Name', output_type, 'output_method')] want_control_area = False  This sets up the description of the widget, icon, inputs and so on. want_control_area is where we say we only want the main window. Both are on by default in Orange and this simply hides the empty control area on the widget’s left side. If your widget has any parameters and controls, leave the control area on and place the buttons there.\nIn __init__ we define widget properties (such as data and queried word) and set the view. I decided to go with a very simple design - I just put everything in the mainArea. For such a basic widget this might be ok, but otherwise you might want to dig deeper into models and use QTableView, QGraphicsScene or something similar. Here we will build just the bare bones of a functioning widget.\ndef __init__(self): super().__init__() self.corpus = None # input data self.word = \"\" # queried word # setting the gui gui.widgetBox(self.mainArea, orientation=\"vertical\") self.input = gui.lineEdit(self.mainArea, self, '', orientation=\"horizontal\", label='Query:') self.input.setFocus() # run method self.search on every text change self.input.textChanged.connect(self.search) # place a text label in the mainArea self.view = QLabel() self.mainArea.layout().addWidget(self.view)  Ok, this now sets the __init__: what the widget remembers and how it looks like. With our buttons in place, the widget needs some methods, too.\nThe first method will update the self.corpus attribute, when the widget receives an input.\ndef set_data(self, data=None): if data is not None and not isinstance(data, Corpus): self.corpus = Corpus.from_table(data.domain, data) self.corpus = data self.search()  At the end we called self.search() method, which we already met in __init__ above. This method is key to our widget, as it will run the search every time the word changes. Moreover, it will run the method on the same query word when the widget is provided with a new data set, which is why we set it also in set_data().\nOk, let’s finally write this method.\ndef search(self): self.word = self.input.text() # self.corpus.tokens will run a default tokenizer, if no tokens are provided on the input result = any(self.word in doc for doc in self.corpus.tokens) self.view.setText(str(result))  This is it. This is our widget. Good job. Creating a new widget can indeed be lot of fun. You can go from a quite basic widget to very intricate, depending on your sense of adventure.\nFinally, you can get the entire widget code in gist.\nHappy programming, everyone! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Recently, I took on a daunting task - programming my first widget. I\u0026rsquo;m not a programmer or a computer science grad, but I\u0026rsquo;ve been looking at Orange code for almost two years now and I thought I could handle it.\nI set to create a simple Concordance widget that displays word contexts in a corpus (the widget will be available in the future release). The widget turned out to be a little more complicated than I originally anticipated, but it was a great exercise in programming." ,
	"author" : "AJDA",
	"summary" : "Recently, I took on a daunting task - programming my first widget. I\u0026rsquo;m not a programmer or a computer science grad, but I\u0026rsquo;ve been looking at Orange code for almost two years now and I thought I could handle it.\nI set to create a simple Concordance widget that displays word contexts in a corpus (the widget will be available in the future release). The widget turned out to be a little more complicated than I originally anticipated, but it was a great exercise in programming.",
	"date" : "Feb 23, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "My First Orange Widget",
	"icon" : ""
},
{
    "uri": "/blog/feature-engineering/",
	"title": "feature engineering",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 3, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "feature engineering",
	"icon" : ""
},
{
    "uri": "/blog/2017/02/03/for-when-you-want-to-transpose-a-data-table/",
	"title": "For When You Want to Transpose a Data Table...",
	"description": "",
	"content": "Sometimes, you need something more. Something different. Something, that helps you look at the world from a different perspective. Sometimes, you simply need to transpose your data.\nSince version 3.3.9, Orange has a Transpose widget that flips your data table around. Columns become rows and rows become columns. This is often useful, if you have, say, biological data.\nRelated: Datasets in Orange Bioinformatics\nToday we will play around with brown-selected.tab, a data set on gene expression levels for 79 experiments. Our data table has genes in rows and experiments in columns, with gene expression levels recorded as values.\nThis representation focuses on exploring genes and allows them to be plotted or to construct a model to predict their functions. But what if I want to explore the experimental conditions and see how different external conditions influence the yeast cells? For this, it would be better to have experiments in rows and genes in columns. We can do this with Transpose.\nTranspose widget took our gene meta attribute and used it for the new column names (YGR270W, YIL075C, etc.). It also appended class values to columns (Proteas). Former columns names (alpha 0, alpha 7, etc.) became our new meta attribute called Feature name.\nOk, we have a transposed data table. Now we ask ourselves: “Do similar experiment types (e.g. heat, cold, alpha, …) behave similarly?”\nLet’s use PCA to transform these many-dimensional experiment vectors into a 2-D representation. We are going to use Scatter Plot to observe experiments (not genes) in a 2-D space. We expect the same experiment types to lie closer together than other experiments. A scatter plot after a 2-D transformation looks like this:\nSpo5 11 lies quite far from the rest, so it could be an experiment to look out for. If we zoom in on the big cluster, we see that similar experiments indeed lie closer together.\nNow, if you are reproducing the result, you probably won’t see these nice colors for class.\nThis is because we used the Create Class widget to help us create new class values. Create Class already available in Orange3-Prototypes add-on and will be included in a future Orange release. You can learn more about it soon… :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Sometimes, you need something more. Something different. Something, that helps you look at the world from a different perspective. Sometimes, you simply need to transpose your data.\nSince version 3.3.9, Orange has a Transpose widget that flips your data table around. Columns become rows and rows become columns. This is often useful, if you have, say, biological data.\nRelated: Datasets in Orange Bioinformatics\nToday we will play around with brown-selected.tab, a data set on gene expression levels for 79 experiments." ,
	"author" : "AJDA",
	"summary" : "Sometimes, you need something more. Something different. Something, that helps you look at the world from a different perspective. Sometimes, you simply need to transpose your data.\nSince version 3.3.9, Orange has a Transpose widget that flips your data table around. Columns become rows and rows become columns. This is often useful, if you have, say, biological data.\nRelated: Datasets in Orange Bioinformatics\nToday we will play around with brown-selected.tab, a data set on gene expression levels for 79 experiments.",
	"date" : "Feb 3, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "For When You Want to Transpose a Data Table...",
	"icon" : ""
},
{
    "uri": "/blog/2017/01/23/preparing-scraped-data/",
	"title": "Preparing Scraped Data",
	"description": "",
	"content": "One of the key questions of every data analysis is how to get the data and put it in the right form(at). In this post I’ll show you how to easily get the data from the web and transfer it to a file Orange can read.\nRelated: Creating a new data table in Orange through Python\nFirst, we’ll have to do some scripting. We’ll use a couple of Python libraries - urllib.requests fetching the data, BeautifulSoup for reading it, csv for writing it and regular expressions for extracting the right data.\nfrom urllib.request import urlopen from bs4 import BeautifulSoup import csv import re  Ok, we’ve imported the all the libraries we’ll need. Now we will scrape the data from our own blog to see how many posts we’ve written throughout the years.\nhtml = urlopen('http://blog.biolab.si') soup = BeautifulSoup(html.read(), 'lxml')  The first line opens the address of the site we want to scrape. In our case this is our blog. The second line retrieves a html response from the site, which is our raw text. It looks like this:\n\u003caside id=\"archives-2\" class=\"widget widget_archive\"\u003e\u003ch3 class=\"widget-title\"\u003eArchives\u003c/h3\u003e \u003cul\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2017/01/'\u003eJanuary 2017\u003c/a\u003e\u0026nbsp;(1)\u003c/li\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2016/12/'\u003eDecember 2016\u003c/a\u003e\u0026nbsp;(3)\u003c/li\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2016/11/'\u003eNovember 2016\u003c/a\u003e\u0026nbsp;(4)\u003c/li\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2016/10/'\u003eOctober 2016\u003c/a\u003e\u0026nbsp;(3)\u003c/li\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2016/09/'\u003eSeptember 2016\u003c/a\u003e\u0026nbsp;(2)\u003c/li\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2016/08/'\u003eAugust 2016\u003c/a\u003e\u0026nbsp;(5)\u003c/li\u003e \u003cli\u003e\u003ca href='http://blog.biolab.si/2016/07/'\u003eJuly 2016\u003c/a\u003e\u0026nbsp;(3)\u003c/li\u003e....  Ok, html is nice, but we can’t really do data analysis with this. We will have to transform this output into something sensible. How about .csv, a simple comma-demilited format Orange can recognize?\nwith open('scraped.csv', 'w') as csvfile: csvwriter = csv.writer(csvfile, delimiter=',')  We created a new file called ‘scraped.csv’ to which we will write our content (‘w’ parameter means write). Then we defined the writer and set the delimiter to comma.\nNow we need to add the header row, so Orange will know what are the column names. We add this just after csvwriter variable.\ncsvwriter.writerow([\"Date\", \"No_of_Blogs\"])  Now we have two columns, one named ‘Date’ and the other ‘No_of_Blogs’. The final step is to extract the data. We have a bunch of lines in html, but the one we’re interested in is in a section ‘aside’ and has an id ‘archives-2’. We will first extract only this section (.find(id='archives-2’) and get all the lines of the archive with the tag ‘li’ (.find_all(‘li’)):\nfor item in soup.find(id=\"archives-2\").find_all('li'):  This is the result of print(item).\n\u003cli\u003e\u003ca href=\"http://blog.biolab.si/2017/01/\"\u003eJanuary 2017\u003c/a\u003e (1)\u003c/li\u003e  Now we need to get the actual content from each line. The first part we need is the date of the archived content. Orange can read dates, but they need to come in the right format. We will extract the date from href part with item.a.get(‘href’). Then we need to extract only digits from it as we’re not interested in the rest of the link. We do this with Regex for finding digits:\ndate = re.findall(r'\\d+', item.a.get('href'))  Regex’s findall function returns a list, in our case containing two items - the year and month of archived content. The second part of our data is the number of blogs archived in a particular month. We will again extract this with a Regex digit search, but this time we will be extracting data from the actual content - ‘item.contents[1]’.\ndigits = re.findall(r'\\d+', item.contents[1])  Finally, we need to write each line to a .csv file we created above.\ncsvwriter.writerow([\"%s-%s-01\" % (date[0], date[1]), digits[0]])  Here, we formatted the date into an ISO-standard format Orange recognizes as time variable (\"%s-%s-01” % (date[0], date[1])), while the second part is simply a count of our blog posts.\nThis is the entire code:\nfrom urllib.request import urlopen from bs4 import BeautifulSoup import csv import re html = urlopen('http://blog.biolab.si') soup = BeautifulSoup(html.read(), 'lxml') with open('scraped.csv', 'w') as csvfile: csvwriter = csv.writer(csvfile, delimiter=',') csvwriter.writerow([\"Date\", \"No_of_Blogs\"]) for item in soup.find(id=\"archives-2\").find_all('li'): date = re.findall(r'\\d+', item.a.get('href')) digits = re.findall(r'\\d+', item.contents[1]) csvwriter.writerow([\"%s-%s-01\" % (date[0], date[1]), digits[0]])  Related: Scripting with Time Variable\nNow let’s load this in Orange. File widget can easily read .csv formats and also correctly identifies the two column types, datetime and numeric. A quick glance into the Data Table…\nEverything looks ok. We can use Timeseries add-on to inspect how many blogs we’ve written each month since 2010. Connect As Timeseries widget to File. Orange will automatically suggest to use Date as our time variable. Finally, we’ll plot the data with Line Chart. This is the curve of our blog activity.\nThe example is extremely simple. A somewhat proficient user can extract much more interesting data than a simple blog count, but one always needs to keep in mind the legal aspects of web scraping. Nevertheless, this is a popular and fruitful way to extract and explore the data!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One of the key questions of every data analysis is how to get the data and put it in the right form(at). In this post I\u0026rsquo;ll show you how to easily get the data from the web and transfer it to a file Orange can read.\nRelated: Creating a new data table in Orange through Python\nFirst, we\u0026rsquo;ll have to do some scripting. We\u0026rsquo;ll use a couple of Python libraries - urllib." ,
	"author" : "AJDA",
	"summary" : "One of the key questions of every data analysis is how to get the data and put it in the right form(at). In this post I\u0026rsquo;ll show you how to easily get the data from the web and transfer it to a file Orange can read.\nRelated: Creating a new data table in Orange through Python\nFirst, we\u0026rsquo;ll have to do some scripting. We\u0026rsquo;ll use a couple of Python libraries - urllib.",
	"date" : "Jan 23, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Preparing Scraped Data",
	"icon" : ""
},
{
    "uri": "/blog/2017/01/13/data-preparation-for-machine-learning/",
	"title": "Data Preparation for Machine Learning",
	"description": "",
	"content": "We’ve said it numerous times and we’re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there’s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly.\nOrange is well equipped for such tasks.\nWidget no. 1: Preprocess\nPreprocess is there to handle a big share of your preprocessing tasks.\n It can normalize numerical data variables. Say we have a fictional data set of people employed in your company. We want to know which employees are more likely to go on holiday, based on the yearly income, years employed in your company and total years of experience in the industry. If you plot this in heat map, you would see a bold yellow line at ‘yearly income’. This obviously happens because yearly income has much higher values than years of experience or years employed by your company. You would naturally like the wage not to overweight the rest of the feature set, so normalization is the way to go. Normalization will transform your values to relative terms, that is, say (depending on the type of normalization) on a scale from 0 to 1. Now Heat Map neatly shows that people who’ve been employed longer and have a higher wage more often go on holidays. (Yes, this is a totally fictional data set, but you see the point.)           no normalization normalized data     It can impute missing values. Average or most frequent missing value imputation might seem as overly simple, but it actually works most of the time. Also, all the learners that require imputation do it implicitly, so the user doesn’t have to configure yet another widget for that. If you want to compare your results against a randomly mixed data set, select ‘Randomize’ or if you want to select relevant features, this is the widget for it.  Preprocessing needs to be used with caution and understanding of your data to avoid losing important information or, worse, overfitting the model. A good example is a case of paramedics, who usually don’t record pulse if it is normal. Missing values here thus cannot be imputed by an average value or random number, but as a distinct value (normal pulse). Domain knowledge is always crucial for data preparation.\nWidget no. 2: Discretize\nFor certain tasks you might want to resort to binning, which is what Discretize does. It effectively distributes your continuous values into a selected number of bins, thus making the variable discrete-like. You can either discretize all your data variables at once, using selected discretization type, or select a particular discretization method for each attribute. The cool thing is the transformation is already displayed in the widget, so you instantly know what you’re getting in the end. A good example of discretization would be having a data set of your customers with their age recorded. It would make little sense to segment customers by each particular age, so binning them into 4 age groups (young, young-adult, middle-aged, senior) would be a great solution. Also some visualizations require feature transformation - Sieve Diagram is currently one such widget. Mosaic Display, however, has the transformation already implemented internally.\noriginal data\nDiscretized data with ‘years employed’ lower or higher then/equal to 8 (same for ‘yearly income’ and ‘experience in the industry’.\nWidget no. 3: Continuize\nThis widget essentially creates new attributes out of your discrete ones. If you have, for example, an attribute with people’s eye color, where values can be either blue, brown or green, you would probably want to have three separate attributes ‘blue’, ‘green’ and ‘brown’ with 0 or 1 if a person has that eye color. Some learners perform much better if data is transformed in such a way. You can also only have attributes where you would presume 0 is a normal condition and would only like to have deviations from the normal state recorded (‘target or first value as base’) or the normal state would be the most common value (‘most frequent value as base’). Continuize widget offers you a lot of room to play. Best thing is to select a small data set with discrete values, connect it to Continuize and then further to Data Table and change the parameters. This is how you can observe the transformations in real time. It is useful for projecting discrete data points in Linear Projection.\nOriginal data.\nContinuized data with two new columns - attribute ‘position’ was replaced by attributes ‘position=office worker’ and ‘position=technical staff’ (same for ‘gender’).\nWidget no. 4: Purge Domain\nGet a broom and sort your data! That’s what Purge Domain does. If all of the values of some attributes are constant, it will remove these attributes. If you have unused (empty) attributes in your data, it will remove them. Effectively, you will get a nice and comprehensive data set in the end.\nOriginal data.\nEmpty columns and columns with the same (constant) value were removed.\nOf course, don’t forget to include all these procedures into your report with the ‘Report’ button! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We\u0026rsquo;ve said it numerous times and we\u0026rsquo;re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there\u0026rsquo;s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve said it numerous times and we\u0026rsquo;re going to say it again. Data preparation is crucial for any data analysis. If your data is messy, there\u0026rsquo;s no way you can make sense of it, let alone a computer. Computers are great at handling large, even enormous data sets, speedy computing and recognizing patterns. But they fail miserably if you give them the wrong input. Also some classification methods work better with binary values, other with continuous, so it is important to know how to treat your data properly.",
	"date" : "Jan 13, 2017",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Preparation for Machine Learning",
	"icon" : ""
},
{
    "uri": "/blog/2016/12/22/the-beauty-of-random-forest/",
	"title": "The Beauty of Random Forest",
	"description": "",
	"content": "It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange. And even more beautiful then a single tree is Orange’s rendering of a forest, that is, a random forest.\nRelated: Pythagorean Trees and Forests\nHere are six trees in the random forest constructed on the housing data set: The random forest for annealing data set includes a set of smaller-sized trees: A Christmas-lit random forest inferred from pen digits data set looks somehow messy in trying to categorize to ten different classes: The power of beauty! No wonder random forests are one of the best machine learning tools. Orange renders them according to the idea of Fabian Beck and colleagues who proposed Pythagoras trees for visualizations of hierarchies. The actual implementation for classification and regression trees for Orange was created by Pavlin Policar.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange." ,
	"author" : "BLAZ",
	"summary" : "It is the time of the year when we adore Christmas trees. But these are not the only trees we, at Orange team, think about. In fact, through almost life-long professional deformation of being a data scientist, when I think about trees I would often think about classification and regression trees. And they can be beautiful as well. Not only for their elegance in explaining the hidden patterns, but aesthetically, when rendered in Orange.",
	"date" : "Dec 22, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "The Beauty of Random Forest",
	"icon" : ""
},
{
    "uri": "/blog/2016/12/16/bdtn-2016-workshop-introduction-to-data-science/",
	"title": "BDTN 2016 Workshop: Introduction to Data Science",
	"description": "",
	"content": "Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.\nRelated: Intro to Data Mining for Life Scientists\nThe task was to teach and explain basic data mining concepts and techniques in four hours. To complete beginners. Not daunting at all…\nLuckily, we had Orange at hand. First, we showed how the program works and how to easily import data into the software. We created a poll using Google Forms on the fly and imported the results from Google Sheets into Orange.\nTo get the first impression of our data, we used Distributions and Scatter Plot. This was just to show how to approach the construction and simple visual exploration on any new data set. Then we delved deep into the workings of classification with Classification Tree and Tree Viewer and showed how easy it is to fall into the trap of overfitting (and how to avoid it). Another topic was clustering and how to relate similar data instances to one another. Finally, we had some fun with ImageAnalytics add-on and observed whether we can detect wrongly labelled microscopy images with machine learning.\nRelated: Data Mining Course in Houston #2\nThese workshops are not only fun, but an amazing learning opportunity for us as well, as they show how our users think and how to even further improve Orange.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.\nRelated: Intro to Data Mining for Life Scientists\nThe task was to teach and explain basic data mining concepts and techniques in four hours." ,
	"author" : "AJDA",
	"summary" : "Every year BEST Ljubljana organizes BEST Days of Technology and Sciences, an event hosting a broad variety of workshops, hackathons and lectures for the students of natural sciences and technology. Introduction to Data Science, organized by our own Laboratory for Bioinformatics, was this year one of them.\nRelated: Intro to Data Mining for Life Scientists\nThe task was to teach and explain basic data mining concepts and techniques in four hours.",
	"date" : "Dec 16, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "BDTN 2016 Workshop: Introduction to Data Science",
	"icon" : ""
},
{
    "uri": "/blog/2016/12/12/dimensionality-reduction-by-manifold-learning/",
	"title": "Dimensionality Reduction by Manifold Learning",
	"description": "",
	"content": "The new Orange release (v. 3.3.9) welcomed a few wonderful additions to its widget family, including Manifold Learning widget. The widget reduces the dimensionality of the high-dimensional data and is thus wonderful in combination with visualization widgets.\nManifold Learning widget has a simple interface with powerful features.\nManifold Learning widget offers five embedding techniques based on scikit-learn library: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding. They each handle the mapping differently and also have a specific set of parameters.\nRelated: Principal Component Analysis (video)\nFor example, a popular t-SNE requires only a metric (e.g. cosine distance). In the demonstration of this widget, we output 2 components, since they are the easiest to visualize and make sense of.\nFirst, let’s load the data and open it in Scatter Plot. Not a very informative visualization, right? The dots from an unrecognizable square in 2D.\nS-curve data in Scatter Plot. Data points form an uninformative square.\nLet’s use embeddings to make things a bit more informative. This is how the data looks like with a t-SNE embedding. The data is starting to have a shape and the data points colored according to regression class reveal a beautiful gradient.\nt-SNE embedding shows an S shape of the data.\nOk, how about MDS? This is beyond our expectations!\nThere’s a plethora of options with embeddings. You can play around with ImageNet embeddings and plot them in 2D or use any of your own high-dimensional data and discover interesting visualizations! Although t-SNE is nowadays probably the most popular dimensionality reduction technique used in combination with scatterplot visualization, do not underestimate the value of other manifold learning techniques. For one, we often find that MDS works fine as well.\nGo, experiment!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The new Orange release (v. 3.3.9) welcomed a few wonderful additions to its widget family, including Manifold Learning widget. The widget reduces the dimensionality of the high-dimensional data and is thus wonderful in combination with visualization widgets.\nManifold Learning widget has a simple interface with powerful features.\nManifold Learning widget offers five embedding techniques based on scikit-learn library: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding. They each handle the mapping differently and also have a specific set of parameters." ,
	"author" : "AJDA",
	"summary" : "The new Orange release (v. 3.3.9) welcomed a few wonderful additions to its widget family, including Manifold Learning widget. The widget reduces the dimensionality of the high-dimensional data and is thus wonderful in combination with visualization widgets.\nManifold Learning widget has a simple interface with powerful features.\nManifold Learning widget offers five embedding techniques based on scikit-learn library: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding. They each handle the mapping differently and also have a specific set of parameters.",
	"date" : "Dec 12, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Dimensionality Reduction by Manifold Learning",
	"icon" : ""
},
{
    "uri": "/blog/2016/11/30/data-mining-for-political-scientists/",
	"title": "Data Mining for Political Scientists",
	"description": "",
	"content": "Being a political scientist, I did not even hear about data mining before I’ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!\nBut hold on! There are still many social scientists out there who haven’t yet heard about the wonderful world of data mining, text mining and machine learning. So I’ve made it my mission to spread the word. And that was the spirit that led me back to my former university - School of Political Sciences, University of Bologna.\nUniversity of Bologna is the oldest university in the world and has one of the best departments for political sciences in Europe. I held a lecture Digital Research - Data Mining for Political Scientists for MIREES students, who are specializing in research and studies in Central and Eastern Europe.\nLecture at University of Bologna\nThe main goal of the lecture was to lay out the possibilities that contemporary technology offers to researchers and to showcase a few simple text mining tasks in Orange. We analysed Trump’s and Clinton’s Twitter timeline and discovered that their tweets are highly distinct from one another and that you can easily find significant words they’re using in their tweets. Moreover, we’ve discovered that Trump is much better at social media than Clinton, creating highly likable and shareable content and inventing his own hashtags. Could that be a tell-tale sign of his recent victory?\nPerhaps. Our future, data-mining savvy political scientists will decide. Below, you can see some examples of the workflows presented at the workshop.\nAuthor predictions from Tweet content. Logistic Regression reports on 92% classification accuracy and AUC score. Confusion Matrix can output misclassified tweets to Corpus Viewer, where we can inspect these tweets further.\nWord Cloud from preprocessed tweets. We removed stopwords and punctuation to find frequencies for meaningful words only.\nWord Enrichment by Author. First we find Donald’s tweets with Select Rows and then compare them to the entire corpus in Word Enrichment. The widget outputs a ranked list of significant words for the provided subset. We do the same for Hillary’s tweets.\nFinding potential topics with LDA.\nFinally, we offered a sneak peek of our recent Tweet Profiler widget. Tweet Profiler is intended for sentiment analysis of tweets and can output classes. probabilities and embeddings. The widget is not yet officially available, but will be included in the upcoming release.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Being a political scientist, I did not even hear about data mining before I\u0026rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!\nBut hold on! There are still many social scientists out there who haven\u0026rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning." ,
	"author" : "AJDA",
	"summary" : "Being a political scientist, I did not even hear about data mining before I\u0026rsquo;ve joined Biolab. And naturally, as with all good things, data mining started to grow on me. Give me some data, connect a bunch of widgets and see the magic happen!\nBut hold on! There are still many social scientists out there who haven\u0026rsquo;t yet heard about the wonderful world of data mining, text mining and machine learning.",
	"date" : "Nov 30, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining for Political Scientists",
	"icon" : ""
},
{
    "uri": "/blog/2016/11/25/celebrity-lookalike-or-how-to-make-students-love-machine-learning/",
	"title": "Celebrity Lookalike or How to Make Students Love Machine Learning",
	"description": "",
	"content": "Recently we’ve been participating at Days of Computer Science, organized by the Museum of Post and Telecommunications and the Faculty of Computer and Information Science, University of Ljubljana, Slovenia. The project brought together pupils and students from around the country and hopefully showed them what computer science is mostly about. Most children would think programming is just typing lines of code. But it’s more than that. It’s a way of thinking, a way to solve problems creatively and efficiently. And even better, computer science can be used for solving a great variety of problems.\nRelated: On teaching data science with Orange\nOrange team has prepared a small demo project called Celebrity Lookalike. We found 65 celebrity photos online and loaded them in Orange. Next we cropped photos to faces and turned them black and white, to avoid bias in background and color. Next we inferred embeddings with ImageNet widget and got 2048 features, which are the penultimate result of the ImageNet neural network.\nWe find faces in photos and turn them to black and white. This eliminates the effect of the background and distinct colors for embeddings.\nStill, we needed a reference photo to find the celebrity lookalike for. Students could take a selfie and similarly extracted black and white face out of it. Embeddings were computed and sent to Neighbors widget. Neighbors finds n closest neighbors based on the defined distance measure to the provided reference. We decided to output 10 closest neighbors by cosine distance.\nCelebrity Lookalike workflow. We load photos, find faces and compute embeddings. We do the same for our Webcam Capture. Then we find 10 closest neighbors and observe the results in Lookalike widget.\nFinally, we used Lookalike widget to display the result. Students found it hilarious when curly boys were the Queen of England and girls with glasses Steve Jobs. They were actively trying to discover how the algorithm works by taking photo of a statue, person with or without glasses, with hats on or by making a funny face.\nHopefully this inspires a new generation of students to become scientists, researchers and to actively find solutions to their problems. Coding or not. :)\n_Note: _Most widgets we have designed for this projects (like Face Detector, Webcam Capture, and Lookalike) are available in Orange3-Prototypes and are not actively maintained. They can, however, be used for personal projects and sheer fun. Orange does not own the copyright of the images.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Recently we\u0026rsquo;ve been participating at Days of Computer Science, organized by the Museum of Post and Telecommunications and the Faculty of Computer and Information Science, University of Ljubljana, Slovenia. The project brought together pupils and students from around the country and hopefully showed them what computer science is mostly about. Most children would think programming is just typing lines of code. But it\u0026rsquo;s more than that. It\u0026rsquo;s a way of thinking, a way to solve problems creatively and efficiently." ,
	"author" : "AJDA",
	"summary" : "Recently we\u0026rsquo;ve been participating at Days of Computer Science, organized by the Museum of Post and Telecommunications and the Faculty of Computer and Information Science, University of Ljubljana, Slovenia. The project brought together pupils and students from around the country and hopefully showed them what computer science is mostly about. Most children would think programming is just typing lines of code. But it\u0026rsquo;s more than that. It\u0026rsquo;s a way of thinking, a way to solve problems creatively and efficiently.",
	"date" : "Nov 25, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Celebrity Lookalike or How to Make Students Love Machine Learning",
	"icon" : ""
},
{
    "uri": "/blog/article/",
	"title": "article",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 18, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "article",
	"icon" : ""
},
{
    "uri": "/blog/2016/11/18/top-100-changemakers-in-central-and-eastern-europe/",
	"title": "Top 100 Changemakers in Central and Eastern Europe",
	"description": "",
	"content": "Recently Orange and one of its inventors, Blaž Zupan, have been recognized as one of the top 100 changemakers in the region. A 2016 New Europe 100 is an annual list of innovators and entrepreneurs in Central and Eastern Europe highlighting novel approaches to pressing problems.\nOrange has been recognized for making data more approachable, which has been our goal from the get-go. The tool is continually being developed with the end user in mind - someone who wants to analyze his/her data quickly, visually, interactively, and efficiently. We’re always thinking hard how to expose valuable information in the data, how to improve the user experience, which defaults are the most appropriate for the method, and, finally, how to intuitively teach people about data mining.\nThis nomination is a great validation of our efforts and it only makes us work harder. Because every research should be fruitful and fun!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Recently Orange and one of its inventors, Blaž Zupan, have been recognized as one of the top 100 changemakers in the region. A 2016 New Europe 100 is an annual list of innovators and entrepreneurs in Central and Eastern Europe highlighting novel approaches to pressing problems.\nOrange has been recognized for making data more approachable, which has been our goal from the get-go. The tool is continually being developed with the end user in mind - someone who wants to analyze his/her data quickly, visually, interactively, and efficiently." ,
	"author" : "AJDA",
	"summary" : "Recently Orange and one of its inventors, Blaž Zupan, have been recognized as one of the top 100 changemakers in the region. A 2016 New Europe 100 is an annual list of innovators and entrepreneurs in Central and Eastern Europe highlighting novel approaches to pressing problems.\nOrange has been recognized for making data more approachable, which has been our goal from the get-go. The tool is continually being developed with the end user in mind - someone who wants to analyze his/her data quickly, visually, interactively, and efficiently.",
	"date" : "Nov 18, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Top 100 Changemakers in Central and Eastern Europe",
	"icon" : ""
},
{
    "uri": "/blog/2016/11/02/orange-at-eurostats-big-data-workshop/",
	"title": "Orange at Eurostat&amp;#39;s Big Data Workshop",
	"description": "",
	"content": "A Eurostat’s Big Data Workshop recently took place in Ljubljana. In a presentation we have showcased Orange as a tool to teach data science.\nThe meeting was organised by Statistical Office of Slovenia and by Eurostat, a Statistical Office of the European Union, and was a primary gathering of representatives from national statistical institutes joined within European Statistical System. The meeting discussed possibilities that big data offers to modern statistics and the role it could play in statistical offices around the world. Say, can one use twitter data to measure costumer satisfaction? Or predict employment rates? Or use traffic information to predict GDP?\nDuring the meeting, Philippe Nieuwbourg from Canada pointed out that the stack of tools for big data analysis, and actually the tool stack for data science, are rather big and are growing larger each day. There is no way that data owners can master data bases, warehouses, Python, R, web development stacks, and similar. Are we alienating the owners and users from their own sources of information?\nOf course not. We were invited to the workshop to show that there are data science tools that can actually connect users and data, and empower the users to explore the data in the ways they have never dreamed before. We claimed that these tools should\n spark the intuition, offer powerful and interactive visualizations, and offer flexibility in design of analysis workflows, say, through visual programming.  Related: Teaching data science with Orange\nWe claimed that with such tools, it takes only a few days to train users to master basic and intermediate concepts of data science. And we claimed that this could be done without diving into complex mathematics and statistics.\nPart of our presentation was a demo in Orange that showed few tricks we use in such training. The presentation included:\n a case study of interactive data exploration by building and visualizing classification tree and forests, and mapping parts of the model to the projection in a scatter plot, a demo how fun it is to draw a data set and then use it to teach about clustering, a presentation how trained deep model can be used to explore and cluster images.  Related: Data Mining Course at Baylor College of Medicine in Houston\nThe Eurostat meeting was very interesting and packed with new ideas. Our thanks to Boro Nikić for inviting us, and thanks to attendees of our session for the many questions and requests we have received during presentation and after the meeting.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "A Eurostat\u0026rsquo;s Big Data Workshop recently took place in Ljubljana. In a presentation we have showcased Orange as a tool to teach data science.\nThe meeting was organised by Statistical Office of Slovenia and by Eurostat, a Statistical Office of the European Union, and was a primary gathering of representatives from national statistical institutes joined within European Statistical System. The meeting discussed possibilities that big data offers to modern statistics and the role it could play in statistical offices around the world." ,
	"author" : "BLAZ",
	"summary" : "A Eurostat\u0026rsquo;s Big Data Workshop recently took place in Ljubljana. In a presentation we have showcased Orange as a tool to teach data science.\nThe meeting was organised by Statistical Office of Slovenia and by Eurostat, a Statistical Office of the European Union, and was a primary gathering of representatives from national statistical institutes joined within European Statistical System. The meeting discussed possibilities that big data offers to modern statistics and the role it could play in statistical offices around the world.",
	"date" : "Nov 2, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at Eurostat&#39;s Big Data Workshop",
	"icon" : ""
},
{
    "uri": "/blog/2016/10/17/tips-for-using-orange/",
	"title": "10 Tips and Tricks for Using Orange",
	"description": "",
	"content": "TIP #1: Follow tutorials and example workflows to get started.\nIt’s difficult to start using new software. Where does one start, especially a total novice in data mining? For this exact reason we’ve prepared Getting Started With Orange - YouTube tutorials for complete beginners. Example workflows on the other hand can be accessed via Help - Examples.\nTIP #2: Make use of Orange documentation.\nYou can access it in three ways:\n Press F1 when the widget is selected. This will open help screen. Select Widget - Help when the widget is selected. It works the same as above. Visit online documentation.  TIP #3:** Embed your help screen.**\nDrag and drop help screen to the side of your Orange canvas. It will become embedded in the canvas. You can also make it narrower, allowing for a full-size analysis while exploring the docs.\nTIP #4: Use right-click.\nRight-click on the canvas and a widget menu will appear. Start typing the widget you’re looking for and press Enter when the widget becomes the top widget. This will place the widget onto the canvas immediately. You can also navigate the menu with up and down.\nTIP #5: Turn off channel names.\nSometimes it is annoying to see channel names above widget links. If you’re already comfortable using Orange, you can turn them off in Options - Settings. Turn off ‘Show channel names between widgets’.\nTIP #6: Hide control pane.\nOnce you’ve set the parameters, you’d probably want to focus just on visualizations. There’s a simple way to do this in Orange. Click on the split between the control pane and visualization pane - you should see a hand appearing instead of a cursor. Click and observe how the control pane gets hidden away neatly. To make it reappear, click the split again.\nTIP #7: Label your data.\nSo you’ve plotted your data, but have no idea what you’re seeing. Use annotation! In some widgets you will see a drop-down menu called Annotation, while in others it will be called a Label. This will mark your data points with suitable labels, making your MDS plots and Scatter Plots much more informative. Scatter Plot also enables you to label only selected points for better clarity.\nTIP #8: Find your plot.\nScrolled around and lost the plot? Zoomed in too much? To re-position the plot click ‘Reset zoom’ and the visualization will jump snugly into the visualization pane. Comes in handy when browsing the subsets and trying to see the bigger picture every now and then.\nTIP #9: Reset widget settings.\nOrange is geared to remember your last settings, thus assisting you in a rapid analysis. However, sometimes you need to start anew. Go to Options - Reset widget settings… and restart Orange. This will return Orange to its original state.\nTIP #10: Use Educational add-on.\nTo learn about how some algorithms work, use Orange3-Educational add-on. It contains 4 widgets that will help you get behind the scenes of some famous algorithms. And since they’re interactive, they’re also a lot of fun!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "TIP #1: Follow tutorials and example workflows to get started.\nIt\u0026rsquo;s difficult to start using new software. Where does one start, especially a total novice in data mining? For this exact reason we\u0026rsquo;ve prepared Getting Started With Orange - YouTube tutorials for complete beginners. Example workflows on the other hand can be accessed via Help - Examples.\nTIP #2: Make use of Orange documentation.\nYou can access it in three ways:" ,
	"author" : "AJDA",
	"summary" : "TIP #1: Follow tutorials and example workflows to get started.\nIt\u0026rsquo;s difficult to start using new software. Where does one start, especially a total novice in data mining? For this exact reason we\u0026rsquo;ve prepared Getting Started With Orange - YouTube tutorials for complete beginners. Example workflows on the other hand can be accessed via Help - Examples.\nTIP #2: Make use of Orange documentation.\nYou can access it in three ways:",
	"date" : "Oct 17, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "10 Tips and Tricks for Using Orange",
	"icon" : ""
},
{
    "uri": "/blog/documentation/",
	"title": "documentation",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 17, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "documentation",
	"icon" : ""
},
{
    "uri": "/blog/canvas/",
	"title": "canvas",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 10, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "canvas",
	"icon" : ""
},
{
    "uri": "/blog/oasys/",
	"title": "oasys",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 10, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "oasys",
	"icon" : ""
},
{
    "uri": "/blog/2016/10/10/the-story-of-shadow-and-orange/",
	"title": "The Story of Shadow and Orange",
	"description": "",
	"content": "This is a long story. I remember when started my PhD in Italy. There I met a researcher and he said to me: »You should do some simulations on x-ray optics beamline.« »Yes, but how should I do that?« He gave me a big tape, it was 1986. I soon realized it was all code. But it was a code called Shadow.\nI started to look at the code, to play with it, do some simulations… Soon my boss told me:\n»You should do a simulation with asymmetric crystals for monochromators.«\n»But asymmetric crystals are not foreseen in this code.«\n»Yes, think about how to do it. You should contact Franco Cerrina, he’s the author of Shadow.«\nI indeed contacted prof. Cerrina and at that time this was not easy, because there was no direct e-mail. What we had was called a digital deck net, Digital Computers Network. I had to go to another laboratory just to send him an e-mail. Soon, he replied: »Come to see me.« I managed to get some funding to go to the US and for the next two years I spent a good amount of time in Madison, Wisconsin.\nI started to work with prof. Cerrina and it was thanks to my work on Shadow that I was called by the European Synchrotron Facility and they offered me a position. But soon I stopped working on Shadow, because I was getting busy with other things.\nIt was only in 2009 that I contacted prof. Cerrina again. We needed to upgrade our software, so I went back to the US two or three times and started working on what is now Shadow3.\nIn 2010 I organized a trip to go visit again with my family for the summer. We booked the house, we booked the trip… And it was ten days before the departure that I learned that Cerrina died. And since everything was already organized, we decided to visit the US anyway.\nThere, I went to Cerrina’s laboratory and met his PhD student, who was keeping his possessions. I said to her:\n»Tell me everything you were doing recently and I will try to recover what I can.« And at that moment, she said many things were on this big old Mac. So I proposed to buy this Mac from her, but my home institution wasn’t happy, they saw no reason to buy a second-hand Mac. Even though it contained some important things Cerrina was working on!\nLuckily, I managed to get it and I was able to recover many things from it. Moreover, I kept maintaining the Shadow code, because it is a standard software in the community. At the very beginning, the source was not public. Then it was eventually published, but the code was very complicated and nobody managed to recompile that. Thus I decided to clean the code and finally we completed the new version of Shadow in 2011.\nThree years ago it was time to update Shadow again, especially the interface. One day I discovered Orange and I thought ‘it looked nice’. In that exact time I met Luca [Rebuffi] in Trieste. He got so excited about Orange that his PhD project became redesigning Shadow’s interface with Orange! And now we have OASYS, which is an adaptation of Orange for optical physics. So I hope that in the future, we will have many more users and also many more developers helping us bring simple tools to the scientific community.\n– Manuel Sanchez del Rio\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This is a long story. I remember when started my PhD in Italy. There I met a researcher and he said to me: »You should do some simulations on x-ray optics beamline.« »Yes, but how should I do that?« He gave me a big tape, it was 1986. I soon realized it was all code. But it was a code called Shadow.\nI started to look at the code, to play with it, do some simulations… Soon my boss told me:" ,
	"author" : "AJDA",
	"summary" : "This is a long story. I remember when started my PhD in Italy. There I met a researcher and he said to me: »You should do some simulations on x-ray optics beamline.« »Yes, but how should I do that?« He gave me a big tape, it was 1986. I soon realized it was all code. But it was a code called Shadow.\nI started to look at the code, to play with it, do some simulations… Soon my boss told me:",
	"date" : "Oct 10, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "The Story of Shadow and Orange",
	"icon" : ""
},
{
    "uri": "/blog/bioorange/",
	"title": "bioorange",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 2, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "bioorange",
	"icon" : ""
},
{
    "uri": "/blog/2016/10/02/intro-to-data-mining-for-life-scientists/",
	"title": "Intro to Data Mining for Life Scientists",
	"description": "",
	"content": "RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.\nWe have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding.\nRelated: Data Mining Course at Baylor College of Medicine in Houston\nIt’s not easy to pack so many new things on data analytics within four hours, but working with Orange helps. This was a hands-on workshop. Students brought their own laptops with Orange and several of its add-ons for bioinformatics and image analytics. We also showed how to prepare one’s own data using Google Forms and designed a questionary, augment it in a class, run it with students and then analyze the questionary with Orange.\nThe hard part of any short course that includes machine learning is how to explain overfitting. The concept is not trivial for data science newcomers, but it is so important it simply cannot be left out. Luckily, Orange has some cool widgets to help us understanding the overfitting. Below is a workflow we have used. We read some data (this time it was a yeast gene expression data set called brown-selected that comes with Orange), “destroyed the data” by randomly permuting the column with class values, trained a classification tree, and observed near perfect results when the model was checked on the training data.\nSure this works, you are probably saying. The models should have been scored on a separate test set! Exactly, and this is what we have done next with Data Sampler, which lead us to cross-validation and Test \u0026 Score widget.\nThis was a great and interesting short course and we were happy to contribute to the success of the student-run MLSC-2016 conference.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.\nWe have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding." ,
	"author" : "BLAZ",
	"summary" : "RNA Club Munich has organized Molecular Life of Stem Cells Conference in Ljubljana this past Thursday, Friday and Saturday. They asked us to organize a four-hour workshop on data mining. And here we were: four of us, Ajda, Anze, Marko and myself (Blaz) run a workshop for 25 students with molecular biology and biochemistry background.\nWe have covered some basic data visualization, modeling (classification) and model scoring, hierarchical clustering and data projection, and finished with a touch of deep-learning by diving into image analysis by deep learning-based embedding.",
	"date" : "Oct 2, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Intro to Data Mining for Life Scientists",
	"icon" : ""
},
{
    "uri": "/blog/2016/09/23/text-mining-version-0-2-0/",
	"title": "Text Mining: version 0.2.0",
	"description": "",
	"content": "Orange3-Text has just recently been polished, updated and enhanced! Our GSoC student Alexey has helped us greatly to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\nWikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries. It serves as an easy data gathering source and it’s great for exploring text mining techniques. Here we’ve simply queried Wikipedia for articles on Slovenia and Germany and displayed them in Corpus Viewer.\nQuery Wikipedia by entering your query word list in the widget. Put each query on a separate line and run Search.\nSimilarity Hashing widget computes similarity hashes for the given corpus, allowing the user to find duplicates, plagiarism or textual borrowing in the corpus. Here’s an example from Wikipedia, which has a pre-defined structure of articles, making our corpus quite similar. We’ve used Wikipedia widget and retrieved 10 articles for the query ‘Slovenia’. Then we’ve used Similarity Hashing to compute hashes for our text. What we got on the output is a table of 64 binary features (predefined in the SimHash widget), which denote a 64-bit hash size. Then we computed similarities in text by sending Similarity Hashing to Distances. Here we’ve selected cosine row distances and sent the output to Hierarchical Clustering. We can see that we have some similar documents, so we can select and inspect them in Corpus Viewer.\nOutput of Similarity Hashing widget.\nWe’ve selected the two most similar documents in Hierarchical Clustering and displayed them in Corpus Viewer.\nTopic Modeling now includes three modeling algorithms, namely Latent Semantic Indexing (LSP), Latent Dirichlet Allocation (LDA), and Hierarchical Dirichlet Process (HDP). Let’s query Twitter for the latest tweets from Hillary Clinton and Donald Trump. First we preprocess the data and send the output to Topic Modeling. The widget suggests 10 topics, with the most significant words denoting each topic, and outputs topic probabilities for each document.\nWe can inspect distances between the topics with Distances (cosine) and Hierarchical Clustering. Seems like topics are not extremely author specific, since Hierarchical Clustering often puts Trump and Clinton in the same cluster. We’ve used Average linkage, but you can play around with different linkages and see if you can get better results.\nExample of comparing text by topics.\nNow we connect Corpus Viewer to Preprocess Text. This is nothing new, but Corpus Viewer now displays also tokens and POS tags. Enable POS Tagger in Preprocess Text. Now open Corpus Viewer and tick the checkbox Show Tokens \u0026 Tags. This will display tagged token at the bottom of each document.\nCorpus Viewer can now display tokens and POS tags below each document.\nThis is just a brief overview of what one can do with the new Orange text mining functionalities. Of course, these are just exemplary workflows. If you did textual analysis with great results using any of these widgets, feel free to share it with us! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange3-Text has just recently been polished, updated and enhanced! Our GSoC student Alexey has helped us greatly to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\nWikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries." ,
	"author" : "AJDA",
	"summary" : "Orange3-Text has just recently been polished, updated and enhanced! Our GSoC student Alexey has helped us greatly to achieve another milestone in Orange development and release the latest 0.2.0 version of our text mining add-on. The new release, which is already available on PyPi, includes Wikipedia and SimHash widgets and a rehaul of Bag of Words, Topic Modeling and Corpus Viewer.\nWikipedia widget allows retrieving sources from Wikipedia API and can handle multiple queries.",
	"date" : "Sep 23, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Text Mining: version 0.2.0",
	"icon" : ""
},
{
    "uri": "/blog/2016/09/15/data-mining-in-houston-2/",
	"title": "Data Mining Course in Houston #2",
	"description": "",
	"content": "This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.\nThe course was designed for students with little or no experience in data science. It consisted of seven two-hour lectures, each followed by a homework assignment. We (Blaz and Janez) lectured on data visualization, classification, regression, clustering, data projection and image analytics. We paid special attention to the problems of overfitting, use of regularization, and proper ways of testing and scoring of modeling methods.\nThe course was hands-on. The lectures were practical. They typically started with some data set and explained data mining techniques through designing data analysis workflows in Orange. Besides some standard machine learning and bioinformatics data sets, we have also painted the data to explore, say, the benefits of different classification techniques or design data sets where k-means clustering would fail.\nThis year, the course benefited from several new Orange widgets. The recently published interactive k-means widget was used to explain the inner working of this clustering algorithm, and polynomial classification widget was helpful in discussion of decision boundaries of classification algorithms. Silhouette plot was used to show how to evaluate and explore the results of clustering. And finally, we explained concepts from deep learning using image embedding to show how already trained networks can be used for clustering and classification of images.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.\nThe course was designed for students with little or no experience in data science." ,
	"author" : "BLAZ",
	"summary" : "This was already the second installment of Introduction to Data Mining Course at Baylor College of Medicine in Houston, Texas. Just like the last year, the course was packed. About 50 graduate students, post-docs and a few faculty attended, making the course one of the largest elective PhD courses from over a hundred offered at this prestigious medical school.\nThe course was designed for students with little or no experience in data science.",
	"date" : "Sep 15, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining Course in Houston #2",
	"icon" : ""
},
{
    "uri": "/blog/gsoc2016/",
	"title": "gsoc2016",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 25, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "gsoc2016",
	"icon" : ""
},
{
    "uri": "/blog/2016/08/25/visualizing-gradient-descent/",
	"title": "Visualizing Gradient Descent",
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nGradient Descent was implemented as a part of my Google Summer of Code project and it is available in the Orange3-Educational add-on. It simulates gradient descent for either Logistic or Linear regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set.\nGradient Descent widget takes data on input and outputs the model and its coefficients.\nThe widget displays the value of the cost function given two parameters of the model. For linear regression, we consider feature from the training set with the parameters being the intercept and the slope. For logistic regression, the widget considers two feature and their associated multiplicative parameters, setting the intercept to zero. Screenshot bellow shows gradient descent on a Iris data set, where we consider petal length and sepal width on the input and predict the probability that iris comes from the family of Iris versicolor.\n The type of the model used (either Logistic regression or Linear regression) Input features (one for X and one for Y axis) and the target class Learning rate is the step size of the gradient descent In a single iteration step, stochastic approach considers only a single data instance (instead of entire training set). Convergence in terms of iterations steps is slower, and we can instruct the widget to display the progress of optimization only after given number of steps (Step size) Step through the algorithm (steps can be reverted with step back button) Run optimization until convergence  Following shows gradient descent for linear regression using The Boston Housing Data Set when trying to predict the median value of a house given its age.\nOn the left we use the regular and on the right the stochastic gradient descent. While the regular descent goes straight to the target, the path of stochastic is not as smooth.\nWe can use the widget to simulate some dangerous, unwanted behavior of gradient descent. The following screenshots show two extreme cases with too high learning rate where optimization function never converges, and a low learning rate where convergence is painfully slow.\nThe two problems as illustrated above are the reason that many implementations of numerical optimization use adaptive learning rates. We can simulate this in the widget by modifying the learning rate for each step of the optimization.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nGradient Descent was implemented as a part of my Google Summer of Code project and it is available in the Orange3-Educational add-on. It simulates gradient descent for either Logistic or Linear regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set." ,
	"author" : "PRIMOZGODEC",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nGradient Descent was implemented as a part of my Google Summer of Code project and it is available in the Orange3-Educational add-on. It simulates gradient descent for either Logistic or Linear regression, depending on the type of the input data. Gradient descent is iterative approach to optimize model parameters that minimize the cost function. In machine learning, the cost function corresponds to prediction error when the model is used on the training data set.",
	"date" : "Aug 25, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Visualizing Gradient Descent",
	"icon" : ""
},
{
    "uri": "/blog/2016/08/19/making-recommendations/",
	"title": "Making recommendations",
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nRecommender systems are everywhere, we can find them on YouTube, Amazon, Netflix, iTunes,… This is because they are crucial component in a competitive retail services.\nHow can I know what you may like if I have almost no information about you? The answer: taking Collaborative filtering (CF) approaches. Basically, this means to combine all the little knowledge we have about users and/or items in order to build a grid of knowledge with which we make recommendation.\nTo help you with that, Biolab has written Orange3-Recommendation - an add-on for Orange3 to train recommendation models, cross-validate them and make predictions.\n Input data First things first. Orange3-Recommendation can read files in native tab-delimited format, or can load data from any of the major standard spreadsheet file type, like CSV and Excel. Native format starts with a header row with feature (column) names. Second header row gives the attribute type, which can be continuous, discrete, string or time. The third header line contains meta information to identify dependent features (class), irrelevant features (ignore) or meta features (meta).\nHere are the first few lines from a data set:\n tid user movie score string discrete discrete continuous meta row=1 col=1 class 1 Breza HarrySally 2 2 Dana Cvetje 5 3 Cene Prometheus 5 4 Ksenija HarrySally 4 5 Albert Matrix 4 ...  The third row is mandatory in this kind of datasets*, in order to know which attributes correspond to the users (row=1) and which ones to the items (col=1). For the case of big datasets, users and items must be specified as continuous attributes due to efficiency issues. (*Note: If the meta attributes _row_ or _col_, some simple heuristics will be applied: users=_column 0_, items=_column 1_, class=_last column_)\nHere are the first few lines from a data set :\n user movie score tid continuous continuous continuous time row=1 col=1 class meta 196 242 3 881250949 186 302 3 891717742 22 377 1 878887116 244 51 2 880606923 166 346 1 886397596 298 474 4 884182806 ...   Training a model This step is pretty simple. To train a model we have to load the data as is described above and connect it to the learner. (Don’t forget to click apply)\n If the model uses side information, we only need to add an extra file.\n In addition, we can set the parameters of our model by double-clicking it:\n By using a fixed seed, we make random numbers predictable. Therefore, this feature is useful if we want to compare results in a deterministic way.\n Cross-validation This is as simple as it seems. The only thing to point out is that side information must be connected to the model.\nStill, cross-validation is a robust way to see how our model is performing. I consider that it’s a good idea to check how our model performs with respect to the baseline. This presents a negligible overload* in our pipeline and makes our analysis more solid. _(*For 1,000,000 ratings, it can take 0.027s)._\nWe can add a baseline leaner to Test\u0026Score and select the model we want to apply.\n Making recommendations The prediction flow is exactly the same as in Orange3.\n Analyzing low-rank matrices Once we’ve output the low-rank matrices, we can play around the vectors in those matrices to discover hidden relations or understand the known ones. For instance, here we plot vector 1 and 2 from the item-feature matrix by simply connecting Data Table with selected instances to the widget Scatter Plot.\nUsing similar approaches we can discover pretty interesting things like similarity between movies or users, how movie genres relate with each other, changes in users’ behavior, when the popularity of a movie has been raised due to a commercial campaign,… and many others.\nFinally, a simple pipeline to do all of the above can be something like this:\nOn the left side we connected several models to Test\u0026Score in order to cross-validate them. Later, we trained a SVD++ model, made some predictions, got the low-rank matrices learnt by the model and plotted some vectors of the Item-feature matrix.\nAnalysis (Advanced users) Here we’ve made a workflow (which can be downloaded here) to perform a really basic analysis on the results obtained through factoring the user and item feature matrices with BRISMF over the movielens100k dataset. (Note: Once downloaded, set the prepared datasets in the folder ‘orange’. Probably you’re gonna get a couple errors. Don’t worry, it’s normal. To solve it, apply the scripts sequentially but don’t forget previously to select all the rows in the related Table.)\nInstead of explaining how this pipeline works, the best thing you can do is to download it and play with it.\nOne of the analysis you can do, is to plot the most popular movies across two first vectors of the matrix descomposition. Later, you can try to find clusters, tweak it a bit and find crossed relations (e.g. male/female Vs._ action/drama)._\nNow let’s focus on the scripting part. Rating models In this tutorial we are going to train a BRISMF model.\n  First we import Orange and the learner that we want to use:\n import Orange from orangecontrib.recommendation import BRISMFLearner    After that, we have to load a dataset:\n data = Orange.data.Table('movielens100k.tab')    Then we set the learner parameters, and finally we train it passing the dataset as an argument (the returned value will be our model trained):\n learner = BRISMFLearner(num_factors=15, num_iter=25, learning_rate=0.07, lmbda=0.1) recommender = learner(data)    Finally, we can make predictions (in this case, for the first three pairs in the dataset):\n prediction = recommender(data[:3]) print(prediction) \u003e\u003e\u003e [ 3.79505151 3.75096513 1.293013 ]     Ranking models At this point we can try something new, let’s make recommendations for a dataset in which only binary relevance is available. For this case, CLiMF is model that will suit our needs.\nimport Orange import numpy as np from orangecontrib.recommendation import CLiMFLearner # Load data data = Orange.data.Table('epinions_train.tab') # Train recommender learner = CLiMFLearner(num_factors=10, num_iter=10, learning_rate=0.0001, lmbda=0.001) recommender = learner(data) # Make recommendations recommender(X=5) \u003e\u003e\u003e [ 494, 803, 180, ..., 25520, 25507, 30815]  Later, we can score the model. In this case we’re using the MeanReciprocalRank:\nimport Orange # Load test dataset testdata = Orange.data.Table('epinions_test.tab') # Sample users num_users = len(recommender.U) num_samples = min(num_users, 1000) # max. number to sample users_sampled = np.random.choice(np.arange(num_users), num_samples) # Compute Mean Reciprocal Rank (MRR) mrr, _ = recommender.compute_mrr(data=testdata, users=users_sampled) print('MRR: %.4f' % mrr) \u003e\u003e\u003e MRR: 0.3975   SGD optimizers This add-on includes several configurations that can be used to modify the updates on the low rank matrices during the stochastic gradient descent optimization.\n SGD: Classical SGD update. Momentum: SGD with inertia. Nesterov momentum: A Momentum that “looks ahead”. AdaGrad: Optimizer that adapts its learning rating during the process. RMSProp: “Leaky” AdaGrad. AdaDelta: Extension of Adagrad that seeks to reduce its aggressive. Adam: Similar to AdaGrad and RMSProp but with an exponentially decaying average of past gradients. Adamax: Similar to Adam, but taking the maximum between the gradient and the velocity.  Do you want to learn more about this? Check our documentation!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nRecommender systems are everywhere, we can find them on YouTube, Amazon, Netflix, iTunes,\u0026hellip; This is because they are crucial component in a competitive retail services.\nHow can I know what you may like if I have almost no information about you? The answer: taking Collaborative filtering (CF) approaches. Basically, this means to combine all the little knowledge we have about users and/or items in order to build a grid of knowledge with which we make recommendation." ,
	"author" : "SALVACARRION",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nRecommender systems are everywhere, we can find them on YouTube, Amazon, Netflix, iTunes,\u0026hellip; This is because they are crucial component in a competitive retail services.\nHow can I know what you may like if I have almost no information about you? The answer: taking Collaborative filtering (CF) approaches. Basically, this means to combine all the little knowledge we have about users and/or items in order to build a grid of knowledge with which we make recommendation.",
	"date" : "Aug 19, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Making recommendations",
	"icon" : ""
},
{
    "uri": "/blog/recommender-system/",
	"title": "recommender  system",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 19, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "recommender  system",
	"icon" : ""
},
{
    "uri": "/blog/gsoc/",
	"title": "gsoc",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 16, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "gsoc",
	"icon" : ""
},
{
    "uri": "/blog/2016/08/16/polynomial-classification/",
	"title": "Visualization of Classification Probabilities",
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nPolynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.\nHere is an example workflow. The data comes from the File widget. With no learner on input, the default is Logistic Regression. Widget outputs learners Coefficients, Classifier (model) and Learner.\nPolynomial Classification widget works on two continuous features only, all other features are ignored. The screenshot shows plot of classification for an Iris data set .\n Set name of the learner. This is the name of learner on output. Set features that logistic regression is performed on. Set class that is classified separately from other classes. Set the degree of a polynom that is used to transform an input data (1 means attributes are not transformed). Select whether see or not contour lines in chart. The density of contours is regulated by Contour step.  The classification for our case fails in separating Iris-versicolor from the other two classes. This is because logistic regression is a linear classifier, and because there is no linear combination of the chosen two attributes that would make for a good decision boundary. We can change that. Polynomial expansion adds features that are polynomial combinations of original ones. For example, if an input data contains features [a, b], polynomial expansion of degree two generates feature space [1, a, b, a2, a b, b2]. With this expansion, the classification boundary looks great.\nPolynomial Classification also works well with other learners. Below we have given it a Classification Tree. This time we have painted the input data using Paint Data, a great data generator used while learning about Orange and data science. The decision boundaries for the tree are all square, a well-known limitation for tree-based learners.\nPolynomial expansion if high degrees may be dangerous. Following example shows overfitting when degree is five. See the two outliers, a blue one on the top and the red one at the lower right of the plot? The classifier was unnecessary able to separate the outliers from the pack, something that will become problematic when classifier will be used on the new data.\nOverfitting is one of the central problems in machine learning. You are welcome to read our previous blog on this problem and possible solutions.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nPolynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.\nHere is an example workflow. The data comes from the File widget." ,
	"author" : "PRIMOZGODEC",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nPolynomial Classification widget is implemented as a part of my Google Summer of Code project along with other widgets in educational add-on (see my previous blog). It visualizes probabilities for two-class classification (target vs. rest) using color gradient and contour lines, and it can do so for any Orange learner.\nHere is an example workflow. The data comes from the File widget.",
	"date" : "Aug 16, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Visualization of Classification Probabilities",
	"icon" : ""
},
{
    "uri": "/blog/2016/08/12/interactive-k-means/",
	"title": "Interactive k-Means",
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nAs a part of my Google Summer of Code project I started developing educational widgets and assemble them in an Educational Add-On for Orange. Educational widgets can be used by students to understand how some key data mining algorithms work and by teachers to demonstrate the working of these algorithms.\nHere I describe an educational widget for interactive k-means clustering, an algorithm that splits the data into clusters by finding cluster centroids such that the distance between data points and their corresponding centroid is minimized. Number of clusters in k-means algorithm is denoted with k and has to be specified manually.\nThe algorithm starts by randomly positioning the centroids in the data space, and then improving their position by repetition of the following two steps:\n Assign each point to the closest centroid. Move centroids to the mean position of points assigned to the centroid.  The widget needs the data that can come from File widget, and outputs the information on clusters (Annotated Data) and centroids:\nEducational widget for k-means works finds clusters based on two continuous features only, all other features are ignored. The screenshot shows plot of an Iris data set and clustering with _k_=3. That is partially cheating, because we know that iris data set has three classes, so that we can check if clusters correspond well to original classes:\n Select two features that are used in k-means Set number of centroids Randomize positions of centroids Show lines between centroids and corresponding points Perform the algorithm step by step. Reassign membership connects points to nearest centroid, Recompute centroids moves centroids. Step back in the algorithm Set speed of automatic stepping Perform the whole algorithm as fast preview  Anytime we can change number of centroids with spinner or with click in desired position in the graph.  If we want to see the correspondence of clusters that are denoted by k-means and classes, we can open Data Table widget where we see that all Iris-setosas are clustered in one cluster and but there are just few Iris-versicolor that are classified is same cluster together with Iris-virginica and vice versa.\nInteractive k-means works great in combination with Paint Data. There, we can design data sets where k-mains fails, and observe why.\nWe could also design data sets where k-means fails under specific initialization of centroids. Ah, I did not tell you that you can freely move the centroids and then restart the algorithm. Below we show the case of centroid initialization and how this leads to non-optimal clustering.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nAs a part of my Google Summer of Code project I started developing educational widgets and assemble them in an Educational Add-On for Orange. Educational widgets can be used by students to understand how some key data mining algorithms work and by teachers to demonstrate the working of these algorithms.\nHere I describe an educational widget for interactive k-means clustering, an algorithm that splits the data into clusters by finding cluster centroids such that the distance between data points and their corresponding centroid is minimized." ,
	"author" : "PRIMOZGODEC",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nAs a part of my Google Summer of Code project I started developing educational widgets and assemble them in an Educational Add-On for Orange. Educational widgets can be used by students to understand how some key data mining algorithms work and by teachers to demonstrate the working of these algorithms.\nHere I describe an educational widget for interactive k-means clustering, an algorithm that splits the data into clusters by finding cluster centroids such that the distance between data points and their corresponding centroid is minimized.",
	"date" : "Aug 12, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Interactive k-Means",
	"icon" : ""
},
{
    "uri": "/blog/2016/08/05/rule-induction-part-i-scripting/",
	"title": "Rule Induction (Part I - Scripting)",
	"description": "",
	"content": "This is a guest blog from the Google Summer of Code project.\nWe’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3.3.8.\nRule induction from examples is recognised as a fundamental component of many machine learning systems. My goal was foremost to implement supervised rule induction algorithms and rule-based classification methods, but also to devise a more general framework of replaceable individual components that users could fine-tune to their needs. To this purpose, separate-and-conquer strategy was applied. In essence, learning instances are covered and removed following a chosen rule. The process is repeated while learning set examples remain. To evaluate found hypotheses and to choose the best rule in each iteration, search heuristics are used (primarily, rule class distribution is the decisive determinant).\nThe use of the created module is straightforward. New rule induction algorithms can be easily introduced, by either utilising predefined components or developing new ones (these include various search algorithms, search strategies, evaluators, and others). Several well-known rule induction algorithms have already been included. Let’s see how they perform!\nClassic CN2 inducer constructs a list of ordered rules (decision list). Here, we load the titanic data set and create a simple classifier, which can already be used to predict data.\nimport Orange data = Orange.data.Table('titanic') learner = Orange.classification.CN2Learner() classifier = learner(data)  Similarly, a set of unordered rules can be constructed using Unordered CN2 inducer. Rules are learnt for each class individually, in regard to the original learning data. To evaluate found hypotheses, Laplace accuracy measure is used. Having first initialised the learner, we then control the algorithm by modifying its parameters. The underlying components are available to us by accessing the rule finder.\ndata = Table('iris.tab') learner = CN2UnorderedLearner() # consider up to 10 solution streams at one time learner.rule_finder.search_algorithm.beam_width = 10 # continuous value space is constrained to reduce computation time learner.rule_finder.search_strategy.bound_continuous = True # found rules must cover at least 15 examples learner.rule_finder.general_validator.min_covered_examples = 15 # found rules must combine at most 2 selectors (conditions) learner.rule_finder.general_validator.max_rule_length = 2 classifier = learner(data)  Induced rules can be quickly reviewed and interpreted. They are each of the form ‘if cond then predict class”. That is, a conjunction of selectors followed by the predicted class.\nfor rule in classifier.rule_list: ... print(rule, rule.curr_class_dist.tolist()) \u003e\u003e\u003e IF petal length\u003c=3.0 AND sepal width\u003e=2.9 THEN iris=Iris-setosa [49, 0, 0] \u003e\u003e\u003e IF petal length\u003e=3.0 AND petal length\u003c=4.8 THEN iris=Iris-versicolor [0, 46, 3] \u003e\u003e\u003e IF petal width\u003e=1.8 AND petal length\u003e=4.9 THEN iris=Iris-virginica [0, 0, 43] \u003e\u003e\u003e IF TRUE THEN iris=Iris-virginica [50, 50, 50] # the default rule  If no other rules fire, default rule (majority classification) is used. Specific to each individual rule inducer, the application of the default rule varies.\nThough rule learning is most frequently used in the context of predictive induction, it can be adapted to subgroup discovery. In contrast, subgroup discovery aims at learning individual patterns or interesting population subgroups, rather than to maximise classification accuracy. Induced rules prove very valuable in terms of their descriptive power. To this end, CN2-SD algorithms were also implemented.\nHopefully, the addition to the Orange software suite will benefit both novice and expert users looking advance their knowledge in a particular area of study, through a better understanding of given predictions and underlying argumentation.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This is a guest blog from the Google Summer of Code project.\nWe’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3." ,
	"author" : "MATEVZKREN",
	"summary" : "This is a guest blog from the Google Summer of Code project.\nWe’ve all heard the saying, “Rules are meant to be broken.” Regardless of how you might feel about the idea, one thing is certain. Rules must first be learnt. My 2016 Google Summer of Code project revolves around doing just that. I am developing classification rule induction techniques for Orange, and here describing the code currently available in the pull request and that will become part of official distribution in an upcoming release 3.",
	"date" : "Aug 5, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Rule Induction (Part I - Scripting)",
	"icon" : ""
},
{
    "uri": "/blog/plot/",
	"title": "plot",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 29, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "plot",
	"icon" : ""
},
{
    "uri": "/blog/2016/07/29/pythagorean-trees-and-forests/",
	"title": "Pythagorean Trees and Forests",
	"description": "",
	"content": "Classification Trees are great, but how about when they overgrow even your 27’’ screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.\nPythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square. Hence Pythagorean Tree. Every square has the color of the prevalent, with opacity indicating the relative proportion of the majority class in the subset. Details are shown in hover balloons.\nClassification Tree with titanic.tab data set.\nPythagorean Tree with titanic.tab data set.\nWhen you hover over a square in Pythagorean Tree, a whole line of parent and child squares/nodes is highlighted. Clicking on a square/node outputs the selected subset, just like in Classification Tree.\nUpon hovering on the square in the tree, the lineage (parent and child nodes) is highlighted. Hover also displays information on the subset, represented by the square. The widget outputs the selected subset.\nAnother amazing addition to Orange’s Visualization set is Pythagorean Forest, which is a visualization of Random Forest algorithm. Random Forest takes N samples from a data set with N instances, but with replacement. Then a tree is grown for each sample, which alleviates the Classification Tree’s tendency to overfit the data. Pythagorean Forest is a concise visualization of Random Forest, with each Pythagorean Tree plotted side by side.\nDifferent trees are grown side by side. Parameters for the algorithm are set in Random Forest widget, then the whole forest is sent to Pythagorean Forest for visualization.\nThis makes Pythagorean Forest a great tool to explain how Random Forest works or to further explore each tree in Pythagorean Tree widget.\nPythagorean trees are a new addition to Orange. Their implementation has been inspired by a recent paper on Generalized Pythagoras Trees for Visualizing Hierarchies by Fabian Beck, Michael Burch, Tanja Munz, Lorenzo Di Silvestro and Daniel Weiskopf that was presented in at the 5th International Conference on Information Visualization Theory and Applications in 2014.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Classification Trees are great, but how about when they overgrow even your 27\u0026rsquo;\u0026rsquo; screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.\nPythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square." ,
	"author" : "AJDA",
	"summary" : "Classification Trees are great, but how about when they overgrow even your 27\u0026rsquo;\u0026rsquo; screen? Can we make the tree fit snugly onto the screen and still tell the whole story? Well, yes we can.\nPythagorean Tree widget will show you the same information as Classification Tree, but way more concisely. Pythagorean Trees represent nodes with squares whose size is proportionate to the number of covered training instances. Once the data is split into two subsets, the corresponding new squares form a right triangle on top of the parent square.",
	"date" : "Jul 29, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Pythagorean Trees and Forests",
	"icon" : ""
},
{
    "uri": "/blog/2016/07/18/network-analysis-with-orange/",
	"title": "Network Analysis with Orange",
	"description": "",
	"content": "Visualizing relations between data instances can tell us a lot about our data. Let’s see how this works in Orange. We have a data set on machine learning and data mining conferences and journals, with the number of shared authors for each publication venue reported. We can estimate similarity between two conferences using the author profile of a conference: two conference would be similar if they attract the same authors. The data set is already 9 years old, but obviously, it’s about the principle. :) We’ve got two data files: one is a distance file with distance scores already calculated by Jaccard index and the other is a standard conferences.tab file.\nConferences.tab data file with the type of the publication venue (conference or journal) and average number of authors and published papers.\nWe load .tab file with the File widget (data set already comes with Orange) and .dst file with the Distance File widget (select ‘Browse documentation data sets’ and choose conferences.dst).\nYou can find conferences.dst in ‘Browse documentation data sets’.\nNow we would like to create a graph from the distance file. Connect Distance File to Network from Distances. In the widget, we’ve selected a high distance threshold, because we would like to get more connections between nodes. We’ve also checked ‘Include also closest neighbors’ to see each node connected with at least one other node.\nWe’ve set a high distance threshold, since we wanted to display connections between most of our nodes.\nWe can visualize our graph in Network Explorer. What we get is a quite uninformative network of conferences with labelled nodes. Now for the fun part. Connect the File widget with Network Explorer and set the link type to ‘Node Data’. This will match the two domains and display additional labelling options in Network Explorer.\nRemove the ‘Node Subset’ link and connect Data to Node Data. This will display other attributes in Network Explorer by which you can label and color your network nodes.\nNodes are colored by event type (conference or journal) and adjusted in size by the average number of authors per event (bigger nodes represent larger events).\nWe’ve colored the nodes by type and set the size of the nodes to the number of authors per conference/paper. Finally, we’ve set the node label to ‘name’. Seems like International Conference on AI and Law and AI and Law journal are connected through the number of shared authors. Same goes for AI in Medicine in Europe conference and AI and Medicine journal. Connections indeed make sense.\nThe entire workflow.\nThere are many other things you can do with the Networks add-on in Orange. You can color nodes by predictions, highlight misclassifications or output only nodes with certain network parameters. But for today, let this be it.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Visualizing relations between data instances can tell us a lot about our data. Let\u0026rsquo;s see how this works in Orange. We have a data set on machine learning and data mining conferences and journals, with the number of shared authors for each publication venue reported. We can estimate similarity between two conferences using the author profile of a conference: two conference would be similar if they attract the same authors. The data set is already 9 years old, but obviously, it\u0026rsquo;s about the principle." ,
	"author" : "AJDA",
	"summary" : "Visualizing relations between data instances can tell us a lot about our data. Let\u0026rsquo;s see how this works in Orange. We have a data set on machine learning and data mining conferences and journals, with the number of shared authors for each publication venue reported. We can estimate similarity between two conferences using the author profile of a conference: two conference would be similar if they attract the same authors. The data set is already 9 years old, but obviously, it\u0026rsquo;s about the principle.",
	"date" : "Jul 18, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Network Analysis with Orange",
	"icon" : ""
},
{
    "uri": "/blog/2016/07/05/rehaul-of-text-mining-add-on/",
	"title": "Rehaul of Text Mining Add-On",
	"description": "",
	"content": "Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we’re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We’ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content.\nTwitter widget offers many functionalities. Since we wanted to get tweets from specific authors, we entered their Twitter handles as queries and set ‘Search by Author’. We only included Author, Content and Date in the query parameters, as we want to predict the author only on the basis of text.\n Provide API keys. Insert queries separated by newline. Search by content, author or both. Set date (1 week limit from tweepy module). Select language you want your tweets to be in. If ‘Max tweets’ is checked, you can set the maximum number of tweets you want to query. Otherwise the widget will provide all tweets matching the query. If ‘Accumulate results’ is checked, new queries will be appended to the old ones. Select what kind of data you want to retrieve. Tweet count. Press ‘Search’ to start your query.  We got 208 tweets on the output. Not bad. Now we need to preprocess them first, before we do any predictions. We transformed all the words into lowercase and split (tokenized) them by words. We didn’t use any normalization (below turned on just as an example) and applied a simple stopword removal.\n Information on the input and output. Transformation applies basic modifications of text. Tokenization splits the corpus into tokens according to the selected method (regexp is set to extract only words by default). Normalization lemmatizes words (do, did, done –\u003e do). Filtering extracts only desired tokens (without stopwords, including only specified words, or by frequency).  Then we passed the tokens through a Bag of Words and observed the results in a Word Cloud.\nThen we simply connected Bag of Words to Test \u0026 Score and used several classifiers to see which one works best. We used Classification Tree and Nearest Neighbors since they are easy to explain even to teenagers. Especially Classification Tree offers a nice visualization in Classification Tree Viewer that makes the idea of the algorithm easy to understand. Moreover we could observe the most distinctive words in the tree.\nDo these make any sense? You be the judge. :)\nWe checked classification results in Test\u0026Score, counted misclassifications in Confusion Matrix and finally observed them in Corpus Viewer. k-NN seems to perform moderately well, while Classification Tree fails miserably. Still, this was trained on barely 200 tweets. Perhaps accumulating results over time might give us much better results. You can now certainly try it on your own! Update your Orange3-Text add-on or install it via ‘pip install Orange3-Text’!\nAbove is the final workflow. Preprocessing on the left. Testing and scoring on the right bottom. Construction of classification tree right and above.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we\u0026rsquo;re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We\u0026rsquo;ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content." ,
	"author" : "AJDA",
	"summary" : "Google Summer of Code is progressing nicely and some major improvements are already live! Our students have been working hard and today we\u0026rsquo;re thanking Alexey for his work on Text Mining add-on. Two major tasks before the midterms were to introduce Twitter widget and rehaul Preprocess Text. Twitter widget was designed to be a part of our summer school program and it worked beautifully. We\u0026rsquo;ve introduced youngsters to the world of data mining through social networks and one of the most exciting things was to see whether we can predict the author from the tweet content.",
	"date" : "Jul 5, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Rehaul of Text Mining Add-On",
	"icon" : ""
},
{
    "uri": "/blog/2016/06/10/scripting-with-time-variable/",
	"title": "Scripting with Time Variable",
	"description": "",
	"content": "It’s always fun to play around with data. And since Orange can, as of a few months ago, read temporal data, we decided to parse some data we had and put it into Orange.\nTimeVariable is an extended class of continuous variable and it works with properly formated ISO standard datetime (Y-M-D h:m:s). Oftentimes our original data is not in the right format and needs to be edited first, so Orange can read it. Python’s own datetime module is of great help. You can give it any date format and tell it how to interpret it in the argument.\nimport datetime date = \"13.03.2013 13:13:31\" new_date = str(datetime.datetime.strptime(date, \"%d.%m.%Y %H:%M:%S\")) \u003e\u003e\u003e '2013-03-13 13:13:31'  Do this for all your datetime attributes. This will transform them into strings that Orange’s TimeVariable can read. Then create a new data table:\nimport Orange from Orange.data import Domain, TimeVariable domain = Domain([TimeVariable(\"timestamp\")]) timestamps = [\"2013-03-13 13:13:31\", \"2014-04-14 14:14:41\", \"2015-05-15 15:15:51\"] #create a new TimeVariable object time_var = TimeVariable() #it's important to parse strings into floats with var.parse(i) #list(zip(data)) then transforms the list into a 2d list of lists time_data = Orange.data.Table(domain, list(zip(time_var.parse(i) for i in timestamps)))  Now say you have some original data you want to append your new data to.\ndata = Orange.data.Table.concatenate([original_data, time_data]) Table.save(data, \"data.tab\")  But what if you want to select only a few attributes from the original data? It can be arranged.\noriginal_data = Orange.data.Table(\"original_data.tab\") new_domain = Domain([\"attribute_1\", \"attribute_2\"], source=original_data.domain) new_data = Orange.data.Table(new_domain, original_data)  Then concatenate again:\ndata = Orange.data.Table.concatenate([new_data, time_data]) Table.save(data, \"selected_data.tab\")  Remember, if your data has string variables, they will always be in meta attributes.\ndomain = Domain([\"some_attribute1\", \"other_attribute2\"], metas=[\"some_string_variable\"])  Have fun scripting!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "It\u0026rsquo;s always fun to play around with data. And since Orange can, as of a few months ago, read temporal data, we decided to parse some data we had and put it into Orange.\nTimeVariable is an extended class of continuous variable and it works with properly formated ISO standard datetime (Y-M-D h:m:s). Oftentimes our original data is not in the right format and needs to be edited first, so Orange can read it." ,
	"author" : "AJDA",
	"summary" : "It\u0026rsquo;s always fun to play around with data. And since Orange can, as of a few months ago, read temporal data, we decided to parse some data we had and put it into Orange.\nTimeVariable is an extended class of continuous variable and it works with properly formated ISO standard datetime (Y-M-D h:m:s). Oftentimes our original data is not in the right format and needs to be edited first, so Orange can read it.",
	"date" : "Jun 10, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Scripting with Time Variable",
	"icon" : ""
},
{
    "uri": "/blog/elettra/",
	"title": "elettra",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 20, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "elettra",
	"icon" : ""
},
{
    "uri": "/blog/esrf/",
	"title": "esrf",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 20, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "esrf",
	"icon" : ""
},
{
    "uri": "/blog/2016/05/20/oasys-orange-canvas-applied-to-optical-physics/",
	"title": "Oasys: Orange Canvas applied to Optical Physics",
	"description": "",
	"content": "This week we’re hosting experts in optical physics from Elettra Sincrotrone Trieste and European Synchrotron Radiation Facility in our laboratory. For a long time they have been interested in developing a user interface that integrates different simulation tools and data analysis software within one environment. It all came true with Orange Canvas and the OASYS system. We’ve already written about this two years ago, when the idea first came up. Now the actual software is ready and is being used by researchers for everyday analysis and prototyping.\nOASYS is basically pure Orange Canvas (Orange but no data mining widgets) that is reconfigured for the needs of optical physicists. What our partners from Italy did (with the help of our lab), was bring optic simulation software used in synchrotron facilities into a single graphical user interface. What is especially incredible is that they managed to transform Orange into a simulation platform for building synchrotron beamlines.\nIn essence, researches in synchrotrons experiment with actual physical objects, such as mirrors and crystals of different shape and size to transmit photons from several light sources of the synchrotron to the experimental endstations. They measure a broad array of material properties through the interaction with the synchrotron light and are trying to simulate different experiment settings before actually building a real-life experiment in the synchrotron. And this is where OASYS truly shines.\nWidgets in this case become parts of the simulation pipeline. Each widget has an input and output beam of light, just like real life devices, and the parameters within the widget are physical properties of a particular experimental object. Thus scientists can model the experiment in advance and do it much quicker and easier than before.\nFurthermore, Orange and OASYS provide a user-friendly GUI that domain experts can quickly get used to. There are anecdotal evidences of renowned physicists, who preferred to do their analysis with outdated simulation tools. However, after using OASYS for just a few days, they were already completely comfortable and could reproduce previously calculated results in a software without any problem. Moreover, they did it within several days instead of weeks as before.\nThis is the power of visual programming - providing a user-friendly interface for automating complicated calculations and quick prototyping.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This week we’re hosting experts in optical physics from Elettra Sincrotrone Trieste and European Synchrotron Radiation Facility in our laboratory. For a long time they have been interested in developing a user interface that integrates different simulation tools and data analysis software within one environment. It all came true with Orange Canvas and the OASYS system. We’ve already written about this two years ago, when the idea first came up. Now the actual software is ready and is being used by researchers for everyday analysis and prototyping." ,
	"author" : "AJDA",
	"summary" : "This week we’re hosting experts in optical physics from Elettra Sincrotrone Trieste and European Synchrotron Radiation Facility in our laboratory. For a long time they have been interested in developing a user interface that integrates different simulation tools and data analysis software within one environment. It all came true with Orange Canvas and the OASYS system. We’ve already written about this two years ago, when the idea first came up. Now the actual software is ready and is being used by researchers for everyday analysis and prototyping.",
	"date" : "May 20, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Oasys: Orange Canvas applied to Optical Physics",
	"icon" : ""
},
{
    "uri": "/blog/orangecanvas/",
	"title": "orangecanvas",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 20, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "orangecanvas",
	"icon" : ""
},
{
    "uri": "/blog/physics/",
	"title": "physics",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 20, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "physics",
	"icon" : ""
},
{
    "uri": "/blog/synchrotron/",
	"title": "synchrotron",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "May 20, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "synchrotron",
	"icon" : ""
},
{
    "uri": "/blog/association-rules/",
	"title": "association rules",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 25, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "association rules",
	"icon" : ""
},
{
    "uri": "/blog/2016/04/25/association-rules-in-orange/",
	"title": "Association Rules in Orange",
	"description": "",
	"content": "Orange is welcoming back one of its more exciting add-ons: Associate! Association rules can help the user quickly and simply discover the underlying relationships and connections between data instances. Yeah!\nThe add-on currently has two widgets: one for Association Rules and the other for Frequent Itemsets. With Frequent Itemsets we first check frequency of items and itemsets in our transaction matrix. This tell us which items (products) and itemsets are the most frequent in our data, so it would make a lot of sense focusing on these products. Let’s use this widget on real Foodmart 2000 data set.\nFirst let’s check our data set. We have 62560 instances with 102 features. That’s a whole lot of transactions. Now we connect Frequent Itemsets to our File widget and observe the results. We went with a quite low minimal support due to the large number of transactions.\nCollapse All will display the most frequent items, so these will be our most important products (‘bestsellers’). Our clients seem to be buying a whole lot of fresh vegetables and fresh fruit. Call your marketing department - you could become the ultimate place to buy fruits and veggies from.\nIf there’s a little arrow on the left side of the item, you can expand it to see all the other items connected to the selected attribute. So if a person buy fresh vegetables, it is most likely to buy fresh fruits as an accompanying product group. Now you can explore frequent itemsets to understand what really sells in your store.\nOk. Now how about some transaction flows? We’re mostly interested in the action-consequence relationship here. In other words, if a person buys one item, what is the most likely second item she will buy? Association Rules will help us discover that.\nOur parameters will again be adjusted for our data set. We probably want low support, since it will be hard to find a few prevailing rules for 62,000+ transactions. However, you want the discovered rules to be true most of the time, so increase the confidence.\nThe table on the right displays a list of rules with 6 different measures of association rule quality:\n support: how often a rule is applicable to a given data set (rule/data) confidence: how frequently items in Y appear in transactions with X or in other words how frequently the rule is true (support for a rule/support of antecedent) coverage: how often antecedent item is found in the data set (support of antecedent/data) strength: (support of consequent/support of antecedent) lift: how frequently a rule is true per consequent item (data * confidence/support of consequent) leverage: the difference between two item appearing in a transaction and the two items appearing independently (support*data - antecedent support * consequent support/data2)  Orange will rank the rules automatically. Now give a quick look at the rules. How about these two rules?\nfresh vegetables, plastic utensils, deli meats, wine –\u003e dried fruit\nfresh vegetables, plastic utensils, bologna, soda –\u003e chocolate candy\nThese seem to picnickers, clients who don’t want to spend a whole lot of time preparing their food. The first group is probably more gourmet, while the second seems to enjoy sweets. A logical step would be to place dried fruit closer to the wine section and the candy bars closer to sodas. What do you say? This already happened in your local supermarket? Coincidence? I don’t think so. :)\nAssociation rules are a powerful way to improve your business by organizing your actual or online store, adjusting marketing strategies to target suitable groups, providing product recommendations and generally understanding your client base better. Just another way Orange can be used as a business intelligence tool!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange is welcoming back one of its more exciting add-ons: Associate! Association rules can help the user quickly and simply discover the underlying relationships and connections between data instances. Yeah!\nThe add-on currently has two widgets: one for Association Rules and the other for Frequent Itemsets. With Frequent Itemsets we first check frequency of items and itemsets in our transaction matrix. This tell us which items (products) and itemsets are the most frequent in our data, so it would make a lot of sense focusing on these products." ,
	"author" : "AJDA",
	"summary" : "Orange is welcoming back one of its more exciting add-ons: Associate! Association rules can help the user quickly and simply discover the underlying relationships and connections between data instances. Yeah!\nThe add-on currently has two widgets: one for Association Rules and the other for Frequent Itemsets. With Frequent Itemsets we first check frequency of items and itemsets in our transaction matrix. This tell us which items (products) and itemsets are the most frequent in our data, so it would make a lot of sense focusing on these products.",
	"date" : "Apr 25, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Association Rules in Orange",
	"icon" : ""
},
{
    "uri": "/blog/2016/04/14/univariate-gsoc-success/",
	"title": "Univariate GSoC Success",
	"description": "",
	"content": "Google Summer of Code application period has come to an end. We’ve received 34 applications, some of which were of truly high quality. Now it’s upon us to select the top performing candidates, but before that we wanted to have an overlook of the candidate pool. We’ve gathered data from our Google Form application and gave it a quick view in Orange.\nFirst, we needed to preprocess the data a bit, since it came in a messy form of strings. Feature Constructor to the rescue! We wanted to extract the OS usage across users. So we first made three new variables named ‘uses linux’, ‘uses windows’ and ‘uses osx’ to represent our three new columns. For each column we searched through ‘OS_of_choice_and_why’, looked up the value of the column, converted it to string, put the string in lowercase, found mentions of either ‘linux’, ‘windows’ or ‘osx’, and voila…. if a mention occurred in the string, we marked the column with 1, else with 0.\nThe expression is just a logical statement in Python and works with booleans (0 if False and 1 if True):\n'linux' in str(OS_of_choice_and_why_.value).lower() or 'ubuntu' in str(OS_of_choice_and_why_.value).lower()  Another thing we might want to do is create three discrete values for ‘‘Dogs or cats’’ question. We want Orange to display ‘dogs’ for someone who replied ‘dogs’, ‘cats’ for someone who replied ‘cats’ and ‘?’ if the questions was a blank or very creative (we had people who wanted to be elephants and butterflies :) ).\nTo create three discrete values you would write:\n0 if 'dogs' in str(Dogs_or_cats_.value).lower() else 1 if 'cats' in str(Dogs_or_cats_.value).lower() else 2  Since we have three values, we need to assign them the corresponding indexes. So if there is ‘dogs’ in the reply, we would get 0 (which we converted to ‘dogs’ in the Feature Constructor’s ‘Values’ box), 1 if there’s ‘cats’ in the reply and 2 if none of the above apply.\nOk, the next step was to sift through a big pile of attributes. We removed personal information for privacy concerns and selected the ones we cared about the most. For example programming skills, years of experience, contributions to OSS and of course whether someone is a dog or a cat person. :) Select Columns sorts the problem. Here you can download a mock-up workflow (same as above, but without sensitive data).\nNow for some lovely charts. Enjoy!\nPython is our lingua franca, experts wanted!\n20 years of programming experience? Hello outlier!\nOSS all the way!\nSome people love dogs and some love cats. Others prefer elephants and butterflies.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Google Summer of Code application period has come to an end. We\u0026rsquo;ve received 34 applications, some of which were of truly high quality. Now it\u0026rsquo;s upon us to select the top performing candidates, but before that we wanted to have an overlook of the candidate pool. We\u0026rsquo;ve gathered data from our Google Form application and gave it a quick view in Orange.\nFirst, we needed to preprocess the data a bit, since it came in a messy form of strings." ,
	"author" : "AJDA",
	"summary" : "Google Summer of Code application period has come to an end. We\u0026rsquo;ve received 34 applications, some of which were of truly high quality. Now it\u0026rsquo;s upon us to select the top performing candidates, but before that we wanted to have an overlook of the candidate pool. We\u0026rsquo;ve gathered data from our Google Form application and gave it a quick view in Orange.\nFirst, we needed to preprocess the data a bit, since it came in a messy form of strings.",
	"date" : "Apr 14, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Univariate GSoC Success",
	"icon" : ""
},
{
    "uri": "/blog/2016/04/01/version-3-3-1-updates-and-features/",
	"title": "Version 3.3.1 - Updates and Features",
	"description": "",
	"content": "About a week ago we issued an updated stable release of Orange, version 3.3.1. We’ve introduced some new functionalities and improved a few old ones.\nHere’s what’s new in this release:\n  New widgets: Distance Matrix for visualizing distance measures in a matrix, Distance Transformation for normalization and inversion of distance matrix, Save Distance Matrix and Distance File for saving and loading distances. Last week we also mentioned a really amazing Silhouette Plot, which helps you visually assess cluster quality.\n  Orange can now read datetime variables in its Time Variable format.\n  Rank outputs scores for each scoring method.\n  Report function had been added to Linear Regression, Univariate Regression, Stochastic Gradient Descent and Distance Transformation widgets.\n  FCBF algorithm has been added to Rank for feature scoring and ReliefF now supports missing target values.\n  Graphs in Classification Tree Viewer can be saved in .dot format.\n  You can view the entire changelog here. :) Enjoy the improvements!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "About a week ago we issued an updated stable release of Orange, version 3.3.1. We\u0026rsquo;ve introduced some new functionalities and improved a few old ones.\nHere\u0026rsquo;s what\u0026rsquo;s new in this release:\n  New widgets: Distance Matrix for visualizing distance measures in a matrix, Distance Transformation for normalization and inversion of distance matrix, Save Distance Matrix and Distance File for saving and loading distances. Last week we also mentioned a really amazing Silhouette Plot, which helps you visually assess cluster quality." ,
	"author" : "AJDA",
	"summary" : "About a week ago we issued an updated stable release of Orange, version 3.3.1. We\u0026rsquo;ve introduced some new functionalities and improved a few old ones.\nHere\u0026rsquo;s what\u0026rsquo;s new in this release:\n  New widgets: Distance Matrix for visualizing distance measures in a matrix, Distance Transformation for normalization and inversion of distance matrix, Save Distance Matrix and Distance File for saving and loading distances. Last week we also mentioned a really amazing Silhouette Plot, which helps you visually assess cluster quality.",
	"date" : "Apr 1, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Version 3.3.1 - Updates and Features",
	"icon" : ""
},
{
    "uri": "/blog/2016/03/23/all-i-see-is-silhouette/",
	"title": "All I See is Silhouette",
	"description": "",
	"content": "Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn’t wait to get it into Orange3. And now we did.\nWhat this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster. Instances with silhouette scores close to 0 are on the border between two clusters. Overall, the quality of the clustering could be assessed by the average silhouette scores of the data instances. But here, we are more interested in the individual silhouettes and their visualization in the silhouette plot.\nUsing the good old iris data set, we are going to assess the silhouettes for each of the data instances. In k-means we set the number of clusters to 3 and send the data to Silhouette plot. Good clusters should include instances with higher silhouette scores. But we’re doing the opposite. In Orange, we are selecting instances with scores close to 0 from the silhouette plot and pass them to other widgets for exploration. No surprise, they are at the periphery of two clusters. This is so perfectly demonstrated in the scatter plot.\nLet’s do something wild now. We’ll use the silhouette on a class attribute of Iris (no clustering here, just using the original class values from the data set). Here is our hypothesis: the data instances with low silhouette values are also those that will be misclassified by some learning algorithm. Say, by a random forest.\nWe will use ten-fold cross validation in Test\u0026Score, send the evaluation results to confusion matrix and select misclassified instances in the widget. Then we will explore the inclusion of these misclassifications in the set of low-silhouette instances in the Venn diagram. The agreement (i.e. the intersection in Venn) between the two techniques is quite high.\nFinally, we can observe these instances in the Scatter Plot. Classifiers indeed have problems with borderline data instances. Our hypothesis was correct.\nSilhouette plot is yet another one of the great visualizations that can help you with data analysis or with understanding certain machine learning concepts. What did we say? Fruitful and fun!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn\u0026rsquo;t wait to get it into Orange3. And now we did.\nWhat this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster." ,
	"author" : "AJDA",
	"summary" : "Silhouette plot is such a nice method for visually assessing cluster quality and the degree of cluster membership that we simply couldn\u0026rsquo;t wait to get it into Orange3. And now we did.\nWhat this visualization displays is the average distance between instances within the cluster and instances in the nearest cluster. For a given data instance, the silhouette close to 1 indicates that the data instance is close to the center of the cluster.",
	"date" : "Mar 23, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "All I See is Silhouette",
	"icon" : ""
},
{
    "uri": "/blog/forestlearner/",
	"title": "forestlearner",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 23, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "forestlearner",
	"icon" : ""
},
{
    "uri": "/blog/2016/03/12/overfitting-and-regularization/",
	"title": "Overfitting and Regularization",
	"description": "",
	"content": "A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit. This widget can also expand the data set by adding columns with powers of original attribute x, thereby augmenting the training set with x^p, where x is our original attribute and p an integer going from 2 to K. The polynomial expansion of data sets allows linear regression model to nicely fit the data, and with higher K to overfit it to extreme, especially if the number of data points in the training set is low.\nWe have already blogged about this experiment a while ago, showing that it is easy to see that linear regression coefficients blow out of proportion with increasing K. This leads to the idea that linear regression should not only minimize the squared error when predicting the value of dependent variable in the training set, but also keep model coefficients low, or better, penalize any high value of coefficients. This procedure is called regularization. Based on the type of penalty (sum of coefficient squared or sum of absolute values), the regularization is referred to L1 or L2, or, ridge and lasso regression.\nIt is quite easy to play with regularized models in Orange by attaching a Linear Regression widget to Polynomial Regression, in this way substituting the default model used in Polynomial Regression with the one designed in Linear Regression widget. This makes available different kinds of regularization. This workflow can be used to show that the regularized models less overfit the data, and that the overfitting depends on the regularization coefficient which governs the degree of penalty stemming from the value of coefficients of the linear model.\nI also use this workflow to show the difference between L1 and L2 regularization. The change of the type of regularization is most pronounced in the table of coefficients (Data Table widget), where with L1 regularization it is clear that this procedure results in many of those being 0. Try this with high value for degree of polynomial expansion, and a data set with about 10 data points. Also, try changing the regularization regularization strength (Linear Regression widget).\nWhile the effects of overfitting and regularization are nicely visible in the plot in Polynomial Regression widget, machine learning models are really about predictions. And the quality of predictions should really be estimated on independent test set. So at this stage of the lecture I needed to introduce the model scoring, that is, a measure that tells me how well my model inferred on the training set performs on the test set. For simplicity, I chose to introduce root mean squared error (RMSE) and then crafted the following workflow.\nHere, I draw the data set (Paint Data, about 20 data instances), assigned y as the target variable (Select Columns), split the data to training and test sets of approximately equal sizes (Data Sampler), and pass training and test data and linear model to the Test \u0026 Score widget. Then I can use linear regression with no regularization, and expect how RMSE changes with changing the degree of the polynomial. I can alternate between Test on train data and Test on test data (Test \u0026 Score widget). In the class I have used the blackboard to record this dependency. For the data from the figure, I got the following table:\n Poly K, RMSE Train, RMSE Test 0, 0.147, 0.138 1, 0.155, 0.192 2, 0.049, 0.063 3, 0.049, 0.063 4, 0.049, 0.067 5, 0.040, 0.408 6, 0.040, 0.574 7, 0.033, 2.681 8, 0.001, 5.734 9, 0.000, 4.776  That’s it. For the class of computer scientists, one may do all this in scripting, but for any other audience, or for any introductory lesson, explaining of regularization with Orange widgets is a lot of fun.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit." ,
	"author" : "BLAZ",
	"summary" : "A week ago I used Orange to explain the effects of regularization. This was the second lecture in the Data Mining class, the first one was on linear regression. My introduction to the benefits of regularization used a simple data set with a single input attribute and a continuous class. I drew a data set in Orange, and then used Polynomial Regression widget (from Prototypes add-on) to plot the linear fit.",
	"date" : "Mar 12, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Overfitting and Regularization",
	"icon" : ""
},
{
    "uri": "/blog/2016/03/03/orange-at-google-summer-of-code-2016/",
	"title": "Orange at Google Summer of Code 2016",
	"description": "",
	"content": "Orange team is extremely excited to be a part of this year’s Google Summer of Code! GSoC is a great opportunity for students around the world to spend their summer contributing to an open-source software, gaining experience and earning money.\nAccepted students will help us develop Orange (or other chosen OSS project) from May to August. Each student is expected to select and define a project of his/her interest and will be ascribed a mentor to guide him/her through the entire process.\nApply here:\nhttps://summerofcode.withgoogle.com/\nOrange’s project proposals (we accept your own ideas as well!):\nhttps://github.com/biolab/orange3/wiki/GSoC-2016\nOur GSoC community forum:\nhttps://groups.google.com/forum/#!forum/orange-gsoc\nSpread the word! (and don’t forget to apply ;) )\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange team is extremely excited to be a part of this year\u0026rsquo;s Google Summer of Code! GSoC is a great opportunity for students around the world to spend their summer contributing to an open-source software, gaining experience and earning money.\nAccepted students will help us develop Orange (or other chosen OSS project) from May to August. Each student is expected to select and define a project of his/her interest and will be ascribed a mentor to guide him/her through the entire process." ,
	"author" : "AJDA",
	"summary" : "Orange team is extremely excited to be a part of this year\u0026rsquo;s Google Summer of Code! GSoC is a great opportunity for students around the world to spend their summer contributing to an open-source software, gaining experience and earning money.\nAccepted students will help us develop Orange (or other chosen OSS project) from May to August. Each student is expected to select and define a project of his/her interest and will be ascribed a mentor to guide him/her through the entire process.",
	"date" : "Mar 3, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at Google Summer of Code 2016",
	"icon" : ""
},
{
    "uri": "/blog/2016/02/26/getting-started-series-pt2/",
	"title": "Getting Started Series: Part Two",
	"description": "",
	"content": "We’ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.\nThe first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:\n  while the second video introduces classification trees and predictive modelling:\n  The seventh video in the series will address how to score classification and regression models by different evaluation methods. Fruits and vegetables data set can be found here.\nIf you have an idea what you’d like to see in the upcoming videos, let us know!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We\u0026rsquo;ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.\nThe first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:\n  while the second video introduces classification trees and predictive modelling:\n  The seventh video in the series will address how to score classification and regression models by different evaluation methods." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve recently published two more videos in our Getting Started with Orange series. The series is intended to introduce beginners to Orange and teach them how to use its components.\nThe first video explains how to do hierarchical clustering and select interesting subsets directly in Orange:\n  while the second video introduces classification trees and predictive modelling:\n  The seventh video in the series will address how to score classification and regression models by different evaluation methods.",
	"date" : "Feb 26, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Getting Started Series: Part Two",
	"icon" : ""
},
{
    "uri": "/blog/2016/01/29/tips-and-tricks-for-data-preparation/",
	"title": "Tips and Tricks for Data Preparation",
	"description": "",
	"content": "Probably the most crucial step in your data analysis is purging and cleaning your data. Here are a couple of cool tricks that will make your data preparation a bit easier.\n  Use a smart text editor. We can recommend Sublime Text as it an extremely versatile editor that supports a broad variety of programming languages and markups, but there are other great tools out there as well. One of the best things you’ll keep coming back to in your editor is ‘Replace’ function that allows you to replace specified values with different ones. You can also use regex to easily find and replace parts of text.\nWe can replace all instances of ‘male’ with ‘man’ in one click.\n  Apply simple hacks. Sometimes when converting files to different formats data can get some background information appended that you cannot properly see. A cheap and dirty trick is to manually select the cells and rows and copy-paste them to a new sheet. This will start with a clean slate and you data will be read properly.\n  Check your settings. When reading .csv files in Excel, you might see all your data squished in one column and literally separated with commas. This can be easily solved with Data –\u003e From Text (Get external data) and a new window will appear. In a Text Import Wizard you can set whether your data is delimited or not (in our case it is), how it is delimited (comma, tab, etc.), whether you have a header or not, what qualifies as text (” is a recommended option), what is your encoding and so on.\n  Manually annotate the data. Orange loves headers and the easiest way to assure your data gets read properly is to set the header yourself. Add two extra rows under your feature names. In the first row, set your variable type and in the second one, your kind. Here’s how to do it properly.\n  Exploit the widgets in Orange. Select Columns is your go-to widget for organizing what gets read as a meta attribute, what is your class variable and which features you want to use in your analysis. Another great widget is Edit domain, where you can set the way the values are displayed in the analysis (say you have “grey” in your data, but you want it to say “gray”). Moreover, you can use Concatenate and Merge widgets to put your data together.\nSet domain with Edit domain widget.\n  What’s your preferred trick?\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Probably the most crucial step in your data analysis is purging and cleaning your data. Here are a couple of cool tricks that will make your data preparation a bit easier.\n  Use a smart text editor. We can recommend Sublime Text as it an extremely versatile editor that supports a broad variety of programming languages and markups, but there are other great tools out there as well. One of the best things you\u0026rsquo;ll keep coming back to in your editor is \u0026lsquo;Replace\u0026rsquo; function that allows you to replace specified values with different ones." ,
	"author" : "AJDA",
	"summary" : "Probably the most crucial step in your data analysis is purging and cleaning your data. Here are a couple of cool tricks that will make your data preparation a bit easier.\n  Use a smart text editor. We can recommend Sublime Text as it an extremely versatile editor that supports a broad variety of programming languages and markups, but there are other great tools out there as well. One of the best things you\u0026rsquo;ll keep coming back to in your editor is \u0026lsquo;Replace\u0026rsquo; function that allows you to replace specified values with different ones.",
	"date" : "Jan 29, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Tips and Tricks for Data Preparation",
	"icon" : ""
},
{
    "uri": "/blog/2016/01/22/predictive-analytics-with-orange/",
	"title": "Making Predictions",
	"description": "",
	"content": "One of the cool things about being a data scientist is being able to predict. That is, predict before we know the actual outcome. I am not talking about verifying your favorite classification algorithm here, and I am not talking about cross-validation or classification accuracies or AUC or anything like that. I am talking about the good old prediction. This is where our very own Predictions widget comes to help.\nPredictions workflow.\nWe will be exploring the Iris data set again, but we’re going to add a little twist to it. Since we’ve worked so much with it already, I’m sure you know all about this data. But now we got three new flowers in the office and of course there’s no label attached to tell us what species of Iris these flowers are. [sigh….] Obviously, we will be measuring petals and sepals and contrasting the results with our data.\nOur new data on three flowers. We have used Google Sheets to enter the data and the copied the sharable link and pasted the link to the File widget.\nBut surely you don’t want to go through all 150 flowers to properly match the three new Irises? So instead, let’s first train a model on the existing data set. We connect the File widget to the chosen classifier (we went with Classification Tree this time) and feed the results into Predictions. Now we write down the measurements for our new flowers into Google Sheets (just like above), load it into Orange with a new File widget and input the fresh data into Predictions. We can observe the predicted class directly in the widget itself.\nPredictions made by classification tree.\nIn the left part of the visualization we have the input data set (our measurements) and in the right part the predictions made with classification tree. By default you see probabilities for all three class values and the predicted class. You can of course use other classifiers as well - it would probably make sense to first evaluate classifiers on the existing data set, find the best one for your and then use it on the new data.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One of the cool things about being a data scientist is being able to predict. That is, predict before we know the actual outcome. I am not talking about verifying your favorite classification algorithm here, and I am not talking about cross-validation or classification accuracies or AUC or anything like that. I am talking about the good old prediction. This is where our very own Predictions widget comes to help." ,
	"author" : "AJDA",
	"summary" : "One of the cool things about being a data scientist is being able to predict. That is, predict before we know the actual outcome. I am not talking about verifying your favorite classification algorithm here, and I am not talking about cross-validation or classification accuracies or AUC or anything like that. I am talking about the good old prediction. This is where our very own Predictions widget comes to help.",
	"date" : "Jan 22, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Making Predictions",
	"icon" : ""
},
{
    "uri": "/blog/2016/01/04/orange-youtube-tutorials/",
	"title": "Orange YouTube Tutorials",
	"description": "",
	"content": "It’s been a long time coming, but finally we’ve created out our first set of YouTube tutorials. In a series ‘Getting Started with Orange’ we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy. Below are the first three videos from this series, more are coming in the following weeks.\n      We are also preparing a series called ‘Data Science with Orange’, which will take you on a journey through the world of data mining and machine learning by explaining predictive modeling, classification, regression, model evaluation and much more.\nFeel free to let us know what tutorials you’d like to see and we’ll do our best to include it in one of the two series. :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "It\u0026rsquo;s been a long time coming, but finally we\u0026rsquo;ve created out our first set of YouTube tutorials. In a series \u0026lsquo;Getting Started with Orange\u0026rsquo; we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy." ,
	"author" : "AJDA",
	"summary" : "It\u0026rsquo;s been a long time coming, but finally we\u0026rsquo;ve created out our first set of YouTube tutorials. In a series \u0026lsquo;Getting Started with Orange\u0026rsquo; we will walk through our software step-by-step. You will learn how to create a workflow, load your data in different formats, visualize and explore the data. These tutorials are meant for complete beginners in both Orange and data mining and come with some handy tricks that will make using Orange very easy.",
	"date" : "Jan 4, 2016",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange YouTube Tutorials",
	"icon" : ""
},
{
    "uri": "/blog/2015/12/28/color-it/",
	"title": "Color it!",
	"description": "",
	"content": "Holiday season is upon us and even the Orange team is in a festive mood. This is why we made a Color widget!\nThis fascinating artsy widget will allow you to play with your data set in a new and exciting way. No more dull visualizations and default color schemes! Set your own colors the way YOU want it to! Care for some magical cyan-to-magenta? Or do you prefer a more festive red-to-green? How about several shades of gray? Color widget is your go-to stop for all things color (did you notice it’s our only widget with a colorful icon?). :)\nColoring works with most visualization widgets, such as scatter plot, distributions, box plot, mosaic display and linear projection. Set the colors for discrete values and gradients for continuous values in this widget, and the same palletes will be used in all downstream widgets. As a bonus, the Color widget also allows you to edit the names of variables and values.\nRemember - the (blue) sky is the limit.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Holiday season is upon us and even the Orange team is in a festive mood. This is why we made a Color widget!\nThis fascinating artsy widget will allow you to play with your data set in a new and exciting way. No more dull visualizations and default color schemes! Set your own colors the way YOU want it to! Care for some magical cyan-to-magenta? Or do you prefer a more festive red-to-green?" ,
	"author" : "AJDA",
	"summary" : "Holiday season is upon us and even the Orange team is in a festive mood. This is why we made a Color widget!\nThis fascinating artsy widget will allow you to play with your data set in a new and exciting way. No more dull visualizations and default color schemes! Set your own colors the way YOU want it to! Care for some magical cyan-to-magenta? Or do you prefer a more festive red-to-green?",
	"date" : "Dec 28, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Color it!",
	"icon" : ""
},
{
    "uri": "/blog/2015/12/19/model-based-feature-scoring/",
	"title": "Model-Based Feature Scoring",
	"description": "",
	"content": "Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression. Same holds for feature weight: any change of parameters of the modeling techniques would change the resulting feature scores.\nIt would thus be great if we could observe these changes and compare feature ranking provided by various machine learning methods. For this purpose, the Rank widget recently got a new input channel called scorer. We can attach any learner that can provide feature scores to the input of Rank, and then observe the ranking in the Rank table.\nSay, for the famous voting data set (File widget, Browse documentation data sets), the last two feature score columns were obtained by random forest and logistic regression with L1 regularization (C=0.1). Try changing the regularization parameter and type to see changes in feature scores.\nFeature weights for logistic and linear regression correspond to the absolute value of coefficients of their linear models. To observe their untransformed values in the table, these widgets now also output a data table with feature weights. (At the time of the writing of this blog, this feature has been implemented for linear regression; other classifiers and regressors that can estimate feature weights will be updated soon).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression." ,
	"author" : "BLAZ",
	"summary" : "Feature scoring and ranking can help in understanding the data in supervised settings. Orange includes a number of standard feature scoring procedures one can access in the Rank widget. Moreover, a number of modeling techniques, like linear or logistic regression, can rank features explicitly through assignment of weights. Trained models like random forests have their own methods for feature scoring. Models inferred by these modeling techniques depend on their parameters, like type and level of regularization for logistic regression.",
	"date" : "Dec 19, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Model-Based Feature Scoring",
	"icon" : ""
},
{
    "uri": "/blog/report/",
	"title": "report",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 11, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "report",
	"icon" : ""
},
{
    "uri": "/blog/2015/12/11/report-is-back-and-better-than-ever/",
	"title": "Report is back! (and better than ever)",
	"description": "",
	"content": "I’m sure you’d agree that reporting your findings when analyzing the data is crucial. Say you have a couple of interesting predictions that you’ve tested with several methods many times and you’d like to share that with the world. Here’s how.\nSave Graph just got company - a Report button! Report works in most widgets, apart from the very obvious ones that simply transmit or display the data (Python Scripting, Edit Domain, Image Viewer, Predictions…).\nWhy is Report so great?\n  Display data and graphs used in your workflow. Whatever you do with your data will be put in the report upon a click of a button.\n  Write comments below each section in your workflow. Put down whatever matters for your research - pitfalls and advantages of a model, why this methodology works, amazing discoveries, etc.\n  Access your workflows. Every step of the analysis recorded in the Report is saved as a workflow and can be accessed by clicking on the Orange icon. Have you spent hours analyzing your data only to find out you made a wrong turn somewhere along the way? No problem. Report saves workflows for each step of the analysis. Perhaps you would like to go back and start again from Bo Plot? Click on the Orange icon next to Box Plot and you will be taken to the workflow you had when you placed that widget in the report. Completely stress-free!\n  Save your reports. The amazing new report that you just made can be saved as .html, .pdf or .report file. Html and PDF are pretty standard, but report format is probably the best thing since sliced bread. Why? Not only it saves your report file for later use, you can also send it to your colleagues and they will be able to access both your report and workflows used in the analysis.\n  Open report. To open a saved report file go to File → Open Report. To view the report you’re working on, go to Options → Show report view or click Shift+R.\n  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "I’m sure you’d agree that reporting your findings when analyzing the data is crucial. Say you have a couple of interesting predictions that you’ve tested with several methods many times and you’d like to share that with the world. Here’s how.\nSave Graph just got company - a Report button! Report works in most widgets, apart from the very obvious ones that simply transmit or display the data (Python Scripting, Edit Domain, Image Viewer, Predictions…)." ,
	"author" : "AJDA",
	"summary" : "I’m sure you’d agree that reporting your findings when analyzing the data is crucial. Say you have a couple of interesting predictions that you’ve tested with several methods many times and you’d like to share that with the world. Here’s how.\nSave Graph just got company - a Report button! Report works in most widgets, apart from the very obvious ones that simply transmit or display the data (Python Scripting, Edit Domain, Image Viewer, Predictions…).",
	"date" : "Dec 11, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Report is back! (and better than ever)",
	"icon" : ""
},
{
    "uri": "/blog/2015/12/04/2uda/",
	"title": "2UDA",
	"description": "",
	"content": "In one of the previous blog posts we mentioned that installing the optional dependency psycopg2 allows Orange to connect to PostgreSQL databases and work directly on the data stored there. It is also possible to transfer a whole table to the client machine, keep it in the local memory, and continue working with it as with any other Orange data set loaded from a file. But the true power of this feature lies in the ability of Orange to leave the bulk of the data on the server, delegate some of the computations to the database, and transfer only the needed results. This helps especially when the connection is too slow to transfer all the data and when the data is too big to fit in the memory of the local machine, since SQL databases are much better equipped to work with large quantities of data residing on the disk.\nIf you want to test this feature it is now even easier to do so! A third party distribution called 2UDA provides a single installer for all major OS platforms that combines Orange and a PostgreSQL 9.5 server along with LibreOffice (optional) and installs all the needed dependencies. The database even comes with some sample data sets that can be used to start testing and using Orange out of the box. 2UDA is also a great way to get the very latest version of PostgreSQL, which is important for Orange as it relies heavily on its new TABLESAMPLE clause. It enables time-based sampling of tables, which is used in Orange to get approximate results quickly and allow responsive and interactive work with big data.\nWe hope this will help us reach an even wider audience and introduce Orange to a whole new group of people managing and storing their data in SQL databases. We believe that having lots of data is a great starting point, but the benefits truly kick in with the ability to easily extract useful information from it.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "In one of the previous blog posts we mentioned that installing the optional dependency psycopg2 allows Orange to connect to PostgreSQL databases and work directly on the data stored there. It is also possible to transfer a whole table to the client machine, keep it in the local memory, and continue working with it as with any other Orange data set loaded from a file. But the true power of this feature lies in the ability of Orange to leave the bulk of the data on the server, delegate some of the computations to the database, and transfer only the needed results." ,
	"author" : "LAN",
	"summary" : "In one of the previous blog posts we mentioned that installing the optional dependency psycopg2 allows Orange to connect to PostgreSQL databases and work directly on the data stored there. It is also possible to transfer a whole table to the client machine, keep it in the local memory, and continue working with it as with any other Orange data set loaded from a file. But the true power of this feature lies in the ability of Orange to leave the bulk of the data on the server, delegate some of the computations to the database, and transfer only the needed results.",
	"date" : "Dec 4, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "2UDA",
	"icon" : ""
},
{
    "uri": "/blog/2015/12/02/hierarchical-clustering-a-simple-explanation/",
	"title": "Hierarchical Clustering: A Simple Explanation",
	"description": "",
	"content": "One of the key techniques of exploratory data mining is clustering – separating instances into distinct groups based on some measure of similarity. We can estimate the similarity between two data instances through euclidean (pythagorean), manhattan (sum of absolute differences between coordinates) and mahalanobis distance (distance from the mean by standard deviation), or, say, through Pearson correlation or Spearman correlation.\nOur main goal when clustering data is to get groups of data instances where:\n each group (Ci) is a a subset of the training data (U): Ci ⊂ U an intersection of all the sets is an empty set: Ci ∩ Cj = 0 a union of all groups equals the train data: Ci ∪ Cj = U  This would be ideal. But we rarely get the data, where separation is so clear. One of the easiest techniques to cluster the data is hierarchical clustering. First, we take an instance from, say, 2D plot. Now we want to find its nearest neighbor. Nearest neighbor of course depends on the measure of distance we choose, but let’s go with euclidean for now as it is the easiest to visualize. First steps of hierarchical clustering.\nEuclidean distance is calculated as:\nNaturally, the shorter the distance the more similar the two instances are. In the beginning, all instances are in their own particular clusters. Then we seek for the closest instances of every instance in the plot. We pin down the closest instance and make a cluster of the original and the closest instance. Now we repeat the process again. What is the closest instances to our new cluster –\u003e add it to the cluster –\u003e find the closest instance. We repeat this procedure until all the instances are grouped in one single cluster.\nWe can write this down also in a form of a pseudocode:\nevery instance is in its own cluster repeat until instances are all in one group: find the closest instances to the group (distance has to be minimum) join closest instances with the group  Visualization of this procedure is called a dendrogram, which is what Hierarchical clustering widget displays in Orange.\nSingle, complete and average linkage.\nAnother thing to consider is the distance between instances when we have already two or more instances in a cluster. Do we go with the closest instance in a cluster or to the furthest one?\n Picture A shows the distances to the closest instance – single linkage. Picture B shows the distance to the furthest instance – complete linkage. Picture C shows the average of all distances in a cluster to the instance – average linkage.  Single vs complete linkage.\nThe downside of single linkage is, even by intuition, creating elongated, stretched clusters. Instances at the top part of the red C are in fact quite different from the lower part of the red C. Complete linkage does much better here as it centers clustering nicely. However, the downside of complete linkage is taking outliers too much into consideration. Naturally, each approach has its own pros and cons and it’s good to know how they work in order to use them correctly. One extra hint: single linkage works great for image recognition, exactly because it can follow the curve.\nThere’s a lot more we could say about hierarchical clustering, but to sum it up, let’s state pros and cons of this method:\n pros: sums up the data, good for small data sets cons: computationally demanding, fails on larger sets  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One of the key techniques of exploratory data mining is clustering – separating instances into distinct groups based on some measure of similarity. We can estimate the similarity between two data instances through euclidean (pythagorean), manhattan (sum of absolute differences between coordinates) and mahalanobis distance (distance from the mean by standard deviation), or, say, through Pearson correlation or Spearman correlation.\nOur main goal when clustering data is to get groups of data instances where:" ,
	"author" : "AJDA",
	"summary" : "One of the key techniques of exploratory data mining is clustering – separating instances into distinct groups based on some measure of similarity. We can estimate the similarity between two data instances through euclidean (pythagorean), manhattan (sum of absolute differences between coordinates) and mahalanobis distance (distance from the mean by standard deviation), or, say, through Pearson correlation or Spearman correlation.\nOur main goal when clustering data is to get groups of data instances where:",
	"date" : "Dec 2, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Hierarchical Clustering: A Simple Explanation",
	"icon" : ""
},
{
    "uri": "/blog/2015/11/27/mining-our-own-data/",
	"title": "Mining our own data",
	"description": "",
	"content": "Recently we’ve made a short survey that was, upon Orange download, asking people how they found out about Orange, what was their data mining level and where do they work. The main purpose of this is to get a better insight into our user base and to figure out what is the profile of people interested in trying Orange.\nHere we have some preliminary results that we’ve managed to gather in the past three weeks or so. Obviously we will use Orange to help us make sense of the data.\nWe’ve downloaded our data from Typeform and appended some background information such as OS and browser. Let’s see what we’ve got in the Data Table widget.\nOk, this is our entire data table. Here we also have the data on people who completed the survey and who didn’t. First, let’s organize the data properly. We’ll do this with Select Columns widget.\nWe removed all the meta attributes as they are not very relevant for our analysis. Next we moved the ‘completed’ attribute into target variable, thus making it our class variable.\nNow we would like to see some basic distributions from our data.\nInteresting. Most of our users are working on Windows, a few on Mac and very few on Linux.\nLet’s investigate further. Now we want to know more about those people who actually completed the survey. Let’s use Select Columns again, this time removing os_type, os_name, agent_name and completed from our data and keeping just the answers. We made “Where do you work?” our class variable, but we could use either one of the three. Another trick is to set in directly in Distributions widget under ‘Group by’.\nOk, let’s again use Distributions - this is such a simple way to get a good sense of your data.\nObviously out of those who found out about Orange in college, most are students, but what’s interesting here is that there are so many. We can also see that out of those who found us on the web, most come from the private sector, followed by academia and researchers. Good. How about the other question?\nAgain, results are not particularly shocking, but it’s great to confirm your hypothesis with real data. Out of beginner level data miners, most are students, while most intermediate users come from the industry.\nA quick look at the Mosaic Display will give us a good overview:\nYup, this sums it up quite nicely. We have lots of beginner levels users and not many expert ones (height of the box). Also most people found out about Orange on the web or in college (width of the box). A thin line on the left shows apriori distribution, thus making it easier to compare expected and actual number of instances. For example, there should be at least some people who are students and have found out about Orange at a conference. But there aren’t - a contrast between how much red there should be in the box (line on the left) and how much there actually is (bigger part of the box) is quite telling. We can even select all the beginner level users who found out about Orange in college and further inspect the data, but be it enough for now.\nOur final workflow:\nObviously, this is a very simple analysis. But even such simple tasks are never boring with good visualization tools such as Distributions and Mosaic Display. You could also use Venn Diagram to find common features of selected subsets or perhaps Sieve Diagram for probabilities.\nWe are very happy to get these data and we would like to thank everyone who completed the survey. If you wish to help us further, please fill out a longer survey that won’t actually take you more than 3 minutes of your time (we timed it!).\nHappy Friday everyone!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Recently we\u0026rsquo;ve made a short survey that was, upon Orange download, asking people how they found out about Orange, what was their data mining level and where do they work. The main purpose of this is to get a better insight into our user base and to figure out what is the profile of people interested in trying Orange.\nHere we have some preliminary results that we\u0026rsquo;ve managed to gather in the past three weeks or so." ,
	"author" : "AJDA",
	"summary" : "Recently we\u0026rsquo;ve made a short survey that was, upon Orange download, asking people how they found out about Orange, what was their data mining level and where do they work. The main purpose of this is to get a better insight into our user base and to figure out what is the profile of people interested in trying Orange.\nHere we have some preliminary results that we\u0026rsquo;ve managed to gather in the past three weeks or so.",
	"date" : "Nov 27, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Mining our own data",
	"icon" : ""
},
{
    "uri": "/blog/2015/10/30/ghostbusters/",
	"title": "Ghostbusters",
	"description": "",
	"content": "Ok, we’ve just recently stumbled across an interesting article on how to deal with non normal (non-Gaussian distributed) data. We have an absolutely paranormal data set of 20 persons with weight, height, paleness, vengefulness, habitation and age attributes (download).\nLet’s check the distribution in Distributions widget.\nOur first attribute is “Weight” and we see a little hump on the left. Otherwise the data would be normally distributed. Ok, so perhaps we have a few children in the data set. Let’s check the age distribution. Whoa, what? Why is the hump now on the right? These distributions look scary. We seem to have a few reaaaaally old people here. What is going on? Perhaps we can figure this out with MDS. This widget projects the data into two dimensions so that the distances between the points correspond to differences between the data instances.\nAha! Now we see that three instances are quite different from all others. Select them and send them to the Data Table for final inspection.\nBusted! We have found three ghosts hiding in our data. They are extremely light (the sheet they are wearing must weight around 2kg), quite vengeful and old.\nNow, joke aside, what would this mean for a general non-normally distributed data? One thing is your data set might be too small. Here we only have 20 instances, thus 3 outlying ghosts have a great impact on the distribution. It is difficult to hide 3 ghosts among 17 normal persons.\nSecondly, why can’t we use Outliers widget to hunt for those ghosts? Again, our data set is too small. With just 20 instances, the estimation variance is so large that it can easily cover a few ghosts under its sheet. We don’t have enough “normal” data to define what is normal and thus detect the paranormal.\nHaven’t we just written two exactly opposite things? Perhaps.\nHappy Halloween everybody! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Ok, we’ve just recently stumbled across an interesting article on how to deal with non normal (non-Gaussian distributed) data. We have an absolutely paranormal data set of 20 persons with weight, height, paleness, vengefulness, habitation and age attributes (download).\nLet’s check the distribution in Distributions widget.\nOur first attribute is “Weight” and we see a little hump on the left. Otherwise the data would be normally distributed. Ok, so perhaps we have a few children in the data set." ,
	"author" : "AJDA",
	"summary" : "Ok, we’ve just recently stumbled across an interesting article on how to deal with non normal (non-Gaussian distributed) data. We have an absolutely paranormal data set of 20 persons with weight, height, paleness, vengefulness, habitation and age attributes (download).\nLet’s check the distribution in Distributions widget.\nOur first attribute is “Weight” and we see a little hump on the left. Otherwise the data would be normally distributed. Ok, so perhaps we have a few children in the data set.",
	"date" : "Oct 30, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Ghostbusters",
	"icon" : ""
},
{
    "uri": "/blog/2015/10/19/sql-for-orange/",
	"title": "SQL for Orange",
	"description": "",
	"content": "We bet you’ve always wanted to use your SQL data in Orange, but you might not be quite sure how to do it. Don’t worry, we’re coming to the rescue.\nThe key to SQL files is installation of ‘psycopg2’ library in Python.\nWINDOWS\nGo to this website and download psycopg2 package. Once your .whl file has downloaded, go to the file directory and run command prompt. Enter “pip install [file name]” and run it.\nMAC OS X, LINUX\nIf you’re on Mac or Linux, install psycopg2 with this.\nUpon opening Orange, you will be able to see a lovely new icon - SQL Table. Then just connect to your server and off you go!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We bet you\u0026rsquo;ve always wanted to use your SQL data in Orange, but you might not be quite sure how to do it. Don\u0026rsquo;t worry, we\u0026rsquo;re coming to the rescue.\nThe key to SQL files is installation of \u0026lsquo;psycopg2\u0026rsquo; library in Python.\nWINDOWS\nGo to this website and download psycopg2 package. Once your .whl file has downloaded, go to the file directory and run command prompt. Enter “pip install [file name]” and run it." ,
	"author" : "AJDA",
	"summary" : "We bet you\u0026rsquo;ve always wanted to use your SQL data in Orange, but you might not be quite sure how to do it. Don\u0026rsquo;t worry, we\u0026rsquo;re coming to the rescue.\nThe key to SQL files is installation of \u0026lsquo;psycopg2\u0026rsquo; library in Python.\nWINDOWS\nGo to this website and download psycopg2 package. Once your .whl file has downloaded, go to the file directory and run command prompt. Enter “pip install [file name]” and run it.",
	"date" : "Oct 19, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "SQL for Orange",
	"icon" : ""
},
{
    "uri": "/blog/2015/10/16/learners-in-python/",
	"title": "Learners in Python",
	"description": "",
	"content": "We’ve already written about classifying instances in Python. However, it’s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.\nTRAINING THE CLASSIFIER\nWe start with simply importing Orange module into Python and loading our data set.\n\u003e\u003e\u003e\u003e import Orange \u003e\u003e\u003e\u003e data = Orange.data.Table(\"titanic\")  We are using ‘titanic.tab’ data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression). Now we want to train our classifier.\n\u003e\u003e\u003e\u003e learner = Orange.classification.LogisticRegressionLearner() \u003e\u003e\u003e\u003e classifier = learner(data) \u003e\u003e\u003e\u003e classifier(data[0])  Python returns the index of the value, as usual.\narray[0.]  To check what’s in the class variable we print:\n\u003e\u003e\u003e\u003eprint(\"Name of the variable: \", data.domain.class_var.name) \u003e\u003e\u003e\u003eprint(\"Class values: \", data.domain.class_var.values) \u003e\u003e\u003e\u003eprint(\"Value of our instance: \", data.domain.class_var.values[0]) Name of the variable: survived Class values: no, yes Value of our instance: no  PREDICTIONS\nIf you want to get predictions for the entire data set, just give the classifier the entire data set.\n\u003e\u003e\u003e\u003e classifier(data) array[0, 0, 0, ..., 1, 1, 1]  If we want to append predictions to the data table, first use classifier on the data, then create a new domain with an additional meta attribute and finally form a new data table with appended predictions:\nsvm = classifier(data) new_domain = Orange.data.Domain(data.domain.attributes, data.domain.class_vars, [data.domain.class_var]) table2 = Orange.data.Table(new_domain, data.X, data.Y, svm.reshape(-1, 1))  We use .reshape to transform vector data into a reshaped array. Then we print out the data.\nprint(table2)  PARAMETERS\nWant to use another classifier? The procedure is the same, simply use:\nOrange.classification.\u003calgorithm-name\u003e()  For most classifiers, you can set a whole range of parameters. Logistic Regression, for example, uses the following:\nlearner = Orange.classification.LogisticRegressionLearner(**penalty**='l2', **dual**=False, **tol**=0.0001, **C**=1.0, **fit_intercept**=True, **intercept_scaling**=1, **class_weight**=None, **random_state**=None **preprocessors**=None)  To check the parameters for the classifier, use:\nprint(Orange.classification.SVMLearner())  PROBABILITIES\nAnother thing you can check with classifiers are the probabilities.\nclassifier(data[0], Orange.classification.Model.ValueProbs) \u003e\u003e\u003e(array([ 0.]), array([[ 1., 0.]]))  The first array is the value for your selected instance (data[0]), while the second array contains probabilities for class values (probability for ‘no’ is 1 and for ‘yes’ 0).\nCLASSIFIERS\nAnd because we care about you, we’re giving you here a full list of classifier names:\nLogisticRegressionLearner()\nNaiveBayesLearner()\nKNNLearner()\nTreeLearner()\nMajorityLearner()\nRandomForestLearner()\nSVMLearner()\nFor other learners, you can find all the parameters and descriptions in the documentation.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We\u0026rsquo;ve already written about classifying instances in Python. However, it\u0026rsquo;s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.\nTRAINING THE CLASSIFIER\nWe start with simply importing Orange module into Python and loading our data set.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; import Orange \u0026gt;\u0026gt;\u0026gt;\u0026gt; data = Orange.data.Table(\u0026quot;titanic\u0026quot;)  We are using \u0026lsquo;titanic.tab\u0026rsquo; data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression)." ,
	"author" : "AJDA",
	"summary" : "We\u0026rsquo;ve already written about classifying instances in Python. However, it\u0026rsquo;s always nice to have a comprehensive list of classifiers and a step-by-step procedure at hand.\nTRAINING THE CLASSIFIER\nWe start with simply importing Orange module into Python and loading our data set.\n\u0026gt;\u0026gt;\u0026gt;\u0026gt; import Orange \u0026gt;\u0026gt;\u0026gt;\u0026gt; data = Orange.data.Table(\u0026quot;titanic\u0026quot;)  We are using \u0026lsquo;titanic.tab\u0026rsquo; data. You can load any data set you want, but it does have to have a categorical class variable (for numeric targets use regression).",
	"date" : "Oct 16, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Learners in Python",
	"icon" : ""
},
{
    "uri": "/blog/2015/10/09/data-mining-course-in-houston/",
	"title": "Data Mining Course in Houston",
	"description": "",
	"content": "We have just completed an Introduction to Data Mining, a graduate course at Baylor College of Medicine in Texas, Houston. The course was given in September and consisted of seven two-hour lectures, each one followed with a homework assignment. The course was attended by about 40 students and some faculty and research staff.\nThis was a challenging course. The audience was new to data mining, and we decided to teach them with the newest, third version of Orange. We also experimented with two course instructors (Blaz and Janez), who, instead of splitting the course into two parts, taught simultaneously, one on the board and the other one helping the students with hands-on exercises. To check whether this worked fine, we ran a student survey at the end of the course. We used Google Sheets and then examined the results with students in the class. Using Orange, of course.\nAnd the outcome? Looks like the students really enjoyed the course\nand the teaching style.\nThe course took advantage of several new widgets in Orange 3, including those for data preprocessing and polynomial regression. The core development team put a lot of effort during the summer to debug and polish this newest version of Orange. Also thanks to the financial support by AXLE EU FP7 and CARE-MI EU FP7** grants and grants by the Slovene Research agency, we were able to finish everything in time.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have just completed an Introduction to Data Mining, a graduate course at Baylor College of Medicine in Texas, Houston. The course was given in September and consisted of seven two-hour lectures, each one followed with a homework assignment. The course was attended by about 40 students and some faculty and research staff.\nThis was a challenging course. The audience was new to data mining, and we decided to teach them with the newest, third version of Orange." ,
	"author" : "BLAZ",
	"summary" : "We have just completed an Introduction to Data Mining, a graduate course at Baylor College of Medicine in Texas, Houston. The course was given in September and consisted of seven two-hour lectures, each one followed with a homework assignment. The course was attended by about 40 students and some faculty and research staff.\nThis was a challenging course. The audience was new to data mining, and we decided to teach them with the newest, third version of Orange.",
	"date" : "Oct 9, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Mining Course in Houston",
	"icon" : ""
},
{
    "uri": "/blog/2015/10/02/a-visit-from-the-tilburg-university/",
	"title": "A visit from the Tilburg University",
	"description": "",
	"content": "Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it. And then consult with statisticians and data mining expert (of course!).\nBiolab team with Marie and Eric, who is standing next to Ivan Cankar - the very serious guy in the middle.\nTo honor this occasion we invite you to check out the Polynomial regression widget, which is specially intended for educational use. There, you can showcase the problem of overfitting through visualization.\nFirst, we set up a workflow.\nThen we paint, say, at most 10 points into the Paint Data widget. (Why at most ten? You’ll see later.)\nNow we open our Polynomial Regression widget, where we play with polynomial degree. Polynomial Degree 1 gives us a line. With coefficient 2 we get a curve that fits only one point. However, with the coefficient 7 we fit all the points with one curve. Yay!\nBut hold on! The curve now becomes very steep. Would the lower end of the curve at about (0.9, -2.2) still be a realistic estimate of our data set? Probably not. Even when we look at the Data Table with coefficient values, they seem to skyrocket.\nThis is a typical danger of overfitting, which is often hard to explain, but with the help of these three widgets becomes as clear as day! Now go out and share the knowledge!!!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it." ,
	"author" : "AJDA",
	"summary" : "Biolab is currently hosting two amazing data scientists from the Tilburg University - dr. Marie Nilsen and dr. Eric Postma, who are preparing a 20-lecture MOOC on data science for non-technical audience. A part of the course will use Orange. The majority of their students is coming from humanities, law, economy and behavioral studies, thus we are discussing options and opportunities for adapting Orange for social scientists. Another great thing is that the course is designed for beginner level data miners, showcasing that anybody can mine the data and learn from it.",
	"date" : "Oct 2, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "A visit from the Tilburg University",
	"icon" : ""
},
{
    "uri": "/blog/2015/09/25/save-your-graphs/",
	"title": "Save your graphs!",
	"description": "",
	"content": "If you are often working with Orange, you probably have noticed a small button at the bottom of most visualization widgets. “Save Graph” now enables you to export graphs, charts, and hierarchical trees to your computer and use them in your reports. Because people need to see it to believe it!\n“Save Graph” will save visualizations to your computer.\nSave Graph function is available in Paint Data, Image Viewer, all visualization widgets, and a few others (list is below).\nWidgets with the “Save Graph” option.\nYou can save visualizations in .png, .dot or .svg format. However - brace yourselves - our team is working on something even better, which will be announced in the following weeks.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "If you are often working with Orange, you probably have noticed a small button at the bottom of most visualization widgets. “Save Graph” now enables you to export graphs, charts, and hierarchical trees to your computer and use them in your reports. Because people need to see it to believe it!\n\u0026ldquo;Save Graph\u0026rdquo; will save visualizations to your computer.\nSave Graph function is available in Paint Data, Image Viewer, all visualization widgets, and a few others (list is below)." ,
	"author" : "AJDA",
	"summary" : "If you are often working with Orange, you probably have noticed a small button at the bottom of most visualization widgets. “Save Graph” now enables you to export graphs, charts, and hierarchical trees to your computer and use them in your reports. Because people need to see it to believe it!\n\u0026ldquo;Save Graph\u0026rdquo; will save visualizations to your computer.\nSave Graph function is available in Paint Data, Image Viewer, all visualization widgets, and a few others (list is below).",
	"date" : "Sep 25, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Save your graphs!",
	"icon" : ""
},
{
    "uri": "/blog/2015/09/11/hubbing-with-hub-widget/",
	"title": "Hubbing with the Hub widget",
	"description": "",
	"content": "So you have painted two data sets and loaded another one from a file, and now you are testing predictions of logistic regression, classification trees and SVM on it? Tired of having to reconnect the Paint data widget and the File widget back and forth whenever you switch between them?\nSay no more! Look no further! Here is the new Hub widget!\nHub widget is the most versatile widget available so far. It accepts several inputs of any type and outputs them to as many other widgets as you want.\nThe Hub widget treats all types with the strictest equality.\n(It also adheres to all applicable EU policies with respect to gender equality, and does not use cookies.)\nThe Hub widget works like charm and is like the amazing cast-to-void-and-back-to-anything idiom in C. This strongful MacGyver of widgets can (almost) convert classification tree into data, or preprocessor into experimental results without ever touching the data. With its amazing capabilities, the Hub widget has the potential to cause an even greater havoc in your workflows than the famous Merge data widget.\nDownload, install - and start hubbing today !!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "So you have painted two data sets and loaded another one from a file, and now you are testing predictions of logistic regression, classification trees and SVM on it? Tired of having to reconnect the Paint data widget and the File widget back and forth whenever you switch between them?\nSay no more! Look no further! Here is the new Hub widget!\nHub widget is the most versatile widget available so far." ,
	"author" : "AJDA",
	"summary" : "So you have painted two data sets and loaded another one from a file, and now you are testing predictions of logistic regression, classification trees and SVM on it? Tired of having to reconnect the Paint data widget and the File widget back and forth whenever you switch between them?\nSay no more! Look no further! Here is the new Hub widget!\nHub widget is the most versatile widget available so far.",
	"date" : "Sep 11, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Hubbing with the Hub widget",
	"icon" : ""
},
{
    "uri": "/blog/2015/09/04/updated-widget-documentation/",
	"title": "Updated Widget Documentation",
	"description": "",
	"content": "Happy news for all passionate Orange users! We’ve uploaded documentation for our Orange 3 widget selection.\nRight click and select “Help” or press F1.\n** **\nIt’s easy to use. To learn more about a particular wigdet, click on the widget. Either use right click and select “Help” or press F1. A new window will open with a widget description and an example for its use. There are also screenshots included as visual help.\nWidget documentation.\n** **\nWe are going to be updating documentation as the widgets continue to develop. Documentation for bioinformatics and data fusion add-ons is expected to be up and running in the following week.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Happy news for all passionate Orange users! We’ve uploaded documentation for our Orange 3 widget selection.\nRight click and select \u0026ldquo;Help\u0026rdquo; or press F1.\n** **\nIt’s easy to use. To learn more about a particular wigdet, click on the widget. Either use right click and select “Help” or press F1. A new window will open with a widget description and an example for its use. There are also screenshots included as visual help." ,
	"author" : "AJDA",
	"summary" : "Happy news for all passionate Orange users! We’ve uploaded documentation for our Orange 3 widget selection.\nRight click and select \u0026ldquo;Help\u0026rdquo; or press F1.\n** **\nIt’s easy to use. To learn more about a particular wigdet, click on the widget. Either use right click and select “Help” or press F1. A new window will open with a widget description and an example for its use. There are also screenshots included as visual help.",
	"date" : "Sep 4, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Updated Widget Documentation",
	"icon" : ""
},
{
    "uri": "/blog/2015/08/28/scatter-plot-projection-rank/",
	"title": "Scatter Plot Projection Rank",
	"description": "",
	"content": "One of the nicest and surely most useful visualization widgets in Orange is Scatter Plot. The widget displays a 2-D plot, where x and y-axes are two attributes from the data.\n2-dimensional scatter plot visualization\nOrange 2.7 has a wonderful functionality called VizRank, that is now implemented also in Orange 3. Rank Projections functionality enables you to find interesting attribute pairs by scoring their average classification accuracy. Click ‘Start Evaluation’ to begin ranking.\nRank Projections before ranking is performed.\nThe functionality will also instantly adapt the visualization to the best scored pair. Select other pairs from the list to compare visualizations.\nRank Projections once the attribute pairs are scored.\nRank suggested petal length and petal width as the best pair and indeed, the visualization below is much clearer (better separated).\nScatter Plot once the visualization is optimized.\nHave fun trying out this and other visualization widgets!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One of the nicest and surely most useful visualization widgets in Orange is Scatter Plot. The widget displays a 2-D plot, where x and y-axes are two attributes from the data.\n2-dimensional scatter plot visualization\nOrange 2.7 has a wonderful functionality called VizRank, that is now implemented also in Orange 3. Rank Projections functionality enables you to find interesting attribute pairs by scoring their average classification accuracy. Click ‘Start Evaluation’ to begin ranking." ,
	"author" : "AJDA",
	"summary" : "One of the nicest and surely most useful visualization widgets in Orange is Scatter Plot. The widget displays a 2-D plot, where x and y-axes are two attributes from the data.\n2-dimensional scatter plot visualization\nOrange 2.7 has a wonderful functionality called VizRank, that is now implemented also in Orange 3. Rank Projections functionality enables you to find interesting attribute pairs by scoring their average classification accuracy. Click ‘Start Evaluation’ to begin ranking.",
	"date" : "Aug 28, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Scatter Plot Projection Rank",
	"icon" : ""
},
{
    "uri": "/blog/2015/08/14/classifying-instances-with-orange-in-python/",
	"title": "Classifying instances with Orange in Python",
	"description": "",
	"content": "Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.\nFirst we’re going to create a new data table with 10 fruits as our instances.\nimport Orange from Orange.data import * color = DiscreteVariable(\"color\", values=[\"orange\", \"green\", \"yellow\"])calories = ContinuousVariable(\"calories\") fiber = ContinuousVariable(\"fiber\") fruit = DiscreteVariable(\"fruit\", values=[\"orange\", \"apple\", \"peach\"]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [\u003c/span\u003e [\"green\", 4, 1.2, \"apple\"], [\"orange\", 5, 1.1, \"orange\"], [\"yellow\", 4, 1.0, \"peach\"], [\"orange\", 4, 1.1, \"orange\"], [\"yellow\", 4, 1.1,\"peach\"], [\"green\", 5, 1.3, \"apple\"], [\"green\", 4, 1.3, \"apple\"], [\"orange\", 5, 1.0, \"orange\"], [\"yellow\", 4.5, 1.3, \"peach\"], [\"green\", 5, 1.0, \"orange\"]]) print(data)  Now we have to select a model for classification. Among the many learners in Orange library, we decided to use the Tree Learner for this example. Since we’re dealing with fruits, we thought it’s only appropriate. :)\nLet’s create a learning algorithm and use it to induce the classifier from the data.\ntree_learner = Orange.classification.TreeLearner() tree = tree_learner(data)  Now we can predict what variety a green fruit with 3.5 calories and 2g of fiber is with the help of our model. To do this, simply call the model and use a list of new data as argument.\nprint(tree([\"green\", 3.5, 2]))  Python returns index as a result:\n1  To check the index, we can call class variable values with the corresponding index:\ndomain.class_var.values[1]  Final result:\n\"apple\"  You can use your own data set to see how this model works for different data types. Let us know how it goes! :)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.\nFirst we’re going to create a new data table with 10 fruits as our instances.\nimport Orange from Orange.data import * color = DiscreteVariable(\u0026quot;color\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;yellow\u0026quot;])calories = ContinuousVariable(\u0026quot;calories\u0026quot;) fiber = ContinuousVariable(\u0026quot;fiber\u0026quot;) fruit = DiscreteVariable(\u0026quot;fruit\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;peach\u0026quot;]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [\u0026lt;/span\u0026gt; [\u0026quot;green\u0026quot;, 4, 1." ,
	"author" : "AJDA",
	"summary" : "Last week we showed you how to create your own data table in Python shell. Now we’re going to take you a step further and show you how to easily classify data with Orange.\nFirst we’re going to create a new data table with 10 fruits as our instances.\nimport Orange from Orange.data import * color = DiscreteVariable(\u0026quot;color\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;green\u0026quot;, \u0026quot;yellow\u0026quot;])calories = ContinuousVariable(\u0026quot;calories\u0026quot;) fiber = ContinuousVariable(\u0026quot;fiber\u0026quot;) fruit = DiscreteVariable(\u0026quot;fruit\u0026quot;, values=[\u0026quot;orange\u0026quot;, \u0026quot;apple\u0026quot;, \u0026quot;peach\u0026quot;]) domain = Domain([color, calories, fiber], class_vars=fruit) data=Table(domain, [\u0026lt;/span\u0026gt; [\u0026quot;green\u0026quot;, 4, 1.",
	"date" : "Aug 14, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Classifying instances with Orange in Python",
	"icon" : ""
},
{
    "uri": "/blog/2015/08/07/creating-a-new-data-table-in-orange-through-python/",
	"title": "Creating a new data table in Orange through Python",
	"description": "",
	"content": "IMPORT DATA\nOne of the first tasks in Orange data analysis is of course loading your data. If you are using Orange through Python, this is as easy as riding a bike:\nimport Orange data = Orange.data.Table(“iris”) print (data)  This will return a neat data table of the famous Iris data set in the console.\nCREATE YOUR OWN DATA TABLE\nWhat if you want to create your own data table from scratch? Even this is surprisingly simple. First, import the Orange data library.\nfrom Orange.data import *  Set all the attributes you wish to see in your data table. For discrete attributes call DiscreteVariable and set the name and the possible values, while for a continuous variable call ContinuousVariable and set only the attribute name.\ncolor = DiscreteVariable(“color”, values=[“orange”, “green”, “yellow”]) calories = ContinuousVariable(“calories”) fiber = ContinuousVariable(“fiber”)] fruit = DiscreteVariable(\"fruit”, values=[”orange\", “apple”, “peach”])  Then set the domain for your data table. See how we set class variable with class_vars?\ndomain = Domain([color, calories, fiber], class_vars=fruit)  Time to input your data!\ndata = Table(domain, [ [“green”, 4, 1.2, “apple”], [\"orange\", 5, 1.1, \"orange\"], [\"yellow\", 4, 1.0, \"peach\"]])  And now print what you have created!\nprint(data)  One final step:\nTable.save(table, \"fruit.tab\")  Your data is safely stored to your computer (in the Python folder)! Good job!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "IMPORT DATA\nOne of the first tasks in Orange data analysis is of course loading your data. If you are using Orange through Python, this is as easy as riding a bike:\nimport Orange data = Orange.data.Table(“iris”) print (data)  This will return a neat data table of the famous Iris data set in the console.\nCREATE YOUR OWN DATA TABLE\nWhat if you want to create your own data table from scratch?" ,
	"author" : "AJDA",
	"summary" : "IMPORT DATA\nOne of the first tasks in Orange data analysis is of course loading your data. If you are using Orange through Python, this is as easy as riding a bike:\nimport Orange data = Orange.data.Table(“iris”) print (data)  This will return a neat data table of the famous Iris data set in the console.\nCREATE YOUR OWN DATA TABLE\nWhat if you want to create your own data table from scratch?",
	"date" : "Aug 7, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Creating a new data table in Orange through Python",
	"icon" : ""
},
{
    "uri": "/blog/2015/07/31/datasets-in-orange-bioinformatics-add-on/",
	"title": "Datasets in Orange Bioinformatics Add-On",
	"description": "",
	"content": "As you might know, Orange comes with several basic widget sets pre-installed. These allow you to upload and explore the data, visualize them, learn from them and make predictions. However, there are also some exciting add-ons available for installation. One of these is a bioinformatics add-on, which is our specialty.\nBioinformatics widget set allows you to pursue complex analysis of gene expression by providing access to several external libraries. There are four widgets intended specifically for this - dictyExpress, GEO Data Sets, PIPAx and GenExpress. GEO Data Sets are sourced from NCBI, PIPAx and dictyExpress from two Biolab projects, and finally GenExpress from Genialis. A lot of the data is freely accessible, while you will need a user account for the rest.\nOnce you open the widget, select the experiments you wish to use for your analysis and view it in the Data Table widget. You can compare these experiments in Data Profiles, visualize them in Volcano Plot, select the most relevant genes in Differential Expression widget and much more.\nThree widgets with experiment data libraries.\nThese databases enable you to start your research just by installing the bioinformatics add-on (Orange → Options → Add-ons…). The great thing is you can easily combine bioinformatics widgets with the basic pre-installed ones. What an easy way to immerse yourself in the exciting world of bioinformatics!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "As you might know, Orange comes with several basic widget sets pre-installed. These allow you to upload and explore the data, visualize them, learn from them and make predictions. However, there are also some exciting add-ons available for installation. One of these is a bioinformatics add-on, which is our specialty.\nBioinformatics widget set allows you to pursue complex analysis of gene expression by providing access to several external libraries. There are four widgets intended specifically for this - dictyExpress, GEO Data Sets, PIPAx and GenExpress." ,
	"author" : "AJDA",
	"summary" : "As you might know, Orange comes with several basic widget sets pre-installed. These allow you to upload and explore the data, visualize them, learn from them and make predictions. However, there are also some exciting add-ons available for installation. One of these is a bioinformatics add-on, which is our specialty.\nBioinformatics widget set allows you to pursue complex analysis of gene expression by providing access to several external libraries. There are four widgets intended specifically for this - dictyExpress, GEO Data Sets, PIPAx and GenExpress.",
	"date" : "Jul 31, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Datasets in Orange Bioinformatics Add-On",
	"icon" : ""
},
{
    "uri": "/blog/2015/07/24/visualizing-misclassifications/",
	"title": "Visualizing Misclassifications",
	"description": "",
	"content": "In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.\nIn Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification. Below we use Iris data set for simplicity, but the same procedure works just as well on all kinds of data sets.\nHere we have three confusion matrices for Naive Bayes (top), Classification Tree (middle) and Logistic Regression (bottom).\nThree misclassification matrices (Naive Bayes, Classification Tree and Logistic Regression)\nWe see that Classification Tree did the best with only 9 misclassified instances. To see which instances were assigned a false class, we select ‘Misclassified’ option in the widget, which highlights misclassifications and feeds them to the Scatter Plot widget. In the graph we thus see the entire data set presented with empty dots and the selected misclassifications with full dots.\nVisualization of misclassified instances in scatter plot.\nFeel free to switch between learners in Confusion Matrix to see how the visualization changes for each of them.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.\nIn Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification." ,
	"author" : "AJDA",
	"summary" : "In data mining classification is one of the key methods for making predictions and gaining important information from our data. We would, for example, use classification for predicting which patients are likely to have the disease based on a given set of symptoms.\nIn Orange an easy way to classify your data is to select several classification widgets (e.g. Naive Bayes, Classification Tree and Linear Regression), compare the prediction quality of each learner with Test Learners and Confusion Matrix and then use the best performing classifier on a new data set for classification.",
	"date" : "Jul 24, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Visualizing Misclassifications",
	"icon" : ""
},
{
    "uri": "/blog/2015/07/20/explorative-data-analysis-with-hierarchical-clustering/",
	"title": "Explorative data analysis with Hierarchical Clustering",
	"description": "",
	"content": "Today we will write about cluster analysis with Hierarchical Clustering widget. We use a well-known Iris data set, which contains 150 Iris flowers, each belonging to one of the three species (setosa, versicolor and virginica). To an untrained eye the three species are very alike, so how could we best tell them apart? The data set contains measurements of sepal and petal dimensions (width and length) and we assume that these gives rise to interesting clustering. But is this so?\nHierarchical Clustering workflow\nTo find clusters, we feed the data from the File widget to Distances and then into Hierarchical Clustering. The last widget in our workflow visualizes hierarchical clustering dendrogram. In the dendrogram, let us annotate the branches with the corresponding Iris species (Annotation = Iris). We see that not all the clusters are composed of the same actual class - there are some mixed clusters with both virginicas and versicolors.\nSelected clusters in Hierarchical Clustering widget\nTo see these clusters, we select them in Hierarchical Clustering widget by clicking on a branch. Selected data will be fed into the output of this widget. Let us inspect the data we have selected by adding Scatter Plot and PCA widgets. If we draw a Data Table directly from Hierarchical Clustering, we see the selected instances and the clusters they belong to. But if we first add the PCA widget, which decomposes the data into principal components, and then connect it to Scatter Plot, we will see the selected instances in the adjusted scatter plot (where principal components are used for x and y-axis).\nSelect other clusters in Hierarchical Clustering widget to see how the scatter plot visualization changes. This allows for an interesting explorative data analysis through a combination of widgets for unsupervised learning and visualizations.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Today we will write about cluster analysis with Hierarchical Clustering widget. We use a well-known Iris data set, which contains 150 Iris flowers, each belonging to one of the three species (setosa, versicolor and virginica). To an untrained eye the three species are very alike, so how could we best tell them apart? The data set contains measurements of sepal and petal dimensions (width and length) and we assume that these gives rise to interesting clustering." ,
	"author" : "AJDA",
	"summary" : "Today we will write about cluster analysis with Hierarchical Clustering widget. We use a well-known Iris data set, which contains 150 Iris flowers, each belonging to one of the three species (setosa, versicolor and virginica). To an untrained eye the three species are very alike, so how could we best tell them apart? The data set contains measurements of sepal and petal dimensions (width and length) and we assume that these gives rise to interesting clustering.",
	"date" : "Jul 20, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Explorative data analysis with Hierarchical Clustering",
	"icon" : ""
},
{
    "uri": "/blog/principal-component-analysis/",
	"title": "principal component analysis",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 20, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "principal component analysis",
	"icon" : ""
},
{
    "uri": "/blog/workflow/",
	"title": "workflow",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 20, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "workflow",
	"icon" : ""
},
{
    "uri": "/blog/2015/07/10/learn-with-paint-data/",
	"title": "Learn with Paint Data",
	"description": "",
	"content": "Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).\nThe widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph.\nSeveral painting tools allow you to manage your data set according to your specific needs; brush will paint several data instances at once, while put allows you paint a single data instance. Select a data subset and view it in the Data Table widget or zoom in to see the position of your points up close. Jitter and magnet are converse tools which allow either to spread the instances or draw them closer together.\nThe data will be represented in a data table with two attributes, where their instances correspond to coordinates in the system. Such data set is great for demonstrating k-means and hierarchical clustering methods. Just like we do below. In the screenshot we see that k-means, with our particular settings, recognizes clusters way better than hierarchical clustering. It returns a score rank, where the best score (the one with the highest value) means the most likely number of clusters. Hierarchical clustering, however, doesn’t even group the right classes together.\nPaint Data widget for comparing precision of k-means and hierarchical clustering methods.\nAnother way to use Paint Data is to observe the performance of classification methods, where we can alter the graph to demonstrate improvement or deterioration of prediction models. By painting the data points we can try to construct the data set, which would be difficult for one but easy for another classifier. Say, why does linear SVM fail on the data set below?\nUse Paint Data to compare prediction quality of several classifiers.\nHappy painting!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).\nThe widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph." ,
	"author" : "AJDA",
	"summary" : "Paint Data widget might initially look like a kids’ game, but in combination with other Orange widgets it becomes a very simple and useful tool for conveying statistical concepts, such as k-means, hierarchical clustering and prediction models (like SVM, logistical regression, etc.).\nThe widget enables you to draw your data on a 2-D plane. You can name the x and y axes, select the number of classes (which are represented by different colors) and then position the points on a graph.",
	"date" : "Jul 10, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Learn with Paint Data",
	"icon" : ""
},
{
    "uri": "/blog/2015/07/03/support-vectors-output-in-svm-widget/",
	"title": "Support vectors output in SVM widget",
	"description": "",
	"content": "Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.\nIdeally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance. This margin touches a small number of data instances that are called support vectors.\nIn Orange 3.0 you can set the SVM classification widget to output also the support vectors and visualize them. We used Iris data set in the File widget and classified data instances with SVM classifier. Then we connected both widgets with Scatterplot and selected Support Vectors in the SVM output channel. This allows us to see support vectors in the Scatterplot widget - they are represented by the bold dots in the graph.\nNow feel free to try it with your own data set!\nSupport vectors output of SVM widget with Iris data set.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.\nIdeally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance." ,
	"author" : "AJDA",
	"summary" : "Did you know that the widget for support vector machines (SVM) classifier can output support vectors? And that you can visualise these in any other Orange widget? In the context of all other data sets, this could provide some extra insight into how this popular classification algorithm works and what it actually does.\nIdeally, that is, in the case of linear seperability, support vector machines (SVM) find a **hyperplane with the largest margin **to any data instance.",
	"date" : "Jul 3, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Support vectors output in SVM widget",
	"icon" : ""
},
{
    "uri": "/blog/2015/06/19/435/",
	"title": "Orange workshops around the world",
	"description": "",
	"content": "Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion. Marinka also got an award for the poster, where data fusion was used to hunt for Dictyostelium bacterial-response genes. This week, we are in Pavia, Italy, also for Matrix Computations in Biomedical Informatics Workshop at AIME 2015, a Conference on Artificial Intelligence in Medicine. During the workshop, we are giving an invited talk on learning latent factor models by data fusion and we’ll also show Orange’s data fusion add-on. Thanks to the workshop organizers, Riccardo Bellazzi, Jimeng Sun and Ping Zhang, the workshop program looks great.\nBlaž with Riccardo and John in Pavia, Italy\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion." ,
	"author" : "AJDA",
	"summary" : "Even though the summer is nigh, we are hardly going to catch a summer break this year. Orange team is busy holding workshops around the world to present the latest widgets and data mining tools to the public. Last week we had a very successful tutorial at [BC]2 in Basel, Switzerland, where Marinka and Blaž presented data fusion. A part of the tutorial was a hands-on workshop with Orange’s new add-on for data fusion.",
	"date" : "Jun 19, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange workshops around the world",
	"icon" : ""
},
{
    "uri": "/blog/2015/06/08/data-fusion-tutorial-at-the-bc2/",
	"title": "Data Fusion Tutorial at the [BC]^2",
	"description": "",
	"content": "We are excited to host a three-hour tutorial on data fusion at the Basel Computational Biology Conference. To this end we have prepared a series of short lectures notes that accompany the recently developed Data Fusion Add-on for Orange.\nWe design the tutorial for data mining researchers and molecular biologists with interest in large-scale data integration. In the tutorial we focus on collective latent factor models, a popular class of approaches for data fusion. We demonstrate the effectiveness of these approaches on several hands-on case studies from recommendation systems and molecular biology.\nThis is a high-risk event. I mean, for us, lecturers. Ok, no bricks will probably fall down. But, in the part of the tutorial, this is the first time we are showing Orange’s data fusion add-on. And not just showing: part of the tutorial is a hands-on session.\nWe would like to acknowledge Biolab members for pushing the widgets through the development pipeline under extreme time constraints. Special thanks to Anze, Ales, Jernej, Andrej, Marko, Aleksandar and all other members of the lab.\nThis post was contributed by Marinka and Blaz.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We are excited to host a three-hour tutorial on data fusion at the Basel Computational Biology Conference. To this end we have prepared a series of short lectures notes that accompany the recently developed Data Fusion Add-on for Orange.\nWe design the tutorial for data mining researchers and molecular biologists with interest in large-scale data integration. In the tutorial we focus on collective latent factor models, a popular class of approaches for data fusion." ,
	"author" : "MARINKAZ",
	"summary" : "We are excited to host a three-hour tutorial on data fusion at the Basel Computational Biology Conference. To this end we have prepared a series of short lectures notes that accompany the recently developed Data Fusion Add-on for Orange.\nWe design the tutorial for data mining researchers and molecular biologists with interest in large-scale data integration. In the tutorial we focus on collective latent factor models, a popular class of approaches for data fusion.",
	"date" : "Jun 8, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Fusion Tutorial at the [BC]^2",
	"icon" : ""
},
{
    "uri": "/blog/data-fusion/",
	"title": "data-fusion",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jun 8, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "data-fusion",
	"icon" : ""
},
{
    "uri": "/blog/2015/06/05/data-fusion-add-on-for-orange/",
	"title": "Data Fusion Add-on for Orange",
	"description": "",
	"content": "Orange is about to get even more exciting! We have created a prototype add-on for data fusion, which will certainly be of interest to many users. Data fusion brings large heterogeneous data sets together to create sensible clusters of related data instances and provides a platform for predictive modelling and recommendation systems.\nThis widget set can be used either to recommend you the next movie to watch based on your demographic characteristics, movies you gave high scores to, your preferred genre, etc. or to suggest you a set of genes that might be relevant for a particular biological function or process. We envision the add-on to be useful for predictive modeling dealing with large heterogeneous data compendia, such as life sciences.\nThe prototype set will be available for download next week, but we are happy to give you a sneak peek below.\nData fusion workflow\n Movie Ratings widget is pre-set to offer data on movie ratings by users with 706 users and 855 movies (10% of the data selected as a subset). We add IMDb Actors to filter the data by matching movie ratings with actors. Then we add the Fusion Graph widget to fuse the data together. Here we have two object types, i.e. users and movies, and one relation between them, i.e. movie ratings. In Latent Factors we see latent data representation demonstrated by red squares at the side. Let’s select a latent matrix associated with Users as our input for the Data Table. In Data Table we see the latent data matrix of Users. The algorithm infers low-dimensional user profiles by collective consideration of entire data collection, i.e. movie ratings and actor information. In our scenario the algorithm has transformed 855 movie titles into 70 movie groupings, i.e. latent components.  Data fusion visualized\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange is about to get even more exciting! We have created a prototype add-on for data fusion, which will certainly be of interest to many users. Data fusion brings large heterogeneous data sets together to create sensible clusters of related data instances and provides a platform for predictive modelling and recommendation systems.\nThis widget set can be used either to recommend you the next movie to watch based on your demographic characteristics, movies you gave high scores to, your preferred genre, etc." ,
	"author" : "AJDA",
	"summary" : "Orange is about to get even more exciting! We have created a prototype add-on for data fusion, which will certainly be of interest to many users. Data fusion brings large heterogeneous data sets together to create sensible clusters of related data instances and provides a platform for predictive modelling and recommendation systems.\nThis widget set can be used either to recommend you the next movie to watch based on your demographic characteristics, movies you gave high scores to, your preferred genre, etc.",
	"date" : "Jun 5, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data Fusion Add-on for Orange",
	"icon" : ""
},
{
    "uri": "/blog/2015/05/29/excel-files-in-orange-3-0/",
	"title": "Excel files in Orange 3.0",
	"description": "",
	"content": "Orange 3.0 version comes with an exciting feature that will simplify reading your data. If the old Orange required conversion from Excel into either tab-delimited or comma-separated files, the new version allows you to open plain .xlsx format data sets in the program. Naturally, the .txt and .csv files are still readable in Orange, so feel free to use data sets in any of the above-mentioned formats.\nSince Orange 3.0 is still in the development mode, you will find a smaller selection of widgets available at the moment, but give it a go and see how it works for Excel type data and whether the existing widgets are sufficient for your data analysis. Please find the daily build for OSX here.\nOrange 3.0 can read Excel files.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange 3.0 version comes with an exciting feature that will simplify reading your data. If the old Orange required conversion from Excel into either tab-delimited or comma-separated files, the new version allows you to open plain .xlsx format data sets in the program. Naturally, the .txt and .csv files are still readable in Orange, so feel free to use data sets in any of the above-mentioned formats.\nSince Orange 3.0 is still in the development mode, you will find a smaller selection of widgets available at the moment, but give it a go and see how it works for Excel type data and whether the existing widgets are sufficient for your data analysis." ,
	"author" : "AJDA",
	"summary" : "Orange 3.0 version comes with an exciting feature that will simplify reading your data. If the old Orange required conversion from Excel into either tab-delimited or comma-separated files, the new version allows you to open plain .xlsx format data sets in the program. Naturally, the .txt and .csv files are still readable in Orange, so feel free to use data sets in any of the above-mentioned formats.\nSince Orange 3.0 is still in the development mode, you will find a smaller selection of widgets available at the moment, but give it a go and see how it works for Excel type data and whether the existing widgets are sufficient for your data analysis.",
	"date" : "May 29, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Excel files in Orange 3.0",
	"icon" : ""
},
{
    "uri": "/blog/2015/05/22/orange-fridays/",
	"title": "Orange Fridays",
	"description": "",
	"content": "You might think “casual Fridays” are the best thing since sliced bread. But what if I were to tell you we have “Orange Fridays” at our lab, where lab members focus solely on debugging Orange software and making improvements to existing features. This is because the new developing version of Orange (3.0) still needs certain widgets to be implemented, such as net explorer, radviz, and survey plot.\nBut there’s more. We are currently hosting an expert on data fusion from the University of Leuven, prof. dr. Yves Moreau, to discuss new venues and niches for the development of Orange. The big debate is how to scale the program to fit large data sets and make it possible to process such sets in a shorter period of time. If you have any ideas and suggestions, please feel free to share them on our community forum.\nprof. dr. Yves Moreau - Prioritization of candidate disease genes and drug—target interactions by genomic data fusion\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "You might think “casual Fridays” are the best thing since sliced bread. But what if I were to tell you we have “Orange Fridays” at our lab, where lab members focus solely on debugging Orange software and making improvements to existing features. This is because the new developing version of Orange (3.0) still needs certain widgets to be implemented, such as net explorer, radviz, and survey plot.\nBut there’s more. We are currently hosting an expert on data fusion from the University of Leuven, prof." ,
	"author" : "AJDA",
	"summary" : "You might think “casual Fridays” are the best thing since sliced bread. But what if I were to tell you we have “Orange Fridays” at our lab, where lab members focus solely on debugging Orange software and making improvements to existing features. This is because the new developing version of Orange (3.0) still needs certain widgets to be implemented, such as net explorer, radviz, and survey plot.\nBut there’s more. We are currently hosting an expert on data fusion from the University of Leuven, prof.",
	"date" : "May 22, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange Fridays",
	"icon" : ""
},
{
    "uri": "/blog/2015/05/05/working-with-sql-data-in-orange-3/",
	"title": "Working with SQL data in Orange 3",
	"description": "",
	"content": "Orange 3 is slowly, but steadily, gaining support for working with data stored in a SQL database. The main focus is to allow huge data sets that do not fit into RAM to be analyzed and visualized efficiently. Many widgets already recognize the type of input data and perform the necessary computations intelligently. This means that data is not downloaded from the database and analyzed locally, but is retained on the remote server, with the computation tasks translated into SQL queries and offloaded to the database engine. This approach takes advantage of the state-of-the-art optimizations relational databases have for working with data that does not fit into working memory, as well as minimizes the transfer of required information to the client.\nWe demonstrate how to explore and visualize data stored in a SQL table on a remote server in the following short video. It shows how to connect to the server and load the data with the SqlTable widget, manipulate the data (Select Columns, Select Rows), obtain the summary statistics (Box plot, Distributions), and visualize the data (Heat map, Mosaic Display).\n  The research leading to these results has received funding from the European Union’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no 318633\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange 3 is slowly, but steadily, gaining support for working with data stored in a SQL database. The main focus is to allow huge data sets that do not fit into RAM to be analyzed and visualized efficiently. Many widgets already recognize the type of input data and perform the necessary computations intelligently. This means that data is not downloaded from the database and analyzed locally, but is retained on the remote server, with the computation tasks translated into SQL queries and offloaded to the database engine." ,
	"author" : "LAN",
	"summary" : "Orange 3 is slowly, but steadily, gaining support for working with data stored in a SQL database. The main focus is to allow huge data sets that do not fit into RAM to be analyzed and visualized efficiently. Many widgets already recognize the type of input data and perform the necessary computations intelligently. This means that data is not downloaded from the database and analyzed locally, but is retained on the remote server, with the computation tasks translated into SQL queries and offloaded to the database engine.",
	"date" : "May 5, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Working with SQL data in Orange 3",
	"icon" : ""
},
{
    "uri": "/blog/2015/02/19/orange-in-pavia-italy/",
	"title": "Orange in Pavia, Italy",
	"description": "",
	"content": "These days, we (Blaz Zupan and Marinka Zitnik, with full background support of entire Bioinformatics Lab) are running a three-day course on Data Mining in Python. Riccardo Bellazzi, a professor at University of Pavia, a world-renown researcher in biomedical informatics, and most of all, a great friend, has invited us to run the elective course for Pavia’s grad students. The enrollment was, he says, overwhelming, as with over 50 students this is by far the best attended grad course at Pavia’s faculty of engineering in the past years.\nWe have opted for the hands-on course and a running it as a workshop. The lectures include a new, development version of Orange 3, and mix it with numpy, scikit-learn, matplotlib, networkx and bunch of other libraries. Course themes are classification, clustering, data projection and network analysis.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "These days, we (Blaz Zupan and Marinka Zitnik, with full background support of entire Bioinformatics Lab) are running a three-day course on Data Mining in Python. Riccardo Bellazzi, a professor at University of Pavia, a world-renown researcher in biomedical informatics, and most of all, a great friend, has invited us to run the elective course for Pavia\u0026rsquo;s grad students. The enrollment was, he says, overwhelming, as with over 50 students this is by far the best attended grad course at Pavia\u0026rsquo;s faculty of engineering in the past years." ,
	"author" : "BLAZ",
	"summary" : "These days, we (Blaz Zupan and Marinka Zitnik, with full background support of entire Bioinformatics Lab) are running a three-day course on Data Mining in Python. Riccardo Bellazzi, a professor at University of Pavia, a world-renown researcher in biomedical informatics, and most of all, a great friend, has invited us to run the elective course for Pavia\u0026rsquo;s grad students. The enrollment was, he says, overwhelming, as with over 50 students this is by far the best attended grad course at Pavia\u0026rsquo;s faculty of engineering in the past years.",
	"date" : "Feb 19, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange in Pavia, Italy",
	"icon" : ""
},
{
    "uri": "/blog/2015/02/12/towards-orange-3/",
	"title": "Towards Orange 3",
	"description": "",
	"content": "We are rushing, full speed ahead, towards Orange 3. A complete revamp of Orange in Python 3 changes its data model to that of numpy, making Orange compatible with an array of Python-based data analytics. We are rewriting all the widgets for visual programming as well. We have two open fronts: the scripting part, and the widget part. So much to do, but it is going well: the closed tasks for widgets are those on the left of Anze (the board full of sticky notes), and those open, in minority, are on Anze’s right. Oh, by the way, it’s Anze who is managing the work and he looks quite happy.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We are rushing, full speed ahead, towards Orange 3. A complete revamp of Orange in Python 3 changes its data model to that of numpy, making Orange compatible with an array of Python-based data analytics. We are rewriting all the widgets for visual programming as well. We have two open fronts: the scripting part, and the widget part. So much to do, but it is going well: the closed tasks for widgets are those on the left of Anze (the board full of sticky notes), and those open, in minority, are on Anze\u0026rsquo;s right." ,
	"author" : "BLAZ",
	"summary" : "We are rushing, full speed ahead, towards Orange 3. A complete revamp of Orange in Python 3 changes its data model to that of numpy, making Orange compatible with an array of Python-based data analytics. We are rewriting all the widgets for visual programming as well. We have two open fronts: the scripting part, and the widget part. So much to do, but it is going well: the closed tasks for widgets are those on the left of Anze (the board full of sticky notes), and those open, in minority, are on Anze\u0026rsquo;s right.",
	"date" : "Feb 12, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Towards Orange 3",
	"icon" : ""
},
{
    "uri": "/blog/2015/01/18/loading-your-data/",
	"title": "Loading your data",
	"description": "",
	"content": "By a popular demand, we have just published a tutorial on how to load the data table into Orange. Besides its own .tab format, Orange can load any tab or comma delimited data set. The details are though in writing header rows that tell Orange about the type and domain of each attribute. The tutorial is a step-by-step description on how to do this and how to transfer the data from popular spreadsheet programs like Excel.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "By a popular demand, we have just published a tutorial on how to load the data table into Orange. Besides its own .tab format, Orange can load any tab or comma delimited data set. The details are though in writing header rows that tell Orange about the type and domain of each attribute. The tutorial is a step-by-step description on how to do this and how to transfer the data from popular spreadsheet programs like Excel." ,
	"author" : "BLAZ",
	"summary" : "By a popular demand, we have just published a tutorial on how to load the data table into Orange. Besides its own .tab format, Orange can load any tab or comma delimited data set. The details are though in writing header rows that tell Orange about the type and domain of each attribute. The tutorial is a step-by-step description on how to do this and how to transfer the data from popular spreadsheet programs like Excel.",
	"date" : "Jan 18, 2015",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Loading your data",
	"icon" : ""
},
{
    "uri": "/blog/2014/10/24/hands-on-orange-at-functional-genomics-workshop/",
	"title": "Hands-on Orange at Functional Genomics Workshop",
	"description": "",
	"content": "Last week we have co-organized a Functional Genomics Workshop. At University of Ljubljana we have hosted an inspiring pack of scientists from the Donnelly Centre for Cellular and Biomolecular Research from Toronto. Part of the event was a hands-on workshop Data mining without programing, where we have used Orange to analyze data from systems biology. Data included a subset of Charlie Boone’s famous yeast interaction data and data from chemical genomics. For the program, info about the speakers, and panckages and šmorn check out workshop’s newspaper.\nIt is always a pleasure seeing a packed lecture room with all laptops running Orange. Attendees were assisted by members of the Biolab in Ljubljana. Hands-on program followed a set of short lectures we have crafted for intended audience – biologists. Everything ran smoothly. At the end, we got excited enough to promise a data import wizard for all those that have problems annotating the data with feature type tags. The deadline: two weeks from the end of the workshop.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Last week we have co-organized a Functional Genomics Workshop. At University of Ljubljana we have hosted an inspiring pack of scientists from the Donnelly Centre for Cellular and Biomolecular Research from Toronto. Part of the event was a hands-on workshop Data mining without programing, where we have used Orange to analyze data from systems biology. Data included a subset of Charlie Boone\u0026rsquo;s famous yeast interaction data and data from chemical genomics." ,
	"author" : "BLAZ",
	"summary" : "Last week we have co-organized a Functional Genomics Workshop. At University of Ljubljana we have hosted an inspiring pack of scientists from the Donnelly Centre for Cellular and Biomolecular Research from Toronto. Part of the event was a hands-on workshop Data mining without programing, where we have used Orange to analyze data from systems biology. Data included a subset of Charlie Boone\u0026rsquo;s famous yeast interaction data and data from chemical genomics.",
	"date" : "Oct 24, 2014",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Hands-on Orange at Functional Genomics Workshop",
	"icon" : ""
},
{
    "uri": "/blog/computervision/",
	"title": "computervision",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Aug 26, 2014",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "computervision",
	"icon" : ""
},
{
    "uri": "/blog/2014/08/26/orange-canvas-applied-to-x-ray-optics/",
	"title": "Orange Canvas applied to x-ray optics",
	"description": "",
	"content": "Orange Canvas is being appropriated by guys who would like to use it as graphical environment for simulating x-ray optics.\nManuel Sanchez del Rio, from The European Synchrotron Facility in Grenoble, France, and Luca Rebuffi from Elettra-Sincrotrone, Trieste, Italy, were looking for a tool that would help them integrate the various tools for x-ray optics simulations, like the popular SHADOW and SRW. They discovered that the data workflow paradigm, like the one used in Orange Canvas, fits their needs perfectly. They took Orange, and replaced the existing widgets with new widgets that represent sources of photons (bending magnets, in the case of ESRF), various optical elements, like lenses and mirrors, and detectors. The channels between the widgets no longer pass data tables, like in the standard Orange, but rays of photons. How cool is this?\nThe result is a system in which the user can arrange the elements in a system that resembles the actual physical system, and then run the simulations using the most powerful tools available in x-ray optics.\nThe tool prototype has been presented at the SPIE Optics + Photonic 2014 in San Diego, the largest meeting of its kind.\nWe’re really excited about this novel use of Orange Canvas.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange Canvas is being appropriated by guys who would like to use it as graphical environment for simulating x-ray optics.\nManuel Sanchez del Rio, from The European Synchrotron Facility in Grenoble, France, and Luca Rebuffi from Elettra-Sincrotrone, Trieste, Italy, were looking for a tool that would help them integrate the various tools for x-ray optics simulations, like the popular SHADOW and SRW. They discovered that the data workflow paradigm, like the one used in Orange Canvas, fits their needs perfectly." ,
	"author" : "BIOLAB",
	"summary" : "Orange Canvas is being appropriated by guys who would like to use it as graphical environment for simulating x-ray optics.\nManuel Sanchez del Rio, from The European Synchrotron Facility in Grenoble, France, and Luca Rebuffi from Elettra-Sincrotrone, Trieste, Italy, were looking for a tool that would help them integrate the various tools for x-ray optics simulations, like the popular SHADOW and SRW. They discovered that the data workflow paradigm, like the one used in Orange Canvas, fits their needs perfectly.",
	"date" : "Aug 26, 2014",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange Canvas applied to x-ray optics",
	"icon" : ""
},
{
    "uri": "/blog/2014/05/29/orange-and-sql/",
	"title": "Orange and SQL",
	"description": "",
	"content": "Orange 3.0 will also support working with data stored in a database.\nWhile we have already talked about this some time ago, we here describe some technical details for anybody interested. This is not a thorough tecnical report, its purpose is only to provide an impression about the architecture of the upcoming version of Orange.\nSo, data tables in Orange 3.0 can refer to data in the working memory or in the database. Any (properly written) code that uses tables should work the same with both storages. When the data is stored in the database, the table is implemented as a “proxy object” with the necessary meta-data for constructing the SQL query to retrieve the data when needed. Operations on the data only modify the meta-data without retrieving any actual data. For instance, construction of a new table with some selected data subset, say all instances that match a certain condition, creates a new proxy with additional conditions for the WHERE clause. Similarly, selecting a subset of features only changes the domain (the list of features), which is later reflected in the columns of the SELECT clause.\nFeatures in this model are no longer described just with their names but also with the part which goes into the query that retrieves or constructs their values. Discretization, for instance, constructs new features which wrap the representation of the continuous features into a CASE statement that assigns a value based on the boundaries of the bins.\nSince the goal was to make the code in modules and widgets oblivious to the storage, we also needed separate implementation of the operations that need to be aware of how the data is stored. For instance, the code that computes the average values of attributes needs to be different for the two storages: for the in-memory data we need to use the corresponding numpy functions and for databases the average is computed on the server.\nWe went through the code of Orange 2.7 and identified the common operations on the data. We found that all data access belongs into the following types:\n basic aggregates like mean, variance, median, minimal and maximal value, distributions of discrete and continuous variables, values at percentiles, contingency matrices, covariance matrices, filtering of rows based on various criteria, including random sampling, selection of columns, construction of variables from values of other variables, matrices of distances (e.g. Euclidean) between all row pairs, individual data rows.  Points 1 to 4 are typical examples of what cannot be done on client but can be efficiently done in the database. The storage (a class derived from Table) now provides specialized methods for computing aggregates, distributions and contingencies, which use numpy for in-memory data and SQL for the data on the database.\nPoints 5 to 7 are implemented “lazily”, by modifying the SQL query describing the data as described above.\nPoint 8 is difficult to implement efficiently in common relational databases and, besides, results in a data matrix that is larger than the actual data. Methods that require such a matrix will need to be reimplemented and be aware of the storage mechanism.\nPoint 9 requires some caution with regard to how the data is retrieved and what it is used for. Access to individual rows should be used sparingly. Sequential retrieval - especially of all rows - needs to be avoided. For efficiency, most methods that did so in the previous versions of Orange will need to be reimplemented to use aggregate data (possibly as approximations) or to be aware of the data storage and execute some operations directly through SQL.\nWe have already ported a number of visualizations and other widgets to the new Orange. Here is one nice example: Mosaic needs to discretize the variables and then compute contingency matrices for discrete variables. Within the above scheme, the widget does not care about the storage mechanism, yet its computation is still as efficient as possible.\nThe described activities were funded in part by the European Union’s Seventh Framework Programme (FP7/2007-2013) under grant agreement n° 318633.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange 3.0 will also support working with data stored in a database.\nWhile we have already talked about this some time ago, we here describe some technical details for anybody interested. This is not a thorough tecnical report, its purpose is only to provide an impression about the architecture of the upcoming version of Orange.\nSo, data tables in Orange 3.0 can refer to data in the working memory or in the database." ,
	"author" : "BIOLAB",
	"summary" : "Orange 3.0 will also support working with data stored in a database.\nWhile we have already talked about this some time ago, we here describe some technical details for anybody interested. This is not a thorough tecnical report, its purpose is only to provide an impression about the architecture of the upcoming version of Orange.\nSo, data tables in Orange 3.0 can refer to data in the working memory or in the database.",
	"date" : "May 29, 2014",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange and SQL",
	"icon" : ""
},
{
    "uri": "/blog/2014/05/26/workshops-at-baylor-college-of-medicine/",
	"title": "Workshops at Baylor College of Medicine",
	"description": "",
	"content": "On May 22nd and May 23rd, we (Blaz Zupan and Janez Demsar, assisted by Marinka Zitnik and Balaji Santhanam) have given two hands-on workshops called Data Mining without Programming at Baylor College of Medicine in Houston, Texas.\nActually, there was a lot of programming, but no Python or alike. The workshop was designed for biomedical students and Baylor’s faculty members. We have presented a visual programming approach for development of data mining workflows for interactive data exploration. A three-hour workshop consisted of 15 data mining lessons on visual data exploration, classification, clustering, network analysis, and gene expression analytics. Each lesson focused on a particular data analysis task that the attendees solved with Orange.\nThe two workshops were organized by Baylor’s Computational and Integrative Biomedical Research Center. Over two days, the event was attended by a large audience of 120 attendees.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "On May 22nd and May 23rd, we (Blaz Zupan and Janez Demsar, assisted by Marinka Zitnik and Balaji Santhanam) have given two hands-on workshops called Data Mining without Programming at Baylor College of Medicine in Houston, Texas.\nActually, there was a lot of programming, but no Python or alike. The workshop was designed for biomedical students and Baylor\u0026rsquo;s faculty members. We have presented a visual programming approach for development of data mining workflows for interactive data exploration." ,
	"author" : "BIOLAB",
	"summary" : "On May 22nd and May 23rd, we (Blaz Zupan and Janez Demsar, assisted by Marinka Zitnik and Balaji Santhanam) have given two hands-on workshops called Data Mining without Programming at Baylor College of Medicine in Houston, Texas.\nActually, there was a lot of programming, but no Python or alike. The workshop was designed for biomedical students and Baylor\u0026rsquo;s faculty members. We have presented a visual programming approach for development of data mining workflows for interactive data exploration.",
	"date" : "May 26, 2014",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Workshops at Baylor College of Medicine",
	"icon" : ""
},
{
    "uri": "/blog/2014/04/29/viewing-images/",
	"title": "Viewing Images",
	"description": "",
	"content": "I am lately having fun with Image Viewer. The widget has been recently updated and can display images stored locally or on the internet. But wait, what images? How on earth can Orange now display images if it can handle mere tabular or basket-based data?\nHere’s an example. I have considered a subset of animals from the [download id=\"864”] data set (comes with Orange installation), and for demonstration purposes selected only a handful of attributes. I have added a new string attribute (“images”) and declared that this is a meta attribute of the type “image”. The values of this attribute are links to images on the web:\nHere is the resulting data set, [download id=\"859”]. I have used this data set in a schema with hierarchical clustering, where upon selection of the part of the clustering tree I can display the associated images:\nTypically and just like above, you would use a string meta attribute to store the link to images. Images can be referred to using a HTTP address, or, if stored locally, using a relative path from the data file location to the image files.\nHere is another example, where all the images were local and we have associated them with a famous digits data set ( download id=\"868” is a data set in the Orange format with the image files). The task for this data set is to classify handwritten digits based on their bitmap representation. In the schema below we wanted to find out which are the most frequent errors some classification algorithm would make, and how do the images of the misclassified digits look like. Turns out that SVM with RBF kernel most often misclassify the digit 9 and confuses it with a digit 3:\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "I am lately having fun with Image Viewer. The widget has been recently updated and can display images stored locally or on the internet. But wait, what images? How on earth can Orange now display images if it can handle mere tabular or basket-based data?\nHere\u0026rsquo;s an example. I have considered a subset of animals from the [download id=\u0026quot;864\u0026rdquo;] data set (comes with Orange installation), and for demonstration purposes selected only a handful of attributes." ,
	"author" : "BIOLAB",
	"summary" : "I am lately having fun with Image Viewer. The widget has been recently updated and can display images stored locally or on the internet. But wait, what images? How on earth can Orange now display images if it can handle mere tabular or basket-based data?\nHere\u0026rsquo;s an example. I have considered a subset of animals from the [download id=\u0026quot;864\u0026rdquo;] data set (comes with Orange installation), and for demonstration purposes selected only a handful of attributes.",
	"date" : "Apr 29, 2014",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Viewing Images",
	"icon" : ""
},
{
    "uri": "/blog/2013/12/20/paint-your-data/",
	"title": "Paint Your Data",
	"description": "",
	"content": "One of the widgets I enjoy very much when teaching introductory course in data mining is the Paint Data widget. When painting in this widget I would intentionally include some clusters, or intentionally obscure them. Or draw them in any strange shape. Then I would discuss with students if these clusters are identified by k-means clustering or by hierarchical clustering. We would also discuss automatic scoring of the quality of clusters, come up with the idea of a silhouette (ok, already invented, but helps if you get this idea on your own as well). And then we would play with various data sets and clustering techniques and their parameters in Orange.\nLike in the following workflow where I drew three clusters which were indeed recognized by k-means clustering. Notice that silhouette scoring correctly identified even the number of clusters. And I also drew the clustered data in the Scatterplot to check if the clusters are indeed where they should be.\nOr like in the workflow below where k-means fails miserably (but someother clustering technique would not).\nPaint Data can also be used in supervised setting, for classification tasks. We can set the intended number of classes, and then chose any of these to paint the data. Below I have used it to create the datasets to check the behavior of several classifiers.\nThere are tons of other workflows where Paint Data can be useful. Give it a try!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "One of the widgets I enjoy very much when teaching introductory course in data mining is the Paint Data widget. When painting in this widget I would intentionally include some clusters, or intentionally obscure them. Or draw them in any strange shape. Then I would discuss with students if these clusters are identified by k-means clustering or by hierarchical clustering. We would also discuss automatic scoring of the quality of clusters, come up with the idea of a silhouette (ok, already invented, but helps if you get this idea on your own as well)." ,
	"author" : "BIOLAB",
	"summary" : "One of the widgets I enjoy very much when teaching introductory course in data mining is the Paint Data widget. When painting in this widget I would intentionally include some clusters, or intentionally obscure them. Or draw them in any strange shape. Then I would discuss with students if these clusters are identified by k-means clustering or by hierarchical clustering. We would also discuss automatic scoring of the quality of clusters, come up with the idea of a silhouette (ok, already invented, but helps if you get this idea on your own as well).",
	"date" : "Dec 20, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Paint Your Data",
	"icon" : ""
},
{
    "uri": "/blog/2013/10/09/brief-history-of-orange-praise-to-donald-michie/",
	"title": "Brief History of Orange, Praise to Donald Michie",
	"description": "",
	"content": "Informatica has recently published our paper on the history of Orange. The paper is a post-publication from a Conference on 100 Years of Alan Turing and 20 Years of Slovene AI Society, where Janez Demšar gave a talk on the topics.\nHistory of Orange goes all the way back to 1997, when late Donald Michie had an idea that machine learning needs an open toolbox for machine learning. To spark the development, we co-organized WebLab97 at beautiful Bled, Slovenia. Workshop’s name reflected Michie’s idea that tool should be a web application where people can submit data mining code, procedures, testing scripts, and data and share them in the joint web workspace.\nDonald Michie, a pioneer of Artificial Intelligence, was always ahead of time. (Check out a great talk by Ivan Bratko on their friendship and adventures in chess and machine learning). At WebLab97, Michie was actually very, very ahead of time. But despite the presence of IBM’s Java team that could guide us in developments of the toolbox, the technology was not ripe and initiative of WebLab was gone as the conference ended. But, at least for us, the idea sparked interest of Janez and myself, and development of what is now Orange begun shortly after.\nOur paper gives brief account of Orange’s history and its developments since WebLab97. For reasons of brevity it does not mention that prior to Qt we have experimented with other GUI platforms. Prior to Qt, we laid our hopes to Pwm Python megawidgets, a library that helped us to construct the first Orange graphical user interface. The GUI part of Orange was called Orange*First. Its screenshot shows a tab for interactive discretisation, thanks to Noriaki Aoki who then proposed that this kind of visualisation should be useful in medical data analysis:\nPS Somehow, I have lost a latex file with a WebLab97 program. It should be on some backup tape, somewhere. The following scan of the first page (and a weblab97.pdf), left in some PPT presentation, is all that I can retrieve. The program of the second day is missing, with keynotes from Tom Mitchell, and much talk about then already a success story of R.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Informatica has recently published our paper on the history of Orange. The paper is a post-publication from a Conference on 100 Years of Alan Turing and 20 Years of Slovene AI Society, where Janez Demšar gave a talk on the topics.\nHistory of Orange goes all the way back to 1997, when late Donald Michie had an idea that machine learning needs an open toolbox for machine learning. To spark the development, we co-organized WebLab97 at beautiful Bled, Slovenia." ,
	"author" : "BLAZ",
	"summary" : "Informatica has recently published our paper on the history of Orange. The paper is a post-publication from a Conference on 100 Years of Alan Turing and 20 Years of Slovene AI Society, where Janez Demšar gave a talk on the topics.\nHistory of Orange goes all the way back to 1997, when late Donald Michie had an idea that machine learning needs an open toolbox for machine learning. To spark the development, we co-organized WebLab97 at beautiful Bled, Slovenia.",
	"date" : "Oct 9, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Brief History of Orange, Praise to Donald Michie",
	"icon" : ""
},
{
    "uri": "/blog/history/",
	"title": "history",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 9, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "history",
	"icon" : ""
},
{
    "uri": "/blog/jmlr/",
	"title": "jmlr",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Oct 3, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "jmlr",
	"icon" : ""
},
{
    "uri": "/blog/2013/10/03/jmlr-publishes-article-on-orange/",
	"title": "JMLR Publishes Article on Orange",
	"description": "",
	"content": "Journal of Machine Learning Research has just published our paper on Orange. In the paper we focus on its Python scripting part. We have last reported on Orange scripting at ECML/PKDD 2004. The manuscript was well received (over 270 citations on Google Scholar), but it is now entirely outdated. This was also our only formal publication on Orange scripting. With publication in JMLR this is now a current description of Orange and will be, for a while :-), Orange’s primary reference.\nHere’s a reference:\nDemšar, J., Curk, T., \u0026 Erjavec, A. et al. Orange: Data Mining Toolbox in Python; Journal of Machine Learning Research 14(Aug):2349−2353, 2013.\nand bibtex entry:\n@article{JMLR:demsar13a, author = {Janez Dem\\v{s}ar and Toma\\v{z} Curk and Ale\\v{s} Erjavec and \\v{C}rt Gorup and Toma\\v{z} Ho\\v{c}evar and Mitar Milutinovi\\v{c} and Martin Mo\\v{z}ina and Matija Polajnar andMarko Toplak and An\\v{z}e Stari\\v{c} and Miha \\v{S}tajdohar and Lan Umek and Lan \\v{Z}agar and Jure \\v{Z}bontar and Marinka \\v{Z}itnik and Bla\\v{z} Zupan}, title = {Orange: Data Mining Toolbox in Python}, journal = {Journal of Machine Learning Research}, year = {2013}, volume = {14}, pages = {2349-2353}, url = {http://jmlr.org/papers/v14/demsar13a.html} }  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Journal of Machine Learning Research has just published our paper on Orange. In the paper we focus on its Python scripting part. We have last reported on Orange scripting at ECML/PKDD 2004. The manuscript was well received (over 270 citations on Google Scholar), but it is now entirely outdated. This was also our only formal publication on Orange scripting. With publication in JMLR this is now a current description of Orange and will be, for a while :-), Orange’s primary reference." ,
	"author" : "BLAZ",
	"summary" : "Journal of Machine Learning Research has just published our paper on Orange. In the paper we focus on its Python scripting part. We have last reported on Orange scripting at ECML/PKDD 2004. The manuscript was well received (over 270 citations on Google Scholar), but it is now entirely outdated. This was also our only formal publication on Orange scripting. With publication in JMLR this is now a current description of Orange and will be, for a while :-), Orange’s primary reference.",
	"date" : "Oct 3, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "JMLR Publishes Article on Orange",
	"icon" : ""
},
{
    "uri": "/blog/future/",
	"title": "future",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 2, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "future",
	"icon" : ""
},
{
    "uri": "/blog/2013/09/02/orange-and-axle-project/",
	"title": "Orange and AXLE project",
	"description": "",
	"content": "Our group at University of Ljubljana is a partner in the EU 7FP project Advanced Analytics for Extremely Large European Databases (AXLE). The project is particularly interesting because of the diverse partners that cover the entire vertical, from studying hardware architectures that would better support extremely large databases (University of Manchester, Barcelona Supercomputing Center) to making the necessary adjustments related to speed and security of databases (2ndQuadrant) to data analytics (our group) to handling and analyzing real data and decision making (Portavita).\nAs a result of the project, Orange will be better connected with databases. Currently, all data is stored in working memory, while the forthcoming Orange 3.0 will be able to handle data that is stored in the database. We are working on a parallel computation architecture. Visualization of large data also presents a big challenge: we cannot transfer large amounts of data from the database to the desktop, and on the other hand it is difficult to provide a rich interactive experience if visualizations are created on the server-side. Also, most visualizations are intrinsically unsuitable for large data sets. For instance, the scatter plot represents each data instance with a symbol. Even when the datum is represented with a single pixel, only a few million data points fits on the computer screen. So in the context of big data, we will have to replace scatterplots with heatmaps.\nWhat have we got so far? Orange 3, which is in early stage of development, features a new architecture, which allows the data to be stored either in memory or on a database. In the latter case, selecting a subset of features or filtering the data does not copy the data but only modifies the queries that are used to access the data when needed. Computation of, for instance, distributions or contingency matrices is performed on the server, so only the minimal amount of data is transferred to the client.\nWe also already have a small suite of widgets that work with this new architecture. Just to wet your appetite, here is the new box plot widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Our group at University of Ljubljana is a partner in the EU 7FP project Advanced Analytics for Extremely Large European Databases (AXLE). The project is particularly interesting because of the diverse partners that cover the entire vertical, from studying hardware architectures that would better support extremely large databases (University of Manchester, Barcelona Supercomputing Center) to making the necessary adjustments related to speed and security of databases (2ndQuadrant) to data analytics (our group) to handling and analyzing real data and decision making (Portavita)." ,
	"author" : "BIOLAB",
	"summary" : "Our group at University of Ljubljana is a partner in the EU 7FP project Advanced Analytics for Extremely Large European Databases (AXLE). The project is particularly interesting because of the diverse partners that cover the entire vertical, from studying hardware architectures that would better support extremely large databases (University of Manchester, Barcelona Supercomputing Center) to making the necessary adjustments related to speed and security of databases (2ndQuadrant) to data analytics (our group) to handling and analyzing real data and decision making (Portavita).",
	"date" : "Sep 2, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange and AXLE project",
	"icon" : ""
},
{
    "uri": "/blog/2013/06/03/network-add-on-published-in-jss/",
	"title": "Network Add-on Published in JSS",
	"description": "",
	"content": "NetExplorer, a widget for network exploration, was in orange for over 5 years. Several network analysis widgets were added to Orange since, and we decided to move the entire network functionality to an Orange Network add-on.\nWe recently published a paper Interactive Network Exploration with Orange in the Journal of Statistical Software. We invite you to read the tutorial on network exploration. It is aimed for beginners in this topic, and includes detailed explanation with images.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "NetExplorer, a widget for network exploration, was in orange for over 5 years. Several network analysis widgets were added to Orange since, and we decided to move the entire network functionality to an Orange Network add-on.\nWe recently published a paper Interactive Network Exploration with Orange in the Journal of Statistical Software. We invite you to read the tutorial on network exploration. It is aimed for beginners in this topic, and includes detailed explanation with images." ,
	"author" : "BIOLAB",
	"summary" : "NetExplorer, a widget for network exploration, was in orange for over 5 years. Several network analysis widgets were added to Orange since, and we decided to move the entire network functionality to an Orange Network add-on.\nWe recently published a paper Interactive Network Exploration with Orange in the Journal of Statistical Software. We invite you to read the tutorial on network exploration. It is aimed for beginners in this topic, and includes detailed explanation with images.",
	"date" : "Jun 3, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Network Add-on Published in JSS",
	"icon" : ""
},
{
    "uri": "/blog/2013/05/25/orange-2-7/",
	"title": "Orange 2.7",
	"description": "",
	"content": "Orange 2.7 is out with a major update in the visual programming environment. Redesigned interface, new widgets, welcome screen with workflow browser. Text annotation and arrow lines in workspace. Preloaded workflows with annotations. Widget menu and search can now be activated through key press (open the Settings to make this option available). Extended or minimised widget tab. Improved widget browsing. Enjoy!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange 2.7 is out with a major update in the visual programming environment. Redesigned interface, new widgets, welcome screen with workflow browser. Text annotation and arrow lines in workspace. Preloaded workflows with annotations. Widget menu and search can now be activated through key press (open the Settings to make this option available). Extended or minimised widget tab. Improved widget browsing. Enjoy!" ,
	"author" : "BLAZ",
	"summary" : "Orange 2.7 is out with a major update in the visual programming environment. Redesigned interface, new widgets, welcome screen with workflow browser. Text annotation and arrow lines in workspace. Preloaded workflows with annotations. Widget menu and search can now be activated through key press (open the Settings to make this option available). Extended or minimised widget tab. Improved widget browsing. Enjoy!",
	"date" : "May 25, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange 2.7",
	"icon" : ""
},
{
    "uri": "/blog/2013/03/04/problems-with-orange-website/",
	"title": "Problems With Orange Website",
	"description": "",
	"content": "Our servers crashed on Friday, March 1st due to technical problems. The Orange website was offline for several hours and Mac bundle was unaccessible until today.\nWe are still reviewing if our other services work. If you notice some problems, please ping us.\nStay tuned and fruitful downloading!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Our servers crashed on Friday, March 1st due to technical problems. The Orange website was offline for several hours and Mac bundle was unaccessible until today.\nWe are still reviewing if our other services work. If you notice some problems, please ping us.\nStay tuned and fruitful downloading!" ,
	"author" : "BIOLAB",
	"summary" : "Our servers crashed on Friday, March 1st due to technical problems. The Orange website was offline for several hours and Mac bundle was unaccessible until today.\nWe are still reviewing if our other services work. If you notice some problems, please ping us.\nStay tuned and fruitful downloading!",
	"date" : "Mar 4, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Problems With Orange Website",
	"icon" : ""
},
{
    "uri": "/blog/website/",
	"title": "website",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Mar 4, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "website",
	"icon" : ""
},
{
    "uri": "/blog/2013/02/14/new-canvas/",
	"title": "New canvas",
	"description": "",
	"content": "Orange Canvas, a visual programming environment for Orange, has been around for a while. Integrating new and new features degraded the quality of code to a point where further development proved to be a daunting task. With ever increasing number of widgets, the existing widget toolbar is becoming harder and harder to use, but improving it is really hard. For that reason, we decided Orange needs a new Canvas, a rewrite, that would keep all of the feature of the existing one, but introduce the needed structure and modularity to the source code.\nThe project started about a year ago, and more than 20 thousand lines of code later, we have something to show you. As of yesterday, the new canvas was merged to the main Orange repository, where it lives alongside the old one. At the moment, it still lacks a lot of testing, some features are not completely implemented, but the main functionality, i.e. visual programming with widgets and links, should work.\nIf you are feeling adventurous, you can try it out yourself. Download the latest version from our website and run:\nWindows:\nC:\\Python27\\python.exe -m Orange.OrangeCanvas.main  Mac OS X bundle:\n/Applications/Orange.app/Contents/MacOS/python -m Orange.OrangeCanvas.main  or, regardless of your operating system,\npython -m Orange.OrangeCanvas.main  with the python that has Orange installed.\nWhat to expect?\nNothing will explode, but short of that, anything might happen. If you stumble upon issues or have helpful suggestions, please post them on our issue tracker. There are some known problems we are aware of; you do not need to report those :).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange Canvas, a visual programming environment for Orange, has been around for a while. Integrating new and new features degraded the quality of code to a point where further development proved to be a daunting task. With ever increasing number of widgets, the existing widget toolbar is becoming harder and harder to use, but improving it is really hard. For that reason, we decided Orange needs a new Canvas, a rewrite, that would keep all of the feature of the existing one, but introduce the needed structure and modularity to the source code." ,
	"author" : "BIOLAB",
	"summary" : "Orange Canvas, a visual programming environment for Orange, has been around for a while. Integrating new and new features degraded the quality of code to a point where further development proved to be a daunting task. With ever increasing number of widgets, the existing widget toolbar is becoming harder and harder to use, but improving it is really hard. For that reason, we decided Orange needs a new Canvas, a rewrite, that would keep all of the feature of the existing one, but introduce the needed structure and modularity to the source code.",
	"date" : "Feb 14, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "New canvas",
	"icon" : ""
},
{
    "uri": "/blog/matrixfactorization/",
	"title": "matrixfactorization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 6, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "matrixfactorization",
	"icon" : ""
},
{
    "uri": "/blog/nmf/",
	"title": "nmf",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 6, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "nmf",
	"icon" : ""
},
{
    "uri": "/blog/2013/02/06/orange-nmf-add-on/",
	"title": "Orange NMF add-on",
	"description": "",
	"content": "Nimfa, a Python library for non-negative matrix factorization (NMF), which was part of Orange GSoC program back in 2011 got its own add-on.\nNimfa provides a plethora of initialization and factorization algorithms, quality measures along with examples on real-world and synthetic data sets. However, until now the analysis was possible only through Python scripting. A recent increase of interest in NMF techniques motivated Fajwel Fogel (a PhD student from INRIA, Paris, SIERRA team) to design and implement several widgets that deal with missing data in target matrices, their normalizations, viewing and assessing the quality of matrix factors returned by different matrix factorization algorithms. He also provided an implementation of robust singular value decomposition (rSVD). All NMF methods call Nimfa library.\nAbove is shown a simple scenario in Orange that applies LSNMF algorithm from Nimfa to decompose a non-negative target matrix and visualizes its basis matrix (W) and coefficient matrix (H) as heat maps. NMF finds a parts-based representation of the data due to the fact that only additive, not subtractive, combinations are allowed, which results in improved interpretability of matrix factors. That is possible because non-negativity constraints are imposed in the NMF model in contrast to SVD, PCA and ICA, which provide only holistic representations. The effect can be easily seen if we investigate heat maps produced by the scenario above. Below are shown the target, basis and coefficient matrices (from left to right, top down), respectively.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Nimfa, a Python library for non-negative matrix factorization (NMF), which was part of Orange GSoC program back in 2011 got its own add-on.\nNimfa provides a plethora of initialization and factorization algorithms, quality measures along with examples on real-world and synthetic data sets. However, until now the analysis was possible only through Python scripting. A recent increase of interest in NMF techniques motivated Fajwel Fogel (a PhD student from INRIA, Paris, SIERRA team) to design and implement several widgets that deal with missing data in target matrices, their normalizations, viewing and assessing the quality of matrix factors returned by different matrix factorization algorithms." ,
	"author" : "BIOLAB",
	"summary" : "Nimfa, a Python library for non-negative matrix factorization (NMF), which was part of Orange GSoC program back in 2011 got its own add-on.\nNimfa provides a plethora of initialization and factorization algorithms, quality measures along with examples on real-world and synthetic data sets. However, until now the analysis was possible only through Python scripting. A recent increase of interest in NMF techniques motivated Fajwel Fogel (a PhD student from INRIA, Paris, SIERRA team) to design and implement several widgets that deal with missing data in target matrices, their normalizations, viewing and assessing the quality of matrix factors returned by different matrix factorization algorithms.",
	"date" : "Feb 6, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange NMF add-on",
	"icon" : ""
},
{
    "uri": "/blog/2013/01/29/writing-orange-add-ons/",
	"title": "Writing Orange Add-ons",
	"description": "",
	"content": "We officially supported add-ons in Orange 2.6. You should start by checking the list of available add-ons. We pull those automatically from the PyPi, which is our preferred distribution channel. Try to install an add-on by either:\n writing “pip install ” in the terminal or from the Orange Canvas GUI. Select “Options / Add-ons…” in the menu.  Everything should just work. Writing add-ons is as easy as writing your own Orange Widgets or Orange Scripts. Just follow this tutorial and you will have your brand-new Orange add-on on PyPi in no time (an hour at most).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We officially supported add-ons in Orange 2.6. You should start by checking the list of available add-ons. We pull those automatically from the PyPi, which is our preferred distribution channel. Try to install an add-on by either:\n writing \u0026ldquo;pip install \u0026rdquo; in the terminal or from the Orange Canvas GUI. Select \u0026ldquo;Options / Add-ons\u0026hellip;\u0026rdquo; in the menu.  Everything should just work. Writing add-ons is as easy as writing your own Orange Widgets or Orange Scripts." ,
	"author" : "BIOLAB",
	"summary" : "We officially supported add-ons in Orange 2.6. You should start by checking the list of available add-ons. We pull those automatically from the PyPi, which is our preferred distribution channel. Try to install an add-on by either:\n writing \u0026ldquo;pip install \u0026rdquo; in the terminal or from the Orange Canvas GUI. Select \u0026ldquo;Options / Add-ons\u0026hellip;\u0026rdquo; in the menu.  Everything should just work. Writing add-ons is as easy as writing your own Orange Widgets or Orange Scripts.",
	"date" : "Jan 29, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Writing Orange Add-ons",
	"icon" : ""
},
{
    "uri": "/blog/2013/01/21/orange-2-6/",
	"title": "Orange 2.6",
	"description": "",
	"content": "A new version of Orange, 2.6, has been uploaded to Python Package Index. Since the version on the Orange website is always up to date (we post daily builds), this may not affect you. Nevertheless, let us explain what we were working on for the last year.\nThe most important improvement to Orange is an implementation of add-on framework that is much more “standard pythonic”. As a consequence, the add-on installation procedure has been simplified for both individual users and system administrators. For developers, the new framework eases the development and distribution of add-ons. This enabled us to make first steps towards the goal of removing the rarely used parts of Orange from the core distribution, which will ultimately result in less external dependencies and less warnings on module import. Orange 2.6 lacks the modules for network analysis (Orange.network) and prediction reliability assesment (Orange.reliability), but fear not: you can get them back by installing the Orange-Network and Orange-Reliability add-ons.\nApart from that, we have been mostly squashing bugs. A fun spare time activity - you can join us anytime by cloning our repository and sending us a pull request. :)\nIf our version numbering system confuses you, let us try to explain. For the last (couple of) year(s), our version numbers have been a mess. Orange2.5a4 was uploaded to pypi almost a year ago, and was followed by a 2.6a2 release that was never available outisde our repository/daily builds. From this day forth, our versioning system should be as follows.\n If you install orange from pypi, the version (Orange.version.full_version) will be something like 2.6 or 2.6.1. If you use our daily builds or build orange yourself from the source available in our repository, your version will be 2.6.1.dev-8804fbc. (minor will be larger by one and .dev- suffix will show the source control revision that was used for the build)  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "A new version of Orange, 2.6, has been uploaded to Python Package Index. Since the version on the Orange website is always up to date (we post daily builds), this may not affect you. Nevertheless, let us explain what we were working on for the last year.\nThe most important improvement to Orange is an implementation of add-on framework that is much more \u0026ldquo;standard pythonic\u0026rdquo;. As a consequence, the add-on installation procedure has been simplified for both individual users and system administrators." ,
	"author" : "BIOLAB",
	"summary" : "A new version of Orange, 2.6, has been uploaded to Python Package Index. Since the version on the Orange website is always up to date (we post daily builds), this may not affect you. Nevertheless, let us explain what we were working on for the last year.\nThe most important improvement to Orange is an implementation of add-on framework that is much more \u0026ldquo;standard pythonic\u0026rdquo;. As a consequence, the add-on installation procedure has been simplified for both individual users and system administrators.",
	"date" : "Jan 21, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange 2.6",
	"icon" : ""
},
{
    "uri": "/blog/2013/01/06/new-scripting-tutorial/",
	"title": "New scripting tutorial",
	"description": "",
	"content": "Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:\nclass SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name='small', m=5): self.name = name self.m = m self.base_learner = base_learner def __call__(self, data, weight=None): gain = Orange.feature.scoring.InfoGain() m = min(self.m, len(data.domain.features)) best = [f for _, f in sorted((gain(x, data), x) \\ for x in data.domain.features)[-m:]] domain = Orange.data.Domain(best + [data.domain.class_var]) model = self.base_learner(Orange.data.Table(domain, data), weight) return Orange.classification.PyClassifier(classifier=model, name=self.name)  The tutorial was first written for Python 2.3. Since, Python and Orange have changed a lot. And so did I. Most of the for loops have become one-liners, list and dictionary comprehension have become a must, and many new and great libraries have emerged. The (boring) tutorial code that used to read\n c = [0] * len(data.domain.classVar.values) for e in data: c[int(e.getclass())] += 1 print \"Instances: \", len(data), \"total\", r = [0.] * len(c) for i in range(len(c)): r[i] = c[i] * 100. / len(data) for i in range(len(data.domain.classVar.values)): print \", %d(%4.1f%s) with class %s\" % (c[i], r[i], '%', data.domain.classVar.values[i]), print  is now replaced with\nprint Counter(str(d.get_class()) for d in data)  Ok. Pretty print is missing, but that, if not in the same line, could be done in another one.\nFor now, the tutorial focuses on data input and output, classification and regression. We plan to use other sections, but you can also give us a hint if there are any you would wish to be included.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:\nclass SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name='small', m=5): self." ,
	"author" : "BLAZ",
	"summary" : "Orange just got a new, completely rewritten scripting tutorial. The tutorial uses Orange class hierarchy as introduced for version 2.5. The tutorial is supposed to be a gentle introduction in Orange scripting. It includes many examples, from really simple ones to those more complex. To give you a hint about the later, here is the code for learner with feature subset selection from:\nclass SmallLearner(Orange.classification.PyLearner): def __init__(self, base_learner=Orange.classification.bayes.NaiveLearner, name='small', m=5): self.",
	"date" : "Jan 6, 2013",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "New scripting tutorial",
	"icon" : ""
},
{
    "uri": "/blog/orange25/",
	"title": "orange25",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Nov 30, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "orange25",
	"icon" : ""
},
{
    "uri": "/blog/2012/11/30/the-easy-way-to-install-add-ons/",
	"title": "The easy way to install add-ons",
	"description": "",
	"content": "The possibility of extending functionality of Orange through add-ons has been present for a long time. In fact, we never provided the toolbox for crunching bioinformatical data as an integral part of Orange; it has always been an add-on. The exact mechanism of distribution of add-ons has changed significantly in the last year to simplify the process for add-on authors and to make it more standards-compliant. Among other things, this enables system administrators to install add-ons system-wide directly from PyPi using easy_install or pip. Unfortunately there were also negative side effects of this process, notably the temporary breakage of the add-on management dialog within the Orange Canvas.\nWe are happy to report that this is now being taken care of and you are encouraged to test the functionality.\nSelect “Add-ons…” in the Options menu. A dialog will open that will list and describe existing add-ons. You can see the same list on the appropriate part of Orange website, but there is more. In the dialog, you can simply pick the add-ons you wish to use, confirm the selection and you should be good to go: widgets that come with the selected add-ons will become available immediately.\nIn case you change your mind, on some systems you can also uninstall add-ons by removing the check marks in front of them. This only works if you have pip installed, which is uncommon on Windows systems.\nThis might be a good time to warn you that the described functionality is new and not thoroughly tested on all the platforms on which Orange runs. If you stumble upon any strange or unwanted behavior, please let us now on the Orange forum, preferrably in the Bugs section.\nNote that the Orange-Text add-on requires a compiler and appropriate libraries on your computer, and it as of now still refuses to be installed using the dialog. This is a known bug.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The possibility of extending functionality of Orange through add-ons has been present for a long time. In fact, we never provided the toolbox for crunching bioinformatical data as an integral part of Orange; it has always been an add-on. The exact mechanism of distribution of add-ons has changed significantly in the last year to simplify the process for add-on authors and to make it more standards-compliant. Among other things, this enables system administrators to install add-ons system-wide directly from PyPi using easy_install or pip." ,
	"author" : "BIOLAB",
	"summary" : "The possibility of extending functionality of Orange through add-ons has been present for a long time. In fact, we never provided the toolbox for crunching bioinformatical data as an integral part of Orange; it has always been an add-on. The exact mechanism of distribution of add-ons has changed significantly in the last year to simplify the process for add-on authors and to make it more standards-compliant. Among other things, this enables system administrators to install add-ons system-wide directly from PyPi using easy_install or pip.",
	"date" : "Nov 30, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "The easy way to install add-ons",
	"icon" : ""
},
{
    "uri": "/blog/2012/11/27/coming-soon-oranges-new-interface/",
	"title": "Coming soon: Orange&amp;#39;s new interface",
	"description": "",
	"content": "Orange will soon get entirely new interface. The GUI will feature new canvas and icons and new presentation of data flow. Orange will be upgraded with on-line help for widgets and tutorials. The prototype is now in testing and should replace the current version of Orange in early 2013.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange will soon get entirely new interface. The GUI will feature new canvas and icons and new presentation of data flow. Orange will be upgraded with on-line help for widgets and tutorials. The prototype is now in testing and should replace the current version of Orange in early 2013." ,
	"author" : "BLAZ",
	"summary" : "Orange will soon get entirely new interface. The GUI will feature new canvas and icons and new presentation of data flow. Orange will be upgraded with on-line help for widgets and tutorials. The prototype is now in testing and should replace the current version of Orange in early 2013.",
	"date" : "Nov 27, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Coming soon: Orange&#39;s new interface",
	"icon" : ""
},
{
    "uri": "/blog/2012/10/23/short-history-of-orange/",
	"title": "Short history of Orange",
	"description": "",
	"content": "Few weeks back we celebrated 20 years of Slovene Artificial Intelligence Society. I have much enjoyed Ivan Bratko’s talk on AI history, and his account of events as triggered by late Donald Michie. Many interesting talks followed, including highlights by Stephen Muggleton and Claude Sammut.\nThe last talk of the event was on Orange. Janez talked about its birth, history and future prospects. You can see his presentation on videolectures and check out the paper with lecture’s notes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Few weeks back we celebrated 20 years of Slovene Artificial Intelligence Society. I have much enjoyed Ivan Bratko\u0026rsquo;s talk on AI history, and his account of events as triggered by late Donald Michie. Many interesting talks followed, including highlights by Stephen Muggleton and Claude Sammut.\nThe last talk of the event was on Orange. Janez talked about its birth, history and future prospects. You can see his presentation on videolectures and check out the paper with lecture\u0026rsquo;s notes." ,
	"author" : "BLAZ",
	"summary" : "Few weeks back we celebrated 20 years of Slovene Artificial Intelligence Society. I have much enjoyed Ivan Bratko\u0026rsquo;s talk on AI history, and his account of events as triggered by late Donald Michie. Many interesting talks followed, including highlights by Stephen Muggleton and Claude Sammut.\nThe last talk of the event was on Orange. Janez talked about its birth, history and future prospects. You can see his presentation on videolectures and check out the paper with lecture\u0026rsquo;s notes.",
	"date" : "Oct 23, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Short history of Orange",
	"icon" : ""
},
{
    "uri": "/blog/2012/06/15/computing-joint-entropy-in-python/",
	"title": "Computing joint entropy (in Python)",
	"description": "",
	"content": "How I wrote a beautiful, general, and super fast joint entropy method (in Python).\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u003e 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).\nimport numpy as np def entropy(X): probs = [np.mean(X == c) for c in set(X)] return np.sum(-p * np.log2(p) for p in probs)  In my next version I extended it to compute the joint entropy of two variables:\ndef entropy(X, Y): probs = [] for c1 in set(X): for c2 in set(Y): probs.append(np.mean(np.logical_and(X == c1, Y == c2))) return np.sum(-p * np.log2(p) for p in probs)  Now wait a minute, it looks like we have a recursion here. I couldn’t stop myself of writing en extended general function to compute the joint entropy of n variables.\ndef entropy(*X, **kwargs): predictions = parse_arg(X[0]) H = kwargs[\"H\"] if \"H\" in kwargs else 0 v = kwargs[\"v\"] if \"v\" in kwargs else np.array([True] * len(predictions)) for c in set(predictions): if len(X) \u003e 1: H = entropy(*X[1:], v=np.logical_and(v, predictions == c), H=H) else: p = np.mean(np.logical_and(v, predictions == c)) H += -p * np.log2(p) if p \u003e 0 else 0 return H  It was the ugliest recursive function I’ve ever written. I couldn’t stop coding, I was hooked. Besides, this method was slow as hell and I need a faster version for my reasearch. I need my data tommorow, not next month. I googled if Python has something that would help me deal with the recursive part. I fould this great method: itertools.product, I’s just what we need. It takes lists and returns a cartesian product of their values. It’s the “nested for loops” in one function.\ndef entropy(*X): n_insctances = len(X[0]) H = 0 for classes in itertools.product(*[set(x) for x in X]): v = np.array([True] * n_insctances) for predictions, c in zip(X, classes): v = np.logical_and(v, predictions == c) p = np.mean(v) H += -p * np.log2(p) if p \u003e 0 else 0 return H  No resursion, but still slow. It’s time to rewrite loops to the Python-like style. As a sharp eye has already noticed, the second for loop with the np.logical_and inside is perfect for the reduce method.\ndef entropy(*X): n_insctances = len(X[0]) H = 0 for classes in itertools.product(*[set(x) for x in X]): v = reduce(np.logical_and, (predictions, c for predictions, c in zip(X, classes))) p = np.mean(v) H += -p * np.log2(p) if p \u003e 0 else 0 return H  Now, we have to remove just one more list comprehension and we have a beautiful, general, and super fast joint etropy method.\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u003e 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "How I wrote a beautiful, general, and super fast joint entropy method (in Python).\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u0026gt; 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings)." ,
	"author" : "BIOLAB",
	"summary" : "How I wrote a beautiful, general, and super fast joint entropy method (in Python).\ndef entropy(*X): return = np.sum(-p * np.log2(p) if p \u0026gt; 0 else 0 for p in (np.mean(reduce(np.logical_and, (predictions == c for predictions, c in zip(X, classes)))) for classes in itertools.product(*[set(x) for x in X])))  I started with the method to compute the entropy of a single variable. Input is a numpy array with discrete values (either integers or strings).",
	"date" : "Jun 15, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Computing joint entropy (in Python)",
	"icon" : ""
},
{
    "uri": "/blog/2012/05/19/kdnuggets-is-asking-if-you-have-been-using-orange-lately/",
	"title": "KDnuggets is asking if you have been using Orange lately",
	"description": "",
	"content": "KDnuggets, one of leading data mining community websites, is having its yearly poll asking its visitors which analytics/data mining software they used in the past 12 months. Among listed is also Orange, our fruity visually pleasing open source pythonic data mining suite. So we are asking you, have you been using Orange lately, that is, in the past 12 months? How do you feel about telling that to the world?\nIf so, we would also like to hear more about how you are using Orange in your projects, research, competitions, or data mining play. We would be glad to publish your story on our blog, or link to your blog post. Feel free to contact us if you are interested.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "KDnuggets, one of leading data mining community websites, is having its yearly poll asking its visitors which analytics/data mining software they used in the past 12 months. Among listed is also Orange, our fruity visually pleasing open source pythonic data mining suite. So we are asking you, have you been using Orange lately, that is, in the past 12 months? How do you feel about telling that to the world?" ,
	"author" : "BIOLAB",
	"summary" : "KDnuggets, one of leading data mining community websites, is having its yearly poll asking its visitors which analytics/data mining software they used in the past 12 months. Among listed is also Orange, our fruity visually pleasing open source pythonic data mining suite. So we are asking you, have you been using Orange lately, that is, in the past 12 months? How do you feel about telling that to the world?",
	"date" : "May 19, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "KDnuggets is asking if you have been using Orange lately",
	"icon" : ""
},
{
    "uri": "/blog/2012/05/15/orange-gsoc-computer-vision-add-on-for-orange/",
	"title": "Orange GSoC: Computer vision add-on for Orange",
	"description": "",
	"content": "This summer I got the chance to develop an add-on for Orange that will introduce basic computer vision functionality, as a part of Google Summer of Code.\nThe add-on will consist of a set of widgets, each with it’s own dedicated purpose, which can be seamlessly connected to provide most commonly used image preprocessing functionality.\nHere is a list of the widgets:\n Widget for viewing image files (add description) Widget for resizing an image Widget for rotation/flipping of the image Widget for converting the color mode (RGB, HSV, Grayscale etc.) Widget for changing the hue/saturation, brightness/contrast and inverting the image Widget for generic transformations through convolution with a matrix  Also, if there is enough time left throughout the GSoC period, a face detection widget will be built in order to demonstrate the power of the underlying libraries.\nThese are all things that have been implemented in Python before. Reimplementing them is of course a rather bad idea, so I will use an library called OpenCV. It is written in C++ and has Python bindings, and is the most widely used computer vision library, by far. So the core of the widgets will be written in it, and the GUI using PyQT, the library used for building the Orange Canvas.\nAlthough working with images is not Oranges’ main thing, the knowledge gathered while developing the add-on will be used to improve in a number of ways: finding a general structure for add-ons developed in the future, improving the way they are distributed and the way they are tested.\nFinally, I want to thank the Orange core team for having faith in me and giving me the chance to spend the summer working on an idea I care about. I’m very grateful for that and I hope I’ll exceed their expectations.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This summer I got the chance to develop an add-on for Orange that will introduce basic computer vision functionality, as a part of Google Summer of Code.\nThe add-on will consist of a set of widgets, each with it\u0026rsquo;s own dedicated purpose, which can be seamlessly connected to provide most commonly used image preprocessing functionality.\nHere is a list of the widgets:\n Widget for viewing image files (add description) Widget for resizing an image Widget for rotation/flipping of the image Widget for converting the color mode (RGB, HSV, Grayscale etc." ,
	"author" : "BIOLAB",
	"summary" : "This summer I got the chance to develop an add-on for Orange that will introduce basic computer vision functionality, as a part of Google Summer of Code.\nThe add-on will consist of a set of widgets, each with it\u0026rsquo;s own dedicated purpose, which can be seamlessly connected to provide most commonly used image preprocessing functionality.\nHere is a list of the widgets:\n Widget for viewing image files (add description) Widget for resizing an image Widget for rotation/flipping of the image Widget for converting the color mode (RGB, HSV, Grayscale etc.",
	"date" : "May 15, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange GSoC: Computer vision add-on for Orange",
	"icon" : ""
},
{
    "uri": "/blog/2012/05/06/orange-gsoc-a-fully-featured-neural-network-library-implementation-with-extension-for-deep-learning/",
	"title": "Orange GSoC: A Fully-Featured Neural Network Library Implementation with Extension for Deep Learning",
	"description": "",
	"content": "This project aims to build a neural network library based on some great existing NN libraries, notably the Flood Library, which already provides a fully functional Multilayer Perceptron (MLP) implementation. The project starts with implementing a robust, efficient feed forward neural network library, and then will extend it in significant ways that add support for state-of-the-art deep learning techniques. Additional extensions include building a PCA framework and improving existing training algorithms and error functional.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This project aims to build a neural network library based on some great existing NN libraries, notably the Flood Library, which already provides a fully functional Multilayer Perceptron (MLP) implementation. The project starts with implementing a robust, efficient feed forward neural network library, and then will extend it in significant ways that add support for state-of-the-art deep learning techniques. Additional extensions include building a PCA framework and improving existing training algorithms and error functional." ,
	"author" : "BIOLAB",
	"summary" : "This project aims to build a neural network library based on some great existing NN libraries, notably the Flood Library, which already provides a fully functional Multilayer Perceptron (MLP) implementation. The project starts with implementing a robust, efficient feed forward neural network library, and then will extend it in significant ways that add support for state-of-the-art deep learning techniques. Additional extensions include building a PCA framework and improving existing training algorithms and error functional.",
	"date" : "May 6, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange GSoC: A Fully-Featured Neural Network Library Implementation with Extension for Deep Learning",
	"icon" : ""
},
{
    "uri": "/blog/multitarget/",
	"title": "multitarget",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 30, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "multitarget",
	"icon" : ""
},
{
    "uri": "/blog/2012/04/30/orange-gsoc-multi-target-learning-for-orange/",
	"title": "Orange GSoC: Multi-Target Learning for Orange",
	"description": "",
	"content": "Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year’s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange’s SimpleTreeLearner and will be integrated with Orange 3.0.\nOnce the clustering trees are implemented and integrated, documentation and unit tests will be written. Additionally I intend to make an experimental study that will compare the effectiveness of clustering trees with established multi-target classifiers (like PLS and chain classifiers) on benchmark data-sets. I will also work on some additional tasks related to multi-target classification that I had not included in my original proposal but Orange’s team thinks would be useful to include. Among these is a chain classifier framework that Orange is currently missing.\nIf any reader is interested in learning more about clustering trees or chain classifiers these articles should cover the basics:\n Top-Down Induction of Clustering Trees (1998), by Hendrik Blockeel, Luc De Raedt, Jan Ramong Classiﬁer Chains for Multi-label Classiﬁcation (2009), by Jesse Read, Bernhard Pfahringer, Geoﬀ Holmes, Eibe Frank  I am a third year undergraduate student at the Faculty of Computer and Information Science in Ljubljana and my project will be mentored by prof. dr. Blaž Zupan. I thank him and the rest of the Orange team for advice and support.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year\u0026rsquo;s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange\u0026rsquo;s SimpleTreeLearner and will be integrated with Orange 3." ,
	"author" : "BIOLAB",
	"summary" : "Orange already supports multi-target classification, but the current implementation of clustering trees is written in Python. One of the five projects Orange has chosen at this year\u0026rsquo;s Google Summer of Code is the implementation of clustering trees in C. The goal of my project is to speed up the building time of clustering trees and lower their spatial complexity, especially when used in random forests. Implementation will be based on Orange\u0026rsquo;s SimpleTreeLearner and will be integrated with Orange 3.",
	"date" : "Apr 30, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange GSoC: Multi-Target Learning for Orange",
	"icon" : ""
},
{
    "uri": "/blog/competition/",
	"title": "competition",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 25, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "competition",
	"icon" : ""
},
{
    "uri": "/blog/2012/04/25/orange-team-wins-jrs-2012-data-mining-competition/",
	"title": "Orange team wins JRS 2012 Data Mining Competition",
	"description": "",
	"content": "Lead by Jure Žbontar, the team from University of Ljubljana wins over 126 other entrants in an international competition in predictive data analytics.\nJure’s team consisted of several Orange developers and computer science students: Miha Zidar, Blaž Zupan, Gregor Majcen, Marinka Žitnik in Matic Potočnik. To win, the team had to predict topics for 10.000 MedLine documents that were represented with over 25.000 algorithmically derived numerical features. Given was training set of another 10.000 documents in the same representation but each labeled with a set of topics. From the training set the task was to develop a model to predict labels for documents in the test set. A particular challenge was guessing the right number of topics to be associated with the documents, as these, at least in the training set, varied from one to a dozen.\nJRS 2012 is just one in a series of competitions recently organized on servers such as TunedIT and Kaggle. The price for winning was $1000 and a trip to Joint Rough Set Symposium in Chengdu, China, to present a winning strategy and developed data mining techiques.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Lead by Jure Žbontar, the team from University of Ljubljana wins over 126 other entrants in an international competition in predictive data analytics.\nJure’s team consisted of several Orange developers and computer science students: Miha Zidar, Blaž Zupan, Gregor Majcen, Marinka Žitnik in Matic Potočnik. To win, the team had to predict topics for 10.000 MedLine documents that were represented with over 25.000 algorithmically derived numerical features. Given was training set of another 10." ,
	"author" : "BLAZ",
	"summary" : "Lead by Jure Žbontar, the team from University of Ljubljana wins over 126 other entrants in an international competition in predictive data analytics.\nJure’s team consisted of several Orange developers and computer science students: Miha Zidar, Blaž Zupan, Gregor Majcen, Marinka Žitnik in Matic Potočnik. To win, the team had to predict topics for 10.000 MedLine documents that were represented with over 25.000 algorithmically derived numerical features. Given was training set of another 10.",
	"date" : "Apr 25, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange team wins JRS 2012 Data Mining Competition",
	"icon" : ""
},
{
    "uri": "/blog/2012/04/24/this-year-five-students-participate-in-google-summer-of-code/",
	"title": "This year five students participate in Google Summer of Code",
	"description": "",
	"content": "This year five students have been accepted to participate in Google Summer of Code and contribute to Orange in their summer time. Congratulations!\n Amela – Widgets for statistics Andrej T. – Computer vision add-on for Orange CoderWilliam – A Fully-Featured Neural Network Library Implementation Based On the Flood Library with Extension for Deep Learning Makarov Dmitry – Text mining add-on for Orange Miran Levar – Multi-Target Learning for Orange  Overall, 1,212 students have been accepted this year to various open source organizations from all around the world.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This year five students have been accepted to participate in Google Summer of Code and contribute to Orange in their summer time. Congratulations!\n Amela – Widgets for statistics Andrej T. – Computer vision add-on for Orange CoderWilliam – A Fully-Featured Neural Network Library Implementation Based On the Flood Library with Extension for Deep Learning Makarov Dmitry – Text mining add-on for Orange Miran Levar – Multi-Target Learning for Orange  Overall, 1,212 students have been accepted this year to various open source organizations from all around the world." ,
	"author" : "BIOLAB",
	"summary" : "This year five students have been accepted to participate in Google Summer of Code and contribute to Orange in their summer time. Congratulations!\n Amela – Widgets for statistics Andrej T. – Computer vision add-on for Orange CoderWilliam – A Fully-Featured Neural Network Library Implementation Based On the Flood Library with Extension for Deep Learning Makarov Dmitry – Text mining add-on for Orange Miran Levar – Multi-Target Learning for Orange  Overall, 1,212 students have been accepted this year to various open source organizations from all around the world.",
	"date" : "Apr 24, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "This year five students participate in Google Summer of Code",
	"icon" : ""
},
{
    "uri": "/blog/icons/",
	"title": "icons",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Apr 9, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "icons",
	"icon" : ""
},
{
    "uri": "/blog/2012/04/09/redesign-of-gui-icons/",
	"title": "Redesign of GUI icons",
	"description": "",
	"content": "Orange GUI is being redesigned. Expect a welcome screen with selection of preloaded widget schemes, simpler access to computational components, and integration with intelligent interface (widget suggestions). For the project we have engaged a designer Peter Čuhalev. To give you a taste of what is going on, here are some icons for widget sets that are being redesigned. There are in B/W, the color will be decided on and added in later stages. Below are just the icons - widget symbols with no frames. Current frames are rounded squares, while it looks like the widget frames for the new GUI will be circles. New icons are designed in a vector format.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange GUI is being redesigned. Expect a welcome screen with selection of preloaded widget schemes, simpler access to computational components, and integration with intelligent interface (widget suggestions). For the project we have engaged a designer Peter Čuhalev. To give you a taste of what is going on, here are some icons for widget sets that are being redesigned. There are in B/W, the color will be decided on and added in later stages." ,
	"author" : "BLAZ",
	"summary" : "Orange GUI is being redesigned. Expect a welcome screen with selection of preloaded widget schemes, simpler access to computational components, and integration with intelligent interface (widget suggestions). For the project we have engaged a designer Peter Čuhalev. To give you a taste of what is going on, here are some icons for widget sets that are being redesigned. There are in B/W, the color will be decided on and added in later stages.",
	"date" : "Apr 9, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Redesign of GUI icons",
	"icon" : ""
},
{
    "uri": "/blog/2012/03/27/orange-is-again-participating-in-gsoc/",
	"title": "Orange is again participating in GSoC",
	"description": "",
	"content": "This year Orange is again participating in Google Summer of Code as a mentoring organization. Student proposal submission period is running and the deadline is on 6th April. We have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year but feel free to suggest your own ideas how you could contribute to Orange and make it even better.\nGoogle Summer of Code is a Google-sponsored program where Google stipends students working for a summer job on an open source projects from all around the world. Student is paid $5000 (and a t-shirt!) for approximately two months of work/contribution to the project. More about the program is available on its homepage.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This year Orange is again participating in Google Summer of Code as a mentoring organization. Student proposal submission period is running and the deadline is on 6th April. We have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year but feel free to suggest your own ideas how you could contribute to Orange and make it even better." ,
	"author" : "BIOLAB",
	"summary" : "This year Orange is again participating in Google Summer of Code as a mentoring organization. Student proposal submission period is running and the deadline is on 6th April. We have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year but feel free to suggest your own ideas how you could contribute to Orange and make it even better.",
	"date" : "Mar 27, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange is again participating in GSoC",
	"icon" : ""
},
{
    "uri": "/blog/2012/02/05/random-decisions-behind-your-back/",
	"title": "Random decisions behind your back",
	"description": "",
	"content": "When Orange builds a decision tree, candidate attributes are evaluated and the best candidate is chosen. But what if two or more share the first place? Most machine learning systems don’t care about it and always take the first, which is unfair and, besides, has strange effects: the induced model and, consequentially, its accuracy depends upon the order of attributes. Which shouldn’t be.\nThis is not an isolated problem. Another instance is when a classifier has to choose between two equally probable classes when there is no additional information (such as classification costs) to help make the prediction. Or selecting random reference examples when computing ReliefF. Returning a modus of a distribution with two or more competing values…\nThe old solution was to make a random selection in such cases. Take a random class (out of the most probable, of course), random attribute, random examples… Although theoretically correct, it comes with a price: the only way to ensure repeatability of experiments is by setting the global random generator, which is not a good practice in component-based systems.\nWhat Orange does now is more cunning. When, for instance, choosing between n equally probable classes, Orange computes something like a hash value from the example to be classified. Its remainder at division by n is then used to select the class. Thus, the class will be random, but always the same for same example.\nA similar trick is used elsewhere. To choose an attribute when building a tree, it simply divides the number of learning examples at that node by the number of candidate attributes and the remainder is used again.\nWhen more random numbers are needed, for instance for selecting m random reference examples for computing ReliefF, the number of examples is used for a random seed for a temporary random generator.\nTo conclude: Orange will sometimes make decisions that will look random. The reason for this is that it is more fair than most of machine learning systems that pick the first (or the last) candidate. But whatever decision is taken, it will be the same if you run the program twice. The message is thus: be aware that this is happenning, but don’t care about it.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "When Orange builds a decision tree, candidate attributes are evaluated and the best candidate is chosen. But what if two or more share the first place? Most machine learning systems don\u0026rsquo;t care about it and always take the first, which is unfair and, besides, has strange effects: the induced model and, consequentially, its accuracy depends upon the order of attributes. Which shouldn\u0026rsquo;t be.\nThis is not an isolated problem. Another instance is when a classifier has to choose between two equally probable classes when there is no additional information (such as classification costs) to help make the prediction." ,
	"author" : "BIOLAB",
	"summary" : "When Orange builds a decision tree, candidate attributes are evaluated and the best candidate is chosen. But what if two or more share the first place? Most machine learning systems don\u0026rsquo;t care about it and always take the first, which is unfair and, besides, has strange effects: the induced model and, consequentially, its accuracy depends upon the order of attributes. Which shouldn\u0026rsquo;t be.\nThis is not an isolated problem. Another instance is when a classifier has to choose between two equally probable classes when there is no additional information (such as classification costs) to help make the prediction.",
	"date" : "Feb 5, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Random decisions behind your back",
	"icon" : ""
},
{
    "uri": "/blog/2012/02/02/new-in-orange-partial-least-squares-regression/",
	"title": "New in Orange: Partial least squares regression",
	"description": "",
	"content": "Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!\nYou can use PLS regression model on single-target or multi-target data sets. Simply load the data set multitarget-synthetic.tab and see that it contains three predictor variables and four responses using this code.\ndata = Orange.data.Table(\"multitarget-synthetic.tab\") print \"Input variables:\" print data.domain.features print \"Response variables:\" print data.domain.class_vars  Output:\nInput variables: \u003cFloatVariable 'X1', FloatVariable 'X2', FloatVariable 'X3'\u003e Response variables: \u003cFloatVariable 'Y1', FloatVariable 'Y2', FloatVariable 'Y3', FloatVariable 'Y4'\u003e  As you can see, all variables in this data set are continuous. The PLS regression is intended forsuch situations although it can be used for discrete input variables as well (using 0-1 continuation). Currently, discrete response variables are not yet supported.\nLet’s try to fit the PLS regression model on our data set.\nlearner = Orange.multitarget.pls.PLSRegressionLearner() classifier = learner(data)\nThe classifier can be now used to predict values of the four responses based onthree predictors. Let’s see how it manages this task on the first data instance.\nactual = data[0].get_classes() predicted = classifier(data[0]) print \"Actual\", \"Predicted\" for a, p in zip(actual, predicted): print \"%6.3f %6.3f\" % (a,p)  Output:\nActual Predicted 0.490 0.613 1.237 0.826 1.808 1.084 0.422 0.534  To test the usefulness of PLS as a multi-target method let’s compare it to a single-target method - linear regression. We did this by comparing Root mean squared error (RMSE) of predicted values for a single response variable. We constructed synthetic data sets and performed the RMSE analysis using this script. The results can be seen in the following output:\n Training set sizes 5 10 20 50 100 200 500 1000 Linear (single-target) 0.5769 0.3128 0.2703 0.2529 0.2493 0.2446 0.2436 0.2442 PLS (multi-target) 0.3663 0.2955 0.2623 0.2517 0.2487 0.2447 0.2441 0.2448  We can see that PLS regression outperforms linear regression when the number of training instances is low. Such situations (low number of instances compared to high number of variables) are quite common when analyzing data sets in bioinformatics. However, with increasing number of training instances, the advantages of PLS regression diminish.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!\nYou can use PLS regression model on single-target or multi-target data sets." ,
	"author" : "BIOLAB",
	"summary" : "Partial least squares regression is a regression technique which supports multiple response variables. PLS regression is very popular in areas such as bioinformatics, chemometrics etc. where the number of observations is usually less than the number of measured variables and where there exists multicollinearity among the predictor variables. In such situations, standard regression techniques would usually fail. The PLS regression is now available in Orange (see documentation)!\nYou can use PLS regression model on single-target or multi-target data sets.",
	"date" : "Feb 2, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "New in Orange: Partial least squares regression",
	"icon" : ""
},
{
    "uri": "/blog/pls/",
	"title": "pls",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 2, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "pls",
	"icon" : ""
},
{
    "uri": "/blog/2012/01/23/orange-2-5a2-available/",
	"title": "Orange 2.5a2 available",
	"description": "",
	"content": "Orange 2.5a2 has been uploaded to PyPI. It now includes basic support for multi-label classification (developed during the Google Summer of Code 2011), some new widget icons and documentation for basket format. Release is also tagged on our Bitbucket repository.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange 2.5a2 has been uploaded to PyPI. It now includes basic support for multi-label classification (developed during the Google Summer of Code 2011), some new widget icons and documentation for basket format. Release is also tagged on our Bitbucket repository." ,
	"author" : "BIOLAB",
	"summary" : "Orange 2.5a2 has been uploaded to PyPI. It now includes basic support for multi-label classification (developed during the Google Summer of Code 2011), some new widget icons and documentation for basket format. Release is also tagged on our Bitbucket repository.",
	"date" : "Jan 23, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange 2.5a2 available",
	"icon" : ""
},
{
    "uri": "/blog/mlc/",
	"title": "mlc",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 9, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "mlc",
	"icon" : ""
},
{
    "uri": "/blog/2012/01/09/multi-label-classification-and-multi-target-prediction-in-orange/",
	"title": "Multi-label classification (and Multi-target prediction) in Orange",
	"description": "",
	"content": "The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been “hidden” in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet …) and added a more general support for multi-target prediction.\nYou can load multi-label tab-delimited data (e.g. emotions.tab) just like any other tab-delimited data:\n\u003e\u003e\u003e zoo = Orange.data.Table('zoo') # single-target \u003e\u003e\u003e emotions = Orange.data.Table('emotions') # multi-label  The difference is that now zoo’s domain has a non-empty class_var field, while a list of emotions’ labels can be obtained through it’s domain’s class_vars:\n\u003e\u003e\u003e zoo.domain.class_var EnumVariable 'type' \u003e\u003e\u003e emotions.domain.class_vars \u003cEnumVariable 'amazed-suprised', EnumVariable 'happy-pleased', EnumVariable 'relaxing-calm', EnumVariable 'quiet-still', EnumVariable 'sad-lonely', EnumVariable 'angry-aggresive'\u003e  A simple example of a multi-label classification learner is a “binary relevance” learner. Let’s try it out.\n\u003e\u003e\u003e learner = Orange.multilabel.BinaryRelevanceLearner() \u003e\u003e\u003e classifier = learner(emotions) \u003e\u003e\u003e classifier(emotions[0]) [\u003corange.Value 'amazed-suprised'='0'\u003e, \u003corange.Value 'happy-pleased'='0'\u003e, \u003corange.Value 'relaxing-calm'='1'\u003e, \u003corange.Value 'quiet-still'='1'\u003e, \u003corange.Value 'sad-lonely'='1'\u003e, \u003corange.Value 'angry-aggresive'='0'\u003e] \u003e\u003e\u003e classifier(emotions[0], Orange.classification.Classifier.GetProbabilities) [\u003c1.000, 0.000\u003e, \u003c0.881, 0.119\u003e, \u003c0.000, 1.000\u003e, \u003c0.046, 0.954\u003e, \u003c0.000, 1.000\u003e, \u003c1.000, 0.000\u003e]  Real values of label variables of emotions[0] instance can be obtained by calling emotions[0].get_classes(), which is analogous to the get_class method in the single-target case.\nFor multi-label classification, we can also perform testing like usual, however, specialised evaluation measures have to be used:\n\u003e\u003e\u003e test = Orange.evaluation.testing.cross_validation([learner], emotions) \u003e\u003e\u003e Orange.evaluation.scoring.mlc_hamming_loss(test) [0.2228780213603148]  In one of the following blog posts, a multi-target regression method PLS that is in the process of implementation will be described.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been \u0026ldquo;hidden\u0026rdquo; in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet \u0026hellip;) and added a more general support for multi-target prediction.\nYou can load multi-label tab-delimited data (e." ,
	"author" : "BIOLAB",
	"summary" : "The last summer, student Wencan Luo participated in Google Summer of Code to implement Multi-label Classification in Orange. He provided a framework, implemented a few algorithms and some prototype widgets. His work has been \u0026ldquo;hidden\u0026rdquo; in our repositories for too long; finally, we have merged part of his code into Orange (widgets are not there yet \u0026hellip;) and added a more general support for multi-target prediction.\nYou can load multi-label tab-delimited data (e.",
	"date" : "Jan 9, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Multi-label classification (and Multi-target prediction) in Orange",
	"icon" : ""
},
{
    "uri": "/blog/multilabel/",
	"title": "multilabel",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 9, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "multilabel",
	"icon" : ""
},
{
    "uri": "/blog/2012/01/06/new-orange-icons/",
	"title": "New Orange icons",
	"description": "",
	"content": "As new and new widgets with new features are added to Orange, icons for them have to be drawn. Most of the time those are just some quick sketches or even missing altogether. But now we are starting to redraw and unify them. A few of them have already been made.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "As new and new widgets with new features are added to Orange, icons for them have to be drawn. Most of the time those are just some quick sketches or even missing altogether. But now we are starting to redraw and unify them. A few of them have already been made." ,
	"author" : "BIOLAB",
	"summary" : "As new and new widgets with new features are added to Orange, icons for them have to be drawn. Most of the time those are just some quick sketches or even missing altogether. But now we are starting to redraw and unify them. A few of them have already been made.",
	"date" : "Jan 6, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "New Orange icons",
	"icon" : ""
},
{
    "uri": "/blog/2012/01/03/parallel-orange/",
	"title": "Parallel Orange?",
	"description": "",
	"content": "We attended a NIPS 2011 workshop on processing and learning from large scale data. Various presenters showed different tools and frameworks that can be used when developing algorithms suitable for dealing with large scale data, but none of them were written in Python and as such, not useful for Orange. We have been looking for a framework that would help us run code in parallel for some time, but so far with no luck.\nWe would like to have a framework that is easy to use, can be used in C as well as in Python and supports multi-level map reduce (cross validation can be viewed as map reduce and random forest that is tested is another map-reduce). Prototypes we have created so far solve this problem by inspecting learners that are used in cross-validation and creating all “subtasks” at the same time. That results in really ugly code we don’t want to commit ;). If you know a framework that would suit our needs, want to implement support for parallel computation by yourself (we will apply to GSoC) or have an idea how to solve this problem, feel free to contact us ;).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We attended a NIPS 2011 workshop on processing and learning from large scale data. Various presenters showed different tools and frameworks that can be used when developing algorithms suitable for dealing with large scale data, but none of them were written in Python and as such, not useful for Orange. We have been looking for a framework that would help us run code in parallel for some time, but so far with no luck." ,
	"author" : "BIOLAB",
	"summary" : "We attended a NIPS 2011 workshop on processing and learning from large scale data. Various presenters showed different tools and frameworks that can be used when developing algorithms suitable for dealing with large scale data, but none of them were written in Python and as such, not useful for Orange. We have been looking for a framework that would help us run code in parallel for some time, but so far with no luck.",
	"date" : "Jan 3, 2012",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Parallel Orange?",
	"icon" : ""
},
{
    "uri": "/blog/2011/12/20/earth-multivariate-adaptive-regression-splines/",
	"title": "Earth - Multivariate adaptive regression splines",
	"description": "",
	"content": "There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.\nSo lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):\nimport Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange.data.Table(\"data.tab\") earth_predictor = earth.EarthLearner(data) X, Y = data.to_numpy(\"A/C\") pl.plot(X, Y, \".r\") linspace = numpy.linspace(min(X), max(X), 20) predictions = [earth_predictor([s, \"?\"]) for s in linspace] pl.plot(linspace, predictions, \"-b\") pl.show()  which produces the following plot:\nWe can also print the model representation using\nprint earth_predictor  which outputs:\nY = 1.013 +1.198 * max(0, X - 0.485) -1.803 * max(0, 0.485 - X) -1.321 * max(0, X - 0.283) -1.609 * max(0, X - 0.640) +1.591 * max(0, X - 0.907)  See Orange.regression.earth reference for full documentation.\n(Edit: Added link to the dataset file)\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.\nSo lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):\nimport Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange." ,
	"author" : "BIOLAB",
	"summary" : "There have recently been some additions to the lineup of Orange learners. One of these is Orange.regression.earth.EarthLearner. It is an Orange interface to the Earth library written by Stephen Milborrow implementing Multivariate adaptive regression splines.\nSo lets take it out for a spin on a simple toy dataset (data.tab - created using the Paint Data widget in the Orange Canvas):\nimport Orange from Orange.regression import earth import numpy from matplotlib import pylab as pl data = Orange.",
	"date" : "Dec 20, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Earth - Multivariate adaptive regression splines",
	"icon" : ""
},
{
    "uri": "/blog/2011/12/20/orange-2-5-code-conversion/",
	"title": "Orange 2.5: code conversion",
	"description": "",
	"content": "Orange 2.5 unifies Orange’s C++ core and Python modules into a single module hierarchy. To use the new module hierarchy, import Orange instead of orange and accompanying orng* modules. While we will maintain backward compatibility in 2.* releases, we nevertheless suggest programmers to use the new interface. The provided conversion tool can help refactor your code to use the new interface.\nThe conversion script, orange2to25.py, resides in Orange’s main directory. To refactor accuracy8.py from the “Orange for beginners” tutorial runpython orange2to25.py -w -o accuracy8_25.py doc/ofb-rst/code/accuracy8.py.\nThe old code\nimport orange import orngTest, orngStat, orngTree # set up the learners bayes = orange.BayesLearner() tree = orngTree.TreeLearner(mForPruning=2) bayes.name = \"bayes\" tree.name = \"tree\" learners = [bayes, tree] # compute accuracies on data data = orange.ExampleTable(\"voting\") res = orngTest.crossValidation(learners, data, folds=10) cm = orngStat.computeConfusionMatrices(res, classIndex=data.domain.classVar.values.index('democrat'))  is refactored to\nimport Orange # set up the learners bayes = Orange.classification.bayes.NaiveLearner() tree = Orange.classification.tree.TreeLearner(mForPruning=2) bayes.name = \"bayes\" tree.name = \"tree\" learners = [bayes, tree] # compute accuracies on data data = Orange.data.Table(\"voting\") res = Orange.evaluation.testing.cross_validation(learners, data, folds=10) cm = Orange.evaluation.scoring.compute_confusion_matrices(res, classIndex=data.domain.classVar.values.index('democrat'))  Read more about the refactoring tool on the wiki and on the help page (python orange2to25.py –help).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange 2.5 unifies Orange\u0026rsquo;s C++ core and Python modules into a single module hierarchy. To use the new module hierarchy, import Orange instead of orange and accompanying orng* modules. While we will maintain backward compatibility in 2.* releases, we nevertheless suggest programmers to use the new interface. The provided conversion tool can help refactor your code to use the new interface.\nThe conversion script, orange2to25.py, resides in Orange\u0026rsquo;s main directory. To refactor accuracy8." ,
	"author" : "MARKO",
	"summary" : "Orange 2.5 unifies Orange\u0026rsquo;s C++ core and Python modules into a single module hierarchy. To use the new module hierarchy, import Orange instead of orange and accompanying orng* modules. While we will maintain backward compatibility in 2.* releases, we nevertheless suggest programmers to use the new interface. The provided conversion tool can help refactor your code to use the new interface.\nThe conversion script, orange2to25.py, resides in Orange\u0026rsquo;s main directory. To refactor accuracy8.",
	"date" : "Dec 20, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange 2.5: code conversion",
	"icon" : ""
},
{
    "uri": "/blog/2011/12/08/random-forest-switches-to-simple-tree-learner-by-default/",
	"title": "Random forest switches to Simple tree learner by default",
	"description": "",
	"content": "Random forest classifiers now use Orange.classification.tree.SimpleTreeLearnerby default, which considerably shortens their construction times.\nUsing a random forest classifier is easy.\nimport Orange iris = Orange.data.Table('iris') forest = Orange.ensemble.forest.RandomForestLearner(iris, trees=200) for instance in iris: print forest(instance), instance.get_class()  The example above loads the iris dataset and trains a random forest classifier with 200 trees. The classifier is then used to label all training examples, printing its prediction alongside the actual class value.\nUsing SimpleTreeLearner insted of TreeLearner substantially reduces the training time. The image below compares construction times of Random Forest classifiers using a SimpleTreeLearner or a TreeLearner as the base learner.\nBy setting the base_learner parameter to TreeLearer it is possible to revert to the original behaviour:\ntree_learner = Orange.classification.tree.TreeLearner() forest_orig = Orange.ensemble.forest.RandomForestLearner(base_learner=tree_learner)  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Random forest classifiers now use Orange.classification.tree.SimpleTreeLearnerby default, which considerably shortens their construction times.\nUsing a random forest classifier is easy.\nimport Orange iris = Orange.data.Table('iris') forest = Orange.ensemble.forest.RandomForestLearner(iris, trees=200) for instance in iris: print forest(instance), instance.get_class()  The example above loads the iris dataset and trains a random forest classifier with 200 trees. The classifier is then used to label all training examples, printing its prediction alongside the actual class value." ,
	"author" : "BIOLAB",
	"summary" : "Random forest classifiers now use Orange.classification.tree.SimpleTreeLearnerby default, which considerably shortens their construction times.\nUsing a random forest classifier is easy.\nimport Orange iris = Orange.data.Table('iris') forest = Orange.ensemble.forest.RandomForestLearner(iris, trees=200) for instance in iris: print forest(instance), instance.get_class()  The example above loads the iris dataset and trains a random forest classifier with 200 trees. The classifier is then used to label all training examples, printing its prediction alongside the actual class value.",
	"date" : "Dec 8, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Random forest switches to Simple tree learner by default",
	"icon" : ""
},
{
    "uri": "/blog/simpletreelearner/",
	"title": "simpletreelearner",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Dec 8, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "simpletreelearner",
	"icon" : ""
},
{
    "uri": "/blog/2011/10/26/gsoc-mentor-summit/",
	"title": "GSoC Mentor Summit",
	"description": "",
	"content": "On 22th and 23th October 2011 there was Google Summer of Code Mentor Summit in Mountain View, California. Google Summer of Code is Google’s program for encouraging students to work on open-source projects during their summer break. Because this year Orange participated in this program too, we decided to participate also in this summit and get to know other mentors, other open-source projects and organizations, exchange our experiences, learn something new, and improve our connections and collaborations with others.\n                                         We went to the meeting together with another Slovenian open-source project: wlan slovenija, an open wireless network initiative. Because the summit itself was held at Google’s premises, where taking photographs was forbidden, photos are mostly from the trip there itself and area around the buildings. There are some photos by others available.\nSummit really satisfied all expectations. We have experienced how it is at Google’s, meet many new people, sessions were great, presenting a lot of interesting issues within open-source deployment and IT in general, and giving some ideas how to solve them. We meet many other researchers doing open-source science and developing different programs, libraries and having similar problems. We have discussed ways of solving them: how to maintain libraries we all use, how to make our projects survive, once research is completed or funding ends and we move to some other research, etc. Cooperation is the key here, but there is often not much time to do that, as it requires extra time and energy, often not a part of research projects.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "On 22th and 23th October 2011 there was Google Summer of Code Mentor Summit in Mountain View, California. Google Summer of Code is Google\u0026rsquo;s program for encouraging students to work on open-source projects during their summer break. Because this year Orange participated in this program too, we decided to participate also in this summit and get to know other mentors, other open-source projects and organizations, exchange our experiences, learn something new, and improve our connections and collaborations with others." ,
	"author" : "BIOLAB",
	"summary" : "On 22th and 23th October 2011 there was Google Summer of Code Mentor Summit in Mountain View, California. Google Summer of Code is Google\u0026rsquo;s program for encouraging students to work on open-source projects during their summer break. Because this year Orange participated in this program too, we decided to participate also in this summit and get to know other mentors, other open-source projects and organizations, exchange our experiences, learn something new, and improve our connections and collaborations with others.",
	"date" : "Oct 26, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "GSoC Mentor Summit",
	"icon" : ""
},
{
    "uri": "/blog/debian/",
	"title": "debian",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 13, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "debian",
	"icon" : ""
},
{
    "uri": "/blog/2011/09/13/debian-packages-support-multiple-python-versions-now/",
	"title": "Debian packages support multiple Python versions now",
	"description": "",
	"content": "We have created Debian packages for multiple Python versions. This means that they work now with both Python 2.6 and 2.7 out of the box, or if you compile them manually, with any (supported) version you have installed on your (Debian-based) system.\nPractically, this means that now you can install them without manual compiling on current Debian and Ubuntu systems. Give it a try, add our Debian package repository, apt-get install python-orange for Orange library/modules and/or orange-canvas for GUI. If you install the later package, type orange in the terminal and Orange canvas will pop-up.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have created Debian packages for multiple Python versions. This means that they work now with both Python 2.6 and 2.7 out of the box, or if you compile them manually, with any (supported) version you have installed on your (Debian-based) system.\nPractically, this means that now you can install them without manual compiling on current Debian and Ubuntu systems. Give it a try, add our Debian package repository, apt-get install python-orange for Orange library/modules and/or orange-canvas for GUI." ,
	"author" : "BIOLAB",
	"summary" : "We have created Debian packages for multiple Python versions. This means that they work now with both Python 2.6 and 2.7 out of the box, or if you compile them manually, with any (supported) version you have installed on your (Debian-based) system.\nPractically, this means that now you can install them without manual compiling on current Debian and Ubuntu systems. Give it a try, add our Debian package repository, apt-get install python-orange for Orange library/modules and/or orange-canvas for GUI.",
	"date" : "Sep 13, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Debian packages support multiple Python versions now",
	"icon" : ""
},
{
    "uri": "/blog/packaging/",
	"title": "packaging",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 13, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "packaging",
	"icon" : ""
},
{
    "uri": "/blog/2011/09/07/3d-visualizations-in-orange/",
	"title": "3D Visualizations in Orange",
	"description": "",
	"content": "Over the summer I worked (and still do) on several new 3D visualization widgets as well as a 3D plotting library they use, which will hopefully simplify making more widgets. The library is designed to be similar in terms of API to the new Qt plotting library Noughmad is working on.\nThe library uses OpenGL 2/3: since Khronos deprecated parts of the old OpenGL API (particularly immediate mode and fixed-function functionality) care has been taken to use only capabilities less likely to go away in the years to come. All the drawing is done using shaders; geometry data is fed to the graphics hardware using Vertex Buffers. The library is fully functional under OpenGL 2.0; when hardware supports newer versions (3+), several optimizations are possible (e.g. geometry processing is done on the GPU rather than on CPU), possibly resulting in improved user experience.\nWidgets I worked on and are reasonably usable:\nScatterPlot3D Its GUI has the same options as the ordinary ScatterPlot (2D),with an additional dropdown for the third attribute (Z) and some new checkboxes (e.g. 2D/3D symbols). The data can be easily rotated, translated and scaled.Supports zoom levels and selections as well. VizRank works.Thanks to hardware acceleration, ScatterPlot3D is quite responsive even with largerdatasets (30k examples).\nLinProj3D LinProj3D is displayed using dark theme (themes are available in all 3D widgets).\nSphereviz3D Sphereviz3D has 2D symbols option enabled (also available in all 3D widgets). VizRank has been modified to work with three dimensions; PCA and SPCA options under FreeViz return first three most important components when used in these widgets.\nFuture Documentation for widgets and the library is still missing. Some additional widgets are being considered, such as NetExplorer3D.\nI wrote few technical details here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Over the summer I worked (and still do) on several new 3D visualization widgets as well as a 3D plotting library they use, which will hopefully simplify making more widgets. The library is designed to be similar in terms of API to the new Qt plotting library Noughmad is working on.\nThe library uses OpenGL 2/3: since Khronos deprecated parts of the old OpenGL API (particularly immediate mode and fixed-function functionality) care has been taken to use only capabilities less likely to go away in the years to come." ,
	"author" : "BIOLAB",
	"summary" : "Over the summer I worked (and still do) on several new 3D visualization widgets as well as a 3D plotting library they use, which will hopefully simplify making more widgets. The library is designed to be similar in terms of API to the new Qt plotting library Noughmad is working on.\nThe library uses OpenGL 2/3: since Khronos deprecated parts of the old OpenGL API (particularly immediate mode and fixed-function functionality) care has been taken to use only capabilities less likely to go away in the years to come.",
	"date" : "Sep 7, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "3D Visualizations in Orange",
	"icon" : ""
},
{
    "uri": "/blog/opengl/",
	"title": "opengl",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Sep 7, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "opengl",
	"icon" : ""
},
{
    "uri": "/blog/2011/09/04/orange-badges-are-here/",
	"title": "Orange badges are here!",
	"description": "",
	"content": "Orange badges are here! They come in two flavors. Tasty!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange badges are here! They come in two flavors. Tasty!" ,
	"author" : "BIOLAB",
	"summary" : "Orange badges are here! They come in two flavors. Tasty!",
	"date" : "Sep 4, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange badges are here!",
	"icon" : ""
},
{
    "uri": "/blog/2011/09/03/gsoc-review-visualizations-with-qt/",
	"title": "GSoC Review: Visualizations with Qt",
	"description": "",
	"content": "During the course of this summer, I created a new plotting library for Orange plot, replacing the use of PyQwt. I can say that I have succesfully completed my project, but the library (and especially the visualization widgets) could still use some more work. The new library supports a similar interface, so little change is needed to convert individual widgets, but it also has several advantages over the old implementation:\n Animations: When using a single curve to show all data points, data changes only move the points instead of replacing them. These moves are now animated, as are color and size changes. Multithreading: All position calculations are done in separate threads, so the interface remains responsive even when an long operation is running in the background. Speed: I removed several occurances of needlessly clearing and repopulating the graph. Simplicity: Because it was written with Orange in mind, the new library has functions that match Orange’s data structures. This leads to simpler code in widgets using the library, and less operations in Python. Appearance: The plot can use the system palette, or a custom color theme. In general, I think it looks much nicer that Qwt-based plots. Documentation: There is an extensive API documentation (will soon be available at Orange 2.5 documentation), as well as two widget examples.  However, there are also disadvantages to using the new library. They are not many, and I’ve been trying to keep them as few and as small as possible, but there still are some.\n Line rendering: For some reason, whenever lines are rendered on the plot, the whole widget starts acting very slow. The effect is even more noticeable when zooming. As far as I can tell, this happens in Qt’s drawing libraries, so there is not much I can do about it. Axis labels: With a large number of long axis labels, the formatting gets rather ugly. This is a minor inconvenience, but it does make the plots look unprofessional.  Fortunately, I have little school obligations this september, so I think I will be able to work on Orange some more, at least until school starts. I have already added gesture support and some minor improvements since the end of the program.\nFinally, I’d like to take this opportunity to thank the Orange team, especially my mentor Miha, for accepting me and helping me throughout the summer. It’s been an interesting project, and I’ll be happy to continue working with the same software and the same team.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "During the course of this summer, I created a new plotting library for Orange plot, replacing the use of PyQwt. I can say that I have succesfully completed my project, but the library (and especially the visualization widgets) could still use some more work. The new library supports a similar interface, so little change is needed to convert individual widgets, but it also has several advantages over the old implementation:" ,
	"author" : "BIOLAB",
	"summary" : "During the course of this summer, I created a new plotting library for Orange plot, replacing the use of PyQwt. I can say that I have succesfully completed my project, but the library (and especially the visualization widgets) could still use some more work. The new library supports a similar interface, so little change is needed to convert individual widgets, but it also has several advantages over the old implementation:",
	"date" : "Sep 3, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "GSoC Review: Visualizations with Qt",
	"icon" : ""
},
{
    "uri": "/blog/2011/09/02/gsoc-review-multi-label-classification-implementation/",
	"title": "GSoC Review: Multi-label Classification Implementation",
	"description": "",
	"content": "Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| \u003e 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| \u003e 2, then it is called a multi-class classification problem (Tsoumakas \u0026 Katakis, 2007).\nMulti-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification. However, currently, Orange can only handle single-label problems. As a result, the project Multi-label classification Implementation has been proposed to extend Orange to support multi-label.\nWe can group the existing methods for multi-label classification into two main categories: a) problem transformation method, and b) algorithm adaptation methods. In the former one, multi-label problems are converted to single-label, and then the traditional binary classification can apply; in the latter case, methods directly classify the multi-label data, instead.\nIn this project, two transformation methods and two algorithm adaptation methods are implemented. Along with the methods, their widgets are also added. As the evaluation metrics for multi-label data are different from the single-label ones, new evaluation measures are supported. The code is available in SVN branch.\nFortunately, benefiting from the Tab file format, the ExampleTable can store multi-label data without any modification. Now, we can add a special value – label into the attributes dictionary of the domain with value 1. In this way, if the attribute description has the keyword label, then it is a label; otherwise, it is a normal feature.\nWhat have been done in this project Transformation methods  br – Binary Relevance Learner (Tsoumakas \u0026 Katakis, 2007) lp – Label Powerset Classification (Tsoumakas \u0026 Katakis, 2007)  Algorithm Adaptation methods  mlknn – Multi-kNN Classification (Zhang \u0026 Zhou, 2007) brknn – BR-kNN Classification (Spyromitros et al. 2008)  Evaluation methods  mlc_hamming_loss – Example-based Hamming Loss (Schapire and Singer 2000) mlc_accuracy, mlc_precision, mlc_recall – Example-based accuracy, precision, recall (Godbole \u0026 Sarawagi, 2004)  Widgets  OWBR – Widget for Binary Relevance Learner OWLP – Widget for Label Powerset Classification OWMLkNN – Widget for Multi-kNN Classification OWBRkNN – Widget for BR-kNN Classification OWTestLearner – Widget for Evaluation  File Format Extension  extend the loadARFF function to support sparse Weka format new support mulan xml and arff format  Plan for the future  add more classification methods for multi-label, such as PT1 to PT6 add feature extraction method add ranking-based evaluation methods  How to use Basically, the way to use multi-label classification and evaluation is nearly the same as the single-label ones. The only difference between them is the different types of input data.\nExample for Classification import Orange data = Orange.data.Table(\"emotions.tab\") classifier = Orange.multilabel.BinaryRelevanceLearner(data) for e in data: c,p = classifier(e,Orange.classification.Classifier.GetBoth) print c,p powerset_cliassifer = Orange.multilabel.LabelPowersetLearner(data) for e in data: c,p = powerset_cliassifer(e,Orange.classification.Classifier.GetBoth) print c,p mlknn_cliassifer = Orange.multilabel.MLkNNLearner(data,k=1) for e in data: c,p = mlknn_cliassifer(e,Orange.classification.Classifier.GetBoth) print c,p br_cliassifer = Orange.multilabel.BRkNNLearner(data,k=1) for e in data: c,p = br_cliassifer(e,Orange.classification.Classifier.GetBoth) print c,p  Example for Evaluation import Orange learners = [ Orange.multilabel.BinaryRelevanceLearner(name=\"br\"), Orange.multilabel.BinaryRelevanceLearner(name=\"br\", \\ base_learner=Orange.classification.knn.kNNLearner), Orange.multilabel.LabelPowersetLearner(name=\"lp\"), Orange.multilabel.LabelPowersetLearner(name=\"lp\", \\ base_learner=Orange.classification.knn.kNNLearner), Orange.multilabel.MLkNNLearner(name=\"mlknn\",k=5), Orange.multilabel.BRkNNLearner(name=\"brknn\",k=5), ] data = Orange.data.Table(\"emotions.xml\") res = Orange.evaluation.testing.cross_validation(learners, data,2) loss = Orange.evaluation.scoring.mlc_hamming_loss(res) accuracy = Orange.evaluation.scoring.mlc_accuracy(res) precision = Orange.evaluation.scoring.mlc_precision(res) recall = Orange.evaluation.scoring.mlc_recall(res) print 'loss=', loss print 'accuracy=', accuracy print 'precision=', precision print 'recall=', recall  References  G. Tsoumakas and I. Katakis. Multi-label classification: An overview”. International Journal of Data Warehousing and Mining, 3(3):1-13, 2007. E. Spyromitros, G. Tsoumakas, and I. Vlahavas, An Empirical Study of Lazy Multilabel Classification Algorithms. Proc. 5th Hellenic Conference on Artificial Intelligence (SETN 2008), Springer, Syros, Greece, 2008. M. Zhang and Z. Zhou. ML-KNN: A lazy learning approach to multi-label learning. Pattern Recognition, 40, 7 (Jul. 2007), 2038-2048. S. Godbole and S. Sarawagi. Discriminative Methods for Multi-labeled Classification, Proceedings of the 8th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2004. R. E. Schapire and Y. Singer. Boostexter: a bossting-based system for text categorization, Machine Learning, vol.39, no.2/3, 2000, pp:135-68.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| \u0026gt; 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| \u0026gt; 2, then it is called a multi-class classification problem (Tsoumakas \u0026amp; Katakis, 2007).\nMulti-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification." ,
	"author" : "BIOLAB",
	"summary" : "Traditional single-label classification is concerned with learning from a set of examples that are associated with a single label l from a set of disjoint labels L, |L| \u0026gt; 1. If |L| = 2, then the learning problem is called a binary classification problem, while if |L| \u0026gt; 2, then it is called a multi-class classification problem (Tsoumakas \u0026amp; Katakis, 2007).\nMulti-label classification methods are increasingly used by many applications, such as textual data classification, protein function classification, music categorization and semantic scene classification.",
	"date" : "Sep 2, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "GSoC Review: Multi-label Classification Implementation",
	"icon" : ""
},
{
    "uri": "/blog/2011/09/01/gsoc-review-mf-matrix-factorization-techniques-for-data-mining/",
	"title": "GSoC Review: MF - Matrix Factorization Techniques for Data Mining",
	"description": "",
	"content": "MF - Matrix Factorization Techniques for Data Mining is a Python scripting library which includes a number of published matrix factorization algorithms, initialization methods, quality and performance measures and facilitates the combination of these to produce new strategies. The library represents a unified and efficient interface to matrix factorization algorithms and methods.\nThe MF works with numpy dense matrices and scipy sparse matrices (where this is possible to save on space). The library has support for multiple runs of the algorithms which can be used for some quality measures. By setting runtime specific options tracking the residuals error within one (or more) run or tracking fitted factorization model is possible. Extensive documentation with working examples which demonstrate real applications, commonly used benchmark data and visualization methods are provided to help with the interpretation and comprehension of the results.\nContent of Current Release Factorization Methods  BD - Bayesian nonnegative matrix factorization Gibbs sampler [Schmidt2009] BMF - Binary matrix factorization [Zhang2007] ICM - Iterated conditional modes nonnegative matrix factorization [Schmidt2009] LFNMF - Fisher nonnegative matrix factorization for learning local features [Wang2004], [Li2001] LSNMF - Alternative nonnegative least squares matrix factorization using projected gradient method for subproblems [Lin2007] NMF - Standard nonnegative matrix factorization with Euclidean / Kullback-Leibler update equations and Frobenius / divergence / connectivity cost functions [Lee2001], [Brunet2004] NSNMF - Nonsmooth nonnegative matrix factorization [Montano2006] PMF - Probabilistic nonnegative matrix factorization [Laurberg2008], [Hansen2008] PSMF - Probabilistic sparse matrix factorization [Dueck2005], [Dueck2004], [Srebro2001], [Li2007] SNMF - Sparse nonnegative matrix factorization based on alternating nonnegativity constrained least squares [Park2007] SNMNMF - Sparse network regularized multiple nonnegative matrix factorization [Zhang2011]  Initialization Methods  Random Fixed NNDSVD [Boutsidis2007] Random C [Albright2006] Random VCol [Albright2006]  Quality and Performance Measures  Distance Residuals Connectivity matrix Consensus matrix Entropy of the fitted NMF model [Park2007] Dominant basis components computation Explained variance Feature score computation representing its specificity to basis vectors [Park2007] Computation of most basis specific features for basis vectors [Park2007] Purity [Park2007] Residual sum of squares - can be used for rank estimate [Hutchins2008], [Frigyesi2008] Sparseness [Hoyer2004] Cophenetic correlation coefficient of consensus matrix - can be used for rank estimate [Brunet2004] Dispersion [Park2007] Factorization rank estimation Selected matrix factorization method specific  Plans for Future General plan for future releases of MF library is to alleviate the usage for non-technical users, increase library stability and provide comprehensive visualization methods. Specifically, in algorithm sense addition of the following could be provided.\n Extending Bayesian methods with variational BD and linearly constrained BD. Adaptation of the PMF model to interval-valued matrices. Nonnegative matrix approximation. Multiplicative iterative schema.  Usage # Import MF library entry point for factorization import mf from scipy.sparse import csr_matrix from scipy import array from numpy import dot # We will try to factorize sparse matrix. Construct sparse matrix in CSR format. V = csr_matrix((array([1,2,3,4,5,6]),array([0,2,2,0,1,2]),array([0,2,3,6])),shape=(3,3)) # Run Standard NMF rank 4 algorithm # Returned object is fitted factorization model. # Through it user can access quality and performance measures. fit = mf.mf(V,method = \"nmf\",max_iter = 30,rank = 4,update = 'divergence',objective = 'div') # Basis matrix. It is sparse, as input V was sparse as well. W = fit.basis() print \"Basis matrix\\n\", W.todense() # Mixture matrix. We print this tiny matrix in dense format. H = fit.coef() print \"Coef\\n\", H.todense() # Return the loss function according to Kullback-Leibler divergence. print \"Distance Kullback-Leibler\", fit.distance(metric = \"kl\") # Compute generic set of measures to evaluate the quality of the factorization sm = fit.summary() # Print sparseness (Hoyer, 2004) of basis and mixture matrix print \"Sparseness W: %5.3f H: %5.3f\" % (sm['sparseness'][0], sm['sparseness'][1]) # Print actual number of iterations performed print \"Iterations\", sm['n_iter'] # Print estimate of target matrix V print \"Estimate\\n\", dot(W.todense(), H.todense())  Examples Examples with visualized results in bioinformatics, image processing, text analysis, recommendation systems are provided in Examples section of Documentation.\nFigure 1: Reordered consensus matrix generated for rank = 2 on Leukemia data set.\nFigure 2: Interpretation of NMF - Divergence basis vectors on Medlars data set. By considering the highest weighted terms in this vector, we can assign a label or topic to basis vector W1, a user might attach the label liver to basis vector W1.\nFigure 3: Basis images of LSNMF obtained after 500 iterations on original face images. The bases trained by LSNMF are additive but not spatially localized for representation of faces.\nRelevant Links  Extensive published documentation with examples Orange wiki MF Project page Github repository with source code Short presentation in pdf format of MF library  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "MF - Matrix Factorization Techniques for Data Mining is a Python scripting library which includes a number of published matrix factorization algorithms, initialization methods, quality and performance measures and facilitates the combination of these to produce new strategies. The library represents a unified and efficient interface to matrix factorization algorithms and methods.\nThe MF works with numpy dense matrices and scipy sparse matrices (where this is possible to save on space)." ,
	"author" : "BIOLAB",
	"summary" : "MF - Matrix Factorization Techniques for Data Mining is a Python scripting library which includes a number of published matrix factorization algorithms, initialization methods, quality and performance measures and facilitates the combination of these to produce new strategies. The library represents a unified and efficient interface to matrix factorization algorithms and methods.\nThe MF works with numpy dense matrices and scipy sparse matrices (where this is possible to save on space).",
	"date" : "Sep 1, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "GSoC Review: MF - Matrix Factorization Techniques for Data Mining",
	"icon" : ""
},
{
    "uri": "/blog/2011/08/24/faster-classification-and-regression-trees/",
	"title": "Faster classification and regression trees",
	"description": "",
	"content": "SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.\nThe motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values.\nComparison with TreeLearner The graph below shows SimpleTreeLearner construction times on datasets bundled with Orange normalized to TreeLearner. Smaller is better.\nThe harmonic mean (average speedup) on all the benchmarks is 11.4.\nUsage The user can set four parameters:\nmaxMajority\nMaximal proportion of majority class.\nminExamples\nMinimal number of examples in leaves.\nmaxDepth\nMaximal depth of tree.\nskipProb\nAt every split an attribute will be skipped with probability skipProb. This parameter is especially useful for building random forests.\nThe code snippet below demonstrates the basic usage of SimpleTreeLearner. It behaves much like any other Orange learner would.\nimport Orange data = Orange.data.Table(\"iris\") # build classifier and classify train data classifier = Orange.classification.tree.SimpleTreeLearner(data, maxMajority=0.8) for ex in data: print classifier(ex) # estimate classification accuracy with cross-validation learner = Orange.classification.tree.SimpleTreeLearner(minExamples=2) result = Orange.evaluation.testing.cross_validation([learner], data) print 'CA:', Orange.evaluation.scoring.CA(result)[0]  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.\nThe motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values." ,
	"author" : "BIOLAB",
	"summary" : "SimpleTreeLearner is an implementation of classification and regression trees that sacrifices flexibility for speed. A benchmark on 42 different datasets reveals that SimpleTreeLearner is 11 times faster than the original TreeLearner.\nThe motivation behind developing a new tree induction algorithm from scratch was to speed up the construction of random forests, but you can also use it as a standalone learner. SimpleTreeLearner uses gain ratio for classification and MSE for regression and can handle unknown values.",
	"date" : "Aug 24, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Faster classification and regression trees",
	"icon" : ""
},
{
    "uri": "/blog/2011/08/19/golden-sublime-triangles-in-orange/",
	"title": "Golden (sublime) triangles in Orange",
	"description": "",
	"content": "Hand in hand with the development of the new visualization framework and the financial crisis we are putting some gold into Orange. The arrows at the ends of the axes are, as of today, small golden triangles. See the changes in owaxis.py!\n- path.moveTo(0, 3) - path.lineTo(0, -3) - path.lineTo(5, 0) + path.moveTo(0, 3.09) + path.lineTo(0, -3.09) + path.lineTo(9.51, 0)  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Hand in hand with the development of the new visualization framework and the financial crisis we are putting some gold into Orange. The arrows at the ends of the axes are, as of today, small golden triangles. See the changes in owaxis.py!\n- path.moveTo(0, 3) - path.lineTo(0, -3) - path.lineTo(5, 0) + path.moveTo(0, 3.09) + path.lineTo(0, -3.09) + path.lineTo(9.51, 0)  " ,
	"author" : "MARKO",
	"summary" : "Hand in hand with the development of the new visualization framework and the financial crisis we are putting some gold into Orange. The arrows at the ends of the axes are, as of today, small golden triangles. See the changes in owaxis.py!\n- path.moveTo(0, 3) - path.lineTo(0, -3) - path.lineTo(5, 0) + path.moveTo(0, 3.09) + path.lineTo(0, -3.09) + path.lineTo(9.51, 0)  ",
	"date" : "Aug 19, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Golden (sublime) triangles in Orange",
	"icon" : ""
},
{
    "uri": "/blog/2011/08/03/orange-at-ismbeccb-2011/",
	"title": "Orange at ISMB/ECCB 2011",
	"description": "",
	"content": "We presented the Orange Bioinformatics add-on at the ISMB/ECCB conference in Vienna, a joined event covering both 19th Annual International Conference on Intelligent Systems for Molecular Biology and 10th European Conference on Computational Biology.\nWe were giving out Orange stickers (with the URL) to the poster’s visitors. There was some interest; in the end we gave out about 10 of them, mostly to biologists, who were excited to perform some of the analysis themselves. Among the visitors was also a developer of a similar tool who seemed slightly surprised that something like this already exists, while another was disappointed because Orange only runs locally.\nSee the poster in action on the photo taken by Gregor Rot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We presented the Orange Bioinformatics add-on at the ISMB/ECCB conference in Vienna, a joined event covering both 19th Annual International Conference on Intelligent Systems for Molecular Biology and 10th European Conference on Computational Biology.\nWe were giving out Orange stickers (with the URL) to the poster\u0026rsquo;s visitors. There was some interest; in the end we gave out about 10 of them, mostly to biologists, who were excited to perform some of the analysis themselves." ,
	"author" : "MARKO",
	"summary" : "We presented the Orange Bioinformatics add-on at the ISMB/ECCB conference in Vienna, a joined event covering both 19th Annual International Conference on Intelligent Systems for Molecular Biology and 10th European Conference on Computational Biology.\nWe were giving out Orange stickers (with the URL) to the poster\u0026rsquo;s visitors. There was some interest; in the end we gave out about 10 of them, mostly to biologists, who were excited to perform some of the analysis themselves.",
	"date" : "Aug 3, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange at ISMB/ECCB 2011",
	"icon" : ""
},
{
    "uri": "/blog/networkx/",
	"title": "networkx",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jul 29, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "networkx",
	"icon" : ""
},
{
    "uri": "/blog/2011/07/29/networkx-in-orange/",
	"title": "NetworkX in Orange",
	"description": "",
	"content": "NetworkX – a popular open-source python library for network analysis has finally found its way into Orange. It is now used as a base class for network representation in all Orange modules and widgets. By that, we offered to the widespread network community a fruitful and fun way to visualize and explore networks, using their existing NetworkX scripts. It has never been easier to combine network analysis and visualization with existing machine learning and data discovery methods.\nComplete documentation is available in the Orange network headquarters. For a brief overview, take a look at the following example. Let us suppose we would like to analyse the data about patients, having one of two types of leukemia. So, we have a data set with 72 patient, 4600+ gene expressions and a class variable. We also have a vast network of human genes, connected if they share a biological function. What we would like to examine is a sub-network with only several hundred most expressed genes from the data set. To show off a bit, we will also use the Orange Bioinformatics add-on. Here is how we do it:\nimport Orange import obiExpression # load leukemia data set table = Orange.data.Table(\"/media/Ox/Projects_Archive/res/BIO/leukemia/leukemiaGSEA.tab\") useAttributeLabels = False ttest = obiExpression.ExpressionSignificance_TTest(table, useAttributeLabels) target = [table.domain.classVar(0), table.domain.classVar(1)] # test for significantly expressed genes score = ttest(target = target) # each gene is scored (t-test, p-value) print score[0] \u003e\u003e\u003e (FloatVariable 'HIST1H4C', (1.8377179790830149, 0.07034778767062116)) # sort by p-value from operator import itemgetter score.sort(key=lambda s: s[1][1]) # select 200 genes with the lowest p-value important_genes = [gene_var.name for gene_var, s in score[:200]] # read the gene network (5000+ genes, dense network) G = Orange.network.readwrite.read('genes_biofunct.gpickle') items = G.items().filter_bool({'gene': important_genes}) indices = [i for i, present in enumerate(items) if present] # build a subraph of 200 most expressed genes G_sub = G.subgraph(indices)  In addition to the power of scripting environment, we also get the benefits of visual data exploration with Orange widgets. However, network widgets are currently under heavy development, so expect some bugs if you dare to try them. Coding should be finished in a month or two, check the blog for progress updates. Here is how to open the network in Nx Explorer widget:\nimport sys import PyQt4 # must have OWNxExplorer in python path! import OWNxExplorer app=PyQt4.QtGui.QApplication(sys.argv) ow=OWNxExplorer.OWNxExplorer() ow.show() # set the network ow.set_graph(G_sub) app.exec_()  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "NetworkX – a popular open-source python library for network analysis has finally found its way into Orange. It is now used as a base class for network representation in all Orange modules and widgets. By that, we offered to the widespread network community a fruitful and fun way to visualize and explore networks, using their existing NetworkX scripts. It has never been easier to combine network analysis and visualization with existing machine learning and data discovery methods." ,
	"author" : "BIOLAB",
	"summary" : "NetworkX – a popular open-source python library for network analysis has finally found its way into Orange. It is now used as a base class for network representation in all Orange modules and widgets. By that, we offered to the widespread network community a fruitful and fun way to visualize and explore networks, using their existing NetworkX scripts. It has never been easier to combine network analysis and visualization with existing machine learning and data discovery methods.",
	"date" : "Jul 29, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "NetworkX in Orange",
	"icon" : ""
},
{
    "uri": "/blog/2011/07/20/orange-gsoc-multi-label-classification-implementation/",
	"title": "Orange GSoC: Multi-label Classification Implementation",
	"description": "",
	"content": "Multi-label classification is one of the three projects of Google Summer Code 2011 for Orange. The main goal is to extend the Orange to support multi-label, including dataset support, two basic multi-label classifications-problem-transformation methods \u0026 algorithm adaptation methods, evaluation measures, GUI support, documentation, testing, and so on.\nMy name is Wencan Luo, from China. I’m very happy to work with my mentor Matija. Until now, we have finished a framework for multi-label support for Orange.\nTo support multi-label data structure, a special value is added into their ‘attributes’ dictionary. In this way, we can know whether the attribute is a type of class without altering the old Example Table class.\nMoreover, a transformation classification method to support multilabel is implemented, named Binary Relevance. All the codes are extended from the old Orange code using Python to be compatible with single-label classification methods.\nIn addition, the evaluator for multilalbel classification is also implemented based on the old single-label evaluator in Orange.evaluator.testing and Orange.evaluator.scoring modules.\nAt last, the widget for Binary Relevance method and Evaluator is implemented.\nMany work has to be done as following:\n one more transformation method two adaptive methods ranking-based evaluator widgets to support the above methods testing  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Multi-label classification is one of the three projects of Google Summer Code 2011 for Orange. The main goal is to extend the Orange to support multi-label, including dataset support, two basic multi-label classifications-problem-transformation methods \u0026amp; algorithm adaptation methods, evaluation measures, GUI support, documentation, testing, and so on.\nMy name is Wencan Luo, from China. I\u0026rsquo;m very happy to work with my mentor Matija. Until now, we have finished a framework for multi-label support for Orange." ,
	"author" : "BIOLAB",
	"summary" : "Multi-label classification is one of the three projects of Google Summer Code 2011 for Orange. The main goal is to extend the Orange to support multi-label, including dataset support, two basic multi-label classifications-problem-transformation methods \u0026amp; algorithm adaptation methods, evaluation measures, GUI support, documentation, testing, and so on.\nMy name is Wencan Luo, from China. I\u0026rsquo;m very happy to work with my mentor Matija. Until now, we have finished a framework for multi-label support for Orange.",
	"date" : "Jul 20, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange GSoC: Multi-label Classification Implementation",
	"icon" : ""
},
{
    "uri": "/blog/2011/07/03/fink-packages-now-also-64-bit/",
	"title": "Fink packages now also 64-bit",
	"description": "",
	"content": "Fink packages (we are using for system-wide Orange installations on Mac OS X) were updated to 64-bit. So if you were using 64-bit Fink installation you will be now able also to use Orange (and our binary Fink repository of already compiled packages). Just use this this installation script to configure your local Fink installation to use our binary Fink repository and add information about Orange packages (they are not available among official Fink packages).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Fink packages (we are using for system-wide Orange installations on Mac OS X) were updated to 64-bit. So if you were using 64-bit Fink installation you will be now able also to use Orange (and our binary Fink repository of already compiled packages). Just use this this installation script to configure your local Fink installation to use our binary Fink repository and add information about Orange packages (they are not available among official Fink packages)." ,
	"author" : "BIOLAB",
	"summary" : "Fink packages (we are using for system-wide Orange installations on Mac OS X) were updated to 64-bit. So if you were using 64-bit Fink installation you will be now able also to use Orange (and our binary Fink repository of already compiled packages). Just use this this installation script to configure your local Fink installation to use our binary Fink repository and add information about Orange packages (they are not available among official Fink packages).",
	"date" : "Jul 3, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Fink packages now also 64-bit",
	"icon" : ""
},
{
    "uri": "/blog/2011/06/30/orange-gsoc-visualizations-with-qt/",
	"title": "Orange GSoC: Visualizations with Qt",
	"description": "",
	"content": "Hello, my name is Miha Čančula and this summer I’m working on Orange as part of Google’s Summer of Code program, mentored by Miha Štajdohar. My task is to replace the current visualization framework based on Qwt with a custom library, depending only on Qt. This library will better support Orange’s very specific visualizations and will replace the unmaintained PyQwt.\nI have a lot of experience with Qt and its graphics classes, both in C++ and Python, but I’m relatively now to Orange. As a physics student, especially now that I’m selecting a computational physics program, this a great learning opportunity for me.\nI think my work is progressing very well, because many visualizations already work with the new library with only minor modifications:\n Scatterplot Linear projections Discretize All the visualizations in VizRank and FreeViz dialogs  The library is written partially in C++, especially the performance-sensitive parts, and partially in Python. It uses the Qt Graphics View Framework, with several reimplemented or wrapped method to preserve the old behavior and API. I have tried to keep the necessary modification to the widgets themselves to a minimum, so the large majority of changes are in the OWGraph class which server as the base class for all graphs.\nGraphs made by Qwt are not very flexible, they only support graphs with cartesian axes. On the other hand, visualization Orange often use custom axes and transformations. That’s why I designed the new library with support for arbitrary axes, curves and other elements. All of these can be extendeng with classes written in Python, C++, or a combination thereof. The required changes to visualizations I already ported to the new OWGraph class were mostly simplifications, with very little new code added.\nFor example, zooming and selection is implemented completely in the new OWGraph class, with the same function and attribute names as before, so visualizations themselves didn’t need any changes at all.\nThe new framework is also able to produce much nicer graphs. I haven’t had the time to customize the looks much, but it’s possible to set colors, line widths, point sizes and symbols from Python. There are still some settings that have no UI configuration, but I will focus on that after making sure that visualization widgets work with the new library.\nCurrently I’m trying to change as many widgets as possible to the new classes. As I said above, I only completed a few of them, but I believe the others won’t require too much work.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Hello, my name is Miha Čančula and this summer I\u0026rsquo;m working on Orange as part of Google\u0026rsquo;s Summer of Code program, mentored by Miha Štajdohar. My task is to replace the current visualization framework based on Qwt with a custom library, depending only on Qt. This library will better support Orange\u0026rsquo;s very specific visualizations and will replace the unmaintained PyQwt.\nI have a lot of experience with Qt and its graphics classes, both in C++ and Python, but I\u0026rsquo;m relatively now to Orange." ,
	"author" : "BIOLAB",
	"summary" : "Hello, my name is Miha Čančula and this summer I\u0026rsquo;m working on Orange as part of Google\u0026rsquo;s Summer of Code program, mentored by Miha Štajdohar. My task is to replace the current visualization framework based on Qwt with a custom library, depending only on Qt. This library will better support Orange\u0026rsquo;s very specific visualizations and will replace the unmaintained PyQwt.\nI have a lot of experience with Qt and its graphics classes, both in C++ and Python, but I\u0026rsquo;m relatively now to Orange.",
	"date" : "Jun 30, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange GSoC: Visualizations with Qt",
	"icon" : ""
},
{
    "uri": "/blog/2011/06/30/debian-packages-for-squeeze/",
	"title": "Debian packages for Squeeze",
	"description": "",
	"content": "We have updated our daily Debian packages to Squeeze (current Debian stable). You just have to reconfigure our package repository in your /etc/apt/sources.list to:\ndeb http://orange.biolab.si/debian squeeze main deb-src http://orange.biolab.si/debian squeeze main  Those packages are compiled for Python 2.6.\nYou can read more about Debian packages in our old blog post.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have updated our daily Debian packages to Squeeze (current Debian stable). You just have to reconfigure our package repository in your /etc/apt/sources.list to:\ndeb http://orange.biolab.si/debian squeeze main deb-src http://orange.biolab.si/debian squeeze main  Those packages are compiled for Python 2.6.\nYou can read more about Debian packages in our old blog post." ,
	"author" : "BIOLAB",
	"summary" : "We have updated our daily Debian packages to Squeeze (current Debian stable). You just have to reconfigure our package repository in your /etc/apt/sources.list to:\ndeb http://orange.biolab.si/debian squeeze main deb-src http://orange.biolab.si/debian squeeze main  Those packages are compiled for Python 2.6.\nYou can read more about Debian packages in our old blog post.",
	"date" : "Jun 30, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Debian packages for Squeeze",
	"icon" : ""
},
{
    "uri": "/blog/2011/06/24/orange-gsoc-mf-techniques-for-data-mining/",
	"title": "Orange GSoC: MF Techniques for Data Mining",
	"description": "",
	"content": "I am one of three students who are working on GSoC projects for Orange this year. The objective of the project Matrix Factorization Techniques for Data Mining is to provide the Orange community with a unified and efficient interface to matrix factorization algorithms and methods.\nFor that purpose I have been developing a library which will include a number of published factorization algorithms and initialization methods and will facilitate combinations of these to produce new strategies. Extensive documentation with working examples that demonstrate applications, commonly used benchmark data and possibly some visualization methods will be provided to help with the interpretation and comprehension of the factorization results.\nMain factorization techniques and their variations included in the library are:\n family of nonnegative matrix factorization algorithms (NMF), including Brunet NMF, sparse NMF, non-smooth NMF, least-squares NMF, local Fisher NMF; probabilistic factorization (PMF) and its sparse variant (PSMF); Bayesian decomposition (BD); iterated conditional modes (ICM) algorithm.  Different multiplicative and update algorithms for NMF will be analyzed which minimize least-squares error or generalized Kullback-Leibler divergence.\nFor those interested some more information with details about algorithms is available at project home page.\nThere is still much work to do but I have been enjoying at it and I am looking forward to continuing with the project.\nThanks to the Orange team and mentor prof. dr. Blaz Zupan for support and advice.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "I am one of three students who are working on GSoC projects for Orange this year. The objective of the project Matrix Factorization Techniques for Data Mining is to provide the Orange community with a unified and efficient interface to matrix factorization algorithms and methods.\nFor that purpose I have been developing a library which will include a number of published factorization algorithms and initialization methods and will facilitate combinations of these to produce new strategies." ,
	"author" : "BIOLAB",
	"summary" : "I am one of three students who are working on GSoC projects for Orange this year. The objective of the project Matrix Factorization Techniques for Data Mining is to provide the Orange community with a unified and efficient interface to matrix factorization algorithms and methods.\nFor that purpose I have been developing a library which will include a number of published factorization algorithms and initialization methods and will facilitate combinations of these to produce new strategies.",
	"date" : "Jun 24, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange GSoC: MF Techniques for Data Mining",
	"icon" : ""
},
{
    "uri": "/blog/2011/06/24/orange-t-shirts/",
	"title": "Orange T-shirts",
	"description": "",
	"content": "If you maybe missed on our Facebook page: we have made our own fruity t-shirts. They are simply awesome and show to everybody around you that you have a taste! Just check the handsomeness:\nWe will be selling them on the website soon for $15 (shipping costs included), but if you want to have one (or more) in advance, drop us a line and we will see what we can do. We have them for all you brave girls and boys and in different sizes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "If you maybe missed on our Facebook page: we have made our own fruity t-shirts. They are simply awesome and show to everybody around you that you have a taste! Just check the handsomeness:\nWe will be selling them on the website soon for $15 (shipping costs included), but if you want to have one (or more) in advance, drop us a line and we will see what we can do." ,
	"author" : "BIOLAB",
	"summary" : "If you maybe missed on our Facebook page: we have made our own fruity t-shirts. They are simply awesome and show to everybody around you that you have a taste! Just check the handsomeness:\nWe will be selling them on the website soon for $15 (shipping costs included), but if you want to have one (or more) in advance, drop us a line and we will see what we can do.",
	"date" : "Jun 24, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange T-shirts",
	"icon" : ""
},
{
    "uri": "/blog/2011/06/14/contact-us/",
	"title": "Contact us!",
	"description": "",
	"content": "We have added a simple contact form so that you can get in direct contact with us. Please do not misuse it (we will simply ignore you). Support and other general questions should be posted in our forum and for issues with Orange you should use our ticketing system to report them.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have added a simple contact form so that you can get in direct contact with us. Please do not misuse it (we will simply ignore you). Support and other general questions should be posted in our forum and for issues with Orange you should use our ticketing system to report them." ,
	"author" : "BIOLAB",
	"summary" : "We have added a simple contact form so that you can get in direct contact with us. Please do not misuse it (we will simply ignore you). Support and other general questions should be posted in our forum and for issues with Orange you should use our ticketing system to report them.",
	"date" : "Jun 14, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Contact us!",
	"icon" : ""
},
{
    "uri": "/blog/2011/05/20/orange-2-5-progress/",
	"title": "Orange 2.5 progress",
	"description": "",
	"content": "We decided that we should reorganize Orange to provide more intuitive interface to the scripting interface. The next release, Orange 2.5 is getting better every day. But fear not, dear reader, we are working hard to ensure that your scripts will still work.\nIn the last morning of the camp in Bohinj we decided to use undercase_separated names instead of CamelCase. We have been steadily converting them and the deprecation utilities by Aleš help a lot. We just list the name changes for class attributes or arguments and their renaming is handled gracefully; they also remain accessible with the old names. Therefore, the code does not need to be duplicated to ensure backwards compatibility.\nA simple example from the documentation of bagging and boosting. The old version first:\nimport orange, orngEnsemble, orngTree import orngTest, orngStat tree = orngTree.TreeLearner(mForPruning=2, name=\"tree\") bs = orngEnsemble.BoostedLearner(tree, name=\"boosted tree\") bg = orngEnsemble.BaggedLearner(tree, name=\"bagged tree\") data = orange.ExampleTable(\"lymphography.tab\") learners = [tree, bs, bg] results = orngTest.crossValidation(learners, data, folds=3) print \"Classification Accuracy:\" for i in range(len(learners)): print (\"%15s: %5.3f\") % (learners[i].name, orngStat.CA(results)[i])  Orange 2.5 version:\nimport Orange tree = Orange.classification.tree.TreeLearner(m_pruning=2, name=\"tree\") bs = Orange.ensemble.boosting.BoostedLearner(tree, name=\"boosted tree\") bg = Orange.ensemble.bagging.BaggedLearner(tree, name=\"bagged tree\") table = Orange.data.Table(\"lymphography.tab\") learners = [tree, bs, bg] results = Orange.evaluation.testing.cross_validation(learners, table, folds=3) print \"Classification Accuracy:\" for i in range(len(learners)): print (\"%15s: %5.3f\") % (learners[i].name, Orange.evaluation.scoring.CA(results)[i])  In new Orange we only need to import a single module, Orange, the root of the new hierarchy.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We decided that we should reorganize Orange to provide more intuitive interface to the scripting interface. The next release, Orange 2.5 is getting better every day. But fear not, dear reader, we are working hard to ensure that your scripts will still work.\nIn the last morning of the camp in Bohinj we decided to use undercase_separated names instead of CamelCase. We have been steadily converting them and the deprecation utilities by Aleš help a lot." ,
	"author" : "MARKO",
	"summary" : "We decided that we should reorganize Orange to provide more intuitive interface to the scripting interface. The next release, Orange 2.5 is getting better every day. But fear not, dear reader, we are working hard to ensure that your scripts will still work.\nIn the last morning of the camp in Bohinj we decided to use undercase_separated names instead of CamelCase. We have been steadily converting them and the deprecation utilities by Aleš help a lot.",
	"date" : "May 20, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange 2.5 progress",
	"icon" : ""
},
{
    "uri": "/blog/2011/04/25/accepted-gsoc-2011-students-announced/",
	"title": "Accepted GSoC 2011 students announced",
	"description": "",
	"content": "Accepted proposals/projects for Google Summer of Code 2011 have been announced. We got 3 students which will this year work on Orange:\n  Marinka Žitnik: Matrix Factorization Techniques for Data Mining\n  Miha Čančula: 2D visualization using PyQt\n  Wencan Luo: Multi-label classification\n  Congrats to all accepted students. We are looking forward working with you. And for all other students: please apply again next year. Your proposals were good, but we just could not accept everybody.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Accepted proposals/projects for Google Summer of Code 2011 have been announced. We got 3 students which will this year work on Orange:\n  Marinka Žitnik: Matrix Factorization Techniques for Data Mining\n  Miha Čančula: 2D visualization using PyQt\n  Wencan Luo: Multi-label classification\n  Congrats to all accepted students. We are looking forward working with you. And for all other students: please apply again next year. Your proposals were good, but we just could not accept everybody." ,
	"author" : "BIOLAB",
	"summary" : "Accepted proposals/projects for Google Summer of Code 2011 have been announced. We got 3 students which will this year work on Orange:\n  Marinka Žitnik: Matrix Factorization Techniques for Data Mining\n  Miha Čančula: 2D visualization using PyQt\n  Wencan Luo: Multi-label classification\n  Congrats to all accepted students. We are looking forward working with you. And for all other students: please apply again next year. Your proposals were good, but we just could not accept everybody.",
	"date" : "Apr 25, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Accepted GSoC 2011 students announced",
	"icon" : ""
},
{
    "uri": "/blog/2011/04/08/student-application-period-for-gsoc-2011-has-ended/",
	"title": "Student application period for GSoC 2011 has ended",
	"description": "",
	"content": "Student application period for Google Summer of Code 2011 has ended. We got 47 proposals from students all around the world. Now it is time for us to evaluate them and select the best proposals and the best students to work this year on Orange.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Student application period for Google Summer of Code 2011 has ended. We got 47 proposals from students all around the world. Now it is time for us to evaluate them and select the best proposals and the best students to work this year on Orange." ,
	"author" : "BIOLAB",
	"summary" : "Student application period for Google Summer of Code 2011 has ended. We got 47 proposals from students all around the world. Now it is time for us to evaluate them and select the best proposals and the best students to work this year on Orange.",
	"date" : "Apr 8, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Student application period for GSoC 2011 has ended",
	"icon" : ""
},
{
    "uri": "/blog/2011/03/29/our-gsoc-2011-posters/",
	"title": "Our GSoC 2011 posters",
	"description": "",
	"content": "We have made our own recruitment posters for this year’s Google Summer of Code inviting students to participate.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have made our own recruitment posters for this year\u0026rsquo;s Google Summer of Code inviting students to participate." ,
	"author" : "BIOLAB",
	"summary" : "We have made our own recruitment posters for this year\u0026rsquo;s Google Summer of Code inviting students to participate.",
	"date" : "Mar 29, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Our GSoC 2011 posters",
	"icon" : ""
},
{
    "uri": "/blog/2011/03/28/data-loading-speedups/",
	"title": "Data loading speedups",
	"description": "",
	"content": "Orange has been loading data faster since the end of February, especially if there are many attributes in the file.\nQuick comparisons between the old new versions, measured on my computer:\n adult.tab (32561 examples, 15 attributes): old version = 1.41s, new version = 0.86s. DLBCL.tab (77 examples, 7071 attributes): old version = 2.72s, new version = 0.93s. GDS1962.tab (104 examples, 31837 attributes): old version = 33.5s, new version = 6.6s.  The speedups were obtained with:\n reuse of a buffer for parsing, skipping type detection for attributes with known types, and by keeping attributes in a different data structure internally.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange has been loading data faster since the end of February, especially if there are many attributes in the file.\nQuick comparisons between the old new versions, measured on my computer:\n adult.tab (32561 examples, 15 attributes): old version = 1.41s, new version = 0.86s. DLBCL.tab (77 examples, 7071 attributes): old version = 2.72s, new version = 0.93s. GDS1962.tab (104 examples, 31837 attributes): old version = 33.5s, new version = 6." ,
	"author" : "MARKO",
	"summary" : "Orange has been loading data faster since the end of February, especially if there are many attributes in the file.\nQuick comparisons between the old new versions, measured on my computer:\n adult.tab (32561 examples, 15 attributes): old version = 1.41s, new version = 0.86s. DLBCL.tab (77 examples, 7071 attributes): old version = 2.72s, new version = 0.93s. GDS1962.tab (104 examples, 31837 attributes): old version = 33.5s, new version = 6.",
	"date" : "Mar 28, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Data loading speedups",
	"icon" : ""
},
{
    "uri": "/blog/2011/03/18/orange-has-been-accepted-into-gsoc-2011/",
	"title": "Orange has been accepted into GSoC 2011",
	"description": "",
	"content": "This year Orange has been accepted into the Google summer of Code program as a mentoring organization. It is one of 175 open-source organizations/projects/groups which will this year mentor students while they will be working on those accepted open source projects.\nWe have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year. Check out also official project page for Orange.\nGoogle Summer of Code is a Google-sponsored program where Google stipends students working for a summer job on an open source projects from all around the world. Student is paid $5000 (and a t-shirt!) for approximately two months of work/contribution to the project. More about the program is available on its homepage.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "This year Orange has been accepted into the Google summer of Code program as a mentoring organization. It is one of 175 open-source organizations/projects/groups which will this year mentor students while they will be working on those accepted open source projects.\nWe have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year." ,
	"author" : "BIOLAB",
	"summary" : "This year Orange has been accepted into the Google summer of Code program as a mentoring organization. It is one of 175 open-source organizations/projects/groups which will this year mentor students while they will be working on those accepted open source projects.\nWe have prepared a page on our Trac with more information about the Google Summer of Code program, especially how the interested students should apply with their proposals. There is also a list of of some ideas we are proposing for this year.",
	"date" : "Mar 18, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Orange has been accepted into GSoC 2011",
	"icon" : ""
},
{
    "uri": "/blog/2011/02/11/biolab-retreat-februar-2011/",
	"title": "Biolab retreat Februar 2011",
	"description": "",
	"content": "From Wednesday, 2nd February 2011, to Saturday, 5th February 2011, we have been on working retreat at Lake Bohinj. The whole Bioinformatics Laboratory of the Faculty of Computer and Information technology has temporary moved to a nice house just few meters from the lake, enjoing the nature and without any distractions. Plan: working on the next version of Orange, Orange 2.5 and documentation rewrite. Orange 2.5 will have a better and restructured Python scripting interface along with great and shinny documentation.\nOverall summary of the retreat: first commit (revision 9743) by Marko on Wednesday, last commit (revision 10181) by Matija at 1:26:49 on Saturday. This gives 439 revisions made during the retreat.\nSome photos to give you a taste how it was.\n                                                                       ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "From Wednesday, 2nd February 2011, to Saturday, 5th February 2011, we have been on working retreat at Lake Bohinj. The whole Bioinformatics Laboratory of the Faculty of Computer and Information technology has temporary moved to a nice house just few meters from the lake, enjoing the nature and without any distractions. Plan: working on the next version of Orange, Orange 2.5 and documentation rewrite. Orange 2.5 will have a better and restructured Python scripting interface along with great and shinny documentation." ,
	"author" : "BIOLAB",
	"summary" : "From Wednesday, 2nd February 2011, to Saturday, 5th February 2011, we have been on working retreat at Lake Bohinj. The whole Bioinformatics Laboratory of the Faculty of Computer and Information technology has temporary moved to a nice house just few meters from the lake, enjoing the nature and without any distractions. Plan: working on the next version of Orange, Orange 2.5 and documentation rewrite. Orange 2.5 will have a better and restructured Python scripting interface along with great and shinny documentation.",
	"date" : "Feb 11, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Biolab retreat Februar 2011",
	"icon" : ""
},
{
    "uri": "/blog/bohinj/",
	"title": "bohinj",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 11, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "bohinj",
	"icon" : ""
},
{
    "uri": "/blog/retreat/",
	"title": "retreat",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Feb 11, 2011",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "retreat",
	"icon" : ""
},
{
    "uri": "/blog/2010/03/04/debian-repository-lives/",
	"title": "Debian repository lives!",
	"description": "",
	"content": "We have made still-experimental-but-probably-working Debian repository with daily built Orange packages. Currently without add-ons.\nTo get access to those packages just add those two lines to your /etc/apt/sources.list (this file contains a list of repositories with packages):\ndeb http://orange.biolab.si/debian lenny main deb-src http://orange.biolab.si/debian lenny main  And then you can install Orange with this command:\naptitude update aptitude install orange-svn  Packages are not signed as they are made automatically so you will probably be warned about this.\nThose packages will probably work also on other Debian-based Linux distributions like Ubuntu, but have not yet been tested there. Please test it and report how it goes.\nYou can also get source of those packages with this command:\napt-get source python-orange-svn  And then build package by yourself in extracted source directory with:\ndpkg-buildpackage  For example this will be useful on amd64 platform for which we currently do not yet provide binary packages. (Edit: now we provide binary packages also for amd64 platform.) But we will once we see how well this system works.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "We have made still-experimental-but-probably-working Debian repository with daily built Orange packages. Currently without add-ons.\nTo get access to those packages just add those two lines to your /etc/apt/sources.list (this file contains a list of repositories with packages):\ndeb http://orange.biolab.si/debian lenny main deb-src http://orange.biolab.si/debian lenny main  And then you can install Orange with this command:\naptitude update aptitude install orange-svn  Packages are not signed as they are made automatically so you will probably be warned about this." ,
	"author" : "BIOLAB",
	"summary" : "We have made still-experimental-but-probably-working Debian repository with daily built Orange packages. Currently without add-ons.\nTo get access to those packages just add those two lines to your /etc/apt/sources.list (this file contains a list of repositories with packages):\ndeb http://orange.biolab.si/debian lenny main deb-src http://orange.biolab.si/debian lenny main  And then you can install Orange with this command:\naptitude update aptitude install orange-svn  Packages are not signed as they are made automatically so you will probably be warned about this.",
	"date" : "Mar 4, 2010",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "blog",
	"LinkTitle" : "Debian repository lives!",
	"icon" : ""
},
{
    "uri": "/blog/2017/06/19/text-preprocessing/",
	"title": "2017/06/19/text-preprocessing/",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "2017/06/19/text-preprocessing/",
	"icon" : ""
},
{
    "uri": "/blog/2020/2020-07-27-story-arcs/",
	"title": "2020/2020-07-27-story-arcs/",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "blog",
	"LinkTitle" : "2020/2020-07-27-story-arcs/",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/adaboost/",
	"title": "AdaBoost",
	"description": "",
	"content": "AdaBoost An ensemble meta-algorithm that combines weak learners and adapts to the ‘hardness’ of each training sample.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Learner: learning algorithm  Outputs\n Learner: AdaBoost learning algorithm Model: trained model  The AdaBoost (short for “Adaptive boosting”) widget is a machine-learning algorithm, formulated by Yoav Freund and Robert Schapire. It can be used with other learning algorithms to boost their performance. It does so by tweaking the weak learners.\nAdaBoost works for both classification and regression.\n The learner can be given a name under which it will appear in other widgets. The default name is “AdaBoost”. Set the parameters. The base estimator is a tree and you can set:  Number of estimators Learning rate: it determines to what extent the newly acquired information will override the old information (0 = the agent will not learn anything, 1 = the agent considers only the most recent information) Fixed seed for random generator: set a fixed seed to enable reproducing the results.   Boosting method.  Classification algorithm (if classification on input): SAMME (updates base estimator’s weights with classification results) or SAMME.R (updates base estimator’s weight with probability estimates). Regression loss function (if regression on input): Linear (), Square (), Exponential ().   Produce a report. Click Apply after changing the settings. That will put the new learner in the output and, if the training examples are given, construct a new model and output it as well. To communicate changes automatically tick Apply Automatically.  Examples For classification, we loaded the iris dataset. We used AdaBoost, Tree and Logistic Regression and evaluated the models’ performance in Test \u0026 Score.\nFor regression, we loaded the housing dataset, sent the data instances to two different models (AdaBoost and Tree) and output them to the Predictions widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "AdaBoost An ensemble meta-algorithm that combines weak learners and adapts to the \u0026lsquo;hardness\u0026rsquo; of each training sample.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Learner: learning algorithm  Outputs\n Learner: AdaBoost learning algorithm Model: trained model  The AdaBoost (short for \u0026ldquo;Adaptive boosting\u0026rdquo;) widget is a machine-learning algorithm, formulated by Yoav Freund and Robert Schapire. It can be used with other learning algorithms to boost their performance. It does so by tweaking the weak learners." ,
	"author" : "",
	"summary" : "AdaBoost An ensemble meta-algorithm that combines weak learners and adapts to the \u0026lsquo;hardness\u0026rsquo; of each training sample.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Learner: learning algorithm  Outputs\n Learner: AdaBoost learning algorithm Model: trained model  The AdaBoost (short for \u0026ldquo;Adaptive boosting\u0026rdquo;) widget is a machine-learning algorithm, formulated by Yoav Freund and Robert Schapire. It can be used with other learning algorithms to boost their performance. It does so by tweaking the weak learners.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "AdaBoost",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/aggregate/",
	"title": "Aggregate",
	"description": "",
	"content": "Aggregate Aggregate data by second, minute, hour, day, week, month, or year.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Aggregated time series.  Aggregate joins together instances at the same level of granularity. In other words, if aggregating by day, all instances from the same day will be merged into one. Aggregation function can be defined separately based on the type of the attribute.\n Interval to aggregate the time series by. Options are: second, minute, hour, day, week, month, or year. Aggregation function for each of the time series in the table. Discrete variables (sequences) can only be aggregated using mode (i.e. most frequent value), whereas string variables can only be aggregated using string concatenation.  See also Moving Transform\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Aggregate Aggregate data by second, minute, hour, day, week, month, or year.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Aggregated time series.  Aggregate joins together instances at the same level of granularity. In other words, if aggregating by day, all instances from the same day will be merged into one. Aggregation function can be defined separately based on the type of the attribute." ,
	"author" : "",
	"summary" : "Aggregate Aggregate data by second, minute, hour, day, week, month, or year.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Aggregated time series.  Aggregate joins together instances at the same level of granularity. In other words, if aggregating by day, all instances from the same day will be merged into one. Aggregation function can be defined separately based on the type of the attribute.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Aggregate",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/align_datasets/",
	"title": "Align Datasets",
	"description": "",
	"content": "Align Datasets Alignment of multiple datasets with a diagram of correlation visualization.\nInputs\n Data: single cell dataset  Outputs\n Transformed Data: aligned data Genes per n. Components   Data source indicator Number of components for canonical correlation analysis. Number of shared genes and the scoring method for alignment. Scoring can be done with Pearson, Spearman and Biweights midcorrelation. Tick the box to use the percent of quantile normalization and dynamic time warping. If Apply automatically is ticked, the results will be communicated automatically. Alternatively, press Apply.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Align Datasets Alignment of multiple datasets with a diagram of correlation visualization.\nInputs\n Data: single cell dataset  Outputs\n Transformed Data: aligned data Genes per n. Components   Data source indicator Number of components for canonical correlation analysis. Number of shared genes and the scoring method for alignment. Scoring can be done with Pearson, Spearman and Biweights midcorrelation. Tick the box to use the percent of quantile normalization and dynamic time warping." ,
	"author" : "",
	"summary" : "Align Datasets Alignment of multiple datasets with a diagram of correlation visualization.\nInputs\n Data: single cell dataset  Outputs\n Transformed Data: aligned data Genes per n. Components   Data source indicator Number of components for canonical correlation analysis. Number of shared genes and the scoring method for alignment. Scoring can be done with Pearson, Spearman and Biweights midcorrelation. Tick the box to use the percent of quantile normalization and dynamic time warping.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Align Datasets",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/annotate_projection/",
	"title": "Annotator",
	"description": "",
	"content": "Annotator The widget provides an option to annotate cells with cell types based on marker genes.\nInputs\n Reference Data: Data set with gene expression values. Secondary Data: Subset of instances (optional). Genes: Marker genes.  Outputs\n Selected Data: Instances selected from the plot. Data: Data with additional columns with annotations, clusters, and projection  This widget receives gene expression data together with mapping to a two-dimensional space and marker genes. It selects the most expressed genes for each cell with the Mann-Whitney U test and computes the p-value of each cell types for a cell based on the selected statistical test. It visualizes groups of cells and for each group, it shows few most present cell types.\n This box contains settings for attribute selection and annotation:  Axis X and Axis Y let you select attributes you will show in the widget. By default t-SNE-x and t-SNE-y are selected which are transformations to lower dimensional space with a method named t-SNE. Scoring method allows you to select the method to score the cell type affiliation to the cell. It has the following options:  -Log(FDR) - negative of the logarithm of an FDR value Marker expression - the sum of expressions of genes typical for cell type. Marker expression % - the proportion of genes typical for a cell type expressed in the cell.   Statistical test allow the user to select between the binomial and hypergeometric statistical test for computing the p-value. FDR threshold sets the threshold for FDR value. Cell types that have an FDR value bellow this threshold are selected. ε for DBSCAN regulates the ε parameter of a DBSCAN algorithm which forms groups in the visualization. When unchecked algorithm estimates this parameter itself. Start run the assigning process. Whenever you change any parameter in this box rerun the process with this button.   Set the color of the displayed points (you will get colors for discrete values and grey-scale points for continuous). Set label, shape, and size to differentiate between points. Set symbol size and opacity for all data points. Set jittering to prevent the dots overlapping. Set the number of labels shown for a cluster. Adjust plot properties:  Show legend displays a legend on the right. Click and drag the legend to move it. Show cluster hull regulate whether the hull around cluster is present or not. Color points by cluster colors the points with cluster specific color. When this option is checked setting color in box 2 is disabled. Show reference data makes data from the Reference data input visible.   Select, zoom, pan, and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom. This group of buttons allows you to get help about the widget, save the created image to your computer in a .svg or .png format, and create the report. The main view shows you data clustered data items. Each cluster is surrounded by a hull and has assigned Cell type labels shown.  Examples In this example Single Cell Datasets widget provides Bone marrow mononuclear cells with AML (sample) gene expression dataset and t-SNE widget maps data into two-dimensional space. Marker Genes widget provides markers genes with cell types to the Annotator widget. The data table receives the output of the Annotator widget and shows items selected in the plot. The widget shows clustered cells projected to the two-dimensional plane. Clusters labels show annotation with the most common label in the cluster.\nThe second example shows how to use Secondary Data input of the widget. We load data with two Single Cell Datasets widgets. First widget loads Cell cycle in mESC (Fluidigm) data which are used as primary data. The second widget loads Cell cycle in mESC (QuartzSeq) dataset for secondary data input. After data are loaded, we normalize them with the Single Cell Preprocess widget and map the reference data to two-dimensional space with the t-SNE projection. Projecting the secondary data is not required since the Annotator widget projects them in the reference space. The Annotator window shows the mapping of secondary data (colored points) to clusters generated on the reference data.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Annotator The widget provides an option to annotate cells with cell types based on marker genes.\nInputs\n Reference Data: Data set with gene expression values. Secondary Data: Subset of instances (optional). Genes: Marker genes.  Outputs\n Selected Data: Instances selected from the plot. Data: Data with additional columns with annotations, clusters, and projection  This widget receives gene expression data together with mapping to a two-dimensional space and marker genes." ,
	"author" : "",
	"summary" : "Annotator The widget provides an option to annotate cells with cell types based on marker genes.\nInputs\n Reference Data: Data set with gene expression values. Secondary Data: Subset of instances (optional). Genes: Marker genes.  Outputs\n Selected Data: Instances selected from the plot. Data: Data with additional columns with annotations, clusters, and projection  This widget receives gene expression data together with mapping to a two-dimensional space and marker genes.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Annotator",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/applydomain/",
	"title": "Apply Domain",
	"description": "",
	"content": "Apply Domain Given dataset and template transforms the dataset.\nInputs\n Data: input dataset Template Data: template for transforming the dataset  Outputs\n Transformed Data: transformed dataset  Apply Domain maps new data into a transformed space. For example, if we transform some data with PCA and wish to observe new data in the same space, we can use Apply Domain to map the new data into the PCA space created from the original data.\nThe widget receives a dataset and a template dataset used to transform the dataset.\nExample We will use iris data from the File widget for this example. To create two separate data sets, we will use Select Rows and set the condition to iris is one of iris-setosa, iris-versicolor. This will output a data set with a 100 rows, half of them belonging to iris-setosa class and the other half to iris-versicolor.\nWe will transform the data with PCA and select the first two components, which explain 96% of variance. Now, we would like to apply the same preprocessing on the ‘new’ data, that is the remaining 50 iris virginicas. Send the unused data from Select Rows to Apply Domain. Make sure to use the Unmatched Data output from Select Rows widget. Then add the Transformed data output from PCA.\nApply Domain will apply the preprocessor to the new data and output it. To add the new data to the old data, use Concatenate. Use Transformed Data output from PCA as Primary Data and Transformed Data from Apply Domain as Additional Data.\nObserve the results in a Data Table or in a Scatter Plot to see the new data in relation to the old one.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Apply Domain Given dataset and template transforms the dataset.\nInputs\n Data: input dataset Template Data: template for transforming the dataset  Outputs\n Transformed Data: transformed dataset  Apply Domain maps new data into a transformed space. For example, if we transform some data with PCA and wish to observe new data in the same space, we can use Apply Domain to map the new data into the PCA space created from the original data." ,
	"author" : "",
	"summary" : "Apply Domain Given dataset and template transforms the dataset.\nInputs\n Data: input dataset Template Data: template for transforming the dataset  Outputs\n Transformed Data: transformed dataset  Apply Domain maps new data into a transformed space. For example, if we transform some data with PCA and wish to observe new data in the same space, we can use Apply Domain to map the new data into the PCA space created from the original data.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Apply Domain",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/arima/",
	"title": "ARIMA Model",
	"description": "",
	"content": "ARIMA Model Model the time series using ARMA, ARIMA, or ARIMAX model.\nInputs\n Time series: Time series as output by As Timeseries widget. Exogenous data: Time series of additional independent variables that can be used in an ARIMAX model.  Outputs\n Time series model: The ARIMA model fitted to input time series. Forecast: The forecast time series. Fitted values: The values that the model was actually fitted to, equals to original values - residuals. Residuals: The errors the model made at each step.  Using this widget, you can model the time series with ARIMA model.\n Model’s name. By default, the name is derived from the model and its parameters. ARIMA’s p, d, q parameters. Use exogenous data. Using this option, you need to connect additional series on the Exogenous data input signal. Number of forecast steps the model should output, along with the desired confidence intervals values at each step.  Example See also VAR Model, Model Evaluation\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "ARIMA Model Model the time series using ARMA, ARIMA, or ARIMAX model.\nInputs\n Time series: Time series as output by As Timeseries widget. Exogenous data: Time series of additional independent variables that can be used in an ARIMAX model.  Outputs\n Time series model: The ARIMA model fitted to input time series. Forecast: The forecast time series. Fitted values: The values that the model was actually fitted to, equals to original values - residuals." ,
	"author" : "",
	"summary" : "ARIMA Model Model the time series using ARMA, ARIMA, or ARIMAX model.\nInputs\n Time series: Time series as output by As Timeseries widget. Exogenous data: Time series of additional independent variables that can be used in an ARIMAX model.  Outputs\n Time series model: The ARIMA model fitted to input time series. Forecast: The forecast time series. Fitted values: The values that the model was actually fitted to, equals to original values - residuals.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "ARIMA Model",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/as_timeseries/",
	"title": "As Timeseries",
	"description": "",
	"content": "As Timeseries Reinterpret a Table object as a Timeseries object.\nInputs\n Data: Any data table.  Outputs\n Time series: Data table reinterpreted as time series.  This widget reinterprets any data table as a time series, so it can be used with the rest of the widgets in this add-on. In the widget, you can set which data attribute represents the time variable.\n The time attribute, the values of which imply measurements’ order and spacing. This can be any continuous attribute. Alternatively, you can specify that the time series sequence is implied by instance order.  Example The input to this widget comes from any data-emitting widget, e.g. the File widget. Note, whenever you do some processing with Orange core widgets, like the Select Columns widget, you need to re-apply the conversion into time series.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "As Timeseries Reinterpret a Table object as a Timeseries object.\nInputs\n Data: Any data table.  Outputs\n Time series: Data table reinterpreted as time series.  This widget reinterprets any data table as a time series, so it can be used with the rest of the widgets in this add-on. In the widget, you can set which data attribute represents the time variable.\n The time attribute, the values of which imply measurements\u0026rsquo; order and spacing." ,
	"author" : "",
	"summary" : "As Timeseries Reinterpret a Table object as a Timeseries object.\nInputs\n Data: Any data table.  Outputs\n Time series: Data table reinterpreted as time series.  This widget reinterprets any data table as a time series, so it can be used with the rest of the widgets in this add-on. In the widget, you can set which data attribute represents the time variable.\n The time attribute, the values of which imply measurements\u0026rsquo; order and spacing.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "As Timeseries",
	"icon" : ""
},
{
    "uri": "/widget-catalog/associate/associationrules/",
	"title": "Association Rules",
	"description": "",
	"content": "Association Rules Induction of association rules.\nInputs\n Data: Data set  Outputs\n Matching Data: Data instances matching the criteria.  This widget implements FP-growth frequent pattern mining algorithm [1] with bucketing optimization [2] for conditional databases of few items. For inducing classification rules, it generates rules for the entire itemset and skips the rules where the consequent does not match one of the class’ values.\n  Information on the data set.\n  In Find association rules you can set criteria for rule induction:\n Minimal support: percentage of the entire data set covered by the entire rule (antecedent and consequent). Minimal confidence: proportion of the number of examples which fit the right side (consequent) among those that fit the left side (antecedent). Max. number of rules: limit the number of rules the algorithm generates. Too many rules can slow down the widget considerably. If Induce classification (itemset → class) rules is ticked, the widget will only generate rules that have a class value on the right-hand side (consequent) of the rule. If Auto find rules is on, the widget will run the search at every change of parameters. Might be slow for data sets with many attributes, so pressing Find rules only when the parameters are set is a good idea.    Filter rules by\n  Antecedent:\n Contains: will filter rules by matching space-separated regular expressions in antecedent items. Min. items: minimum number of items that have to appear in an antecedent. Max. items: maximum number of items that can appear in an antecedent.    Consequent:\n Contains: will filter rules by matching space-separated regular expressions in consequent items. Min. items: minimum number of items that have to appear in a consequent. Max. items: maximum number of items that can appear in a consequent.    If Apply these filters in search is ticked, the widget will limit the rule generation only to rules that match the filters. If unchecked, all rules are generated, but only the matching are shown.\n  If Auto send selection is on, data instances that match the selected association rules are output automatically. Alternatively press Send selection.\n  Example Association Rules can be used directly with the File widget.\nReferences and further reading [1]: J. Han, J. Pei, Y. Yin, R. Mao. (2004) Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern Tree Approach.\n[2]: R. Agrawal, C. Aggarwal, V. Prasad. (2000) Depth first generation of long patterns.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Association Rules Induction of association rules.\nInputs\n Data: Data set  Outputs\n Matching Data: Data instances matching the criteria.  This widget implements FP-growth frequent pattern mining algorithm [1] with bucketing optimization [2] for conditional databases of few items. For inducing classification rules, it generates rules for the entire itemset and skips the rules where the consequent does not match one of the class\u0026rsquo; values.\n  Information on the data set." ,
	"author" : "",
	"summary" : "Association Rules Induction of association rules.\nInputs\n Data: Data set  Outputs\n Matching Data: Data instances matching the criteria.  This widget implements FP-growth frequent pattern mining algorithm [1] with bucketing optimization [2] for conditional databases of few items. For inducing classification rules, it generates rules for the entire itemset and skips the rules where the consequent does not match one of the class\u0026rsquo; values.\n  Information on the data set.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Association Rules",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/average/",
	"title": "Average Spectra",
	"description": "",
	"content": "Average Spectra Average spectra.\nInputs\n Data: input dataset  Outputs\n Averages: averaged dataset  The Average Spectra widget enables you to calculate average spectra. It can output the average of the entire dataset, or average into groups defined by a Categorical feature.\nUse Group by to output averages defined by a Categorical feature.\nColumns of non-Numerical data will return a value if every row in that group has the same value, otherwise it will return Unknown.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Average Spectra Average spectra.\nInputs\n Data: input dataset  Outputs\n Averages: averaged dataset  The Average Spectra widget enables you to calculate average spectra. It can output the average of the entire dataset, or average into groups defined by a Categorical feature.\nUse Group by to output averages defined by a Categorical feature.\nColumns of non-Numerical data will return a value if every row in that group has the same value, otherwise it will return Unknown." ,
	"author" : "",
	"summary" : "Average Spectra Average spectra.\nInputs\n Data: input dataset  Outputs\n Averages: averaged dataset  The Average Spectra widget enables you to calculate average spectra. It can output the average of the entire dataset, or average into groups defined by a Categorical feature.\nUse Group by to output averages defined by a Categorical feature.\nColumns of non-Numerical data will return a value if every row in that group has the same value, otherwise it will return Unknown.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Average Spectra",
	"icon" : ""
},
{
    "uri": "/workflows/Bag-of-Words/",
	"title": "Bag of Words",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Bag of Words",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/bagofwords-widget/",
	"title": "Bag of Words",
	"description": "",
	"content": "Bag of Words Generates a bag of words from the input corpus.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with bag of words features appended.  Bag of Words model creates a corpus with word counts for each data instance (document). The count can be either absolute, binary (contains or does not contain) or sublinear (logarithm of the term frequency). Bag of words model is required in combination with Word Enrichment and could be used for predictive modelling.\n Parameters for bag of words model:  Term Frequency:  Count: number of occurrences of a word in a document Binary: word appears or does not appear in the document Sublinear: logarithm of term frequency (count)   Document Frequency:  (None) IDF: inverse document frequency Smooth IDF: adds one to document frequencies to prevent zero division.   Regulariation:  (None) L1 (Sum of elements): normalizes vector length to sum of elements L2 (Euclidean): normalizes vector length to sum of squares     Produce a report. If Commit Automatically is on, changes are communicated automatically. Alternatively press Commit.  Example In the first example we will simply check how the bag of words model looks like. Load book-excerpts.tab with Corpus widget and connect it to Bag of Words. Here we kept the defaults - a simple count of term frequencies. Check what the Bag of Words outputs with Data Table. The final column in white represents term frequencies for each document.\nIn the second example we will try to predict document category. We are still using the book-excerpts.tab data set, which we sent through Preprocess Text with default parameters. Then we connected Preprocess Text to Bag of Words to obtain term frequencies by which we will compute the model.\nConnect Bag of Words to Test \u0026 Score for predictive modelling. Connect SVM or any other classifier to Test \u0026 Score as well (both on the left side). Test \u0026 Score will now compute performance scores for each learner on the input. Here we got quite impressive results with SVM. Now we can check, where the model made a mistake.\nAdd Confusion Matrix to Test \u0026 Score. Confusion matrix displays correctly and incorrectly classified documents. Select Misclassified will output misclassified documents, which we can further inspect with Corpus Viewer.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Bag of Words Generates a bag of words from the input corpus.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with bag of words features appended.  Bag of Words model creates a corpus with word counts for each data instance (document). The count can be either absolute, binary (contains or does not contain) or sublinear (logarithm of the term frequency). Bag of words model is required in combination with Word Enrichment and could be used for predictive modelling." ,
	"author" : "",
	"summary" : "Bag of Words Generates a bag of words from the input corpus.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with bag of words features appended.  Bag of Words model creates a corpus with word counts for each data instance (document). The count can be either absolute, binary (contains or does not contain) or sublinear (logarithm of the term frequency). Bag of words model is required in combination with Word Enrichment and could be used for predictive modelling.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Bag of Words",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/barplot/",
	"title": "Bar Plot",
	"description": "",
	"content": "Bar Plot Visualizes comparisons among discrete categories.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Bar Plot widget visualizes numeric variables and compares them by a categorical variable. The widget is useful for observing outliers, distributions within groups, and comparing categories.\n Parameters of the plot. Values are the numeric variable to plot. Group by is the variable for grouping the data. Annotations are categorical labels below the plot. Color is the categorical variable whose values are used for coloring the bars. Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom. If Send automatically is ticked, changes are communicated automatically. Alternatively, press Send. Access help, save image, produce a report, or adjust visual settings. On the right, the information on input and output are shown.  Example The Bar Plot widget is most commonly used immediately after the File widget to compare categorical values. In this example, we have used heart-disease data to inspect our variables.\nFirst, we have observed cholesterol values of patient from our data set. We grouped them by diameter narrowing, which defines patients with a heart disease (1) and those without (0). We use the same variable for coloring the bars.\nThen, we selected patients over 60 years of age with Select Rows. We sent the subset to Bar Plot to highlight these patients in the widget. The big outlier with a high cholesterol level is apparently over 60 years old.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Bar Plot Visualizes comparisons among discrete categories.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Bar Plot widget visualizes numeric variables and compares them by a categorical variable. The widget is useful for observing outliers, distributions within groups, and comparing categories.\n Parameters of the plot." ,
	"author" : "",
	"summary" : "Bar Plot Visualizes comparisons among discrete categories.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Bar Plot widget visualizes numeric variables and compares them by a categorical variable. The widget is useful for observing outliers, distributions within groups, and comparing categories.\n Parameters of the plot.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Bar Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/batch_effect_removal/",
	"title": "Batch Effect Removal",
	"description": "",
	"content": "Batch Effect Removal Batch effect normalization on Single Cell data set.\nData\n Data: Single cell dataset.  Outputs\n Data: Single cell dataset.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Batch Effect Removal Batch effect normalization on Single Cell data set.\nData\n Data: Single cell dataset.  Outputs\n Data: Single cell dataset.  " ,
	"author" : "",
	"summary" : "Batch Effect Removal Batch effect normalization on Single Cell data set.\nData\n Data: Single cell dataset.  Outputs\n Data: Single cell dataset.  ",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Batch Effect Removal",
	"icon" : ""
},
{
    "uri": "/workflows/Box-Plot/",
	"title": "Box Plot",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Box Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/boxplot/",
	"title": "Box Plot",
	"description": "",
	"content": "Box Plot Shows distribution of attribute values.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Box Plot widget shows the distributions of attribute values. It is a good practice to check any new data with this widget to quickly discover any anomalies, such as duplicated values (e.g., gray and grey), outliers, and alike. Bars can be selected - for example, values for categorical data or the quantile range for numeric data.\n Select the variable you want to plot. Tick Order by relevance to subgroups to order variables by Chi2 or ANOVA over the selected subgroup. Choose Subgroups to see box plots displayed by a discrete subgroup. Tick Order by relevance to variable to order subgroups by Chi2 or ANOVA over the selected variable. When instances are grouped by a subgroup, you can change the display mode. Annotated boxes will display the end values, the mean and the median, while comparing medians and compare means will, naturally, compare the selected value between subgroups.  The mean (the dark blue vertical line). The thin blue line represents the standard deviation. Values of the first (25%) and the third (75%) quantile. The blue highlighted area represents the values between the first and the third quartile. The median (yellow vertical line).  For discrete attributes, the bars represent the number of instances with each particular attribute value. The plot shows the number of different animal types in the Zoo dataset: there are 41 mammals, 13 fish, 20 birds, and so on.\nDisplay shows:\n Stretch bars: Shows relative values (proportions) of data instances. The unticked box shows absolute values. Show box labels: Display discrete values above each bar. Sort by subgroup frequencies: Sort subgroups by their descending frequency.  Examples The Box Plot widget is most commonly used immediately after the File widget to observe the statistical properties of a dataset. In the first example, we have used heart-disease data to inspect our variables.\nBox Plot is also useful for finding the properties of a specific dataset, for instance, a set of instances manually defined in another widget (e.g. Scatter Plot or instances belonging to some cluster or a classification tree node. Let us now use zoo data and create a typical clustering workflow with Distances and Hierarchical Clustering.\nNow define the threshold for cluster selection (click on the ruler at the top). Connect Box Plot to Hierarchical Clustering, tick Order by relevance, and select Cluster as a subgroup. This will order attributes by how well they define the selected subgroup, in our case, a cluster. It seems like our clusters indeed correspond very well with the animal type!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Box Plot Shows distribution of attribute values.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Box Plot widget shows the distributions of attribute values. It is a good practice to check any new data with this widget to quickly discover any anomalies, such as duplicated values (e.g., gray and grey), outliers, and alike." ,
	"author" : "",
	"summary" : "Box Plot Shows distribution of attribute values.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Box Plot widget shows the distributions of attribute values. It is a good practice to check any new data with this widget to quickly discover any anomalies, such as duplicated values (e.g., gray and grey), outliers, and alike.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Box Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/calibratedlearner/",
	"title": "Calibrated Learner",
	"description": "",
	"content": "Calibrated Learner Wraps another learner with probability calibration and decision threshold optimization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Base Learner: learner to calibrate  Outputs\n Learner: calibrated learning algorithm Model: trained model using the calibrated learner  This learner produces a model that calibrates the distribution of class probabilities and optimizes decision threshold. The widget works only for binary classification tasks.\n  The name under which it will appear in other widgets. Default name is composed of the learner, calibration and optimization parameters.\n  Probability calibration:\n Sigmoid calibration Isotonic calibration No calibration    Decision threshold optimization:\n Optimize classification accuracy Optimize F1 score No threshold optimization    Press Apply to commit changes. If Apply Automatically is ticked, changes are committed automatically.\n  Example A simple example with Calibrated Learner. We are using the titanic data set as the widget requires binary class values (in this case they are ‘survived’ and ‘not survived’).\nWe will use Logistic Regression as the base learner which will we calibrate with the default settings, that is with sigmoid optimization of distribution values and by optimizing the CA.\nComparing the results with the uncalibrated Logistic Regression model we see that the calibrated model performs better.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Calibrated Learner Wraps another learner with probability calibration and decision threshold optimization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Base Learner: learner to calibrate  Outputs\n Learner: calibrated learning algorithm Model: trained model using the calibrated learner  This learner produces a model that calibrates the distribution of class probabilities and optimizes decision threshold. The widget works only for binary classification tasks.\n  The name under which it will appear in other widgets." ,
	"author" : "",
	"summary" : "Calibrated Learner Wraps another learner with probability calibration and decision threshold optimization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Base Learner: learner to calibrate  Outputs\n Learner: calibrated learning algorithm Model: trained model using the calibrated learner  This learner produces a model that calibrates the distribution of class probabilities and optimizes decision threshold. The widget works only for binary classification tasks.\n  The name under which it will appear in other widgets.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Calibrated Learner",
	"icon" : ""
},
{
    "uri": "/widget-catalog/evaluate/calibrationplot/",
	"title": "Calibration Plot",
	"description": "",
	"content": "Calibration Plot Shows the match between classifiers’ probability predictions and actual class probabilities.\nInputs\n Evaluation Results: results of testing classification algorithms  The Calibration Plotplots class probabilities against those predicted by the classifier(s).\n Select the desired target class from the drop down menu. Choose which classifiers to plot. The diagonal represents optimal behavior; the closer the classifier’s curve gets, the more accurate its prediction probabilities are. Thus we would use this widget to see whether a classifier is overly optimistic (gives predominantly positive results) or pessimistic (gives predominantly negative results). If Show rug is enabled, ticks are displayed at the bottom and the top of the graph, which represent negative and positive examples respectively. Their position corresponds to the classifier’s probability prediction and the color shows the classifier. At the bottom of the graph, the points to the left are those which are (correctly) assigned a low probability of the target class, and those to the right are incorrectly assigned high probabilities. At the top of the graph, the instances to the right are correctly assigned high probabilities and vice versa. Press Save Image if you want to save the created image to your computer in a .svg or .png format. Produce a report.  Example At the moment, the only widget which gives the right type of signal needed by the Calibration Plot is Test \u0026 Score. The Calibration Plot will hence always follow Test \u0026 Score and, since it has no outputs, no other widgets follow it.\nHere is a typical example, where we compare three classifiers (namely Naive Bayes, Tree and Constant) and input them into Test \u0026 Score. We used the Titanic dataset. Test \u0026 Score then displays evaluation results for each classifier. Then we draw Calibration Plot and ROC Analysis widgets from Test \u0026 Score to further analyze the performance of classifiers. Calibration Plot enables you to see prediction accuracy of class probabilities in a plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Calibration Plot Shows the match between classifiers\u0026rsquo; probability predictions and actual class probabilities.\nInputs\n Evaluation Results: results of testing classification algorithms  The Calibration Plotplots class probabilities against those predicted by the classifier(s).\n Select the desired target class from the drop down menu. Choose which classifiers to plot. The diagonal represents optimal behavior; the closer the classifier\u0026rsquo;s curve gets, the more accurate its prediction probabilities are. Thus we would use this widget to see whether a classifier is overly optimistic (gives predominantly positive results) or pessimistic (gives predominantly negative results)." ,
	"author" : "",
	"summary" : "Calibration Plot Shows the match between classifiers\u0026rsquo; probability predictions and actual class probabilities.\nInputs\n Evaluation Results: results of testing classification algorithms  The Calibration Plotplots class probabilities against those predicted by the classifier(s).\n Select the desired target class from the drop down menu. Choose which classifiers to plot. The diagonal represents optimal behavior; the closer the classifier\u0026rsquo;s curve gets, the more accurate its prediction probabilities are. Thus we would use this widget to see whether a classifier is overly optimistic (gives predominantly positive results) or pessimistic (gives predominantly negative results).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Calibration Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/geo/choroplethmap/",
	"title": "Choropleth Map",
	"description": "",
	"content": "Choropleth Map A thematic map in which areas are shaded in proportion to the measurement of the statistical variable being displayed.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the map. Data: data with an additional column showing whether a point is selected  Choropleth provides an easy way to visualize how a measurement varies across a geographic area or show the level of variability within a region. There are several levels of granularity available, from countries to states, counties, or municipalities.\n Set latitude and longitude attributes, if the widget didn’t recognize them automatically. Set Attribute to color the region by. Set Agg. which by default counts the number of occurrences of the region in the data. Count defined shows which regions appear in the data. Sum, Mean, Median, Maximal, Minimal and Std. (standard deviation) work for numeric data, while Mode works for categorical. Set Detail level to countries, states (US)/counties/Bundesländer/provinces or counties (US)/municipalities. Adjust plot properties:  Bin width for discretize displayed color. Opacity sets transparency of regions. Show legend displays a legend on the right. Click and drag the legend to move it.   Select, zoom, pan and zoom to fit are the options for exploring the map. The manual selection of data instances works as an angular/square selection tool. Scroll in or out for zoom. If Send automatically is ticked, changes are communicated automatically. Alternatively, press Send.  Example We will use HDI data from the Datasets widget. Open the widget, find HDI data and double click. Choropleth widget requires latitude and longitude pairs, so we will use Geocoding to extract this information. We used the attribute Country and found lat/lon pairs that Choropleth can use.\nChoropleth will automatically look for attributes named latitude, longitude, lat, lon or similar. It will use them for plotting. Alternatively, set the attributes manually.\nSince HDI attribute is our target variable, it will automatically be used for coloring. We change it in the Attribute dropdown to Life expectancy. We have set the level of aggregation to Mean, but since we have only one value per country, we could use Sum or Median just as well.\nThe widget shows life expectancy as reported by the United Nations per country. Yellow countries are those with a high Life expectancy and blue ones are the ones with a low life expectancy.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Choropleth Map A thematic map in which areas are shaded in proportion to the measurement of the statistical variable being displayed.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the map. Data: data with an additional column showing whether a point is selected  Choropleth provides an easy way to visualize how a measurement varies across a geographic area or show the level of variability within a region." ,
	"author" : "",
	"summary" : "Choropleth Map A thematic map in which areas are shaded in proportion to the measurement of the statistical variable being displayed.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the map. Data: data with an additional column showing whether a point is selected  Choropleth provides an easy way to visualize how a measurement varies across a geographic area or show the level of variability within a region.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Choropleth Map",
	"icon" : ""
},
{
    "uri": "/citation/",
	"title": "Citation",
	"description": "",
	"content": "If you are using Orange in your research, please cite:\nDemsar J, Curk T, Erjavec A, Gorup C, Hocevar T, Milutinovic M, Mozina M, Polajnar M, Toplak M, Staric A, Stajdohar M, Umek L, Zagar L, Zbontar J, Zitnik M, Zupan B (2013) Orange: Data Mining Toolbox in Python, Journal of Machine Learning Research 14(Aug): 2349−2353.\nBibTeX entry:\n @article{JMLR:demsar13a, author = {Janez Dem\\v{s}ar and Toma\\v{z} Curk and Ale\\v{s} Erjavec and \\v{C}rt Gorup and Toma\\v{z} Ho\\v{c}evar and Mitar Milutinovi\\v{c} and Martin Mo\\v{z}ina and Matija Polajnar and Marko Toplak and An\\v{z}e Stari\\v{c} and Miha \\v{S}tajdohar and Lan Umek and Lan \\v{Z}agar and Jure \\v{Z}bontar and Marinka \\v{Z}itnik and Bla\\v{z} Zupan}, title = {Orange: Data Mining Toolbox in Python}, journal = {Journal of Machine Learning Research}, year = {2013}, volume = {14}, pages = {2349-2353}, url = {http://jmlr.org/papers/v14/demsar13a.html} }  Orange is developed by Bioinformatics Lab at University of Ljubljana, Slovenia, in collaboration with the open source community. Orange is an open source project. If you include it within your programs, please comply with the license. Contact us if you would like to use Orange under other licenses.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "If you are using Orange in your research, please cite:\nDemsar J, Curk T, Erjavec A, Gorup C, Hocevar T, Milutinovic M, Mozina M, Polajnar M, Toplak M, Staric A, Stajdohar M, Umek L, Zagar L, Zbontar J, Zitnik M, Zupan B (2013) Orange: Data Mining Toolbox in Python, Journal of Machine Learning Research 14(Aug): 2349−2353.\nBibTeX entry:\n @article{JMLR:demsar13a, author = {Janez Dem\\v{s}ar and Toma\\v{z} Curk and Ale\\v{s} Erjavec and \\v{C}rt Gorup and Toma\\v{z} Ho\\v{c}evar and Mitar Milutinovi\\v{c} and Martin Mo\\v{z}ina and Matija Polajnar and Marko Toplak and An\\v{z}e Stari\\v{c} and Miha \\v{S}tajdohar and Lan Umek and Lan \\v{Z}agar and Jure \\v{Z}bontar and Marinka \\v{Z}itnik and Bla\\v{z} Zupan}, title = {Orange: Data Mining Toolbox in Python}, journal = {Journal of Machine Learning Research}, year = {2013}, volume = {14}, pages = {2349-2353}, url = {http://jmlr." ,
	"author" : "",
	"summary" : "If you are using Orange in your research, please cite:\nDemsar J, Curk T, Erjavec A, Gorup C, Hocevar T, Milutinovic M, Mozina M, Polajnar M, Toplak M, Staric A, Stajdohar M, Umek L, Zagar L, Zbontar J, Zitnik M, Zupan B (2013) Orange: Data Mining Toolbox in Python, Journal of Machine Learning Research 14(Aug): 2349−2353.\nBibTeX entry:\n @article{JMLR:demsar13a, author = {Janez Dem\\v{s}ar and Toma\\v{z} Curk and Ale\\v{s} Erjavec and \\v{C}rt Gorup and Toma\\v{z} Ho\\v{c}evar and Mitar Milutinovi\\v{c} and Martin Mo\\v{z}ina and Matija Polajnar and Marko Toplak and An\\v{z}e Stari\\v{c} and Miha \\v{S}tajdohar and Lan Umek and Lan \\v{Z}agar and Jure \\v{Z}bontar and Marinka \\v{Z}itnik and Bla\\v{z} Zupan}, title = {Orange: Data Mining Toolbox in Python}, journal = {Journal of Machine Learning Research}, year = {2013}, volume = {14}, pages = {2349-2353}, url = {http://jmlr.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "citation",
	"LinkTitle" : "Citation",
	"icon" : ""
},
{
    "uri": "/workflows/Classification/",
	"title": "Classification",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Classification",
	"icon" : ""
},
{
    "uri": "/workflows/Classification-Tree/",
	"title": "Classification Tree",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Classification Tree",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/cluster_analysis/",
	"title": "Cluster Analysis",
	"description": "",
	"content": "Cluster Analysis Display differentially expressed genes that characterize the cluster.\nInputs\n Data: Data set. Custom Gene Sets: Genes to compare.  Outputs\n Selected Data: Data selected in the widget.  Cluster Analysis widget displays differentially expressed genes that characterize the cluster, and corresponding gene terms that describe differentially expressed genes.\n Info Cluster Indicator Batch Indicator Gene Scoring Gene Sets Custom Gene Sets Filter Genes Filter Gene Sets  Example TODO: Example\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Cluster Analysis Display differentially expressed genes that characterize the cluster.\nInputs\n Data: Data set. Custom Gene Sets: Genes to compare.  Outputs\n Selected Data: Data selected in the widget.  Cluster Analysis widget displays differentially expressed genes that characterize the cluster, and corresponding gene terms that describe differentially expressed genes.\n Info Cluster Indicator Batch Indicator Gene Scoring Gene Sets Custom Gene Sets Filter Genes Filter Gene Sets  Example TODO: Example" ,
	"author" : "",
	"summary" : "Cluster Analysis Display differentially expressed genes that characterize the cluster.\nInputs\n Data: Data set. Custom Gene Sets: Genes to compare.  Outputs\n Selected Data: Data selected in the widget.  Cluster Analysis widget displays differentially expressed genes that characterize the cluster, and corresponding gene terms that describe differentially expressed genes.\n Info Cluster Indicator Batch Indicator Gene Scoring Gene Sets Custom Gene Sets Filter Genes Filter Gene Sets  Example TODO: Example",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Cluster Analysis",
	"icon" : ""
},
{
    "uri": "/workflows/Clustering/",
	"title": "Clustering",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Clustering",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/cn2ruleinduction/",
	"title": "CN2 Rule Induction",
	"description": "",
	"content": "CN2 Rule Induction Induce rules from data using CN2 algorithm.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: CN2 learning algorithm CN2 Rule Classifier: trained model  The CN2 algorithm is a classification technique designed for the efficient induction of simple, comprehensible rules of form “if cond then predict class”, even in domains where noise may be present.\nCN2 Rule Induction works only for classification.\n Name under which the learner appears in other widgets. The default name is CN2 Rule Induction. Rule ordering:  Ordered: induce ordered rules (decision list). Rule conditions are found and the majority class is assigned in the rule head. Unordered: induce unordered rules (rule set). Learn rules for each class individually, in regard to the original learning data.   Covering algorithm:  Exclusive: after covering a learning instance, remove it from further consideration. Weighted: after covering a learning instance, decrease its weight (multiplication by gamma) and in-turn decrease its impact on further iterations of the algorithm.   Rule search:  Evaluation measure: select a heuristic to evaluate found hypotheses:  Entropy (measure of unpredictability of content) Laplace Accuracy Weighted Relative Accuracy   Beam width; remember the best rule found thus far and monitor a fixed number of alternatives (the beam).   Rule filtering:  Minimum rule coverage: found rules must cover at least the minimum required number of covered examples. Unordered rules must cover this many target class examples. Maximum rule length: found rules may combine at most the maximum allowed number of selectors (conditions). Default alpha: significance testing to prune out most specialised (less frequently applicable) rules in regard to the initial distribution of classes. Parent alpha: significance testing to prune out most specialised (less frequently applicable) rules in regard to the parent class distribution.   Tick ‘Apply Automatically’ to auto-communicate changes to other widgets and to immediately train the classifier if learning data is connected. Alternatively, press ‘Apply‘ after configuration.  Examples For the example below, we have used zoo dataset and passed it to CN2 Rule Induction. We can review and interpret the built model with CN2 Rule Viewer widget.\nThe second workflow tests evaluates CN2 Rule Induction and Tree in Test \u0026 Score.\nReferences  Fürnkranz, Johannes. “Separate-and-Conquer Rule Learning”, Artificial Intelligence Review 13, 3-54, 1999. Clark, Peter and Tim Niblett. “The CN2 Induction Algorithm”, Machine Learning Journal, 3 (4), 261-283, 1989. Clark, Peter and Robin Boswell. “Rule Induction with CN2: Some Recent Improvements”, Machine Learning - Proceedings of the 5th European Conference (EWSL-91),151-163, 1991. Lavrač, Nada et al. “Subgroup Discovery with CN2-SD”,Journal of Machine Learning Research 5, 153-188, 2004  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "CN2 Rule Induction Induce rules from data using CN2 algorithm.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: CN2 learning algorithm CN2 Rule Classifier: trained model  The CN2 algorithm is a classification technique designed for the efficient induction of simple, comprehensible rules of form \u0026ldquo;if cond then predict class\u0026rdquo;, even in domains where noise may be present.\nCN2 Rule Induction works only for classification.\n Name under which the learner appears in other widgets." ,
	"author" : "",
	"summary" : "CN2 Rule Induction Induce rules from data using CN2 algorithm.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: CN2 learning algorithm CN2 Rule Classifier: trained model  The CN2 algorithm is a classification technique designed for the efficient induction of simple, comprehensible rules of form \u0026ldquo;if cond then predict class\u0026rdquo;, even in domains where noise may be present.\nCN2 Rule Induction works only for classification.\n Name under which the learner appears in other widgets.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "CN2 Rule Induction",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/cn2ruleviewer/",
	"title": "CN2 Rule Viewer",
	"description": "",
	"content": "CN2 Rule Viewer CN2 Rule Viewer\nInputs\n Data: dataset to filter CN2 Rule Classifier: CN2 Rule Classifier, including a list of induced rules  Outputs\n Filtered Data: data instances covered by all selected rules  A widget that displays CN2 classification rules. If data is also connected, upon rule selection, one can analyze which instances abide to the conditions.\n Original order of induced rules can be restored. When rules are many and complex, the view can appear packed. For this reason, compact view was implemented, which allows a flat presentation and a cleaner inspection of rules. Click Report to bring up a detailed description of the rule induction algorithm and its parameters, the data domain, and induced rules.  Additionally, upon selection, rules can be copied to clipboard by pressing the default system shortcut (ctrl+C, cmd+C).\nExamples In the schema below, the most common use of the widget is presented. First, the data is read and a CN2 rule classifier is trained. We are using titanic dataset for the rule construction. The rules are then viewed using the Rule Viewer. To explore different CN2 algorithms and understand how adjusting parameters influences the learning process, Rule Viewer should be kept open and in sight, while setting the CN2 learning algorithm (the presentation will be updated promptly).\nSelecting a rule outputs filtered data instances. These can be viewed in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "CN2 Rule Viewer CN2 Rule Viewer\nInputs\n Data: dataset to filter CN2 Rule Classifier: CN2 Rule Classifier, including a list of induced rules  Outputs\n Filtered Data: data instances covered by all selected rules  A widget that displays CN2 classification rules. If data is also connected, upon rule selection, one can analyze which instances abide to the conditions.\n Original order of induced rules can be restored. When rules are many and complex, the view can appear packed." ,
	"author" : "",
	"summary" : "CN2 Rule Viewer CN2 Rule Viewer\nInputs\n Data: dataset to filter CN2 Rule Classifier: CN2 Rule Classifier, including a list of induced rules  Outputs\n Filtered Data: data instances covered by all selected rules  A widget that displays CN2 classification rules. If data is also connected, upon rule selection, one can analyze which instances abide to the conditions.\n Original order of induced rules can be restored. When rules are many and complex, the view can appear packed.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "CN2 Rule Viewer",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/color/",
	"title": "Color",
	"description": "",
	"content": "Color Set color legend for variables.\nInputs\n Data: input data set  Outputs\n Data: data set with a new color legend  The Color widget sets the color legend for visualizations.\n A list of discrete variables. Set the color of each variable by double-clicking on it. The widget also enables renaming variables by clicking on their names. A list of continuous variables. Click on the color strip to choose a different palette. To use the same palette for all variables, change it for one variable and click Copy to all that appears on the right. The widget also enables renaming variables by clicking on their names. Produce a report. Apply changes. If Apply automatically is ticked, changes will be communicated automatically. Alternatively, just click Apply.  Palettes for numeric variables are grouped and tagged by their properties.\n  Diverging palettes have two colors on its ends and a central color (white or black) in the middle. Such palettes are particularly useful when the the values can be positive or negative, as some widgets (for instance the Heat map) will put the 0 at the middle point in the palette.\n  Linear palettes are constructed so that human perception of the color change is linear with the change of the value.\n  Color blind palettes cover different types of color blindness, and can also be linear or diverging.\n  In isoluminant palettes, all colors have equal brightness.\n  Rainbow palettes are particularly nice in widgets that bin numeric values in visualizations.\n  Example We chose to work with the heart_disease data set. We opened the color palette and selected two new colors for diameter narrowing variable. Then we opened the Scatter Plot widget and viewed the changes made to the scatter plot.\nTo see the effect of color palettes for numeric variables, we color the points in the scatter plot by cholesterol and change the palette for this attribute in the Color widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Color Set color legend for variables.\nInputs\n Data: input data set  Outputs\n Data: data set with a new color legend  The Color widget sets the color legend for visualizations.\n A list of discrete variables. Set the color of each variable by double-clicking on it. The widget also enables renaming variables by clicking on their names. A list of continuous variables. Click on the color strip to choose a different palette." ,
	"author" : "",
	"summary" : "Color Set color legend for variables.\nInputs\n Data: input data set  Outputs\n Data: data set with a new color legend  The Color widget sets the color legend for visualizations.\n A list of discrete variables. Set the color of each variable by double-clicking on it. The widget also enables renaming variables by clicking on their names. A list of continuous variables. Click on the color strip to choose a different palette.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Color",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/concatenate/",
	"title": "Concatenate",
	"description": "",
	"content": "Concatenate Concatenates data from multiple sources.\nInputs\n Primary Data: data set that defines the attribute set Additional Data: additional data set  Outputs\n Data: concatenated data  The widget concatenates multiple sets of instances (data sets). The merge is “vertical”, in a sense that two sets of 10 and 5 instances yield a new set of 15 instances.\n Set the attribute merging method. Add the identification of source data sets to the output data set. Produce a report. If Apply automatically is ticked, changes are communicated automatically. Otherwise, click Apply.  If one of the tables is connected to the widget as the primary table, the resulting table will contain its own attributes. If there is no primary table, the attributes can be either a union of all attributes that appear in the tables specified as Additional Tables, or their intersection, that is, a list of attributes common to all the connected tables.\nExample As shown below, the widget can be used for merging data from two separate files. Let’s say we have two data sets with the same attributes, one containing instances from the first experiment and the other instances from the second experiment and we wish to join the two data tables together. We use the Concatenate widget to merge the data sets by attributes (appending new rows under existing attributes).\nBelow, we used a modified Zoo data set. In the first File widget, we loaded only the animals beginning with the letters A and B and in the second one only the animals beginning with the letter C. Upon concatenation, we observe the new data in the Data Table widget, where we see the complete table with animals from A to C.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Concatenate Concatenates data from multiple sources.\nInputs\n Primary Data: data set that defines the attribute set Additional Data: additional data set  Outputs\n Data: concatenated data  The widget concatenates multiple sets of instances (data sets). The merge is “vertical”, in a sense that two sets of 10 and 5 instances yield a new set of 15 instances.\n Set the attribute merging method. Add the identification of source data sets to the output data set." ,
	"author" : "",
	"summary" : "Concatenate Concatenates data from multiple sources.\nInputs\n Primary Data: data set that defines the attribute set Additional Data: additional data set  Outputs\n Data: concatenated data  The widget concatenates multiple sets of instances (data sets). The merge is “vertical”, in a sense that two sets of 10 and 5 instances yield a new set of 15 instances.\n Set the attribute merging method. Add the identification of source data sets to the output data set.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Concatenate",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/concordance/",
	"title": "Concordance",
	"description": "",
	"content": "Concordance Display the context of the word.\nInputs\n Corpus: A collection of documents.  Outputs\n Selected Documents: Documents containing the queried word. Concordances: A table of concordances.  Concordance finds the queried word in a text and displays the context in which this word is used. Results in a single color come from the same document. The widget can output selected documents for further analysis or a table of concordances for the queried word. Note that the widget finds only exact matches of a word, which means that if you query the word ‘do’, the word ‘doctor’ won’t appear in the results.\n Information:  Documents: number of documents on the input. Tokens: number of tokens on the input. Types: number of unique tokens on the input. Matching: number of documents containing the queried word.   Number of words: the number of words displayed on each side of the queried word. Queried word. If Auto commit is on, selected documents are communicated automatically. Alternatively press Commit.  Examples Concordance can be used for displaying word contexts in a corpus. First, we load book-excerpts.tab in Corpus. Then we connect Corpus to Concordance and search for concordances of a word ‘doctor’. The widget displays all documents containing the word ‘doctor’ together with their surrounding (contextual) words.\nNow we can select those documents that contain interesting contexts and output them to Corpus Viewer to inspect them further.\nIn the second example, we will output concordances instead. We will keep the book-excerpts.tab in Corpus and the connection to Concordance. Our queried word remains ‘doctor’.\nThis time, we will connect Data Table to Concordance and select Concordances output instead. In the Data Table, we get a list of concordances for the queried word and the corresponding documents. Now, we will save this table with Save Data widget, so we can use it in other projects or for further analysis.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Concordance Display the context of the word.\nInputs\n Corpus: A collection of documents.  Outputs\n Selected Documents: Documents containing the queried word. Concordances: A table of concordances.  Concordance finds the queried word in a text and displays the context in which this word is used. Results in a single color come from the same document. The widget can output selected documents for further analysis or a table of concordances for the queried word." ,
	"author" : "",
	"summary" : "Concordance Display the context of the word.\nInputs\n Corpus: A collection of documents.  Outputs\n Selected Documents: Documents containing the queried word. Concordances: A table of concordances.  Concordance finds the queried word in a text and displays the context in which this word is used. Results in a single color come from the same document. The widget can output selected documents for further analysis or a table of concordances for the queried word.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Concordance",
	"icon" : ""
},
{
    "uri": "/workflows/Confusion-Matrix/",
	"title": "Confusion Matrix",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Confusion Matrix",
	"icon" : ""
},
{
    "uri": "/widget-catalog/evaluate/confusionmatrix/",
	"title": "Confusion Matrix",
	"description": "",
	"content": "Confusion Matrix Shows proportions between the predicted and actual class.\nInputs\n Evaluation results: results of testing classification algorithms  Outputs\n Selected Data: data subset selected from confusion matrix Data: data with the additional information on whether a data instance was selected  The Confusion Matrix gives the number/proportion of instances between the predicted and actual class. The selection of the elements in the matrix feeds the corresponding instances into the output signal. This way, one can observe which specific instances were misclassified and how.\nThe widget usually gets the evaluation results from Test \u0026 Score; an example of the schema is shown below.\n When evaluation results contain data on multiple learning algorithms, we have to choose one in the Learners box. The snapshot shows the confusion matrix for Tree and Naive Bayesian models trained and tested on the iris data. The right-hand side of the widget contains the matrix for the naive Bayesian model (since this model is selected on the left). Each row corresponds to a correct class, while columns represent the predicted classes. For instance, four instances of Iris-versicolor were misclassified as Iris-virginica. The rightmost column gives the number of instances from each class (there are 50 irises of each of the three classes) and the bottom row gives the number of instances classified into each class (e.g., 48 instances were classified into virginica). In Show, we select what data we would like to see in the matrix.  Number of instances shows correctly and incorrectly classified instances numerically. Proportions of predicted shows how many instances classified as, say, Iris-versicolor are in which true class; in the table we can read the 0% of them are actually setosae, 88.5% of those classified as versicolor are versicolors, and 7.7% are virginicae. Proportions of actual shows the opposite relation: of all true versicolors, 92% were classified as versicolors and 8% as virginicae.    In Select, you can choose the desired output.  Correct sends all correctly classified instances to the output by selecting the diagonal of the matrix. Misclassified selects the misclassified instances. None annuls the selection. As mentioned before, one can also select individual cells of the table to select specific kinds of misclassified instances (e.g. the versicolors classified as virginicae).   When sending selected instances, the widget can add new attributes, such as predicted classes or their probabilities, if the corresponding options Predictions and/or Probabilities are checked. The widget outputs every change if Send Automatically is ticked. If not, the user will need to click Send Selected to commit the changes. Produce a report.  Example The following workflow demonstrates what this widget can be used for.\nTest \u0026 Score gets the data from File and two learning algorithms from Naive Bayes and Tree. It performs cross-validation or some other train-and-test procedures to get class predictions by both algorithms for all (or some) data instances. The test results are fed into the Confusion Matrix, where we can observe how many instances were misclassified and in which way.\nIn the output, we used Data Table to show the instances we selected in the confusion matrix. If we, for instance, click Misclassified, the table will contain all instances which were misclassified by the selected method.\nThe Scatter Plot gets two sets of data. From the File widget it gets the complete data, while the confusion matrix sends only the selected data, misclassifications for instance. The scatter plot will show all the data, with bold symbols representing the selected data.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Confusion Matrix Shows proportions between the predicted and actual class.\nInputs\n Evaluation results: results of testing classification algorithms  Outputs\n Selected Data: data subset selected from confusion matrix Data: data with the additional information on whether a data instance was selected  The Confusion Matrix gives the number/proportion of instances between the predicted and actual class. The selection of the elements in the matrix feeds the corresponding instances into the output signal." ,
	"author" : "",
	"summary" : "Confusion Matrix Shows proportions between the predicted and actual class.\nInputs\n Evaluation results: results of testing classification algorithms  Outputs\n Selected Data: data subset selected from confusion matrix Data: data with the additional information on whether a data instance was selected  The Confusion Matrix gives the number/proportion of instances between the predicted and actual class. The selection of the elements in the matrix feeds the corresponding instances into the output signal.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Confusion Matrix",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/constant/",
	"title": "Constant",
	"description": "",
	"content": "Constant Predict the most frequent class or mean value from the training set.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: majority/mean learning algorithm Model: trained model  This learner produces a model that always predicts themajority for classification tasks and mean value for regression tasks.\nFor classification, when predicting the class value with Predictions, the widget will return relative frequencies of the classes in the training set. When there are two or more majority classes, the classifier chooses the predicted class randomly, but always returns the same class for a particular example.\nFor regression, it learns the mean of the class variable and returns a predictor with the same mean value.\nThe widget is typically used as a baseline for other models.\nThis widget provides the user with two options:\n The name under which it will appear in other widgets. Default name is “Constant”. Produce a report.  If you change the widget’s name, you need to click Apply. Alternatively, tick the box on the left side and changes will be communicated automatically.\nExamples In a typical classification example, we would use this widget to compare the scores of other learning algorithms (such as kNN) with the default scores. Use iris dataset and connect it to Test \u0026 Score. Then connect Constant and kNN to Test \u0026 Score and observe how well kNN performs against a constant baseline.\nFor regression, we use Constant to construct a predictor in Predictions. We used the housing dataset. In Predictions, you can see that Mean Learner returns one (mean) value for all instances.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Constant Predict the most frequent class or mean value from the training set.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: majority/mean learning algorithm Model: trained model  This learner produces a model that always predicts themajority for classification tasks and mean value for regression tasks.\nFor classification, when predicting the class value with Predictions, the widget will return relative frequencies of the classes in the training set." ,
	"author" : "",
	"summary" : "Constant Predict the most frequent class or mean value from the training set.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: majority/mean learning algorithm Model: trained model  This learner produces a model that always predicts themajority for classification tasks and mean value for regression tasks.\nFor classification, when predicting the class value with Predictions, the widget will return relative frequencies of the classes in the training set.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Constant",
	"icon" : ""
},
{
    "uri": "/contact/",
	"title": "Contact",
	"description": "",
	"content": "Perhaps we have already answered your question in the FAQ. If the answer isn’t there, feel free to write to us.\n We prefer to address any support requests and other general questions about Orange in our Discord chatroom.\nPlease report bugs, issues, and anything unexpected on our GitHub issue tracker.\nAlternatively, for questions regarding the graphical user interface, you may consult Data Science Stack Exchange. For questions on the scripting layer (Python), please consult Stack Overflow.\n For other inquiries of professional nature, such as business proposals, reach us directly through the form below.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Perhaps we have already answered your question in the FAQ. If the answer isn’t there, feel free to write to us.\n We prefer to address any support requests and other general questions about Orange in our Discord chatroom.\nPlease report bugs, issues, and anything unexpected on our GitHub issue tracker.\nAlternatively, for questions regarding the graphical user interface, you may consult Data Science Stack Exchange. For questions on the scripting layer (Python), please consult Stack Overflow." ,
	"author" : "",
	"summary" : "Perhaps we have already answered your question in the FAQ. If the answer isn’t there, feel free to write to us.\n We prefer to address any support requests and other general questions about Orange in our Discord chatroom.\nPlease report bugs, issues, and anything unexpected on our GitHub issue tracker.\nAlternatively, for questions regarding the graphical user interface, you may consult Data Science Stack Exchange. For questions on the scripting layer (Python), please consult Stack Overflow.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "contact",
	"LinkTitle" : "Contact",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/continuize/",
	"title": "Continuize",
	"description": "",
	"content": "Continuize Turns discrete variables (attributes) into numeric (“continuous”) dummy variables.\nInputs\n Data: input data set  Outputs\n Data: transformed data set  The Continuize widget receives a data set in the input and outputs the same data set in which the discrete variables (including binary variables) are replaced with continuous ones.\n  Define the treatment of non-binary categorical variables.\nExamples in this section will assume that we have a discrete attribute status with the values low, middle and high, listed in that order. Options for their transformation are:\n  First value as base: a N-valued categorical variable will be transformed into N-1 numeric variables, each serving as an indicator for one of the original values except for the base value. The base value is the first value in the list. By default, the values are ordered alphabetically; their order can be changed in Edit Domain.\nIn the above case, the three-valued variable status is transformed into two numeric variables, status=middle with values 0 or 1 indicating whether the original variable had value middle on a particular example, and similarly, status=high.\n  Most frequent value as base: similar to the above, except that the most frequent value is used as a base. So, if the most frequent value in the above example is middle, then middle is considered as the base and the two newly constructed variables are status=low and status=high.\n  One attribute per value: this option constructs one numeric variable per each value of the original variable. In the above case, we would get variables status=low, status=middle and status=high.\n  Ignore multinomial attributes: removes non-binary categorical variables from the data.\n  Treat as ordinal: converts the variable into a single numeric variable enumerating the original values. In the above case, the new variable would have the value of 0 for low, 1 for middle and 2 for high. Again note that the order of values can be set in Edit Domain.\n  Divide by number of values: same as above, except that values are normalized into range 0-1. In our example, the values of the new variable would be 0, 0.5 and 1.\n    Define the treatment of continuous attributes. Besised the option to Leave them as they are, we can Normalize by span, which will subtract the lowest value found in the data and divide by the span, so all values will fit into [0, 1]. Option Normalize by standard deviation subtracts the average and divides by the standard deviation.\n  Define the treatment of class attributes (outcomes, targets). Besides leaving it as it is, the available options mirror those for multinomial attributes, except for those that would split the outcome into multiple outcome variables.\n  This option defines the ranges of new variables. In the above text, we supposed the range from 0 to 1.\n  Produce a report.\n  If Apply automatically is ticked, changes are committed automatically. Otherwise, you have to press Apply after each change.\n  Examples First, let’s see what is the output of the Continuize widget. We feed the original data (the Heart disease data set) into the Data Table and see how they look like. Then we continuize the discrete values and observe them in another Data Table.\nIn the second example, we show a typical use of this widget - in order to properly plot the linear projection of the data, discrete attributes need to be converted to continuous ones and that is why we put the data through the Continuize widget before drawing it. The attribute “chest pain” originally had four values and was transformed into three continuous attributes; similar happened to gender, which was transformed into a single attribute “gender=female”.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Continuize Turns discrete variables (attributes) into numeric (\u0026ldquo;continuous\u0026rdquo;) dummy variables.\nInputs\n Data: input data set  Outputs\n Data: transformed data set  The Continuize widget receives a data set in the input and outputs the same data set in which the discrete variables (including binary variables) are replaced with continuous ones.\n  Define the treatment of non-binary categorical variables.\nExamples in this section will assume that we have a discrete attribute status with the values low, middle and high, listed in that order." ,
	"author" : "",
	"summary" : "Continuize Turns discrete variables (attributes) into numeric (\u0026ldquo;continuous\u0026rdquo;) dummy variables.\nInputs\n Data: input data set  Outputs\n Data: transformed data set  The Continuize widget receives a data set in the input and outputs the same data set in which the discrete variables (including binary variables) are replaced with continuous ones.\n  Define the treatment of non-binary categorical variables.\nExamples in this section will assume that we have a discrete attribute status with the values low, middle and high, listed in that order.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Continuize",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/corpus-widget/",
	"title": "Corpus",
	"description": "",
	"content": "Corpus Load a corpus of text documents, (optionally) tagged with categories, or change the data input signal to the corpus.\nInputs\n Data: Input data (optional)  Outputs\n Corpus: A collection of documents.  Corpus widget can work in two modes:\n When no data on input, it reads text corpora from files and sends a corpus instance to its output channel. History of the most recently opened files is maintained in the widget. The widget also includes a directory with sample corpora that come pre-installed with the add-on. The widget reads data from Excel (.xlsx), comma-separated (.csv) and native tab-delimited (.tab) files. When the user provides data to the input, it transforms data into the corpus. Users can select which features are used as text features.   Browse through previously opened data files, or load any of the sample ones. Browse for a data file. Reloads currently selected data file. Select the variable that is shown as a document title in Corpus Viewer. Features that will be used in text analysis. Features that won’t be used in text analysis. Browse through the datasets that come together with an add-on. Access help, make a report and get information on the loaded data set.  You can drag and drop features between the two boxes and also change the order in which they appear.\nExample The first example shows a very simple use of Corpus widget. Place Corpus onto canvas and connect it to Corpus Viewer. We’ve used book-excerpts.tab data set, which comes with the add-on, and inspected it in Corpus Viewer.\nThe second example demonstrates how to quickly visualize your corpus with Word Cloud. We could connect Word Cloud directly to Corpus, but instead, we decided to apply some preprocessing with Preprocess Text. We are again working with book-excerpts.tab. We’ve put all text to lowercase, tokenized (split) the text to words only, filtered out English stopwords and selected 100 most frequent tokens.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Corpus Load a corpus of text documents, (optionally) tagged with categories, or change the data input signal to the corpus.\nInputs\n Data: Input data (optional)  Outputs\n Corpus: A collection of documents.  Corpus widget can work in two modes:\n When no data on input, it reads text corpora from files and sends a corpus instance to its output channel. History of the most recently opened files is maintained in the widget." ,
	"author" : "",
	"summary" : "Corpus Load a corpus of text documents, (optionally) tagged with categories, or change the data input signal to the corpus.\nInputs\n Data: Input data (optional)  Outputs\n Corpus: A collection of documents.  Corpus widget can work in two modes:\n When no data on input, it reads text corpora from files and sends a corpus instance to its output channel. History of the most recently opened files is maintained in the widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Corpus",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/corpustonetwork/",
	"title": "Corpus to Network",
	"description": "",
	"content": "Corpus to Network Creates a network from given corpus. Network nodes can be either documents or words (ngrams).\nInputs\n Corpus: A collection of documents.  Outputs\n Network: A network generated from input corpus. Node data: Additional data about nodes.  Corpus to Network can operate either on documents or words (ngrams). If nodes are documents, there’s an edge between two documents if number of words (ngrams) that appears in both documents is at least Threshold. If nodes are words (ngrams), there’s an edge between two words if the number of times they both appear inside of a window (of size 2 * Window size + 1) is at least Threshold. Only words that have frequency higher than Frequency Threshold will be included as nodes. This is a word co-occurrence network.\n Widget parameters:  Node type: controls whether nodes will be documents or words (ngrams) Threshold: controls threshold needed for defining the edge between two nodes (see explanation above) Window size: controls window size (see explanation above), applicable only if Node type is set to Word Frequency Threshold: controls threshold for word frequency (see explanation above), applicable only if Node type is set to Word   Button for starting computation after stopping it or changing parameters while the widget is running. If clicked, it is replaced with Stop button.  Examples In first example, we will inspect how the widget works. Load book-excerpts.tab using Corpus widget and connect it to Corpus to Network. We see that the input contains 140 documents and the output is network with 140 nodes and 9730 edges. This is a complete graph, which is expected because the Threshold parameter is set to 1 (it is expected that each document shares at least 1 word). Change it a little to see how the number of edges changes.\nLet’s now create a word co-occurrence network and visualize it. Load book-excerpts.tab using Corpus widget, connect it to Preprocess Text with default parameters and connect Preprocess Text to Corpus to Network. Set Node type to Word, Threshold to 10, Window size to 3 and Frequency Threshold to 200 and press Start. The output network now contains only words (ngrams) with frequency over 200. The edges between those words are created if the words co-occur in at least 10 windows of width 7 (2 * Window size + 1). Connect Corpus to Network widget to Network Explorer, double click on a connection between them and connect Node data to Node data. Now open the Network Explorer widget, set Label to word to annotate the nodes and Size to word_frequency to make node sizes correspond to their frequencies. We get a nice visualization of most frequent words in corpus and their connections.\nIn Document Embedding widget, we saw how we can predict document category using it. Let’s now try to improve the score even further by adding features obtained from network. We will keep working on book-excerpts.tab loaded with Corpus widget and sent through Preprocess Text with default parameters. Connect Preprocess Text to Document Embedding to obtain features for predictive modelling. Here we set aggregator to Sum.\nThe first part was the same. Let’s now obtain some features from network. Connect Document Embedding widget to Corpus to Network, set Node type to Document, Threshold to 50 and press Start. Connect Corpus to Network to Network Analysis widget. Double click on the connection and connect Node data to Items so that the output data contains previously obtained embedding features. Open Network Analysis widget and uncheck everything under the Graph-level indices tab and check everything under the Node-level indices tab. You can connect Data Table widget to inspect the output.\nNow connect Network Analysis widget to Test and Score and also connect learner of choice to the left side of Test and Score. We chose SVM and changed kernel to Linear. Test and Score will now compute performance of each learner on the input. We can see that we obtained even better results by adding network features. Let’s connect Test and Score to Confusion Matrix. New features helped us correctly classify two more examples.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Corpus to Network Creates a network from given corpus. Network nodes can be either documents or words (ngrams).\nInputs\n Corpus: A collection of documents.  Outputs\n Network: A network generated from input corpus. Node data: Additional data about nodes.  Corpus to Network can operate either on documents or words (ngrams). If nodes are documents, there\u0026rsquo;s an edge between two documents if number of words (ngrams) that appears in both documents is at least Threshold." ,
	"author" : "",
	"summary" : "Corpus to Network Creates a network from given corpus. Network nodes can be either documents or words (ngrams).\nInputs\n Corpus: A collection of documents.  Outputs\n Network: A network generated from input corpus. Node data: Additional data about nodes.  Corpus to Network can operate either on documents or words (ngrams). If nodes are documents, there\u0026rsquo;s an edge between two documents if number of words (ngrams) that appears in both documents is at least Threshold.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Corpus to Network",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/corpusviewer/",
	"title": "Corpus Viewer",
	"description": "",
	"content": "Corpus Viewer Displays corpus content.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Documents containing the queried word.  Corpus Viewer is meant for viewing text files (instances of Corpus). It will always output an instance of corpus. If RegExp filtering is used, the widget will output only matching documents.\n Information:  Documents: number of documents on the input Preprocessed: if preprocessor is used, the result is True, else False. Reports also on the number of tokens and types (unique tokens). POS tagged: if POS tags are on the input, the result is True, else False. N-grams range: if N-grams are set in Preprocess Text, results are reported, default is 1-1 (one-grams). Matching: number of documents matching the RegExp Filter. All documents are output by default.   RegExp Filter: Python regular expression for filtering documents. By default no documents are filtered (entire corpus is on the output). Search Features: features by which the RegExp Filter is filtering. Use Ctrl (Cmd) to select multiple features. Display Features: features that are displayed in the viewer. Use Ctrl (Cmd) to select multiple features. Show Tokens \u0026 Tags: if tokens and POS tag are present on the input, you can check this box to display them. If Auto commit is on, changes are communicated automatically. Alternatively press Commit.  Example Corpus Viewer can be used for displaying all or some documents in corpus. In this example, we will first load book-excerpts.tab, that already comes with the add-on, into Corpus widget. Then we will preprocess the text into words, filter out the stopwords, create bi-grams and add POS tags (more on preprocessing in Preprocess Text. Now we want to see the results of preprocessing. In Corpus Viewer we can see, how many unique tokens we got and what they are (tick Show Tokens \u0026 Tags). Since we used also POS tagger to show part-of-speech labels, they will be displayed alongside tokens underneath the text.\nNow we will filter out just the documents talking about a character Bill. We use regular expression \\bBill\\b to find the documents containing only the word Bill. You can output matching or non-matching documents, view them in another Corpus Viewer or further analyse them.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Corpus Viewer Displays corpus content.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Documents containing the queried word.  Corpus Viewer is meant for viewing text files (instances of Corpus). It will always output an instance of corpus. If RegExp filtering is used, the widget will output only matching documents.\n Information:  Documents: number of documents on the input Preprocessed: if preprocessor is used, the result is True, else False." ,
	"author" : "",
	"summary" : "Corpus Viewer Displays corpus content.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Documents containing the queried word.  Corpus Viewer is meant for viewing text files (instances of Corpus). It will always output an instance of corpus. If RegExp filtering is used, the widget will output only matching documents.\n Information:  Documents: number of documents on the input Preprocessed: if preprocessor is used, the result is True, else False.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Corpus Viewer",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/correlations/",
	"title": "Correlations",
	"description": "",
	"content": "Correlations Compute all pairwise attribute correlations.\nInputs\n Data: input dataset  Outputs\n Data: input dataset Features: selected pair of features Correlations: data table with correlation scores  Correlations computes Pearson or Spearman correlation scores for all pairs of features in a dataset. These methods can only detect monotonic relationship.\n Correlation measure:  Pairwise Pearson correlation. Pairwise Spearman correlation.   Filter for finding attribute pairs. A list of attribute pairs with correlation coefficient. Press Finished to stop computation for large datasets. Access widget help and produce report.  Example Correlations can be computed only for numeric (continuous) features, so we will use housing as an example data set. Load it in the File widget and connect it to Correlations. Positively correlated feature pairs will be at the top of the list and negatively correlated will be at the bottom.\nGo to the most negatively correlated pair, DIS-NOX. Now connect Scatter Plot to Correlations and set two outputs, Data to Data and Features to Features. Observe how the feature pair is immediately set in the scatter plot. Looks like the two features are indeed negatively correlated.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Correlations Compute all pairwise attribute correlations.\nInputs\n Data: input dataset  Outputs\n Data: input dataset Features: selected pair of features Correlations: data table with correlation scores  Correlations computes Pearson or Spearman correlation scores for all pairs of features in a dataset. These methods can only detect monotonic relationship.\n Correlation measure:  Pairwise Pearson correlation. Pairwise Spearman correlation.   Filter for finding attribute pairs. A list of attribute pairs with correlation coefficient." ,
	"author" : "",
	"summary" : "Correlations Compute all pairwise attribute correlations.\nInputs\n Data: input dataset  Outputs\n Data: input dataset Features: selected pair of features Correlations: data table with correlation scores  Correlations computes Pearson or Spearman correlation scores for all pairs of features in a dataset. These methods can only detect monotonic relationship.\n Correlation measure:  Pairwise Pearson correlation. Pairwise Spearman correlation.   Filter for finding attribute pairs. A list of attribute pairs with correlation coefficient.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Correlations",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/correlogram/",
	"title": "Correlogram",
	"description": "",
	"content": "Correlogram Visualize variables’ auto-correlation.\nInputs\n Time series: Time series as output by As Timeseries widget.  In this widget, you can visualize the autocorrelation coefficients for the selected time series.\n Select the series to calculate autocorrelation for. See the autocorrelation coefficients. Choose to calculate the coefficients using partial autocorrelation function (PACF) instead. Choose to plot the 95% significance interval (dotted horizontal line). Coefficients that are outside of this interval might be significant.  See also Periodogram\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Correlogram Visualize variables\u0026rsquo; auto-correlation.\nInputs\n Time series: Time series as output by As Timeseries widget.  In this widget, you can visualize the autocorrelation coefficients for the selected time series.\n Select the series to calculate autocorrelation for. See the autocorrelation coefficients. Choose to calculate the coefficients using partial autocorrelation function (PACF) instead. Choose to plot the 95% significance interval (dotted horizontal line). Coefficients that are outside of this interval might be significant." ,
	"author" : "",
	"summary" : "Correlogram Visualize variables\u0026rsquo; auto-correlation.\nInputs\n Time series: Time series as output by As Timeseries widget.  In this widget, you can visualize the autocorrelation coefficients for the selected time series.\n Select the series to calculate autocorrelation for. See the autocorrelation coefficients. Choose to calculate the coefficients using partial autocorrelation function (PACF) instead. Choose to plot the 95% significance interval (dotted horizontal line). Coefficients that are outside of this interval might be significant.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Correlogram",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/correspondenceanalysis/",
	"title": "Correspondence Analysis",
	"description": "",
	"content": "Correspondence Analysis Correspondence analysis for categorical multivariate data.\nInputs\n Data: input dataset  Outputs\n Coordinates: coordinates of all components  Correspondence Analysis (CA) computes the CA linear transformation of the input data. While it is similar to PCA, CA computes linear transformation on discrete rather than on continuous data.\n Select the variables you want to see plotted. Select the component for each axis. Inertia values (percentage of independence from transformation, i.e. variables are in the same dimension). Produce a report.  Example Below, is a simple comparison between the Correspondence Analysis and Scatter Plot widgets on the Titanic dataset. While the Scatter Plot shows fairly well which class and sex had a good survival rate and which one didn’t, Correspondence Analysis can plot several variables in a 2-D graph, thus making it easy to see the relations between variable values. It is clear from the graph that “no”, “male” and “crew” are related to each other. The same goes for “yes”, “female” and “first”.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Correspondence Analysis Correspondence analysis for categorical multivariate data.\nInputs\n Data: input dataset  Outputs\n Coordinates: coordinates of all components  Correspondence Analysis (CA) computes the CA linear transformation of the input data. While it is similar to PCA, CA computes linear transformation on discrete rather than on continuous data.\n Select the variables you want to see plotted. Select the component for each axis. Inertia values (percentage of independence from transformation, i." ,
	"author" : "",
	"summary" : "Correspondence Analysis Correspondence analysis for categorical multivariate data.\nInputs\n Data: input dataset  Outputs\n Coordinates: coordinates of all components  Correspondence Analysis (CA) computes the CA linear transformation of the input data. While it is similar to PCA, CA computes linear transformation on discrete rather than on continuous data.\n Select the variables you want to see plotted. Select the component for each axis. Inertia values (percentage of independence from transformation, i.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Correspondence Analysis",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/createclass/",
	"title": "Create Class",
	"description": "",
	"content": "Create Class Create class attribute from a string attribute.\nInputs\n Data: input dataset  Outputs\n Data: dataset with a new class variable  Create Class creates a new class attribute from an existing discrete or string attribute. The widget matches the string value of the selected attribute and constructs a new user-defined value for matching instances.\n The attribute the new class is constructed from. Matching:  Name: the name of the new class value Substring: regex-defined substring that will match the values from the above-defined attribute Instances: the number of instances matching the substring Press ‘+’ to add a new class value   Name of the new class column. Match only at the beginning will begin matching from the beginning of the string. Case sensitive will match by case, too. Produce a report. Press Apply to commit the results.  Example Here is a simple example with the auto-mpg dataset. Pass the data to Create Class. Select car_name as a column to create the new class from. Here, we wish to create new values that match the car brand. First, we type ford as the new value for the matching strings. Then we define the substring that will match the data instances. This means that all instances containing ford in their car_name, will now have a value ford in the new class column. Next, we define the same for honda and fiat. The widget will tell us how many instance are yet unmatched (remaining instances). We will name them other, but you can continue creating new values by adding a condition with ‘+’.\nWe named our new class column car_brand and we matched at the beginning of the string.\nFinally, we can observe the new column in a Data Table or use the value as color in the Scatter Plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Create Class Create class attribute from a string attribute.\nInputs\n Data: input dataset  Outputs\n Data: dataset with a new class variable  Create Class creates a new class attribute from an existing discrete or string attribute. The widget matches the string value of the selected attribute and constructs a new user-defined value for matching instances.\n The attribute the new class is constructed from. Matching:  Name: the name of the new class value Substring: regex-defined substring that will match the values from the above-defined attribute Instances: the number of instances matching the substring Press \u0026lsquo;+\u0026rsquo; to add a new class value   Name of the new class column." ,
	"author" : "",
	"summary" : "Create Class Create class attribute from a string attribute.\nInputs\n Data: input dataset  Outputs\n Data: dataset with a new class variable  Create Class creates a new class attribute from an existing discrete or string attribute. The widget matches the string value of the selected attribute and constructs a new user-defined value for matching instances.\n The attribute the new class is constructed from. Matching:  Name: the name of the new class value Substring: regex-defined substring that will match the values from the above-defined attribute Instances: the number of instances matching the substring Press \u0026lsquo;+\u0026rsquo; to add a new class value   Name of the new class column.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Create Class",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/createinstance/",
	"title": "Create Instance",
	"description": "",
	"content": "Create Instance Interactively creates an instance from a sample dataset.\nInputs\n Data: input dataset Reference: refrence dataset  Outputs\n Data: input dataset appended the created instance  The Create Instance widget creates a new instance, based on the input data. The widget displays all variables of the input dataset in a table of two columns. The column Variable represents the variable’s name, meanwhile the column Value enables setting the variable’s value. Each value is initially set to median value of the variable. The values can be manually set to Median, Mean, Random or Input by clicking the corresponding button. For easier searching through the variables, the table has filter attached. When clicking upon one of the mentioned buttons, only filtered variables are considered. One can also set the value by right-clicking a row and selecting an option in a context menu.\n Filter table by variable name. The column represents a variable’s name and type. The table can be sorted by clicking the columns header. Provides controls for value editing. Set filtered variables’ values to:  Median: median value of variable in the input dataset Mean: mean value of variable in the input dataset Random: random value in a range of variable in the input dataset Input: median value of variable in the reference dataset   If Append this instance to input data is ticked, the created instance is appended to the input dataset. Otherwise, a single instance appears on the output. To distinguish between created and original data, Source ID variable is added. If Apply automatically is ticked, changes are committed automatically. Otherwise, you have to press Apply after each change. Produce a report. Information on input and reference dataset. Information on output dataset.  Example The Create Instance is usually used to examine a model performance on some arbitrary data. The basic usage is shown in the following workflow, where a (Housing) dataset is used to fit a Linear Regression model, which is than used to predict a target value for data, created by the Create Instance widget. Inserting a Rank widget between File and Create Instance enables outputting (and therefore making predictions on) the most important features. A Select Column widget is inserted to omit the actual target value.\nThe next example shows how to check whether the created instance is some kind of outlier. The creates instance is feed to PCA whose first and second componens are then examined in a Scatter Plot. The created instance is colored red in the plot and it could be considered as an outlier if it appears far from the original data (blue).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Create Instance Interactively creates an instance from a sample dataset.\nInputs\n Data: input dataset Reference: refrence dataset  Outputs\n Data: input dataset appended the created instance  The Create Instance widget creates a new instance, based on the input data. The widget displays all variables of the input dataset in a table of two columns. The column Variable represents the variable\u0026rsquo;s name, meanwhile the column Value enables setting the variable\u0026rsquo;s value." ,
	"author" : "",
	"summary" : "Create Instance Interactively creates an instance from a sample dataset.\nInputs\n Data: input dataset Reference: refrence dataset  Outputs\n Data: input dataset appended the created instance  The Create Instance widget creates a new instance, based on the input data. The widget displays all variables of the input dataset in a table of two columns. The column Variable represents the variable\u0026rsquo;s name, meanwhile the column Value enables setting the variable\u0026rsquo;s value.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Create Instance",
	"icon" : ""
},
{
    "uri": "/workflows/Cross-Validation/",
	"title": "Cross Validation",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Cross Validation",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/csvfileimport/",
	"title": "CSV File Import",
	"description": "",
	"content": "CSV File Import Import a data table from a CSV formatted file.\nOutputs\n Data: dataset from the .csv file Data Frame: pandas DataFrame object  The CSV File Import widget reads comma-separated files and sends the dataset to its output channel. File separators can be commas, semicolons, spaces, tabs or manually-defined delimiters. The history of most recently opened files is maintained in the widget.\nData Frame output can be used in the Python Script widget by connecting it to the in_object input (e.g. df = in_object). Then it can be used a regular DataFrame.\nImport Options The import window where the user sets the import parameters. Can be re-opened by pressing Import Options in the widget.\nRight click on the column name to set the column type. Right click on the row index (on the left) to mark a row as a header, skipped or a normal data row.\n File encoding. Default is UTF-8. See Encoding subchapter for details. Import settings:  Cell delimiter:  Tab Comma Semicolon Space Other (set the delimiter in the field to the right)   Quote character: either \" or ‘. Defines what is considered a text. Number separators:  Grouping: delimiters for thousands, e.g. 1,000 Decimal: delimiters for decimals, e.g. 1.234     Column type: select the column in the preview and set its type. Column type can be set also by right-clicking on the selected column.  Auto: Orange will automatically try to determine column type. (default) Numeric: for continuous data types, e.g. (1.23, 1.32, 1.42, 1.32) Categorical: for discrete data types, e.g. (brown, green, blue) Text: for string data types, e.g. (John, Olivia, Mike, Jane) Datetime: for time variables, e.g. (1970-01-01) Ignore: do not output the column.   Pressing Reset will return the settings to the previously set state (saved by pressing OK in the Import Options dialogue). Restore Defaults will set the settings to their default values. Cancel aborts the import, while OK imports the data and saves the settings.  Widget The widget once the data is successfully imported.\n The folder icon opens the dialogue for import the local .csv file. It can be used to either load the first file or change the existing file (load new data). The File dropdown stores paths to previously loaded data sets. Information on the imported data set. Reports on the number of instances (rows), variables (features or columns) and meta variables (special columns). Import Options re-opens the import dialogue where the user can set delimiters, encodings, text fields and so on. Cancel aborts data import. Reload imports the file once again, adding to the data any changes made in the original file.  Encoding The dialogue for settings custom encodings list in the Import Options - Encoding dropdown. Select Customize Encodings List… to change which encodings appear in the list. To save the changes, simply close the dialogue. Closing and reopening Orange (even with Reset widget settings) will not re-set the list. To do this, press Restore Defaults. To have all the available encodings in the list, press Select all.\nExample CSV File Import works almost exactly like the File widget, with the added options for importing different types of .csv files. In this workflow, the widget read the data from the file and sends it to the Data Table for inspection.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "CSV File Import Import a data table from a CSV formatted file.\nOutputs\n Data: dataset from the .csv file Data Frame: pandas DataFrame object  The CSV File Import widget reads comma-separated files and sends the dataset to its output channel. File separators can be commas, semicolons, spaces, tabs or manually-defined delimiters. The history of most recently opened files is maintained in the widget.\nData Frame output can be used in the Python Script widget by connecting it to the in_object input (e." ,
	"author" : "",
	"summary" : "CSV File Import Import a data table from a CSV formatted file.\nOutputs\n Data: dataset from the .csv file Data Frame: pandas DataFrame object  The CSV File Import widget reads comma-separated files and sends the dataset to its output channel. File separators can be commas, semicolons, spaces, tabs or manually-defined delimiters. The history of most recently opened files is maintained in the widget.\nData Frame output can be used in the Python Script widget by connecting it to the in_object input (e.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "CSV File Import",
	"icon" : ""
},
{
    "uri": "/workflows/Data/",
	"title": "Data",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Data",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/datainfo/",
	"title": "Data Info",
	"description": "",
	"content": "Data Info Displays information on a selected dataset.\nInputs\n Data: input dataset  A simple widget that presents information on dataset size, features, targets, meta attributes, and location.\n Information on dataset size Information on discrete and continuous features Information on targets Information on meta attributes Information on where the data is stored Produce a report.  Example Below, we compare the basic statistics of two Data Info widgets - one with information on the entire dataset and the other with information on the (manually) selected subset from the Scatter Plot widget. We used the Iris dataset.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Data Info Displays information on a selected dataset.\nInputs\n Data: input dataset  A simple widget that presents information on dataset size, features, targets, meta attributes, and location.\n Information on dataset size Information on discrete and continuous features Information on targets Information on meta attributes Information on where the data is stored Produce a report.  Example Below, we compare the basic statistics of two Data Info widgets - one with information on the entire dataset and the other with information on the (manually) selected subset from the Scatter Plot widget." ,
	"author" : "",
	"summary" : "Data Info Displays information on a selected dataset.\nInputs\n Data: input dataset  A simple widget that presents information on dataset size, features, targets, meta attributes, and location.\n Information on dataset size Information on discrete and continuous features Information on targets Information on meta attributes Information on where the data is stored Produce a report.  Example Below, we compare the basic statistics of two Data Info widgets - one with information on the entire dataset and the other with information on the (manually) selected subset from the Scatter Plot widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Data Info",
	"icon" : ""
},
{
    "uri": "/workflows/Data-Loading/",
	"title": "Data Loading",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Data Loading",
	"icon" : ""
},
{
    "uri": "/",
	"title": "Data Mining",
	"description": "Fruitful and Fun",
	"content": "Open source machine learning and data visualization.\nBuild data analysis workflows visually, with a large, diverse toolbox.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Open source machine learning and data visualization.\nBuild data analysis workflows visually, with a large, diverse toolbox." ,
	"author" : "",
	"summary" : "Open source machine learning and data visualization.\nBuild data analysis workflows visually, with a large, diverse toolbox.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "home",
	"LinkTitle" : "Data Mining",
	"icon" : ""
},
{
    "uri": "/workflows/Data-Sampler/",
	"title": "Data Sampler",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Data Sampler",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/datasampler/",
	"title": "Data Sampler",
	"description": "",
	"content": "Data Sampler Selects a subset of data instances from an input dataset.\nInputs\n Data: input dataset  Outputs\n Data Sample: sampled data instances Remaining Data: out-of-sample data  The Data Sampler widget implements several data sampling methods. It outputs a sampled and a complementary dataset (with instances from the input set that are not included in the sampled dataset). The output is processed after the input dataset is provided and Sample Data is pressed.\n Information on the input and output dataset. The desired sampling method:  Fixed proportion of data returns a selected percentage of the entire data (e.g. 70% of all the data) Fixed sample size returns a selected number of data instances with a chance to set Sample with replacement, which always samples from the entire dataset (does not subtract instances already in the subset). With replacement, you can generate more instances than available in the input dataset. Cross Validation partitions data instances into the specified number of complementary subsets. Following a typical validation schema, all subsets except the one selected by the user are output as Data Sample, and the selected subset goes to Remaining Data. (Note: In older versions, the outputs were swapped. If the widget is loaded from an older workflow, it switches to compatibility mode.) Bootstrap infers the sample from the population statistic.   Replicable sampling maintains sampling patterns that can be carried across users, while stratify sample mimics the composition of the input dataset. Press Sample Data to output the data sample.  If all data instances are selected (by setting the proportion to 100 % or setting the fixed sample size to the entire data size), output instances are still shuffled.\nExamples First, let’s see how the Data Sampler works. We will use the iris data from the File widget. We see there are 150 instances in the data. We sampled the data with the Data Sampler widget and we chose to go with a fixed sample size of 5 instances for simplicity. We can observe the sampled data in the Data Table widget (Data Table (in-sample)). The second Data Table (Data Table (out-of-sample)) shows the remaining 145 instances that weren’t in the sample. To output the out-of-sample data, double-click the connection between the widgets and rewire the output to Remaining Data –\u003e Data.\nNow, we will use the Data Sampler to split the data into training and testing part. We are using the iris data, which we loaded with the File widget. In Data Sampler, we split the data with Fixed proportion of data, keeping 70% of data instances in the sample.\nThen we connected two outputs to the Test \u0026 Score widget, Data Sample –\u003e Data and Remaining Data –\u003e Test Data. Finally, we added Logistic Regression as the learner. This runs logistic regression on the Data input and evaluates the results on the Test Data.\nOver/Undersampling Data Sampler can also be used to oversample a minority class or undersample majority class in the data. Let us show an example for oversampling. First, separate the minority class using a Select Rows widget. We are using the iris data from the File widget. The data set has 150 data instances, 50 of each class. Let us oversample, say, iris-setosa.\nIn Select Rows, set the condition to iris is iris-setosa. This will output 50 instances of the iris-setosa class. Now, connect Matching Data into the Data Sampler, select Fixed sample size, set it to, say, 100 and select Sample with replacement. Upon pressing Sample Data, the widget will output 100 instances of iris-setosa class, some of which will be duplicated (because we used Sample with replacement).\nFinally, use Concatenate to join the oversampled instances and the Unmatched Data output of the Select Rows widget. This outputs a data set with 200 instances. We can observe the final results in the Distributions.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Data Sampler Selects a subset of data instances from an input dataset.\nInputs\n Data: input dataset  Outputs\n Data Sample: sampled data instances Remaining Data: out-of-sample data  The Data Sampler widget implements several data sampling methods. It outputs a sampled and a complementary dataset (with instances from the input set that are not included in the sampled dataset). The output is processed after the input dataset is provided and Sample Data is pressed." ,
	"author" : "",
	"summary" : "Data Sampler Selects a subset of data instances from an input dataset.\nInputs\n Data: input dataset  Outputs\n Data Sample: sampled data instances Remaining Data: out-of-sample data  The Data Sampler widget implements several data sampling methods. It outputs a sampled and a complementary dataset (with instances from the input set that are not included in the sampled dataset). The output is processed after the input dataset is provided and Sample Data is pressed.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Data Sampler",
	"icon" : ""
},
{
    "uri": "/workflows/Data-Table/",
	"title": "Data Table",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Data Table",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/datatable/",
	"title": "Data Table",
	"description": "",
	"content": "Data Table Displays attribute-value data in a spreadsheet.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the table  The Data Table widget receives one or more datasets in its input and presents them as a spreadsheet. Data instances may be sorted by attribute values. The widget also supports manual selection of data instances.\n The name of the dataset (usually the input data file). Data instances are in rows and their attribute values in columns. In this example, the dataset is sorted by the attribute “sepal length”. Info on current dataset size and number and types of attributes Values of continuous attributes can be visualized with bars; colors can be attributed to different classes. Data instances (rows) can be selected and sent to the widget’s output channel. Use the Restore Original Order button to reorder data instances after attribute-based sorting. Produce a report. While auto-send is on, all changes will be automatically communicated to other widgets. Otherwise, press Send Selected Rows.  Example We used two File widgets to read the Iris and Glass dataset (provided in Orange distribution), and send them to the Data Table widget.\nSelected data instances in the first Data Table are passed to the second Data Table. Notice that we can select which dataset to view (iris or glass). Changing from one dataset to another alters the communicated selection of data instances if Commit on any change is selected.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Data Table Displays attribute-value data in a spreadsheet.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the table  The Data Table widget receives one or more datasets in its input and presents them as a spreadsheet. Data instances may be sorted by attribute values. The widget also supports manual selection of data instances.\n The name of the dataset (usually the input data file). Data instances are in rows and their attribute values in columns." ,
	"author" : "",
	"summary" : "Data Table Displays attribute-value data in a spreadsheet.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the table  The Data Table widget receives one or more datasets in its input and presents them as a spreadsheet. Data instances may be sorted by attribute values. The widget also supports manual selection of data instances.\n The name of the dataset (usually the input data file). Data instances are in rows and their attribute values in columns.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Data Table",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/databases_update/",
	"title": "Databases Update",
	"description": "",
	"content": "Databases Update Updates local systems biology databases, like gene ontologies, annotations, gene names, protein interaction networks, and similar.\nInputs\n None  Outputs\n None  With the bioinformatics add-on you can access several databases directly from Orange. The widget can also be used to update and manage locally stored databases. To get a more detailed information on the particular database hover on its name.\n Find the desired database. A list of available databases described with data source, update availability, date of your last update and file size. A large Update button will be displayed next to the database that needs to be updated. Update All will update and Download All will download all of the available databases from the serverfiles. Cancel will abort the action. Add a data set from the local machine.  To add a new file to the database, select the domain and the organism of the data. Give the data set a name and, optionally, tag it with appropriate tags. Finally, use the Select File button to load the local file. Press OK to complete the process. The data will be stored in a cached folder locally. To see the full path to the data, hover on the data set name.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Databases Update Updates local systems biology databases, like gene ontologies, annotations, gene names, protein interaction networks, and similar.\nInputs\n None  Outputs\n None  With the bioinformatics add-on you can access several databases directly from Orange. The widget can also be used to update and manage locally stored databases. To get a more detailed information on the particular database hover on its name.\n Find the desired database. A list of available databases described with data source, update availability, date of your last update and file size." ,
	"author" : "",
	"summary" : "Databases Update Updates local systems biology databases, like gene ontologies, annotations, gene names, protein interaction networks, and similar.\nInputs\n None  Outputs\n None  With the bioinformatics add-on you can access several databases directly from Orange. The widget can also be used to update and manage locally stored databases. To get a more detailed information on the particular database hover on its name.\n Find the desired database. A list of available databases described with data source, update availability, date of your last update and file size.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Databases Update",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/datasets/",
	"title": "Datasets",
	"description": "",
	"content": "Datasets Load a dataset from an online repository.\nOutputs\n Data: output dataset  Datasets widget retrieves selected dataset from the server and sends it to the output. File is downloaded to the local memory and thus instantly available even without the internet connection. Each dataset is provided with a description and information on the data size, number of instances, number of variables, target and tags.\n Information on the number of datasets available and the number of them downloaded to the local memory. Content of available datasets. Each dataset is described with the size, number of instances and variables, type of the target variable and tags. Formal description of the selected dataset. If Send Data Automatically is ticked, selected dataset is communicated automatically. Alternatively, press Send Data.  Example Orange workflows can start with Datasets widget instead of File widget. In the example below, the widget retrieves a dataset from an online repository (Kickstarter data), which is subsequently sent to both the Data Table and the Distributions.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Datasets Load a dataset from an online repository.\nOutputs\n Data: output dataset  Datasets widget retrieves selected dataset from the server and sends it to the output. File is downloaded to the local memory and thus instantly available even without the internet connection. Each dataset is provided with a description and information on the data size, number of instances, number of variables, target and tags.\n Information on the number of datasets available and the number of them downloaded to the local memory." ,
	"author" : "",
	"summary" : "Datasets Load a dataset from an online repository.\nOutputs\n Data: output dataset  Datasets widget retrieves selected dataset from the server and sends it to the output. File is downloaded to the local memory and thus instantly available even without the internet connection. Each dataset is provided with a description and information on the data size, number of instances, number of variables, target and tags.\n Information on the number of datasets available and the number of them downloaded to the local memory.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Datasets",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/DBSCAN/",
	"title": "DBSCAN",
	"description": "",
	"content": "DBSCAN Groups items using the DBSCAN clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute  The widget applies the DBSCAN clustering algorithm to the data and outputs a new dataset with cluster indices as a meta attribute. The widget also shows the sorted graph with distances to k-th nearest neighbors. With k values set to Core point neighbors as suggested in the methods article. This gives the user the idea of an ideal selection for Neighborhood distance setting. As suggested by authors this parameter should be set to the first value in the first “valley” in the graph.\n Set minimal number of core neighbors for a cluster and *maximal neighborhood distance. Set the distance metric that is used in grouping the items. If Apply Automatically is ticked, the widget will commit changes automatically. Alternatively, click Apply. The graph shows the distance to the k-th nearest neighbor. k is set by the Core point neighbor option. With moving the black slider left and right you can select the right Neighbourhood distance.  Example In the following example, we connected the File widget with selected Iris dataset to the DBSCAN widget. In the DBSCAN widget, we set Core points neighbors parameter to 5. And select the Neighbourhood distance to the value in the first “valley” in the graph. We show clusters in the Scatter Plot widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "DBSCAN Groups items using the DBSCAN clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute  The widget applies the DBSCAN clustering algorithm to the data and outputs a new dataset with cluster indices as a meta attribute. The widget also shows the sorted graph with distances to k-th nearest neighbors. With k values set to Core point neighbors as suggested in the methods article." ,
	"author" : "",
	"summary" : "DBSCAN Groups items using the DBSCAN clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute  The widget applies the DBSCAN clustering algorithm to the data and outputs a new dataset with cluster indices as a meta attribute. The widget also shows the sorted graph with distances to k-th nearest neighbors. With k values set to Core point neighbors as suggested in the methods article.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "DBSCAN",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/dicty_express/",
	"title": "dictyExpress",
	"description": "",
	"content": "dictyExpress Gives access to dictyExpress databases.\nInputs\n None  Outputs\n Data: Selected experiment (time-course gene expression data).  dictyExpress widget gives a direct access to the dictyExpress database. It allows you to download the data from selected experiments in Dictyostelium by Baylor College of Medicine. The widget requires internet connection to work.\n Log into the database to access personal files. Define the output. Genes from experiments can be either in rows or in columns. Press Commit to output the data and Refresh to update the list of experiments. List of available experiments. Use Filter to find a particular experiment.  Example dictyExpress widget can be used to retrieve data from a database, just like GEO Data Sets and similar to the File widget. We have retrieved the D. discoideum vs. D. purpureum data and sent it to the output by pressing Commit. We have observed the data in a Data Table and in a Heat Map, where we used Merge by k-means and clustering by rows to find similar genes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "dictyExpress Gives access to dictyExpress databases.\nInputs\n None  Outputs\n Data: Selected experiment (time-course gene expression data).  dictyExpress widget gives a direct access to the dictyExpress database. It allows you to download the data from selected experiments in Dictyostelium by Baylor College of Medicine. The widget requires internet connection to work.\n Log into the database to access personal files. Define the output. Genes from experiments can be either in rows or in columns." ,
	"author" : "",
	"summary" : "dictyExpress Gives access to dictyExpress databases.\nInputs\n None  Outputs\n Data: Selected experiment (time-course gene expression data).  dictyExpress widget gives a direct access to the dictyExpress database. It allows you to download the data from selected experiments in Dictyostelium by Baylor College of Medicine. The widget requires internet connection to work.\n Log into the database to access personal files. Define the output. Genes from experiments can be either in rows or in columns.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "dictyExpress",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/difference/",
	"title": "Difference",
	"description": "",
	"content": "Difference Make the time series stationary by replacing it with 1st or 2nd order discrete difference along its values.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Differences of input time series.   Order of differencing. Can be 1 or 2. The shift before differencing. Value of 1 equals to discrete differencing. You can use higher values to compute the difference between now and this many steps ahead. Invert the differencing direction. Select the series to difference.  To integrate the differences back into the original series (e.g. the forecasts), use the Moving Transform widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Difference Make the time series stationary by replacing it with 1st or 2nd order discrete difference along its values.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Differences of input time series.   Order of differencing. Can be 1 or 2. The shift before differencing. Value of 1 equals to discrete differencing. You can use higher values to compute the difference between now and this many steps ahead." ,
	"author" : "",
	"summary" : "Difference Make the time series stationary by replacing it with 1st or 2nd order discrete difference along its values.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Differences of input time series.   Order of differencing. Can be 1 or 2. The shift before differencing. Value of 1 equals to discrete differencing. You can use higher values to compute the difference between now and this many steps ahead.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Difference",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/differential_expression/",
	"title": "Differential Expression",
	"description": "",
	"content": "Differential Expression Plots differential gene expression for selected experiments.\nInputs\n Data: Data set.  Outputs\n Data Subset: Differentially expressed genes. Remaining Data Subset: Genes that were not differentially expressed. Selected Genes: Genes from the select data with scores appended.  This widget plots a differential gene expression graph for a sample target. It takes gene expression data as an input (from dictyExpress, GEO Data Sets, etc.) and outputs a selected data subset (normally the most interesting genes).\n  Information of the data input and output. The first line shows the number of samples and genes in the data set. The second line displays the selected sample target (read around which the graph is plotted). The third line shows the number of undefined genes (missing data) and the fourth the number of genes in the output.\n  Select the plotting method in Scoring method:\n Fold change: final to initial value ratio log2 (fold change): binary logarithmic transformation of fold change values T-test: parametric test of null hypothesis T-test (P-value): parametric test of null hypothesis with P-value as criterium ANOVA: variance distribution ANOVA (P-value): variance distribution with P-value as criterium Signal to NoiseRatio: biological signal to noise ratio Mann-Whitney: non-parametric test of null hypothesis with P-value as criterium Hypergeometric test: for binary expression data.    Select Target Labels. Labels depend on the attributes in the input. In Values you can change the sample target (default value is the first value on the list, alphabetically or numerically).\n  Selection box controls the output data.\n By setting the Lower threshold and Upper threshold values you are outputting the data outside this interval (the most interesting expression levels). You can also manually place the threshold lines by dragging left or right in the plot. If you click Compute null distribution box, the widget will calculate null distribution and display it in the plot. Permutations field allows you to set the precision of null distribution (the more permutations the more precise the distribution), while alpha-value will be the allowed probability of false positives. Press Select to output this data. The final option is to set the number of best ranked genes and output them with Select.    When Auto commit is on is ticked, the widget will automatically apply the changes. Alternatively press Commit. If the Add gene scores to output is ticked, the widget will append an additional column with gene scores to the data.\n  Example From the GEO Data Sets widget, we selected Breast cancer and docetaxel treatment (GDS360) with 14 treatment resistant and 10 treatment sensitive tumors. Then we used the Differential Expression widget to select the most interesting genes. We left the upper and lower threshold at default (1 and -1) and output the data. Then we observed the selected data subset in a Data Table. The table shows selected genes with an additional gene score label.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Differential Expression Plots differential gene expression for selected experiments.\nInputs\n Data: Data set.  Outputs\n Data Subset: Differentially expressed genes. Remaining Data Subset: Genes that were not differentially expressed. Selected Genes: Genes from the select data with scores appended.  This widget plots a differential gene expression graph for a sample target. It takes gene expression data as an input (from dictyExpress, GEO Data Sets, etc.) and outputs a selected data subset (normally the most interesting genes)." ,
	"author" : "",
	"summary" : "Differential Expression Plots differential gene expression for selected experiments.\nInputs\n Data: Data set.  Outputs\n Data Subset: Differentially expressed genes. Remaining Data Subset: Genes that were not differentially expressed. Selected Genes: Genes from the select data with scores appended.  This widget plots a differential gene expression graph for a sample target. It takes gene expression data as an input (from dictyExpress, GEO Data Sets, etc.) and outputs a selected data subset (normally the most interesting genes).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Differential Expression",
	"icon" : ""
},
{
    "uri": "/workflows/Dimensionality-Reduction/",
	"title": "Dimensionality Reduction",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Dimensionality Reduction",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/discretize/",
	"title": "Discretize",
	"description": "",
	"content": "Discretize Discretizes continuous attributes from an input dataset.\nInputs\n Data: input dataset  Outputs\n Data: dataset with discretized values  The Discretize widget discretizes continuous attributes with a selected method.\n The basic version of the widget is rather simple. It allows choosing between three different discretizations.  Entropy-MDL, invented by Fayyad and Irani is a top-down discretization, which recursively splits the attribute at a cut maximizing information gain, until the gain is lower than the minimal description length of the cut. This discretization can result in an arbitrary number of intervals, including a single interval, in which case the attribute is discarded as useless (removed). Equal-frequency splits the attribute into a given number of intervals, so that they each contain approximately the same number of instances. Equal-width evenly splits the range between the smallest and the largest observed value. The Number of intervals can be set manually. The widget can also be set to leave the attributes continuous or to remove them.   To treat attributes individually, go to Individual Attribute Settings. They show a specific discretization of each attribute and allow changes. First, the top left list shows the cut-off points for each attribute. In the snapshot, we used the entropy-MDL discretization, which determines the optimal number of intervals automatically; we can see it discretized the age into seven intervals with cut-offs at 21.50, 23.50, 27.50, 35.50, 43.50, 54.50 and 61.50, respectively, while the capital-gain got split into many intervals with several cut-offs. The final weight (fnlwgt), for instance, was left with a single interval and thus removed. On the right, we can select a specific discretization method for each attribute. Attribute “fnlwgt” would be removed by the MDL-based discretization, so to prevent its removal, we select the attribute and choose, for instance, Equal-frequency discretization. We could also choose to leave the attribute continuous. Produce a report. Tick Apply automatically for the widget to automatically commit changes. Alternatively, press Apply.  Example In the schema below, we show the Iris dataset with continuous attributes (as in the original data file) and with discretized attributes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Discretize Discretizes continuous attributes from an input dataset.\nInputs\n Data: input dataset  Outputs\n Data: dataset with discretized values  The Discretize widget discretizes continuous attributes with a selected method.\n The basic version of the widget is rather simple. It allows choosing between three different discretizations.  Entropy-MDL, invented by Fayyad and Irani is a top-down discretization, which recursively splits the attribute at a cut maximizing information gain, until the gain is lower than the minimal description length of the cut." ,
	"author" : "",
	"summary" : "Discretize Discretizes continuous attributes from an input dataset.\nInputs\n Data: input dataset  Outputs\n Data: dataset with discretized values  The Discretize widget discretizes continuous attributes with a selected method.\n The basic version of the widget is rather simple. It allows choosing between three different discretizations.  Entropy-MDL, invented by Fayyad and Irani is a top-down discretization, which recursively splits the attribute at a cut maximizing information gain, until the gain is lower than the minimal description length of the cut.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Discretize",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/distancefile/",
	"title": "Distance File",
	"description": "",
	"content": "Distance File Loads an existing distance file.\nOutputs\n Distance File: distance matrix   Choose from a list of previously saved distance files. Browse for saved distance files. Reload the selected distance file. Information about the distance file (number of points, labelled/unlabelled). Browse documentation datasets. Produce a report.  Example When you want to use a custom-set distance file that you’ve saved before, open the Distance File widget and select the desired file with the Browse icon. This widget loads the existing distance file. In the snapshot below, we loaded the transformed Iris distance matrix from the Save Distance Matrix example. We displayed the transformed data matrix in the Distance Map widget. We also decided to display a distance map of the original Iris dataset for comparison.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Distance File Loads an existing distance file.\nOutputs\n Distance File: distance matrix   Choose from a list of previously saved distance files. Browse for saved distance files. Reload the selected distance file. Information about the distance file (number of points, labelled/unlabelled). Browse documentation datasets. Produce a report.  Example When you want to use a custom-set distance file that you\u0026rsquo;ve saved before, open the Distance File widget and select the desired file with the Browse icon." ,
	"author" : "",
	"summary" : "Distance File Loads an existing distance file.\nOutputs\n Distance File: distance matrix   Choose from a list of previously saved distance files. Browse for saved distance files. Reload the selected distance file. Information about the distance file (number of points, labelled/unlabelled). Browse documentation datasets. Produce a report.  Example When you want to use a custom-set distance file that you\u0026rsquo;ve saved before, open the Distance File widget and select the desired file with the Browse icon.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Distance File",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/distancemap/",
	"title": "Distance Map",
	"description": "",
	"content": "Distance Map Visualizes distances between items.\nInputs\n Distances: distance matrix  Outputs\n Data: instances selected from the matrix Features: attributes selected from the matrix  The Distance Map visualizes distances between objects. The visualization is the same as if we printed out a table of numbers, except that the numbers are replaced by colored spots.\nDistances are most often those between instances (\"rows” in the Distances widget) or attributes (\"columns” in Distances widget). The only suitable input for Distance Map is the Distances widget. For the output, the user can select a region of the map and the widget will output the corresponding instances or attributes. Also note that the Distances widget ignores discrete values and calculates distances only for continuous data, thus it can only display distance map for discrete data if you Continuize them first.\nThe snapshot shows distances between columns in the heart disease data, where smaller distances are represented with light and larger with dark orange. The matrix is symmetric and the diagonal is a light shade of orange - no attribute is different from itself. Symmetricity is always assumed, while the diagonal may also be non-zero.\n Element sorting arranges elements in the map by  None (lists instances as found in the dataset) Clustering (clusters data by similarity) Clustering with ordered leaves (maximizes the sum of similarities of adjacent elements)   Colors  Colors (select the color palette for your distance map) Low and High are thresholds for the color palette (low for instances or attributes with low distances and high for instances or attributes with high distances).   Select Annotations. If Send Selected Automatically is on, the data subset is communicated automatically, otherwise you need to press Send Selected. Press Save Image if you want to save the created image to your computer. Produce a report.  Normally, a color palette is used to visualize the entire range of distances appearing in the matrix. This can be changed by setting the low and high threshold. In this way we ignore the differences in distances outside this interval and visualize the interesting part of the distribution.\nBelow, we visualized the most correlated attributes (distances by columns) in the heart disease dataset by setting the color threshold for high distances to the minimum. We get a predominantly black square, where attributes with the lowest distance scores are represented by a lighter shade of the selected color schema (in our case: orange). Beside the diagonal line, we see that in our example ST by exercise and major vessels colored are the two attributes closest together.\nThe user can select a region in the map with the usual click-and-drag of the cursor. When a part of the map is selected, the widget outputs all items from the selected cells.\nExamples The first workflow shows a very standard use of the Distance Map widget. We select 70% of the original Iris data as our sample and view the distances between rows in Distance Map.\nIn the second example, we use the heart disease data again and select a subset of women only from the Scatter Plot. Then, we visualize distances between columns in the Distance Map. Since the subset also contains some discrete data, the Distances widget warns us it will ignore the discrete features, thus we will see only continuous instances/attributes in the map.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Distance Map Visualizes distances between items.\nInputs\n Distances: distance matrix  Outputs\n Data: instances selected from the matrix Features: attributes selected from the matrix  The Distance Map visualizes distances between objects. The visualization is the same as if we printed out a table of numbers, except that the numbers are replaced by colored spots.\nDistances are most often those between instances (\u0026quot;rows\u0026rdquo; in the Distances widget) or attributes (\u0026quot;columns\u0026rdquo; in Distances widget)." ,
	"author" : "",
	"summary" : "Distance Map Visualizes distances between items.\nInputs\n Distances: distance matrix  Outputs\n Data: instances selected from the matrix Features: attributes selected from the matrix  The Distance Map visualizes distances between objects. The visualization is the same as if we printed out a table of numbers, except that the numbers are replaced by colored spots.\nDistances are most often those between instances (\u0026quot;rows\u0026rdquo; in the Distances widget) or attributes (\u0026quot;columns\u0026rdquo; in Distances widget).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Distance Map",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/distancematrix/",
	"title": "Distance Matrix",
	"description": "",
	"content": "Distance Matrix Visualizes distance measures in a distance matrix.\nInputs\n Distances: distance matrix  Outputs\n Distances: distance matrix Table: distance measures in a distance matrix  The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing the distances, taken pairwise, between the elements of a set. The number of elements in the dataset defines the size of the matrix. Data matrices are essential for hierarchical clustering and they are extremely useful in bioinformatics as well, where they are used to represent protein structures in a coordinate-independent manner.\n Elements in the dataset and the distances between them. Label the table. The options are: none, enumeration, according to variables. Produce a report. Click Send to communicate changes to other widgets. Alternatively, tick the box in front of the Send button and changes will be communicated automatically (Send Automatically).  The only two suitable inputs for Distance Matrix are the Distances widget and the Distance Transformation widget. The output of the widget is a data table containing the distance matrix. The user can decide how to label the table and the distance matrix (or instances in the distance matrix) can then be visualized or displayed in a separate data table.\nExample The example below displays a very standard use of the Distance Matrix widget. We compute the distances between rows in the sample from the Iris dataset and output them in the Distance Matrix. It comes as no surprise that Iris Virginica and Iris Setosa are the furthest apart.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Distance Matrix Visualizes distance measures in a distance matrix.\nInputs\n Distances: distance matrix  Outputs\n Distances: distance matrix Table: distance measures in a distance matrix  The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing the distances, taken pairwise, between the elements of a set. The number of elements in the dataset defines the size of the matrix. Data matrices are essential for hierarchical clustering and they are extremely useful in bioinformatics as well, where they are used to represent protein structures in a coordinate-independent manner." ,
	"author" : "",
	"summary" : "Distance Matrix Visualizes distance measures in a distance matrix.\nInputs\n Distances: distance matrix  Outputs\n Distances: distance matrix Table: distance measures in a distance matrix  The Distance Matrix widget creates a distance matrix, which is a two-dimensional array containing the distances, taken pairwise, between the elements of a set. The number of elements in the dataset defines the size of the matrix. Data matrices are essential for hierarchical clustering and they are extremely useful in bioinformatics as well, where they are used to represent protein structures in a coordinate-independent manner.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Distance Matrix",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/distancetransformation/",
	"title": "Distance Transformation",
	"description": "",
	"content": "Distance Transformation Transforms distances in a dataset.\nInputs\n Distances: distance matrix  Outputs\n Distances: transformed distance matrix  The Distances Transformation widget is used for the normalization and inversion of distance matrices. The normalization of data is necessary to bring all the variables into proportion with one another.\n Choose the type of Normalization:  No normalization To interval [0, 1] To interval [-1, 1] Sigmoid function: 1/(1+exp(-X))   Choose the type of Inversion:  No inversion -X 1 - X max(X) - X 1/X   Produce a report. After changing the settings, you need to click Apply to commit changes to other widgets. Alternatively, tick Apply automatically.  Example In the snapshot below, you can see how transformation affects the distance matrix. We loaded the Iris dataset and calculated the distances between rows with the help of the Distances widget. In order to demonstrate how Distance Transformation affects the Distance Matrix, we created the workflow below and compared the transformed distance matrix with the “original” one.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Distance Transformation Transforms distances in a dataset.\nInputs\n Distances: distance matrix  Outputs\n Distances: transformed distance matrix  The Distances Transformation widget is used for the normalization and inversion of distance matrices. The normalization of data is necessary to bring all the variables into proportion with one another.\n Choose the type of Normalization:  No normalization To interval [0, 1] To interval [-1, 1] Sigmoid function: 1/(1+exp(-X))   Choose the type of Inversion:  No inversion -X 1 - X max(X) - X 1/X   Produce a report." ,
	"author" : "",
	"summary" : "Distance Transformation Transforms distances in a dataset.\nInputs\n Distances: distance matrix  Outputs\n Distances: transformed distance matrix  The Distances Transformation widget is used for the normalization and inversion of distance matrices. The normalization of data is necessary to bring all the variables into proportion with one another.\n Choose the type of Normalization:  No normalization To interval [0, 1] To interval [-1, 1] Sigmoid function: 1/(1+exp(-X))   Choose the type of Inversion:  No inversion -X 1 - X max(X) - X 1/X   Produce a report.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Distance Transformation",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/distances/",
	"title": "Distances",
	"description": "",
	"content": "Distances Computes distances between rows/columns in a dataset.\nInputs\n Data: input dataset  Outputs\n Distances: distance matrix  The Distances widget computes distances between rows or columns in a dataset. By default, the data will be normalized to ensure equal treatment of individual features. Normalization is always done column-wise.\nSparse data can only be used with Euclidean, Manhattan and Cosine metric.\nThe resulting distance matrix can be fed further to Hierarchical Clustering for uncovering groups in the data, to Distance Map or Distance Matrix for visualizing the distances (Distance Matrix can be quite slow for larger data sets), to MDS for mapping the data instances using the distance matrix and finally, saved with Save Distance Matrix. Distance file can be loaded with Distance File.\nDistances work well with Orange add-ons, too. The distance matrix can be fed to Network from Distances (Network add-on) to convert the matrix into a graph and to Duplicate Detection (Text add-on) to find duplicate documents in the corpus.\n  Choose whether to measure distances between rows or columns.\n  Choose the Distance Metric:\n Euclidean (“straight line”, distance between two points) Manhattan (the sum of absolute differences for all attributes) Cosine (the cosine of the angle between two vectors of an inner product space) Jaccard (the size of the intersection divided by the size of the union of the sample sets) Spearman(linear correlation between the rank of the values, remapped as a distance in a [0, 1] interval) Spearman absolute(linear correlation between the rank of the absolute values, remapped as a distance in a [0, 1] interval) Pearson (linear correlation between the values, remapped as a distance in a [0, 1] interval) Pearson absolute (linear correlation between the absolute values, remapped as a distance in a [0, 1] interval) Hamming (the number of features at which the corresponding values are different) Bhattacharyya distance (Similarity between two probability distributions, not a real distance as it doesn’t obey triangle inequality.)  Normalize the features. Normalization is always done column-wise. Values are zero centered and scaled. In case of missing values, the widget automatically imputes the average value of the row or the column. The widget works for both numeric and categorical data. In case of categorical data, the distance is 0 if the two values are the same (‘green’ and ‘green’) and 1 if they are not (‘green’ and ‘blue’).\n  Tick Apply Automatically to automatically commit changes to other widgets. Alternatively, press ‘Apply’.\n  Examples The first example shows a typical use of the Distances widget. We are using the iris.tab data from the File widget. We compute distances between data instances (rows) and pass the result to the Hierarchical Clustering. This is a simple workflow to find groups of data instances.\nAlternatively, we can compute distance between columns and find how similar our features are.\nThe second example shows how to visualize the resulting distance matrix. A nice way to observe data similarity is in a Distance Map or in MDS.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Distances Computes distances between rows/columns in a dataset.\nInputs\n Data: input dataset  Outputs\n Distances: distance matrix  The Distances widget computes distances between rows or columns in a dataset. By default, the data will be normalized to ensure equal treatment of individual features. Normalization is always done column-wise.\nSparse data can only be used with Euclidean, Manhattan and Cosine metric.\nThe resulting distance matrix can be fed further to Hierarchical Clustering for uncovering groups in the data, to Distance Map or Distance Matrix for visualizing the distances (Distance Matrix can be quite slow for larger data sets), to MDS for mapping the data instances using the distance matrix and finally, saved with Save Distance Matrix." ,
	"author" : "",
	"summary" : "Distances Computes distances between rows/columns in a dataset.\nInputs\n Data: input dataset  Outputs\n Distances: distance matrix  The Distances widget computes distances between rows or columns in a dataset. By default, the data will be normalized to ensure equal treatment of individual features. Normalization is always done column-wise.\nSparse data can only be used with Euclidean, Manhattan and Cosine metric.\nThe resulting distance matrix can be fed further to Hierarchical Clustering for uncovering groups in the data, to Distance Map or Distance Matrix for visualizing the distances (Distance Matrix can be quite slow for larger data sets), to MDS for mapping the data instances using the distance matrix and finally, saved with Save Distance Matrix.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Distances",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/distributions/",
	"title": "Distributions",
	"description": "",
	"content": "Distributions Displays value distributions for a single attribute.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether an instance is selected Histogram Data: bins and instance counts from the histogram  The Distributions widget displays the value distribution of discrete or continuous attributes. If the data contains a class variable, distributions may be conditioned on the class.\nThe graph shows how many times (e.g., in how many instances) each attribute value appears in the data. If the data contains a class variable, class distributions for each of the attribute values will be displayed (like in the snapshot below). To create this graph, we used the Zoo dataset.\n A list of variables for display. Sort categories by frequency orders displayed values by frequency. Set Bin width with the slider. Precision scale is set to sensible intervals. Fitted distribution fits selected distribution to the plot. Options are Normal, Beta, Gamma, Rayleigh, Pareto, Exponential, Kernel density. Columns:   Split by displays value distributions for instances of a certain class. Stack columns displays one column per bin, colored by proportions of class values. Show probabilities shows probabilities of class values at selected variable. Show cumulative distribution cumulatively stacks frequencies.  If Apply Automatically is ticked, changes are communicated automatically. Alternatively, click Apply.  For continuous attributes, the attribute values are also displayed as a histogram. It is possible to fit various distributions to the data, for example, a Gaussian kernel density estimation. Hide bars hides histogram bars and shows only distribution (old behavior of Distributions).\nFor this example, we used the Iris dataset.\nIn class-less domains, the bars are displayed in blue. We used the Housing dataset.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Distributions Displays value distributions for a single attribute.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether an instance is selected Histogram Data: bins and instance counts from the histogram  The Distributions widget displays the value distribution of discrete or continuous attributes. If the data contains a class variable, distributions may be conditioned on the class." ,
	"author" : "",
	"summary" : "Distributions Displays value distributions for a single attribute.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether an instance is selected Histogram Data: bins and instance counts from the histogram  The Distributions widget displays the value distribution of discrete or continuous attributes. If the data contains a class variable, distributions may be conditioned on the class.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Distributions",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/documentembedding/",
	"title": "Document Embedding",
	"description": "",
	"content": "Document Embedding Embeds documents from input corpus into vector space by using pretrained fastText models described in E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, Learning Word Vectors for 157 Languages. Proceedings of the International Conference on Language Resources and Evaluation, 2018.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with new features appended.  Document Embedding parses ngrams of each document in corpus, obtains embedding for each ngram using pretrained model for chosen language and obtains one vector for each document by aggregating ngram embeddings using one of offered aggregators. Note that method will work on any ngrams but it will give best results if corpus is preprocessed such that ngrams are words (because model was trained to embed words).\n Widget parameters:  Language: widget will use a model trained on documents in chosen language. Aggregator: operation to perform on ngram embeddings to aggregate them into a single document vector.   Cancel current execution. If Apply automatically is checked, changes in parameters are sent automatically. Alternatively press Apply.  Examples In first example, we will inspect how the widget works. Load book-excerpts.tab using Corpus widget and connect it to Document Embedding. Check the output data by connecting Document Embedding to Data Table. We see additional 300 features that we widget has appended.\nIn the second example we will try to predict document category. We will keep working on book-excerpts.tab loaded with Corpus widget and sent through Preprocess Text with default parameters. Connect Preprocess Text to Document Embedding to obtain features for predictive modelling. Here we set aggregator to Sum.\nConnect Document Embedding to Test and Score and also connect learner of choice to the left side of Test and Score. We chose SVM and changed kernel to Linear. Test and Score will now compute performance of each learner on the input. We can see that we achieved great results.\nLet’s now inspect confusion matrix. Connect Test and Score to Confusion Matrix. Clicking on Select Misclassified will output documents that were misclassified. We can further inspect them by connecting Corpus Viewer to Confusion Matrix.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Document Embedding Embeds documents from input corpus into vector space by using pretrained fastText models described in E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, Learning Word Vectors for 157 Languages. Proceedings of the International Conference on Language Resources and Evaluation, 2018.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with new features appended.  Document Embedding parses ngrams of each document in corpus, obtains embedding for each ngram using pretrained model for chosen language and obtains one vector for each document by aggregating ngram embeddings using one of offered aggregators." ,
	"author" : "",
	"summary" : "Document Embedding Embeds documents from input corpus into vector space by using pretrained fastText models described in E. Grave, P. Bojanowski, P. Gupta, A. Joulin, T. Mikolov, Learning Word Vectors for 157 Languages. Proceedings of the International Conference on Language Resources and Evaluation, 2018.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with new features appended.  Document Embedding parses ngrams of each document in corpus, obtains embedding for each ngram using pretrained model for chosen language and obtains one vector for each document by aggregating ngram embeddings using one of offered aggregators.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Document Embedding",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/docmap/",
	"title": "Document Map",
	"description": "",
	"content": "Document Map Displays geographic locations mentioned in the text.\nInputs\n Data: Data set.  Outputs\n Corpus: Documents containing mentions of selected geographical regions.  Document Map widget shows geolocations from textual (string) data. It finds mentions of geographic names (countries and capitals) and displays distributions (frequency of mentions) of these names on a map. It works with any Orange widget that outputs a data table and that contains at least one string attribute. The widget outputs selected data instances, that is all documents containing mentions of a selected country (or countries).\n Select the meta attribute you want to search geolocations by. The widget will find all mentions of geolocations in a text and display distributions on a map. Select the type of map you wish to display. The options are World, Europe and USA. You can zoom in and out of the map by pressing + and - buttons on a map or by mouse scroll. The legend for the geographic distribution of data. Countries with the boldest color are most often mentioned in the selected region attribute (highest frequency).  To select documents mentioning a specific country, click on a country and the widget will output matching documents. To select more than one country hold Ctrl/Cmd upon selection.\nExample Document Map widget can be used for simply visualizing distributions of geolocations or for a more complex interactive data analysis. Here, we’ve queried NY Times for articles on Slovenia for the time period of the last year (2015-2016). First we checked the results with Corpus Viewer.\nThen we sent the data to Document Map to see distributions of geolocations by country attribute. The attribute already contains country tags for each article, which is why NY Times is great in combinations with Document Map. We selected Germany, which sends all the documents tagged with Germany to the output. Remember, we queried NY Times for articles on Slovenia.\nWe can again inspect the output with Corpus Viewer. But there’s a more interesting way of visualizing the data. We’ve sent selected documents to Preprocess Text, where we’ve tokenized text to words and removed stopwords.\nFinally, we can inspect the top words appearing in last year’s documents on Slovenia and mentioning also Germany with Word Cloud.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Document Map Displays geographic locations mentioned in the text.\nInputs\n Data: Data set.  Outputs\n Corpus: Documents containing mentions of selected geographical regions.  Document Map widget shows geolocations from textual (string) data. It finds mentions of geographic names (countries and capitals) and displays distributions (frequency of mentions) of these names on a map. It works with any Orange widget that outputs a data table and that contains at least one string attribute." ,
	"author" : "",
	"summary" : "Document Map Displays geographic locations mentioned in the text.\nInputs\n Data: Data set.  Outputs\n Corpus: Documents containing mentions of selected geographical regions.  Document Map widget shows geolocations from textual (string) data. It finds mentions of geographic names (countries and capitals) and displays distributions (frequency of mentions) of these names on a map. It works with any Orange widget that outputs a data table and that contains at least one string attribute.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Document Map",
	"icon" : ""
},
{
    "uri": "/docs/",
	"title": "Documentation",
	"description": "",
	"content": "For a list of frequently asked questions, see FAQ. Also, feel free to reach out to us in our Discord chatroom.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "For a list of frequently asked questions, see FAQ. Also, feel free to reach out to us in our Discord chatroom." ,
	"author" : "",
	"summary" : "For a list of frequently asked questions, see FAQ. Also, feel free to reach out to us in our Discord chatroom.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "docs",
	"LinkTitle" : "Documentation",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/dot_matrix/",
	"title": "Dot Matrix",
	"description": "",
	"content": "Dot Matrix Perform cluster analysis.\nInputs\n Data: Single cell dataset.  Outputs\n Selected Data Data: Single cell dataset. Contingency Table  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Dot Matrix Perform cluster analysis.\nInputs\n Data: Single cell dataset.  Outputs\n Selected Data Data: Single cell dataset. Contingency Table  " ,
	"author" : "",
	"summary" : "Dot Matrix Perform cluster analysis.\nInputs\n Data: Single cell dataset.  Outputs\n Selected Data Data: Single cell dataset. Contingency Table  ",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Dot Matrix",
	"icon" : ""
},
{
    "uri": "/download/",
	"title": "Download",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "download",
	"LinkTitle" : "Download",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/duplicatedetection/",
	"title": "Duplicate Detection",
	"description": "",
	"content": "Duplicate Detection Detect \u0026 remove duplicates from a corpus.\nInputs\n Distances: A distance matrix.  Outputs\n Corpus Without Duplicated: Corpus with duplicates removed. Duplicates Cluster: Documents belonging to selected cluster. Corpus: Corpus with appended cluster labels.  Duplicate Detection uses clustering to find duplicates in the corpus. It is great with the Twitter widget for removing retweets and other similar documents.\nTo set the level of similarity, drag the line vertical line left or right in the visualization. The further left the line, the more similar the documents have to be in order to be considered duplicates. You can also set the threshold manually in the control area.\n Information on unique and duplicate documents. Linkage used for clustering (Single, Average, Complete, Weighted and Ward). Distance threshold sets the similarity cutoff. The lower the value, the more similar the data instances have to be to belong to the same cluster. You can also set the cutoff by dragging the vertical line in the plot. Cluster labels can be appended as attributes, class or metas. List of clusters at the selected threshold. They are sorted by size by default. Click on the cluster to observe its content on the output.  Example This simple example uses iris data to find identical data instances. Load iris with the File widget and pass it to Distances. In Distances, use Euclidean distance for computing the distance matrix. Pass distances to Duplicate Detection.\nIt looks like cluster C147 contain three duplicate entries. Let us select it in the widget and observe it in a Data Table. Remember to set the output to Duplicates Cluster. IThe three data instances are identical. To use the data set without duplicates, use the first output, Corpus Without Duplicates.\nThe same procedure can be used also for corpora. Remember to use the Bag of Words between Corpus and Distances.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Duplicate Detection Detect \u0026amp; remove duplicates from a corpus.\nInputs\n Distances: A distance matrix.  Outputs\n Corpus Without Duplicated: Corpus with duplicates removed. Duplicates Cluster: Documents belonging to selected cluster. Corpus: Corpus with appended cluster labels.  Duplicate Detection uses clustering to find duplicates in the corpus. It is great with the Twitter widget for removing retweets and other similar documents.\nTo set the level of similarity, drag the line vertical line left or right in the visualization." ,
	"author" : "",
	"summary" : "Duplicate Detection Detect \u0026amp; remove duplicates from a corpus.\nInputs\n Distances: A distance matrix.  Outputs\n Corpus Without Duplicated: Corpus with duplicates removed. Duplicates Cluster: Documents belonging to selected cluster. Corpus: Corpus with appended cluster labels.  Duplicate Detection uses clustering to find duplicates in the corpus. It is great with the Twitter widget for removing retweets and other similar documents.\nTo set the level of similarity, drag the line vertical line left or right in the visualization.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Duplicate Detection",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/editdomain/",
	"title": "Edit Domain",
	"description": "",
	"content": "Edit Domain Rename features and their values.\nInputs\n Data: input dataset  Outputs\n Data: dataset with edited domain  This widget can be used to edit/change a dataset’s domain - rename features, rename or merge values of categorical features, add a categorical value, and assign labels.\n All features (including meta attributes) from the input dataset are listed in the Variables list. Selecting one feature displays an editor on the right. Editing options:  Change the name of the feature. Change the type of the feature. For example, convert a string variable to categorical. Unlink variable from its source variable. This option removes existing computation for a variable (say for Cluster how clustering was computed), making it ‘plain’. This enables merging variables with same names in Merge Data. Change the value names for discrete features in the Values list box. Double-click to edit the name. Add, remove or edit additional feature annotations in the Labels box. Add a new label with the + button and add the Key and Value for the new entry. Key will be displayed in the top left corner of the Data Table, while values will appear below the specified column. Remove an existing label with the - button.   Reorder or merge values of categorical features. To reorder the values (for example, to display them in Distributions, use the up and down keys at the bottom of the box. To add or remove a value, use + and - buttons. Select two or more variables and click = to merge them into a single value. Use the M button to merge variables on condition. Rename the output table. Useful for displaying table names in Venn Diagram. To revert the changes made to the selected feature, press the Reset Selected button while the feature is selected in the Variables list. Pressing Reset All will remove all the changes to the domain. Press Apply to send the new domain to the output.  Merging options\n Group selected values: selected cateogorical values become a single variable. Group values with less than N occurrences: values which appear less than N times in the data, will be grouped into a single value. Group values with less than % occurrences: values which appear less then X % of the time in the data, will be grouped into a single value. Group all except N most frequent values: all values but the N most frequent will be grouped into a single variable. New value name: the name of the grouped value.  Example Below, we demonstrate how to simply edit an existing domain. We selected the heart_disease.tab dataset and edited the gender attribute. Where in the original we had the values female and male, we changed it into F for female and M for male. Then we used the down key to switch the order of the variables. Finally, we added a label to mark that the attribute is binary. We can observe the edited data in the Data Table widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Edit Domain Rename features and their values.\nInputs\n Data: input dataset  Outputs\n Data: dataset with edited domain  This widget can be used to edit/change a dataset\u0026rsquo;s domain - rename features, rename or merge values of categorical features, add a categorical value, and assign labels.\n All features (including meta attributes) from the input dataset are listed in the Variables list. Selecting one feature displays an editor on the right." ,
	"author" : "",
	"summary" : "Edit Domain Rename features and their values.\nInputs\n Data: input dataset  Outputs\n Data: dataset with edited domain  This widget can be used to edit/change a dataset\u0026rsquo;s domain - rename features, rename or merge values of categorical features, add a categorical value, and assign labels.\n All features (including meta attributes) from the input dataset are listed in the Variables list. Selecting one feature displays an editor on the right.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Edit Domain",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/enklik-anketa/",
	"title": "EnKlik Anketa",
	"description": "",
	"content": "EnKlik Anketa Import data from EnKlikAnketa (1ka.si) public URL.\nOutputs\n Data: survey results  The EnKlik Anketa widget retrieves survey results obtained from the EnKlikAnketa service. You need to create a public link to to retrieve the results. Go to the survey you wish to retrieve, then select Data (Podatki) tab and create a public link (javna povezava) at the top right corner.\nThen insert the link into the Public link URL field. The link should look something like this: https://www.1ka.si/podatki/123456/78A9B1CD/.\n A public link to the survey results. To observe the results live, set the reload rate (5s - 5 min). Attribute list. You can change the attribute type and role, just like in the File widget. Survey meta information. Tick the box on the left to commit the changes automatically. Alternatively, click Commit. Access widget help.  Example EnKlik Anketa widget is great for observing results from online surveys. We have created a sample survey and imported it into the widget. We have 41 responses and we have asked 8 questions, 7 of which were recognized as features and 1 as a meta attribute.\nThe widget sets questions from the survey as feature names. This, however, might be slightly impractical for analytical purposes, as we can see in the Data Table. We will shorten the names with Edit Domain widget.\nEdit Domain enables us to change attribute names and even rename attribute values for discrete attributes. Now our attribute names are much easier to work with, as we can see in Data Table (1).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "EnKlik Anketa Import data from EnKlikAnketa (1ka.si) public URL.\nOutputs\n Data: survey results  The EnKlik Anketa widget retrieves survey results obtained from the EnKlikAnketa service. You need to create a public link to to retrieve the results. Go to the survey you wish to retrieve, then select Data (Podatki) tab and create a public link (javna povezava) at the top right corner.\nThen insert the link into the Public link URL field." ,
	"author" : "",
	"summary" : "EnKlik Anketa Import data from EnKlikAnketa (1ka.si) public URL.\nOutputs\n Data: survey results  The EnKlik Anketa widget retrieves survey results obtained from the EnKlikAnketa service. You need to create a public link to to retrieve the results. Go to the survey you wish to retrieve, then select Data (Podatki) tab and create a public link (javna povezava) at the top right corner.\nThen insert the link into the Public link URL field.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "EnKlik Anketa",
	"icon" : ""
},
{
    "uri": "/faq/",
	"title": "Faq",
	"description": "",
	"content": "I am having trouble installing Orange. Make sure you are downloading the latest version. You can also try to download a standalone installer or Anaconda distribution.\nStill no luck? Check our issue tracker for similar issues or report a new one.\nI am having trouble installing an add-on. Make sure you are using the latest version of Orange. Encountering issues, please raise them on the relevant add-on's issue tracker. Alternatively, you can try with Anaconda distribution and install the add-on in the terminal. You can use pip or conda to install a package directly. E.g.:\npip install orange3-text conda install orange3-timeseries  I get an error when using Orange. First, make sure you are using the latest version of Orange. If this doesn’t fix the issue, you should be able to see an error window pop up, prompting you to submit the issue to the developers. Please use this option as it allows us to recreate the issue and fix it. If this is not possible, please report the issue to our issue tracker.\nI believe there’s a bug in Orange. First, make sure you are using the latest version of Orange. If the bug is still present, please submit an issue to our issue tracker. Follow the template and describe the issue as well as possible.\nBefore submitting a new issue, check if someone has already reported the same problem. Upvote the issue with a +1 and/or add your comment. Only consider opening a new issue if you cannot find a corresponding existing open or closed issue.\nThere’s a feature I wish to see in Orange. Please submit a pull request with the feature. We welcome all contributions! Once you’ve made a PR, developer(s) will leave comments and when everything is fixed and tested, the pull request will be merged into Orange. See our Contribute guide for more information.\nIf you are not a developer, you can submit a feature request to our issue tracker. Describe what you wish to see in Orange and, if possible, provide a visual example.\nBefore submitting a new feature request, check if someone has already requested the same feature. Upvote the request with a +1 and/or add your comment. Only consider opening a new request if you cannot find a corresponding existing request.\nI would like to learn how to use Orange. Where can I find the resources? Widget documentation is available on our website. You can also access the same documentation directly from Orange by selecting the widget and pressing F1.\nWe also provide an e-mail course ( subscribe here), beginner-level YouTube tutorials and a regularly updated blog.\nI have a question concerning data mining, statistics and machine learning algorithms. Visit Data Science StackExchange and ask the community!\nWhat data does Orange support? Core Orange supports Excel, comma- and tab-delimited files (.xlsx, .csv, .tab). It also reads online data, such as Google Spreadsheets. Orange3-ImageAnalytics add-on can import images (.jpg, .tiff, .png). Orange3-Corpus add-on can import text files (.txt, .docx, .odt). SQL widget supports PostgreSQL and MSSQL databases.\nI want to use Orange on big data. Orange provides the SQL widget, which samples data instances and enables exploratory data analysis on large datasets. You need psycopg2 module installed to be able to see the widget. Install it directly in Python with\npip install psycopg2  command. You can also use 2UDA package for easier installation. Please note that Orange supports only PostgreSQL and MSSQL databases for now.\nCan I export Orange workflow as a Python script? Unfortunately, no. We’ve debated this long and hard. A (limited) functionality of this type would probably be possible, but would be a very big project to do well and would cost more than the expected benefits.\nI wish to propose a collaboration and/or a project with the Orange team. Please submit a contact request and we will respond as soon as we can.\nI wish to develop a widget / an add-on for Orange. We are always happy to receive contributions! Development documentation for widgets and add-ons is available on the web. Next, submit a pull request and our team will review it. If you have specific questions, use Gitter for direct communication with the developers.\nI wish to commission a custom module and/or an add-on for Orange. Please submit a contact request to discuss the idea and rates.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "I am having trouble installing Orange. Make sure you are downloading the latest version. You can also try to download a standalone installer or Anaconda distribution.\nStill no luck? Check our issue tracker for similar issues or report a new one.\nI am having trouble installing an add-on. Make sure you are using the latest version of Orange. Encountering issues, please raise them on the relevant add-on\u0026#39;s issue tracker. Alternatively, you can try with Anaconda distribution and install the add-on in the terminal." ,
	"author" : "",
	"summary" : "I am having trouble installing Orange. Make sure you are downloading the latest version. You can also try to download a standalone installer or Anaconda distribution.\nStill no luck? Check our issue tracker for similar issues or report a new one.\nI am having trouble installing an add-on. Make sure you are using the latest version of Orange. Encountering issues, please raise them on the relevant add-on\u0026#39;s issue tracker. Alternatively, you can try with Anaconda distribution and install the add-on in the terminal.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "faq",
	"LinkTitle" : "Faq",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/featureconstructor/",
	"title": "Feature Constructor",
	"description": "",
	"content": "Feature Constructor Add new features to your dataset.\nInputs\n Data: input dataset  Outputs\n Data: dataset with additional features  The Feature Constructor allows you to manually add features (columns) into your dataset. The new feature can be a computation of an existing one or a combination of several (addition, subtraction, etc.). You can choose what type of feature it will be (discrete, continuous or string) and what its parameters are (name, value, expression). For continuous variables you only have to construct an expression in Python.\n List of constructed variables Add or remove variables New feature name Expression in Python Select a feature Select a function Produce a report Press Send to communicate changes  For discrete variables, however, there’s a bit more work. First add or remove the values you want for the new feature. Then select the base value and the expression. In the example below, we have constructed an expression with ‘if lower than’ and defined three conditions; the program ascribes 0 (which we renamed to lower) if the original value is lower than 6, 1 (mid) if it is lower than 7 and 2 (higher) for all the other values. Notice that we use an underscore for the feature name (e.g. petal_length).\n List of variable definitions Add or remove variables New feature name Expression in Python Select a feature Select a function Assign values Produce a report Press Send to communicate changes  Example With the Feature Constructor you can easily adjust or combine existing features into new ones. Below, we added one new discrete feature to the Titanic dataset. We created a new attribute called Financial status and set the values to be rich if the person belongs to the first class (status = first) and not rich for everybody else. We can see the new dataset with Data Table widget.\nHints If you are unfamiliar with Python math language, here’s a quick introduction.\n +, - to add, subtract * to multiply / to divide % to divide and return the remainder ** for exponent (for square root square by 0.5) // for floor division \u003c, \u003e, \u003c=, \u003e= less than, greater than, less or equal, greater or equal == for equal != for not equal  As in the example: (value) if (feature name) \u003c (value), else (value) if (feature name) \u003c (value), else (value)\n[Use value 1 if feature is less than specified value, else use value 2 if feature is less than specified value 2, else use value 3.]\nSee more here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Feature Constructor Add new features to your dataset.\nInputs\n Data: input dataset  Outputs\n Data: dataset with additional features  The Feature Constructor allows you to manually add features (columns) into your dataset. The new feature can be a computation of an existing one or a combination of several (addition, subtraction, etc.). You can choose what type of feature it will be (discrete, continuous or string) and what its parameters are (name, value, expression)." ,
	"author" : "",
	"summary" : "Feature Constructor Add new features to your dataset.\nInputs\n Data: input dataset  Outputs\n Data: dataset with additional features  The Feature Constructor allows you to manually add features (columns) into your dataset. The new feature can be a computation of an existing one or a combination of several (addition, subtraction, etc.). You can choose what type of feature it will be (discrete, continuous or string) and what its parameters are (name, value, expression).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Feature Constructor",
	"icon" : ""
},
{
    "uri": "/workflows/Feature-Ranking/",
	"title": "Feature Ranking",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Feature Ranking",
	"icon" : ""
},
{
    "uri": "/workflows/Feature-Selection/",
	"title": "Feature Selection",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Feature Selection",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/featurestatistics/",
	"title": "Feature Statistics",
	"description": "",
	"content": "Feature Statistics Show basic statistics for data features.\nInputs\n Data: input data  Outputs\n Reduced data: table containing only selected features Statistics: table containing statistics of the selected features  The Feature Statistics widget provides a quick way to inspect and find interesting features in a given data set.\nThe Feature Statistics widget on the heart-disease data set. The feature exerc ind ang was manually changed to a meta variable for illustration purposes.\n Info on the current data set size and number and types of features The histograms on the right can be colored by any feature. If the selected feature is categorical, a discrete color palette is used (as shown in the example). If the selected feature is numerical, a continuous color palette is used. The table on the right contains statistics about each feature in the data set. The features can be sorted by each statistic, which we now describe. The feature type - can be one of categorical, numeric, time and string. The name of the feature. A histogram of feature values. If the feature is numeric, we appropriately discretize the values into bins. If the feature is categorical, each value is assigned its own bar in the histogram. The central tendency of the feature values. For categorical features, this is the mode. For numeric features, this is mean value. The dispersion of the feature values. For categorical features, this is the entropy of the value distribution. For numeric features, this is the coefficient of variation. The minimum value. This is computed for numerical and ordinal categorical features. The maximum value. This is computed for numerical and ordinal categorical features. The number of missing values in the data.  Notice also that some rows are colored differently. White rows indicate regular features, gray rows indicate class variables and the lighter gray indicates meta variables.\nExample The Feature Statistics widget is most often used after the File widget to inspect and find potentially interesting features in the given data set. In the following examples, we use the heart-disease data set.\nOnce we have found a subset of potentially interesting features, or we have found features that we would like to exclude, we can simply select the features we want to keep. The widget outputs a new data set with only these features.\nAlternatively, if we want to store feature statistics, we can use the Statistics output and manipulate those values as needed. In this example, we simply select all the features and display the statistics in a table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Feature Statistics Show basic statistics for data features.\nInputs\n Data: input data  Outputs\n Reduced data: table containing only selected features Statistics: table containing statistics of the selected features  The Feature Statistics widget provides a quick way to inspect and find interesting features in a given data set.\nThe Feature Statistics widget on the heart-disease data set. The feature exerc ind ang was manually changed to a meta variable for illustration purposes." ,
	"author" : "",
	"summary" : "Feature Statistics Show basic statistics for data features.\nInputs\n Data: input data  Outputs\n Reduced data: table containing only selected features Statistics: table containing statistics of the selected features  The Feature Statistics widget provides a quick way to inspect and find interesting features in a given data set.\nThe Feature Statistics widget on the heart-disease data set. The feature exerc ind ang was manually changed to a meta variable for illustration purposes.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Feature Statistics",
	"icon" : ""
},
{
    "uri": "/features/",
	"title": "Features",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "features",
	"LinkTitle" : "Features",
	"icon" : ""
},
{
    "uri": "/home/testimonials2/",
	"title": "Ferenc Borondics, Ph.D.",
	"description": "",
	"content": "",
	"image" : "/images/Borondics_Ferenc_small.jpg",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/Borondics_Ferenc_small.jpg",
	"kind" : "page",
	"type" : "testimonials",
	"LinkTitle" : "Ferenc Borondics, Ph.D.",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/file/",
	"title": "File",
	"description": "",
	"content": "File Reads attribute-value data from an input file.\nOutputs\n Data: dataset from the file  The File widget reads the input data file (data table with data instances) and sends the dataset to its output channel. The history of most recently opened files is maintained in the widget. The widget also includes a directory with sample datasets that come pre-installed with Orange.\nThe widget reads data from Excel (.xlsx), simple tab-delimited (.txt), comma-separated files (.csv) or URLs. For other formats see Other Formats section below.\n Browse through previously opened data files, or load any of the sample ones. Browse for a data file. Reloads currently selected data file. Insert data from URL addresses, including data from Google Sheets. Information on the loaded dataset: dataset size, number and types of data features. Additional information on the features in the dataset. Features can be edited by double-clicking on them. The user can change the attribute names, select the type of variable per each attribute (Continuous, Nominal, String, Datetime), and choose how to further define the attributes (as Features, Targets or Meta). The user can also decide to ignore an attribute. Browse documentation datasets. Produce a report.  Example Most Orange workflows would probably start with the File widget. In the schema below, the widget is used to read the data that is sent to both the Data Table and the Box Plot widget.\nLoading your data  Orange can import any comma, .xlsx or tab-delimited data file or URL. Use the File widget and then, if needed, select class and meta attributes. To specify the domain and the type of the attribute, attribute names can be preceded with a label followed by a hash. Use c for class and m for meta attribute, i to ignore a column, and C, D, S for continuous, discrete and string attribute types. Examples: C#mpg, mS#name, i#dummy. Orange’s native format is a tab-delimited text file with three header rows. The first row contains attribute names, the second the type (continuous, discrete or string), and the third the optional element (class, meta or time).  Read more on loading your data here.\nOther Formats Supported formats and the widgets to load them:\n distance matrix: Distance File predictive model: Load Model network: Network File from Network add-on images: Import Images from Image Analytics add-on text/corpus: Corpus or Import Documents from Text add-on single cell data: Load Data from Single Cell add-on several spectroscopy files: Multifile from Spectroscopy add-on  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "File Reads attribute-value data from an input file.\nOutputs\n Data: dataset from the file  The File widget reads the input data file (data table with data instances) and sends the dataset to its output channel. The history of most recently opened files is maintained in the widget. The widget also includes a directory with sample datasets that come pre-installed with Orange.\nThe widget reads data from Excel (.xlsx), simple tab-delimited (." ,
	"author" : "",
	"summary" : "File Reads attribute-value data from an input file.\nOutputs\n Data: dataset from the file  The File widget reads the input data file (data table with data instances) and sends the dataset to its output channel. The history of most recently opened files is maintained in the widget. The widget also includes a directory with sample datasets that come pre-installed with Orange.\nThe widget reads data from Excel (.xlsx), simple tab-delimited (.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "File",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/filter/",
	"title": "Filter",
	"description": "",
	"content": "Filter Filter cells/genes.\nInputs\n Data: input dataset  Outputs\n Data: filtered dataset  The Filter widget filters cells or genes by the number of positive measurements. One can select genes in visualization to remove the unfrequent and overly frequent cells/genes.\n Information on the input and output. Filter by cells (rows), genes (columns) or data (remove zero measurments). If the box is ticked, data points are displayed. Filter by lower and upper threshold. Apply changes. If Commit automatically is ticked, changes will be communicated automatically. Alternatively, click Commit. Access help.  Example The Filter widget is used for filtering uninteresting cells, genes or data. By uninteresting we mean too frequent, too unfrequent or where data is zero (no expression). This allows us to have leaner data sets, which speeds up computation and enables easier analysis of results.\nWe have used ingle Cell Datasets to load Cell cycle in mESC (Fluidigm) data set. Then we used Filter widget to narrow down the selection of genes from 38,293 to 11,932. The width of our data table (number of columns) has descreased significantly. We have set the selection here manually (20 for lower and 170 for upper threshold), but you can also set the selection in the visualization by dragging the green field up or down.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Filter Filter cells/genes.\nInputs\n Data: input dataset  Outputs\n Data: filtered dataset  The Filter widget filters cells or genes by the number of positive measurements. One can select genes in visualization to remove the unfrequent and overly frequent cells/genes.\n Information on the input and output. Filter by cells (rows), genes (columns) or data (remove zero measurments). If the box is ticked, data points are displayed. Filter by lower and upper threshold." ,
	"author" : "",
	"summary" : "Filter Filter cells/genes.\nInputs\n Data: input dataset  Outputs\n Data: filtered dataset  The Filter widget filters cells or genes by the number of positive measurements. One can select genes in visualization to remove the unfrequent and overly frequent cells/genes.\n Information on the input and output. Filter by cells (rows), genes (columns) or data (remove zero measurments). If the box is ticked, data points are displayed. Filter by lower and upper threshold.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Filter",
	"icon" : ""
},
{
    "uri": "/home/testimonials3/",
	"title": "Francesca Vitali, Ph.D.",
	"description": "",
	"content": "",
	"image" : "/images/Vitali_Francesca_small.jpg",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/Vitali_Francesca_small.jpg",
	"kind" : "page",
	"type" : "testimonials",
	"LinkTitle" : "Francesca Vitali, Ph.D.",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/freeviz/",
	"title": "FreeViz",
	"description": "",
	"content": "FreeViz Displays FreeViz projection.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: FreeViz vectors  FreeViz uses a paradigm borrowed from particle physics: points in the same class attract each other, those from different class repel each other, and the resulting forces are exerted on the anchors of the attributes, that is, on unit vectors of each of the dimensional axis. The points cannot move (are projected in the projection space), but the attribute anchors can, so the optimization process is a hill-climbing optimization where at the end the anchors are placed such that forces are in equilibrium. The button Optimize is used to invoke the optimization process. The result of the optimization may depend on the initial placement of the anchors, which can be set in a circle, arbitrary or even manually. The later also works at any stage of optimization, and we recommend to play with this option in order to understand how a change of one anchor affects the positions of the data points. In any linear projection, projections of unit vector that are very short compared to the others indicate that their associated attribute is not very informative for particular classification task. Those vectors, that is, their corresponding anchors, may be hidden from the visualization using Radius slider in Show anchors box.\n Two initial positions of anchors are possible: random and circular. Optimization moves anchors in an optimal position. Set the color of the displayed points (you will get colors for discrete values and grey-scale points for continuous). Set label, shape and size to differentiate between points. Set symbol size and opacity for all data points. Anchors inside a circle are hidden. Circle radius can be be changed using a slider. Adjust plot properties:  Set jittering to prevent the dots from overlapping (especially for discrete attributes). Show legend displays a legend on the right. Click and drag the legend to move it. Show class density colors the graph by class (see the screenshot below). Label only selected points allows you to select individual data instances and label them.   Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom. If Send automatically is ticked, changes are communicated automatically. Alternatively, press Send. Save Image saves the created image to your computer in a .svg or .png format. Produce a report.  Manually move anchors One can manually move anchors. Use a mouse pointer and hover above the end of an anchor. Click the left button and then you can move selected anchor where ever you want.\nSelection Selection can be used to manually defined subgroups in the data. Use Shift modifier when selecting data instances to put them into a new group. Shift + Ctrl (or Shift + Cmd on macOs) appends instances to the last group.\nSignal data outputs a data table with an additional column that contains group indices.\nExplorative Data Analysis The FreeViz, as the rest of Orange widgets, supports zooming-in and out of part of the plot and a manual selection of data instances. These functions are available in the lower left corner of the widget. The default tool is Select, which selects data instances within the chosen rectangular area. Pan enables you to move the plot around the pane. With Zoom you can zoom in and out of the pane with a mouse scroll, while Reset zoom resets the visualization to its optimal size. An example of a simple schema, where we selected data instances from a rectangular region and sent them to the Data Table widget, is shown below.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "FreeViz Displays FreeViz projection.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: FreeViz vectors  FreeViz uses a paradigm borrowed from particle physics: points in the same class attract each other, those from different class repel each other, and the resulting forces are exerted on the anchors of the attributes, that is, on unit vectors of each of the dimensional axis." ,
	"author" : "",
	"summary" : "FreeViz Displays FreeViz projection.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: FreeViz vectors  FreeViz uses a paradigm borrowed from particle physics: points in the same class attract each other, those from different class repel each other, and the resulting forces are exerted on the anchors of the attributes, that is, on unit vectors of each of the dimensional axis.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "FreeViz",
	"icon" : ""
},
{
    "uri": "/widget-catalog/associate/frequentitemsets/",
	"title": "Frequent Itemsets",
	"description": "",
	"content": "Frequent Itemsets Finds frequent itemsets in the data.\nInputs\n Data: Data set  Outputs\n Matching Data: Data instances matching the criteria.  The widget finds frequent items in a data set based on a measure of support for the rule.\n Information on the data set. ‘Expand all’ expands the frequent itemsets tree, while ‘Collapse all’ collapses it. In Find itemsets by you can set criteria for itemset search:  Minimal support: a minimal ratio of data instances that must support (contain) the itemset for it to be generated. For large data sets it is normal to set a lower minimal support (e.g. between 2%-0.01%). Max. number of itemsets: limits the upward quantity of generated itemsets. Itemsets are generated in no particular order. If Auto find itemsets is on, the widget will run the search at every change of parameters. Might be slow for large data sets, so pressing Find itemsets only when the parameters are set is a good idea.   Filter itemsets: If you’re looking for a specific item or itemsets, filter the results by regular expressions. Separate regular expressions by comma to filter by more than one word.  Contains: will filter itemsets by regular expressions. Min. items: minimum number of items that have to appear in an itemset. If 1, all the itemsets will be displayed. Increasing it to, say, 4, will only display itemsets with four or more items. Max. items: maximum number of items that are to appear in an itemset. If you wish to find, say, only itemsets with less than 5 items in it, you’d set this parameter to 5. If Apply these filters in search is ticked, the widget will filter the results in real time. Preferably not ticked for large data sets.   If Auto send selection is on, changes are communicated automatically. Alternatively press Send selection.  Example Frequent Itemsets can be used directly with the File widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Frequent Itemsets Finds frequent itemsets in the data.\nInputs\n Data: Data set  Outputs\n Matching Data: Data instances matching the criteria.  The widget finds frequent items in a data set based on a measure of support for the rule.\n Information on the data set. \u0026lsquo;Expand all\u0026rsquo; expands the frequent itemsets tree, while \u0026lsquo;Collapse all\u0026rsquo; collapses it. In Find itemsets by you can set criteria for itemset search:  Minimal support: a minimal ratio of data instances that must support (contain) the itemset for it to be generated." ,
	"author" : "",
	"summary" : "Frequent Itemsets Finds frequent itemsets in the data.\nInputs\n Data: Data set  Outputs\n Matching Data: Data instances matching the criteria.  The widget finds frequent items in a data set based on a measure of support for the rule.\n Information on the data set. \u0026lsquo;Expand all\u0026rsquo; expands the frequent itemsets tree, while \u0026lsquo;Collapse all\u0026rsquo; collapses it. In Find itemsets by you can set criteria for itemset search:  Minimal support: a minimal ratio of data instances that must support (contain) the itemset for it to be generated.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Frequent Itemsets",
	"icon" : ""
},
{
    "uri": "/home/testimonials1/",
	"title": "Gad Shaulsky, Ph.D.",
	"description": "",
	"content": "",
	"image" : "/images/Shaulsky_Gad_small.jpg",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "/images/Shaulsky_Gad_small.jpg",
	"kind" : "page",
	"type" : "testimonials",
	"LinkTitle" : "Gad Shaulsky, Ph.D.",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/gene_set_enrichment/",
	"title": "Gene Set Enrichment",
	"description": "",
	"content": "Gene Set Enrichment Enrich gene sets.\nInputs\n Data: Data set. Custom Gene Sets: Genes to compare. Reference Genes: Genes used as reference.  Outputs\n Matched Genes: Gene that match.  TODO Description.\n Info Custom Gene Set Term Column Reference Gene Sets If Commit Automatically is ticked, results will be automatically sent to the output. Alternatively, press Commit. Filtering.  Example TODO Example\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Gene Set Enrichment Enrich gene sets.\nInputs\n Data: Data set. Custom Gene Sets: Genes to compare. Reference Genes: Genes used as reference.  Outputs\n Matched Genes: Gene that match.  TODO Description.\n Info Custom Gene Set Term Column Reference Gene Sets If Commit Automatically is ticked, results will be automatically sent to the output. Alternatively, press Commit. Filtering.  Example TODO Example" ,
	"author" : "",
	"summary" : "Gene Set Enrichment Enrich gene sets.\nInputs\n Data: Data set. Custom Gene Sets: Genes to compare. Reference Genes: Genes used as reference.  Outputs\n Matched Genes: Gene that match.  TODO Description.\n Info Custom Gene Set Term Column Reference Gene Sets If Commit Automatically is ticked, results will be automatically sent to the output. Alternatively, press Commit. Filtering.  Example TODO Example",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Gene Set Enrichment",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/genes/",
	"title": "Genes",
	"description": "",
	"content": "Genes Match input gene ID’s with corresponding Entrez ID’s.\nInputs\n Data: Data set.  Outputs\n Data: Instances with meta data that the user has manually selected in the widget. Genes: All genes from the input with included gene info summary and matcher result.  To work with widgets in the bioinformatics add-on data sets must be properly annotated. We need to specify:\n Location of genes in a table (rows, columns) ID from the NCBI Gene database (Entrez ID) Organism (Taxonomy ID)  Genes is a useful widget that presents information on the genes from the NCBI Gene database and outputs annotated data table. You can also select a subset and feed it to other widgets. By clicking on the gene Entrez ID in the list, you will be taken to the NCBI site with the information on the gene.\nExample First we load brown-selected.tab (from Browse documentation data sets) with the File widget and feed our data to the Genes widget. Orange recognized the organism correctly, but we have to tell it where our gene labels are. To do this, we tick off Stored as feature (column) name and select gene attribute from the list. Then we can observe gene info provided from the NCBI Gene database. In the Data Table we can see the Entrez ID column included as a meta attribute. The data is also properly annotated (see Data Attributes section in Data Info widget).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Genes Match input gene ID\u0026rsquo;s with corresponding Entrez ID\u0026rsquo;s.\nInputs\n Data: Data set.  Outputs\n Data: Instances with meta data that the user has manually selected in the widget. Genes: All genes from the input with included gene info summary and matcher result.  To work with widgets in the bioinformatics add-on data sets must be properly annotated. We need to specify:\n Location of genes in a table (rows, columns) ID from the NCBI Gene database (Entrez ID) Organism (Taxonomy ID)  Genes is a useful widget that presents information on the genes from the NCBI Gene database and outputs annotated data table." ,
	"author" : "",
	"summary" : "Genes Match input gene ID\u0026rsquo;s with corresponding Entrez ID\u0026rsquo;s.\nInputs\n Data: Data set.  Outputs\n Data: Instances with meta data that the user has manually selected in the widget. Genes: All genes from the input with included gene info summary and matcher result.  To work with widgets in the bioinformatics add-on data sets must be properly annotated. We need to specify:\n Location of genes in a table (rows, columns) ID from the NCBI Gene database (Entrez ID) Organism (Taxonomy ID)  Genes is a useful widget that presents information on the genes from the NCBI Gene database and outputs annotated data table.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Genes",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/geo_data_sets/",
	"title": "GEO Data Sets",
	"description": "",
	"content": "GEO Data Sets Provides access to data sets from gene expression omnibus GEO DataSets.\nInputs\n None  Outputs\n Expression data: Data set selected in the widget with genes or samples in rows.  GEO DataSets is a database of gene expression curated profiles maintained by NCBI and included in the Gene Expression Omnibus. This Orange widget provides access to all its data sets and outputs a data set selected for further processing. For convenience, each dowloaded data set is stored locally.\n Information on the GEO data set collection. Cached data sets are the ones currently stored on the computer. Output features. If Samples in rows is selected, genes (or spots) will be used as attributes. Alternatively samples will be used as attributes. Merge spots of same gene averages measures of the same gene. Finally, in the Data set name you can rename the output data. GEO title will be used as a default name. If Auto commit is on, then the selected data set will be automatically communicated to other widgets. Alternatively, click Commit. Filter allows you to search for the data set. Below you see a list of GEO data sets with an ID number (link to the NCBI Data Set Browser), title, organism used in the experiment, number of samples, features, genes, subsets and a reference number for the PubMed journal (link to the article abstract). Short description of the experiment from which the data set is sourced. Select which Sample Annotations will be used in the output.  Example GEO Data Sets is similar to the File widget, since it is used to load the data. In the example below we selected Caffeine effect: time course and dose response dataset from the GEO data base. Do not forget to press Commit to output the data. We can inspect the data in Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "GEO Data Sets Provides access to data sets from gene expression omnibus GEO DataSets.\nInputs\n None  Outputs\n Expression data: Data set selected in the widget with genes or samples in rows.  GEO DataSets is a database of gene expression curated profiles maintained by NCBI and included in the Gene Expression Omnibus. This Orange widget provides access to all its data sets and outputs a data set selected for further processing." ,
	"author" : "",
	"summary" : "GEO Data Sets Provides access to data sets from gene expression omnibus GEO DataSets.\nInputs\n None  Outputs\n Expression data: Data set selected in the widget with genes or samples in rows.  GEO DataSets is a database of gene expression curated profiles maintained by NCBI and included in the Gene Expression Omnibus. This Orange widget provides access to all its data sets and outputs a data set selected for further processing.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "GEO Data Sets",
	"icon" : ""
},
{
    "uri": "/widget-catalog/geo/geomap/",
	"title": "Geo Map",
	"description": "",
	"content": "Geo Map Show data points on a map.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  Geo Map widget visualizes geo-spatial data on a map. It works on datasets containing latitude and longitude variables in WGS 84 (EPSG:4326) format. We can use it much like we use Scatter Plot widget.\n Set the type of map: OpenStreetMap, Black and White, Topographic, Satellite, Print, Dark. Set latitude and longitude attributes, if the widget didn’t recognize them automatically. Latitude values should be between -85.0511(S) and 85.0511(N) (a limitation of the projections onto flat maps) and longitude values between -180(W) and 180(E). Set color, shape, size and label to differentiate between points. Set symbol size, opacity and jittering for all data points. Adjust plot properties:  Show color region colors the graph by class (color must be selected). Show legend displays a legend on the right. Click and drag the legend to move it. Freeze map freezes the map so it doesn’t update when input data changes.   Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Scroll in or out for zoom. If Send automatically is ticked, changes are communicated automatically. Alternatively, press Send.  Examples In this simple example we visualize the Philadelphia Crime dataset that we can find in the Datasets widget. We connect the output of that widget to the Map widget. Latitude and longitude variables get automatically detected and we additionally select the crime type variable for color. We can observe how different crimes are more present in specific areas of the city.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Geo Map Show data points on a map.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  Geo Map widget visualizes geo-spatial data on a map. It works on datasets containing latitude and longitude variables in WGS 84 (EPSG:4326) format. We can use it much like we use Scatter Plot widget." ,
	"author" : "",
	"summary" : "Geo Map Show data points on a map.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  Geo Map widget visualizes geo-spatial data on a map. It works on datasets containing latitude and longitude variables in WGS 84 (EPSG:4326) format. We can use it much like we use Scatter Plot widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Geo Map",
	"icon" : ""
},
{
    "uri": "/widget-catalog/geo/geocoding/",
	"title": "Geocoding",
	"description": "",
	"content": "Geocoding Encode region names into geographical coordinates, or reverse-geocode latitude and longitude pairs into regions.\nInputs\n Data: An input data set.  Outputs\n Coded Data: Data set with new meta attributes.  Geocoding widget extracts latitude/longitude pairs from region names or synthesizes latitude/longitude to return region name. If the region is large, say a country, encoder with return the latitude and longitude of geometric centre.\n Use region names to extract the corresponding latitude/longitude pairs:  Region identifier: attribute with the information on region names. Can be discrete or string. Identifier type: define how the data is coded. Supports ISO codes and some major cities and countries.   Use latitude and longitude pairs to retrieve region names:  Latitude attribute. Longitude attribute. Administrative level of the region you wish to extract.   Extend coded data adds additional information on the region of interest. For countries, for example, one would get economy, type, continent, etc. If Apply Automatically is ticked, the changes will be communicated automatically. Alternatively, press Apply. Unmatched identifiers editor. Match regions names that couldn’t be matched automatically with their corresponding name.  Example We will use HDI data from the Datasets widget. Open the widget, find HDI data, select it and press Send. First, let us observe the data in a Data Table. We have a meta attribute names Country, which contains country names. Now we would like to plot this on a map, but Geo Map widget requires latitude and longitude pairs. Geocoding will help us extract this information from country names.\nConnect Geocoding to Datasets. Region identifier in our case is the attribute Country and the identifier type is Country name. If our data contained major European cities, we would have to select this from the dropdown. On the right there is the Unmatched identifier editor, which shows those instances, for which Geocoding couldn’t find corresponding latitude/longitude pairs. We can help the widget by providing a custom replacement. Click on the field and start typing Korea. The widget will suggest two countries, Democratic Republic of Korea and South Korea. Select the one you wish to use here.\nFinally, we can observe the data in the second Data Table. We can see our data now has two additional attributes, one for the latitude and one for the longitude of the region of interest. Now, you can plot the data on the map!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Geocoding Encode region names into geographical coordinates, or reverse-geocode latitude and longitude pairs into regions.\nInputs\n Data: An input data set.  Outputs\n Coded Data: Data set with new meta attributes.  Geocoding widget extracts latitude/longitude pairs from region names or synthesizes latitude/longitude to return region name. If the region is large, say a country, encoder with return the latitude and longitude of geometric centre.\n Use region names to extract the corresponding latitude/longitude pairs:  Region identifier: attribute with the information on region names." ,
	"author" : "",
	"summary" : "Geocoding Encode region names into geographical coordinates, or reverse-geocode latitude and longitude pairs into regions.\nInputs\n Data: An input data set.  Outputs\n Coded Data: Data set with new meta attributes.  Geocoding widget extracts latitude/longitude pairs from region names or synthesizes latitude/longitude to return region name. If the region is large, say a country, encoder with return the latitude and longitude of geometric centre.\n Use region names to extract the corresponding latitude/longitude pairs:  Region identifier: attribute with the information on region names.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Geocoding",
	"icon" : ""
},
{
    "uri": "/getting-started/",
	"title": "Getting started",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "getting-started",
	"LinkTitle" : "Getting started",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/go_browser/",
	"title": "GO Browser",
	"description": "",
	"content": "GO Browser Provides access to Gene Ontology database.\nInputs\n Cluster Data: Data on clustered genes. Reference Data: Data with genes for the reference set (optional).  Outputs\n Data on Selected Genes: Data on genes from the selected GO node. Enrichment Report: Data on GO enrichment analysis.  GO Browser widget provides access to Gene Ontology database. Gene Ontology (GO) classifies genes and gene products to terms organized in a graph structure called an ontology. The widget takes any data on genes as an input (it is best to input statistically significant genes, for example from the output of the Differential Expression widget) and shows a ranked list of GO terms with p-values. This is a great tool for finding biological processes that are over- or under-represented in a particular gene set. The user can filter input data by selecting terms in a list.\nINPUT tab\n  Information on the input data set. Ontology/Annotation Info reports the current status of the GO database.\n  Select the reference. You can either have the entire genome as reference or a reference set from the input.\n  Select the ontology where you want to calculate the enrichment. There are three Aspect options:\n Biological process Cellular component) Molecular function    A ranked tree (upper pane) and list (lower pane) of GO terms for the selected aspect:\n GO term Cluster: number of genes from the input that are also annotated to a particular GO term (and its proportion in all the genes from that term). Reference: number of genes that are annotated to a particular GO term (and its proportion in the entire genome). P-value: probability of seeing as many or more genes at random. The closer the p-value is to zero, the more significant a particular GO term is. Value is written in e notation). FDR: false discovery rate - a multiple testing correction that means a proportion of false discoveries among all discoveries up to that FDR value. Genes: genes in a biological process. Enrichment level    FILTER tab\n  Filter GO Term Nodes by:\n Genes is a minimal number of genes mapped to a term P-value is a max term p-value FDR: is a max term false discovery rate    Significance test specifies distribution to use for null hypothesis:\n Binomial: use a binomial distribution Hypergeometric: use a hypergeometric distribution    Evidence codes in annotation show how the annotation to a particular term is supported.\n  SELECT tab\n Annotated genes outputs genes that are:\n Directly or Indirectly annotated (direct and inherited annotations) Directly annotated (inherited annotations won’t be in the output)    Output:\n All selected genes: outputs genes annotated to all selected GO terms Term-specific genes: outputs genes that appear in only one of selected GO terms Common term genes: outputs genes common to all selected GO terms Add GO Term as class: adds GO terms as class attribute    Example In the example below we have used GEO Data Sets widget, in which we have selected Caffeine effects: time course and dose response data set, and connected it to a Differential Expression. Differential analysis allows us to select genes with the highest statistical relevance (we used ANOVA scoring and agent label) and feed them to GO Browser. This widget lists four biological processes for our selected genes. Say we are interested in finding out more about monosaccharide transmembrane transport as this term has a high enrichment rate. To learn more about which genes are annotated to this GO term, select it in the view and observe the results in a Data Table, where we see all the genes participating in this process listed. The other output of GO Browser widget is enrichment report, which we observe in the second Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "GO Browser Provides access to Gene Ontology database.\nInputs\n Cluster Data: Data on clustered genes. Reference Data: Data with genes for the reference set (optional).  Outputs\n Data on Selected Genes: Data on genes from the selected GO node. Enrichment Report: Data on GO enrichment analysis.  GO Browser widget provides access to Gene Ontology database. Gene Ontology (GO) classifies genes and gene products to terms organized in a graph structure called an ontology." ,
	"author" : "",
	"summary" : "GO Browser Provides access to Gene Ontology database.\nInputs\n Cluster Data: Data on clustered genes. Reference Data: Data with genes for the reference set (optional).  Outputs\n Data on Selected Genes: Data on genes from the selected GO node. Enrichment Report: Data on GO enrichment analysis.  GO Browser widget provides access to Gene Ontology database. Gene Ontology (GO) classifies genes and gene products to terms organized in a graph structure called an ontology.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "GO Browser",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/google-sheets/",
	"title": "Google Sheets",
	"description": "",
	"content": "Google Sheets Read data from a Google Sheets spreadsheet.\nOutputs\n Data: data set from the Google Sheets service.  Description The widget reads data from the Google Sheets service. To use the widget, click the Share button in a selected spreadsheet, copy the provided link and paste it into the widget’s URL line. Press enter to load the data. To observe the data in real time, use the Reload function.\n Enter the link to the spreadsheet. Press Enter to load the data. Set reload if you wish to observe the updates in real time. Information on the data set: name and attributes. If Commit Automatically is ticked, the data will be automatically communicated downstream. Alternatively, press Commit.  Example This widget is used for loading the data. We have used the link from the Google Sheets: https://goo.gl/jChYki. This is a fictional data on hamsters and rabbits, of which some have the disease and some don’t. Use the Data Table to observe the loaded data in a spreadsheet.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Google Sheets Read data from a Google Sheets spreadsheet.\nOutputs\n Data: data set from the Google Sheets service.  Description The widget reads data from the Google Sheets service. To use the widget, click the Share button in a selected spreadsheet, copy the provided link and paste it into the widget\u0026rsquo;s URL line. Press enter to load the data. To observe the data in real time, use the Reload function." ,
	"author" : "",
	"summary" : "Google Sheets Read data from a Google Sheets spreadsheet.\nOutputs\n Data: data set from the Google Sheets service.  Description The widget reads data from the Google Sheets service. To use the widget, click the Share button in a selected spreadsheet, copy the provided link and paste it into the widget\u0026rsquo;s URL line. Press enter to load the data. To observe the data in real time, use the Reload function.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Google Sheets",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/gradientboosting/",
	"title": "Gradient Boosting",
	"description": "",
	"content": "Gradient Boosting Predict using gradient boosting on decision trees.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: gradient boosting learning algorithm Model: trained model  Gradient Boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n Specify the name of the model. The default name is “Gradient Boosting”. Select a gradient boosting method:  Gradient Boosting (scikit-learn) Extreme Gradient Boosting (xgboost) Extreme Gradient Boosting Random Forest (xgboost) Gradient Boosting (catboost)   Basic properties:  Number of trees: Specify how many gradient boosted trees will be included. A large number usually results in better performance. Learning rate: Specify the boosting learning rate. Learning rate shrinks the contribution of each tree. Replicable training: Fix the random seed, which enables replicability of the results. Regularization: Specify the L2 regularization term. Available only for xgboost and catboost methods.   Growth control:  Limit depth of individual trees: Specify the maximum depth of the individual tree. Do not split subsets smaller than: Specify the smallest subset that can be split. Available only for scikit-learn methods.   Subsampling:  Fraction of training instances: Specify the percentage of the training instances for fitting the individual tree. Available for scikit-learn and xgboost methods. Fraction of features for each tree: Specify the percentage of features to use when constructing each tree. Available for xgboost and catboost methods. Fraction of features for each level: Specify the percentage of features to use for each level. Available only for xgboost methods. Fraction of features for each split: Specify the percentage of features to use for each split. Available only for xgboost methods.   Click Apply to communicate the changes to other widgets. Alternatively, tick the box on the left side of the Apply button and changes will be communicated automatically.  Example For a classification tasks, we use the heart disease data. Here, we compare all available methods in the Test \u0026 Score widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Gradient Boosting Predict using gradient boosting on decision trees.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: gradient boosting learning algorithm Model: trained model  Gradient Boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n Specify the name of the model. The default name is \u0026ldquo;Gradient Boosting\u0026rdquo;." ,
	"author" : "",
	"summary" : "Gradient Boosting Predict using gradient boosting on decision trees.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: gradient boosting learning algorithm Model: trained model  Gradient Boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.\n Specify the name of the model. The default name is \u0026ldquo;Gradient Boosting\u0026rdquo;.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Gradient Boosting",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/gradient-descent/",
	"title": "Gradient Descent",
	"description": "",
	"content": "Gradient Descent Educational widget that shows the gradient descent algorithm on a logistic or linear regression.\nInputs\n Data: input data set  Outputs\n Data: data with columns selected in the widget Classifier: model produced at the current step of the algorithm. Coefficients: coefficients at the current step of the algorithm.  Description This widget incrementally shows steps of gradient descent for a logistic or linear regression. Gradient descent is demonstrated on two attributes that are selected by the user.\nGradient descent is performed on logistic regression if the class in the data set is categorical and linear regression if the class is numeric.\n Select two attributes (x and y) on which the gradient descent algorithm is preformed. Select the target class. It is the class that is classified against all other classes. Learning rate is a step size in the gradient descent With stochastic checkbox you can select whether gradient descent is stochastic or not. If stochastic is checked you can set step size that is amount of steps of stochastic gradient descent performed in one press on step button. Restart: start algorithm from the beginning Step: perform one step of the algorithm Step back: make a step back in the algorithm Run: automatically perform several steps until the algorithm converges Speed: set speed of the automatic stepping Save Image saves the image to the computer in a .svg or .png format. Report includes widget parameters and visualization in the report.  Example In Orange we connected File widget with Iris data set to the Gradient Descent widget. Iris data set has discrete class, so Logistic regression will be used this time. We connected outputs of the widget to Predictions widget to see how the data are classified and the Data Table widget where we inspect coefficients of logistic regression.\nWe open the Gradient Descent widget and set X to sepal width and Y to sepal length. Target class is set to Iris-virginica. We set learning rate to 0.02. With a click in the graph we set the initial coefficients (red dot).\nWe perform step of the algorithm by pressing the Step button. When we get bored with clicking we can finish stepping by pressing on the Run button.\nIf we want to go back in the algorithm we can do it by pressing Step back button. This will also change the model. Current model uses positions of last coefficients (red-yellow dot).\nIn the end we want to see the predictions for input data so we can open the Predictions widget. Predictions are listed in the left column. We can compare this predictions to the real classes.\nIf we want to demonstrate the linear regression we can change the data set to Housing. This data set has a continuous class variable. When using linear regression we can select only one feature which means that our function is linear. Another parameter that is plotted in the graph is intercept of a linear function.\nThis time we selected INDUS as an independent variable. In the widget we can make the same actions as before. In the end we can also check the predictions for each point with the Predictions widget. And check coefficients of linear regression in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Gradient Descent Educational widget that shows the gradient descent algorithm on a logistic or linear regression.\nInputs\n Data: input data set  Outputs\n Data: data with columns selected in the widget Classifier: model produced at the current step of the algorithm. Coefficients: coefficients at the current step of the algorithm.  Description This widget incrementally shows steps of gradient descent for a logistic or linear regression. Gradient descent is demonstrated on two attributes that are selected by the user." ,
	"author" : "",
	"summary" : "Gradient Descent Educational widget that shows the gradient descent algorithm on a logistic or linear regression.\nInputs\n Data: input data set  Outputs\n Data: data with columns selected in the widget Classifier: model produced at the current step of the algorithm. Coefficients: coefficients at the current step of the algorithm.  Description This widget incrementally shows steps of gradient descent for a logistic or linear regression. Gradient descent is demonstrated on two attributes that are selected by the user.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Gradient Descent",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/granger_causality/",
	"title": "Granger Causality",
	"description": "",
	"content": "Granger Causality Test if one time series Granger-causes (i.e. can be an indicator of) another time series.\nInputs\n Time series: Time series as output by As Timeseries widget.  This widgets performs a series of statistical tests to determine the series that cause other series so we can use the former to forecast the latter.\n Desired level of confidence. Maximum lag to test to. Runs the test. Denotes the minimum lag at which one series can be said to cause another. In the first line of the example above, if we have the monthly unemployment rate time series for Austria, we can say something about unemployment rate in Hungary 10 months ahead. The causing (antecedent) series. The effect (consequent) series.  The time series that Granger-cause the series you are interested in are good candidates to have in the same VAR model. But careful, even if one series is said to Granger-cause another, this doesn’t mean there really exists a causal relationship. Mind your conclusions.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Granger Causality Test if one time series Granger-causes (i.e. can be an indicator of) another time series.\nInputs\n Time series: Time series as output by As Timeseries widget.  This widgets performs a series of statistical tests to determine the series that cause other series so we can use the former to forecast the latter.\n Desired level of confidence. Maximum lag to test to. Runs the test. Denotes the minimum lag at which one series can be said to cause another." ,
	"author" : "",
	"summary" : "Granger Causality Test if one time series Granger-causes (i.e. can be an indicator of) another time series.\nInputs\n Time series: Time series as output by As Timeseries widget.  This widgets performs a series of statistical tests to determine the series that cause other series so we can use the former to forecast the latter.\n Desired level of confidence. Maximum lag to test to. Runs the test. Denotes the minimum lag at which one series can be said to cause another.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Granger Causality",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/heatmap/",
	"title": "Heat Map",
	"description": "",
	"content": "Heat Map Plots a heat map for a pair of attributes.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot  Heat map is a graphical method for visualizing attribute values by class in a two-way matrix. It only works on datasets containing continuous variables. The values are represented by color: the higher a certain value is, the darker the represented color. By combining class and attributes on x and y axes, we see where the attribute values are the strongest and where the weakest, thus enabling us to find typical features (discrete) or value range (continuous) for each class.\n The color scheme legend. Low and High are thresholds for the color palette (low for attributes with low values and high for attributes with high values). Selecting one of diverging palettes, which have two extreme colors and a neutral (black or white) color at the midpoint, enables an option to set a meaningful mid-point value (default is 0). Merge data. Sort columns and rows:  No Sorting (lists attributes as found in the dataset) Clustering (clusters data by similarity) Clustering with ordered leaves (maximizes the sum of similarities of adjacent elements)   Set what is displayed in the plot in Annotation \u0026 Legend.  If Show legend is ticked, a color chart will be displayed above the map. If Stripes with averages is ticked, a new line with attribute averages will be displayed on the left. Row Annotations adds annotations to each instance on the right. Column Label Positions places column labels in a selected place (None, Top, Bottom, Top and Bottom).   If Keep aspect ratio is ticked, each value will be displayed with a square (proportionate to the map). If Send Automatically is ticked, changes are communicated automatically. Alternatively, click Send. Save image saves the image to your computer in a .svg or .png format. Produce a report.  Example The Heat Map below displays attribute values for the Housing dataset. The aforementioned dataset concerns the housing values in the suburbs of Boston.\nThe first thing we see in the map are the ‘B’ and ‘Tax’ attributes, which are the only two colored in dark orange. The ‘B’ attribute provides information on the proportion of blacks by town and the ‘Tax’ attribute informs us about the full-value property-tax rate per $10,000. In order to get a clearer heat map, we then use the Select Columns widget and remove the two attributes from the dataset. Then we again feed the data to the Heat map. The new projection offers additional information.\nBy removing ‘B’ and ‘Tax’, we can see other deciding factors, namely ‘Age’ and ‘ZN’. The ‘Age’ attribute provides information on the proportion of owner-occupied units built prior to 1940 and the ‘ZN’ attribute informs us about the proportion of non-retail business acres per town.\nThe Heat Map widget is a nice tool for discovering relevant features in the data. By removing some of the more pronounced features, we came across new information, which was hiding in the background.\nReferences Housing Dataset\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Heat Map Plots a heat map for a pair of attributes.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot  Heat map is a graphical method for visualizing attribute values by class in a two-way matrix. It only works on datasets containing continuous variables. The values are represented by color: the higher a certain value is, the darker the represented color. By combining class and attributes on x and y axes, we see where the attribute values are the strongest and where the weakest, thus enabling us to find typical features (discrete) or value range (continuous) for each class." ,
	"author" : "",
	"summary" : "Heat Map Plots a heat map for a pair of attributes.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot  Heat map is a graphical method for visualizing attribute values by class in a two-way matrix. It only works on datasets containing continuous variables. The values are represented by color: the higher a certain value is, the darker the represented color. By combining class and attributes on x and y axes, we see where the attribute values are the strongest and where the weakest, thus enabling us to find typical features (discrete) or value range (continuous) for each class.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Heat Map",
	"icon" : ""
},
{
    "uri": "/workflows/Hierarchical-Clustering/",
	"title": "Hierarchical Clustering",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Hierarchical Clustering",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/hierarchicalclustering/",
	"title": "Hierarchical Clustering",
	"description": "",
	"content": "Hierarchical Clustering Groups items using a hierarchical clustering algorithm.\nInputs\n Distances: distance matrix  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether an instance is selected  The widget computes hierarchical clustering of arbitrary types of objects from a matrix of distances and shows a corresponding dendrogram.\n The widget supports four ways of measuring distances between clusters:  Single linkage computes the distance between the closest elements of the two clusters Average linkage computes the average distance between elements of the two clusters Weighted linkage uses the WPGMA method Complete linkage computes the distance between the clusters’ most distant elements   Labels of nodes in the dendrogram can be chosen in the Annotation box. Huge dendrograms can be pruned in the Pruning box by selecting the maximum depth of the dendrogram. This only affects the display, not the actual clustering. The widget offers three different selection methods:  Manual (Clicking inside the dendrogram will select a cluster. Multiple clusters can be selected by holding Ctrl/Cmd. Each selected cluster is shown in a different color and is treated as a separate cluster in the output.) Height ratio (Clicking on the bottom or top ruler of the dendrogram places a cutoff line in the graph. Items to the right of the line are selected.) Top N (Selects the number of top nodes.)   Use Zoom and scroll to zoom in or out. If the items being clustered are instances, they can be added a cluster index (Append cluster IDs). The ID can appear as an ordinary Attribute, Class attribute or a Meta attribute. In the second case, if the data already has a class attribute, the original class is placed among meta attributes. The data can be automatically output on any change (Auto send is on) or, if the box isn’t ticked, by pushing Send Data. Clicking this button produces an image that can be saved. Produce a report.  Examples The workflow below shows the output of Hierarchical Clustering for the Iris dataset in Data Table widget. We see that if we choose Append cluster IDs in hierarchical clustering, we can see an additional column in the Data Table named Cluster. This is a way to check how hierarchical clustering clustered individual instances.\nIn the second example, we loaded the Iris dataset again, but this time we added the Scatter Plot, showing all the instances from the File widget, while at the same time receiving the selected instances signal from Hierarchical Clustering. This way we can observe the position of the selected cluster(s) in the projection.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Hierarchical Clustering Groups items using a hierarchical clustering algorithm.\nInputs\n Distances: distance matrix  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether an instance is selected  The widget computes hierarchical clustering of arbitrary types of objects from a matrix of distances and shows a corresponding dendrogram.\n The widget supports four ways of measuring distances between clusters:  Single linkage computes the distance between the closest elements of the two clusters Average linkage computes the average distance between elements of the two clusters Weighted linkage uses the WPGMA method Complete linkage computes the distance between the clusters\u0026rsquo; most distant elements   Labels of nodes in the dendrogram can be chosen in the Annotation box." ,
	"author" : "",
	"summary" : "Hierarchical Clustering Groups items using a hierarchical clustering algorithm.\nInputs\n Distances: distance matrix  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether an instance is selected  The widget computes hierarchical clustering of arbitrary types of objects from a matrix of distances and shows a corresponding dendrogram.\n The widget supports four ways of measuring distances between clusters:  Single linkage computes the distance between the closest elements of the two clusters Average linkage computes the average distance between elements of the two clusters Weighted linkage uses the WPGMA method Complete linkage computes the distance between the clusters\u0026rsquo; most distant elements   Labels of nodes in the dendrogram can be chosen in the Annotation box.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Hierarchical Clustering",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/hyperspectra/",
	"title": "HyperSpectra",
	"description": "",
	"content": "HyperSpectra Plots 2D map of hyperspectra.\nInputs\n Data: input dataset  Outputs\n Selection: spectra from selected area Data: dataset with information whether a spectrum was selected or not  The HyperSpectra widget plots hyperspectra that were read from the .map file. To use this widget with infrared spectral data, you need to transform it with Reshape Map widget.\nAt the top, HyperSpectra shows a 2D map of a slice of the spectra. At the bottom, a spectra plot is shown with the red line indicating the wavenumber slice we are observing at the top.\n Image values: define the transformation (usually an integral) of the spectra or use a feature to use as values for the plot. The former transformation can be an integral from 0, integral from baseline, peak from 0, peak from baseline, closest value, X-value of maximum from 0 or X-value of maximum from baseline. The hyperspectral plot of the slice of the spectra.  Zoom in (Z): zoom in to the area selected from the hyperspectral plot Zoom to fit (backspace): return to the original plot Select (square) (S): select an area from the plot by clicking at the top left corner and then the bottom right corner of the desired selection area Select (polygon) (P): select an area by circumscribing a polygon Save graph (Mod + S): save the visualization as a .png, .svg or .pdf file. Axis x: define the attribute for the x axis Axis y: define the attribute for the y axis Color: select the color for the plot   The spectral plot of the selected image region. It behaves like the Spectra widget. Region selectors for the chosen integration method. Split between image and spectral view: move it to increase the image size.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "HyperSpectra Plots 2D map of hyperspectra.\nInputs\n Data: input dataset  Outputs\n Selection: spectra from selected area Data: dataset with information whether a spectrum was selected or not  The HyperSpectra widget plots hyperspectra that were read from the .map file. To use this widget with infrared spectral data, you need to transform it with Reshape Map widget.\nAt the top, HyperSpectra shows a 2D map of a slice of the spectra." ,
	"author" : "",
	"summary" : "HyperSpectra Plots 2D map of hyperspectra.\nInputs\n Data: input dataset  Outputs\n Selection: spectra from selected area Data: dataset with information whether a spectrum was selected or not  The HyperSpectra widget plots hyperspectra that were read from the .map file. To use this widget with infrared spectral data, you need to transform it with Reshape Map widget.\nAt the top, HyperSpectra shows a 2D map of a slice of the spectra.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "HyperSpectra",
	"icon" : ""
},
{
    "uri": "/widget-catalog/image-analytics/imageembedding/",
	"title": "Image Embedding",
	"description": "",
	"content": "Image Embedding Image embedding through deep neural networks.\nInputs\n Images: List of images.  Outputs\n Embeddings: Images represented with a vector of numbers. Skipped Images: List of images where embeddings were not calculated.  Image Embedding reads images and uploads them to a remote server or evaluate them locally. Deep learning models are used to calculate a feature vector for each image. It returns an enhanced data table with additional columns (image descriptors).\nImages can be imported with Import Images widget or as paths to images in a spreadsheet. In this case the column with images paths needs a three-row header with type=image label in the third row.\nImage Embedding offers several embedders, each trained for a specific task. Images are sent to a server or they are evaluated locally on the user’s computer, where vectors representations are computed. SqueezeNet embedder offers a fast evaluation on users computer which does not require an internet connection. If you decide to use other embedders than SqueezeNet, you will need an internet connection. Images sent to the server are not stored anywhere.\n  Information on the number of embedded images and images skipped.\n  Settings:\n  Image attribute: attribute containing images you wish to embed\n  Embedder:\n SqueezeNet: Small and fast model for image recognition trained on ImageNet. Inception v3: Google’s Inception v3 model trained on ImageNet. VGG-16: 16-layer image recognition model trained on ImageNet. VGG-19: 19-layer image recognition model trained on ImageNet. Painters: A model trained to predict painters from artwork images. DeepLoc: A model trained to analyze yeast cell images.      Tick the box on the left to start the embedding automatically. Alternatively, click Apply. To cancel the embedding, click Cancel.\n  Access help.\n  Embedders InceptionV3 is Google’s deep neural network for image recognition. It is trained on the ImageNet data set. The model we are using is available here. For the embedding, we use the activations of the penultimate layer of the model, which represents images with vectors.\nSqueezeNet is a deep model for image recognition that achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. The model is trained on the ImageNet dataset. We re-implemented the SqueezeNet by using weights from the author’s pretrained model. We use activations from pre-softmax (flatten10) layer as an embedding.\nVGG16 and VGG19 are deep neural networks for image recognition proposed by Visual Geometry Group from the University of Oxford. They are trained on the ImageNet data set. We use a community implementation of networks with original weights. As an embedding, we use activations of the penultimate layer - fc7.\nImage Embedding also includes Painters, an embedder that was trained on 79,433 images of paintings by 1,584 painters and won Kaggle’s Painter by Numbers competition. Activations of the penultimate layer of the network are used as an embedding.\nDeepLoc is a convolutional network trained on 21,882 images of single cells that were manually assigned to one of 15 localization compartments. We use the pre-trained network proposed by authors. The embeddings are activations of penultimate layer fc_2.\nAn article by Godec et al. (2019) explains how the embeddings work and how to use it in Orange.\nExample Let us first import images from a folder with Import Images. We have three images of an orange, a banana and a strawberry in a folder called Fruits. From Import Images we will send a data table containing a column with image paths to Image Embedding.\nWe will use the default embedder SqueezeNet. The widget will automatically start retrieving image vectors from the server.\nOnce the computation is done, you can observe the enhanced data in a Data Table. With the retrieved embeddings, you can continue with any machine learning method Orange offers. Below is an example for clustering.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Image Embedding Image embedding through deep neural networks.\nInputs\n Images: List of images.  Outputs\n Embeddings: Images represented with a vector of numbers. Skipped Images: List of images where embeddings were not calculated.  Image Embedding reads images and uploads them to a remote server or evaluate them locally. Deep learning models are used to calculate a feature vector for each image. It returns an enhanced data table with additional columns (image descriptors)." ,
	"author" : "",
	"summary" : "Image Embedding Image embedding through deep neural networks.\nInputs\n Images: List of images.  Outputs\n Embeddings: Images represented with a vector of numbers. Skipped Images: List of images where embeddings were not calculated.  Image Embedding reads images and uploads them to a remote server or evaluate them locally. Deep learning models are used to calculate a feature vector for each image. It returns an enhanced data table with additional columns (image descriptors).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Image Embedding",
	"icon" : ""
},
{
    "uri": "/widget-catalog/image-analytics/imagegrid/",
	"title": "Image Grid",
	"description": "",
	"content": "Image Grid Displays images in a similarity grid.\nInputs\n Embeddings: Image embeddings from Image Embedding widget. Data Subset: A subset of embeddings or images.  Outputs\n Images: Images from the dataset with an additional column specifying if the image is selected or the group, if there are several. Selected Images: Selected images with an additional column specifying the group.  The Image Grid widget can display images from a dataset in a similarity grid - images with similar content are placed closer to each other. It can be used for image comparison, while looking for similarities or discrepancies between selected data instances (e.g. bacterial growth or bitmap representations of handwriting).\n Image Filename Attribute: Attribute containing paths to images. Image cell fit: Resize scales the images to grid, while Crop crops them to squares. Grid size: Set the size of the grid. Click Set size automatically to optimize the projection. Tick the box to commit the changes automatically. Alternatively, click Apply. Information on the input. Access help, save image, and report (in that order).  Example Image Grid can be used to visualize similarity of images in a 2D projection. We have used 5 images of fruits and vegetables, namely orange, banana, strawberry, broccoli and cauliflower.\nWe loaded the images with Import Images and embedded them with Inception v3 embedder in Image Embedding.\nFinally, we visualized the images in Image Grid. It is obvious that broccoli and cauliflower and much more alike than strawberry and banana.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Image Grid Displays images in a similarity grid.\nInputs\n Embeddings: Image embeddings from Image Embedding widget. Data Subset: A subset of embeddings or images.  Outputs\n Images: Images from the dataset with an additional column specifying if the image is selected or the group, if there are several. Selected Images: Selected images with an additional column specifying the group.  The Image Grid widget can display images from a dataset in a similarity grid - images with similar content are placed closer to each other." ,
	"author" : "",
	"summary" : "Image Grid Displays images in a similarity grid.\nInputs\n Embeddings: Image embeddings from Image Embedding widget. Data Subset: A subset of embeddings or images.  Outputs\n Images: Images from the dataset with an additional column specifying if the image is selected or the group, if there are several. Selected Images: Selected images with an additional column specifying the group.  The Image Grid widget can display images from a dataset in a similarity grid - images with similar content are placed closer to each other.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Image Grid",
	"icon" : ""
},
{
    "uri": "/widget-catalog/image-analytics/imageviewer/",
	"title": "Image Viewer",
	"description": "",
	"content": "Image Viewer Displays images that come with a data set.\nInputs\n Data: A data set with images.  Outputs\n Data: Images that come with the data. Selected images: Images selected in the widget.  The Image Viewer widget can display images from a data set, which are stored locally or on the internet. The widget will look for an attribute with type=image in the third header row. It can be used for image comparison, while looking for similarities or discrepancies between selected data instances (e.g. bacterial growth or bitmap representations of handwriting).\n Information on the data set Select the column with image data (links). Select the column with image titles. Zoom in or out. Saves the visualization in a file. Tick the box on the left to commit changes automatically. Alternatively, click Send.  Examples A very simple way to use this widget is to connect the File widget with Image Viewer and see all the images that come with your data set. You can also visualize images from Import Images.\nAlternatively, you can visualize only selected instances, as shown in the example below.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Image Viewer Displays images that come with a data set.\nInputs\n Data: A data set with images.  Outputs\n Data: Images that come with the data. Selected images: Images selected in the widget.  The Image Viewer widget can display images from a data set, which are stored locally or on the internet. The widget will look for an attribute with type=image in the third header row. It can be used for image comparison, while looking for similarities or discrepancies between selected data instances (e." ,
	"author" : "",
	"summary" : "Image Viewer Displays images that come with a data set.\nInputs\n Data: A data set with images.  Outputs\n Data: Images that come with the data. Selected images: Images selected in the widget.  The Image Viewer widget can display images from a data set, which are stored locally or on the internet. The widget will look for an attribute with type=image in the third header row. It can be used for image comparison, while looking for similarities or discrepancies between selected data instances (e.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Image Viewer",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/importdocuments/",
	"title": "Import Documents",
	"description": "",
	"content": "Import Documents Import text documents from folders.\nInputs\n None  Outputs\n Corpus: A collection of documents from the local machine.  Import Documents widget retrieves text files from folders and creates a corpus. The widget reads .txt, .docx, .odt, .pdf and .xml files. If a folder contains subfolders, they will be used as class labels.\n Folder being loaded. Load folder from a local machine. Reload the data. Number of documents retrieved.  If the widget cannot read the file for some reason, the file will be skipped. Files that were successfully retrieved will still be on the output.\nExample To retrieve the data, select the folder icon on the right side of the widget. Select the folder you wish to turn into corpus. Once the loading is finished, you will see how many documents the widget retrieved. To inspect them, connect the widget to Corpus Viewer. We’ve used a set of Kennedy’s speeches in a plain text format.\nNow let us try it with subfolders. We have placed Kennedy’s speeches in two folders - pre-1962 and post-1962. If I load the parent folder, these two subfolders will be used as class labels. Check the output of the widget in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Import Documents Import text documents from folders.\nInputs\n None  Outputs\n Corpus: A collection of documents from the local machine.  Import Documents widget retrieves text files from folders and creates a corpus. The widget reads .txt, .docx, .odt, .pdf and .xml files. If a folder contains subfolders, they will be used as class labels.\n Folder being loaded. Load folder from a local machine. Reload the data. Number of documents retrieved." ,
	"author" : "",
	"summary" : "Import Documents Import text documents from folders.\nInputs\n None  Outputs\n Corpus: A collection of documents from the local machine.  Import Documents widget retrieves text files from folders and creates a corpus. The widget reads .txt, .docx, .odt, .pdf and .xml files. If a folder contains subfolders, they will be used as class labels.\n Folder being loaded. Load folder from a local machine. Reload the data. Number of documents retrieved.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Import Documents",
	"icon" : ""
},
{
    "uri": "/widget-catalog/image-analytics/importimages/",
	"title": "Import Images",
	"description": "",
	"content": "Import Images Import images from a directory(s).\nOutputs\n Data: Dataset describing one image in each row.  Import Images walks through a directory and returs one row per located image. Columns include image name, path to image, width, height and image size. Column with image path is later used as an attribute for image visualization and embedding.\n Currently loaded folder. Select the folder to load. Click Reload to update imported images. Information on the input. Access help.  You can load a folder containing subfolders. In this case Orange will consider each folder as a class value. In the example above, Import Images loaded 26 images belonging to two categories. These two categories will be used as class values.\nExample Import Images is likely the first widget you will use in image analysis. It loads images and creates class values from folders. In this example we used Import Images to load 26 painting belonging to either Monet or Manet.\nWe can observe the result in a Data Table. See how Orange added an extra class attribute with values Monet and Manet?\nNow we can proceed with standard machine learning methods. We will send images to Image Embedding, where we will use Painters embedder to retrieve image vectors.\nThen we will use Test \u0026 Score and Logistic Regression, to build a model for predicting the author of a painting. We get a perfect score? How come? It turns out, these were the images the Painters embedder was trained on, so a high accuracy is expected.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Import Images Import images from a directory(s).\nOutputs\n Data: Dataset describing one image in each row.  Import Images walks through a directory and returs one row per located image. Columns include image name, path to image, width, height and image size. Column with image path is later used as an attribute for image visualization and embedding.\n Currently loaded folder. Select the folder to load. Click Reload to update imported images." ,
	"author" : "",
	"summary" : "Import Images Import images from a directory(s).\nOutputs\n Data: Dataset describing one image in each row.  Import Images walks through a directory and returs one row per located image. Columns include image name, path to image, width, height and image size. Column with image path is later used as an attribute for image visualization and embedding.\n Currently loaded folder. Select the folder to load. Click Reload to update imported images.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Import Images",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/impute/",
	"title": "Impute",
	"description": "",
	"content": "Impute Replaces unknown values in the data.\nInputs\n Data: input dataset Learner: learning algorithm for imputation  Outputs\n Data: dataset with imputed values  Some Orange’s algorithms and visualizations cannot handle unknown values in the data. This widget does what statisticians call imputation: it substitutes missing values by values either computed from the data or set by the user. The default imputation is (1-NN).\n  In the top-most box, Default method, the user can specify a general imputation technique for all attributes.\n Don’t Impute does nothing with the missing values. Average/Most-frequent uses the average value (for continuous attributes) or the most common value (for discrete attributes). As a distinct value creates new values to substitute the missing ones. Model-based imputer constructs a model for predicting the missing value, based on values of other attributes; a separate model is constructed for each attribute. The default model is 1-NN learner, which takes the value from the most similar example (this is sometimes referred to as hot deck imputation). This algorithm can be substituted by one that the user connects to the input signal Learner for Imputation. Note, however, that if there are discrete and continuous attributes in the data, the algorithm needs to be capable of handling them both; at the moment only 1-NN learner can do that. (In the future, when Orange has more regressors, the Impute widget may have separate input signals for discrete and continuous models.) Random values computes the distributions of values for each attribute and then imputes by picking random values from them. Remove examples with missing values removes the example containing missing values. This check also applies to the class attribute if Impute class values is checked.    It is possible to specify individual treatment for each attribute, which overrides the default treatment set. One can also specify a manually defined value used for imputation. In the screenshot, we decided not to impute the values of “normalized-losses” and “make”, the missing values of “aspiration” will be replaced by random values, while the missing values of “body-style” and “drive-wheels” are replaced by “hatchback” and “fwd\",respectively. If the values of “length”, “width” or “height” are missing, the example is discarded. Values of all other attributes use the default method set above (model-based imputer, in our case).\n  The imputation methods for individual attributes are the same as default methods.\n  Restore All to Default resets the individual attribute treatments to default.\n  Produce a report.\n  All changes are committed immediately if Apply automatically is checked. Otherwise, Apply needs to be ticked to apply any new settings.\n  Example To demonstrate how the Impute widget works, we played around with the Iris dataset and deleted some of the data. We used the Impute widget and selected the Model-based imputer to impute the missing values. In another Data Table, we see how the question marks turned into distinct values (“Iris-setosa, “Iris-versicolor”).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Impute Replaces unknown values in the data.\nInputs\n Data: input dataset Learner: learning algorithm for imputation  Outputs\n Data: dataset with imputed values  Some Orange\u0026rsquo;s algorithms and visualizations cannot handle unknown values in the data. This widget does what statisticians call imputation: it substitutes missing values by values either computed from the data or set by the user. The default imputation is (1-NN).\n  In the top-most box, Default method, the user can specify a general imputation technique for all attributes." ,
	"author" : "",
	"summary" : "Impute Replaces unknown values in the data.\nInputs\n Data: input dataset Learner: learning algorithm for imputation  Outputs\n Data: dataset with imputed values  Some Orange\u0026rsquo;s algorithms and visualizations cannot handle unknown values in the data. This widget does what statisticians call imputation: it substitutes missing values by values either computed from the data or set by the user. The default imputation is (1-NN).\n  In the top-most box, Default method, the user can specify a general imputation technique for all attributes.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Impute",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/integrate-spectra/",
	"title": "Integrate Spectra",
	"description": "",
	"content": "Integrate Spectra Integrate spectra in various ways.\nInputs\n Data: input dataset  Outputs\n Integrated Data: data with integrals appended Preprocessor: preprocessing method  The Integrate Spectra widget allows you to add integrals to your data by selecting regions of interest and integrating them with several methods.\n Add integral:  Integral from 0: Integral from baseline: Peak from 0: Peak from baseline: Closest value: X-value of maximum from 0: X-value of maximum from baseline   Toggle preview. Preview plot with its editor menu like in the Spectra widget. Show a subsample of the spectra (implemented for performance). Output integrals as meta attributes. Otherwise only integrals will be output. Commit to send the changes to the output.  Example This is a simple example on how to use the Integrate Spectra widget. The widget provides many options for integrating spectral areas and the results are appended as additional columns to the data.\nWe are using the liver spectroscopy data set from the Datasets widget. In Integrate Spectra we have selected integral from 0 and set the lower and upper limit with the red lines. We could also do it by setting the Low limit and High limit values on the left.\nTo observe the integrated area, we need to press the triangular play button next to the method. To output the data, we need to press Commit.\nFinally, we can observe the additional column with the integral values of the area in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Integrate Spectra Integrate spectra in various ways.\nInputs\n Data: input dataset  Outputs\n Integrated Data: data with integrals appended Preprocessor: preprocessing method  The Integrate Spectra widget allows you to add integrals to your data by selecting regions of interest and integrating them with several methods.\n Add integral:  Integral from 0: Integral from baseline: Peak from 0: Peak from baseline: Closest value: X-value of maximum from 0: X-value of maximum from baseline   Toggle preview." ,
	"author" : "",
	"summary" : "Integrate Spectra Integrate spectra in various ways.\nInputs\n Data: input dataset  Outputs\n Integrated Data: data with integrals appended Preprocessor: preprocessing method  The Integrate Spectra widget allows you to add integrals to your data by selecting regions of interest and integrating them with several methods.\n Add integral:  Integral from 0: Integral from baseline: Peak from 0: Peak from baseline: Closest value: X-value of maximum from 0: X-value of maximum from baseline   Toggle preview.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Integrate Spectra",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/interactive-kmeans/",
	"title": "Interactive k-Means",
	"description": "",
	"content": "Interactive k-Means Educational widget that shows the working of a k-means clustering.\nInputs\n Data: input data set  Outputs\n Data: data set with cluster annotation Centroids: centroids position  Description The aim of this widget is to show the working of a k-means clustering algorithm on two attributes from a data set. The widget applies k-means clustering to the selected two attributes step by step. Users can step through the algorithm and see how it works.\n Select attributes for x and y axis. Number of centroids: set the number of centroids. Randomize: randomly assigns position of centroids. If you want to add centroid on a particular position in the graph, click on this position. If you want to move the centroid, drag and drop it on the desired position. Show membership lines: if ticked, connection between data points and closest centroids are shown. Recompute centroids or Reassign membership: step through different stages of the algorithm. Recompute centroids moves centroids to new positions, based on the most central position of the data assigned to the centroid. Reassign membership reassigns data points to the centroid they are the closest to. Step back: make a step back in the algorithm. Run: step through the algorithm automatically. Speed: set the speed of automatic stepping. Save Image saves the image to the computer in a .svg or .png format.  Example Here are two possible schemas that show how the Interactive k-Means widget can be used. You can load the data from File or use any other data source, such as Paint Data. Interactive k-Means widget also produces a data table with results of clustering and a table with centroids positions. These data can be inspected with the Data Table widget.\nLet us demonstrate the working of the widget on Iris data set. We provide the data using File. Then we open Interactive k-Means. Say, we will demonstrate k-Means on petal length and petal width attributes, so we set them as X and Y parameters. We also decided to perform clustering for 3 clusters. This is set as the Number of centroids.\nIf we are not satisfied with positions of centroids we can change them with a click on the Randomize button. Then we perform the first recomputing of centroids with a click on the Recompute centroids. We get the following image.\nThe next step is to reassign membership of all points to the closest centroid. This is performed with a click on the Reassign membership button.\nThen we repeat these two steps until the algorithm converges. This is the final result.\nPerhaps we are not satisfied with the result because we noticed that maybe classification into 4 clusters would be better. So we decided to add a new centroid. We can do this by increasing the number of centroids in the control menu or with a click on the position in the graph where we want to place the centroid. We decided to add it with a click. The new centroid is the orange one.\nNow we can repeat running the algorithm until it converges again, but before that we will move the new centroid to change the behavior of the algorithm. We grabbed the orange centroid and moved it to the desired position.\nThen we press Run and observe the centroids while the algorithm converges again.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Interactive k-Means Educational widget that shows the working of a k-means clustering.\nInputs\n Data: input data set  Outputs\n Data: data set with cluster annotation Centroids: centroids position  Description The aim of this widget is to show the working of a k-means clustering algorithm on two attributes from a data set. The widget applies k-means clustering to the selected two attributes step by step. Users can step through the algorithm and see how it works." ,
	"author" : "",
	"summary" : "Interactive k-Means Educational widget that shows the working of a k-means clustering.\nInputs\n Data: input data set  Outputs\n Data: data set with cluster annotation Centroids: centroids position  Description The aim of this widget is to show the working of a k-means clustering algorithm on two attributes from a data set. The widget applies k-means clustering to the selected two attributes step by step. Users can step through the algorithm and see how it works.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Interactive k-Means",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/interferogram-to-spectrum/",
	"title": "Interferogram to Spectrum",
	"description": "",
	"content": "Interferogram to Spectrum Performs Fast Fourier Transform on an interferogram, including zero filling, apodization and phase correction.\nInputs\n Interferogram: input interferogram  Outputs\n Spectra: dataset with spectra Phases: phases  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Interferogram to Spectrum Performs Fast Fourier Transform on an interferogram, including zero filling, apodization and phase correction.\nInputs\n Interferogram: input interferogram  Outputs\n Spectra: dataset with spectra Phases: phases  " ,
	"author" : "",
	"summary" : "Interferogram to Spectrum Performs Fast Fourier Transform on an interferogram, including zero filling, apodization and phase correction.\nInputs\n Interferogram: input interferogram  Outputs\n Spectra: dataset with spectra Phases: phases  ",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Interferogram to Spectrum",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/interpolate/",
	"title": "Interpolate",
	"description": "",
	"content": "Interpolate Interpolate spectra.\nInputs\n Data: input dataset Points: a reference data set  Outputs\n Interpolated Data: aligned dataset  The Interpolate widget enables you to align datasets with different wavenumbers. It has automatic interpolation or you can provide the reference data set to align with.\n Enable automatic interpolation: creates a new domain, which consequently enables interpolation of values on test data. Linear interval:  Min: minimum cutoff Max: maximum cutoff Delta: the difference between the cutoffs   Reference data: the data is aligned to the reference data  Examples The first example shows how to use Interpolate to align the train and test data set of spectral data. We will use collagen-interpolate-train.tab to train our model. Let us load the data with the File widget. Then connect it to Test \u0026 Score and add a learner, say, Logistic Regression. The scores in Test \u0026 Score look great.\nNow we would like to test on a separate data set, which has different wavenumbers. We will use collagen-interpolate-test.tab for testing. If we connect this data directly to Test \u0026 Score and select the option Test on test data, our results will be horrible. What has happened?\nWell, Orange couldn’t find any similarity between the two datasets, since the wavenumbers differ. This is why we need to interpolate first, to align the two data sets to the same scale. I will insert the Interpolate widget between File - Test and Test \u0026 Score. I will also provide the File - Train as the reference data set and select this as an option in Interpolate. Now the results in Test \u0026 Score are much better.\nThe second use case is a tad more advanced. We will use Interpolate to determine how much granularity we can afford to lose in our measurement. Say we wish to perform a diagnostic much faster. Could we measure only every 10th wavenumber? Or every 50th?\nWe will use the Liver spectroscopy data from the Datasets widget. Connect the widget to Interpolate and use the Linear interval option. The delta is set to 10. Then observe the performance of predictive models in Test \u0026 Score. Use any classifier you want; we chose Logistic Regression and Random Forest. The AUC is quite high.\nNow, set the delta to, say, 50 and observe how the AUC changes. Not much. Try setting the delta to 100 or 150. The AUC is still high, which means the classifier is stable even at such a low resolution. This is a nice way to determine how much granularity you can afford to lose to be still able to achieve a good separation between class values.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Interpolate Interpolate spectra.\nInputs\n Data: input dataset Points: a reference data set  Outputs\n Interpolated Data: aligned dataset  The Interpolate widget enables you to align datasets with different wavenumbers. It has automatic interpolation or you can provide the reference data set to align with.\n Enable automatic interpolation: creates a new domain, which consequently enables interpolation of values on test data. Linear interval:  Min: minimum cutoff Max: maximum cutoff Delta: the difference between the cutoffs   Reference data: the data is aligned to the reference data  Examples The first example shows how to use Interpolate to align the train and test data set of spectral data." ,
	"author" : "",
	"summary" : "Interpolate Interpolate spectra.\nInputs\n Data: input dataset Points: a reference data set  Outputs\n Interpolated Data: aligned dataset  The Interpolate widget enables you to align datasets with different wavenumbers. It has automatic interpolation or you can provide the reference data set to align with.\n Enable automatic interpolation: creates a new domain, which consequently enables interpolation of values on test data. Linear interval:  Min: minimum cutoff Max: maximum cutoff Delta: the difference between the cutoffs   Reference data: the data is aligned to the reference data  Examples The first example shows how to use Interpolate to align the train and test data set of spectral data.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Interpolate",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/interpolate/",
	"title": "Interpolate",
	"description": "",
	"content": "Interpolate Induce missing values in the time series by interpolation.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: The input time series with the chosen default interpolation method for when the algorithms require interpolated time series (without missing values). Interpolated time series: The input time series with any missing values interpolated according to the chosen interpolation method.  Most time series algorithms assume, you don’t have any missing values in your data. In this widget, you can chose the interpolation method to impute the missing values with. By default, it’s linear interpolation (fast and reasonable default).\n  Interpolation type. You can select one of linear, cubic spline, nearest, or mean interpolation:\n Linear interpolation replaces missing values with linearly-spaced values between the two nearest defined data points. Spline interpolation fits a cubic polynomial to the points around the missing values. This is a painfully slow method that usually gives best results. Nearest interpolation replaces missing values with the previous defined value. Mean interpolation replaces missing values with the series’ mean.    Multi-variate interpolation interpolates the whole series table as a two-dimensional plane instead of as separate single-dimensional series.\n  Missing values on the series’ end points (head and tail) are always interpolated using nearest method. Unless the interpolation method is set to nearest, discrete time series (i.e. sequences) are always imputed with the series’ mode (most frequent value).\nExample Pass a time series with missing values in, get interpolated time series out.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Interpolate Induce missing values in the time series by interpolation.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: The input time series with the chosen default interpolation method for when the algorithms require interpolated time series (without missing values). Interpolated time series: The input time series with any missing values interpolated according to the chosen interpolation method.  Most time series algorithms assume, you don\u0026rsquo;t have any missing values in your data." ,
	"author" : "",
	"summary" : "Interpolate Induce missing values in the time series by interpolation.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: The input time series with the chosen default interpolation method for when the algorithms require interpolated time series (without missing values). Interpolated time series: The input time series with any missing values interpolated according to the chosen interpolation method.  Most time series algorithms assume, you don\u0026rsquo;t have any missing values in your data.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Interpolate",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/kmeans/",
	"title": "k-Means",
	"description": "",
	"content": "k-Means Groups items using the k-Means clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute  The widget applies the k-Means clustering algorithm to the data and outputs a new dataset in which the cluster index is used as a class attribute. The original class attribute, if it exists, is moved to meta attributes. Scores of clustering results for various k are also shown in the widget.\n Select the number of clusters.  Fixed: algorithm clusters data in a specified number of clusters. Optimized: widget shows clustering scores for the selected cluster range:  Silhouette (contrasts average distance to elements in the same cluster with the average distance to elements in other clusters) Inter-cluster distance (measures distances between clusters,normally between centroids) Distance to centroids (measures distances to the arithmetic means of clusters)     Select the initialization method (the way the algorithm begins clustering):  k-Means++ (first center is selected randomly, subsequent are chosen from the remaining points with probability proportioned to squared distance from the closest center) Random initialization (clusters are assigned randomly at first and then updated with further iterations) Re-runs (how many times the algorithm is run from random initial positions; the result with the lowest within-cluster sum of squares will be used) and maximal iterations (the maximum number of iterations within each algorithm run) can be set manually.   The widget outputs a new dataset with appended cluster information. Select how to append cluster information (as class, feature or meta attribute) and name the column. If Apply Automatically is ticked, the widget will commit changes automatically. Alternatively, click Apply. Produce a report. Check scores of clustering results for various k.  Examples We are going to explore the widget with the following schema.\nFirst, we load the Iris dataset, divide it into three clusters and show it in the Data Table, where we can observe which instance went into which cluster. The interesting parts are the Scatter Plot and Select Rows.\nSince k-Means added the cluster index as a class attribute, the scatter plot will color the points according to the clusters they are in.\nWhat we are really interested in is how well the clusters induced by the (unsupervised) clustering algorithm match the actual classes in the data. We thus take Select Rows widget, in which we can select individual classes and have the corresponding points marked in the scatter plot. The match is perfect for setosa, and pretty good for the other two classes.\nYou may have noticed that we left the Remove unused values/attributes and Remove unused classes in Select Rows unchecked. This is important: if the widget modifies the attributes, it outputs a list of modified instances and the scatter plot cannot compare them to the original data.\nPerhaps a simpler way to test the match between clusters and the original classes is to use the Distributions widget.\nThe only (minor) problem here is that this widget only visualizes normal (and not meta) attributes. We solve this by using Select Columns: we reinstate the original class Iris as the class and put the cluster index among the attributes.\nThe match is perfect for setosa: all instances of setosa are in the third cluster (blue). 48 versicolors are in the second cluster (red), while two ended up in the first. For virginicae, 36 are in the first cluster and 14 in the second.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "k-Means Groups items using the k-Means clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute  The widget applies the k-Means clustering algorithm to the data and outputs a new dataset in which the cluster index is used as a class attribute. The original class attribute, if it exists, is moved to meta attributes. Scores of clustering results for various k are also shown in the widget." ,
	"author" : "",
	"summary" : "k-Means Groups items using the k-Means clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute  The widget applies the k-Means clustering algorithm to the data and outputs a new dataset in which the cluster index is used as a class attribute. The original class attribute, if it exists, is moved to meta attributes. Scores of clustering results for various k are also shown in the widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "k-Means",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/kegg_pathways/",
	"title": "KEGG Pathways",
	"description": "",
	"content": "KEGG Pathways Diagrams of molecular interactions, reactions, and relations.\nInputs\n Data: Data set. Reference: Referential data set.  Outputs\n Selected Data: Data subset. Unselected Data: Remaining data.  KEGG Pathways widget displays diagrams of molecular interactions, reactions and relations from the KEGG Pathways Database. It takes data on gene expression as an input, matches the genes to the biological processes and displays a list of corresponding pathways. To explore the pathway, the user can click on any process from the list or arrange them by P-value to get the most relevant processes at the top.\n Information on the input genes. If you have a separate reference set in the input, tick From signal to use these data as reference. To have pathways listed and displayed by vertical descent, tick Show pathways in full orthology. To fit the image to screen, tick Resize to fit. Untick the box if you wish to explore the pathways. To clear all locally cached KEGG data, press Clear cache. When Auto commit is on, the widget will automatically apply the changes. Alternatively press Commit. A list of pathways either as processes or in full orthology. Click on the process to display the pathway. You can sort the data by P-value to get the most relevant results at the top.  Example This simple example shows how to visualize interactions with KEGG Pathways. We have loaded the Caffeine effect: time courses and dose response (GDS2914) data with the GEO Data Sets widget. Then we have observed the pathways in KEGG Pathways. We have used reference from signal and selected AGE-RAGE signaling pathway in diabetic complications.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "KEGG Pathways Diagrams of molecular interactions, reactions, and relations.\nInputs\n Data: Data set. Reference: Referential data set.  Outputs\n Selected Data: Data subset. Unselected Data: Remaining data.  KEGG Pathways widget displays diagrams of molecular interactions, reactions and relations from the KEGG Pathways Database. It takes data on gene expression as an input, matches the genes to the biological processes and displays a list of corresponding pathways. To explore the pathway, the user can click on any process from the list or arrange them by P-value to get the most relevant processes at the top." ,
	"author" : "",
	"summary" : "KEGG Pathways Diagrams of molecular interactions, reactions, and relations.\nInputs\n Data: Data set. Reference: Referential data set.  Outputs\n Selected Data: Data subset. Unselected Data: Remaining data.  KEGG Pathways widget displays diagrams of molecular interactions, reactions and relations from the KEGG Pathways Database. It takes data on gene expression as an input, matches the genes to the biological processes and displays a list of corresponding pathways. To explore the pathway, the user can click on any process from the list or arrange them by P-value to get the most relevant processes at the top.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "KEGG Pathways",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/knn/",
	"title": "kNN",
	"description": "",
	"content": "kNN Predict according to the nearest training instances.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: kNN learning algorithm Model: trained model  The kNN widget uses the kNN algorithm that searches for k closest training examples in feature space and uses their average as prediction.\n A name under which it will appear in other widgets. The default name is “kNN”. Set the number of nearest neighbors, the distance parameter (metric) and weights as model criteria.  Metric can be:  Euclidean (“straight line”, distance between two points) Manhattan (sum of absolute differences of all attributes) Maximal (greatest of absolute differences between attributes) Mahalanobis (distance between point and distribution).   The Weights you can use are:  Uniform: all points in each neighborhood are weighted equally. Distance: closer neighbors of a query point have a greater influence than the neighbors further away.     Produce a report. When you change one or more settings, you need to click Apply, which will put a new learner on the output and, if the training examples are given, construct a new model and output it as well. Changes can also be applied automatically by clicking the box on the left side of the Apply button.  Examples The first example is a classification task on iris dataset. We compare the results of k-Nearest neighbors with the default model Constant, which always predicts the majority class.\nThe second example is a regression task. This workflow shows how to use the Learner output. For the purpose of this example, we used the housing dataset. We input the kNN prediction model into Predictions and observe the predicted values.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "kNN Predict according to the nearest training instances.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: kNN learning algorithm Model: trained model  The kNN widget uses the kNN algorithm that searches for k closest training examples in feature space and uses their average as prediction.\n A name under which it will appear in other widgets. The default name is \u0026ldquo;kNN\u0026rdquo;. Set the number of nearest neighbors, the distance parameter (metric) and weights as model criteria." ,
	"author" : "",
	"summary" : "kNN Predict according to the nearest training instances.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: kNN learning algorithm Model: trained model  The kNN widget uses the kNN algorithm that searches for k closest training examples in feature space and uses their average as prediction.\n A name under which it will appear in other widgets. The default name is \u0026ldquo;kNN\u0026rdquo;. Set the number of nearest neighbors, the distance parameter (metric) and weights as model criteria.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "kNN",
	"icon" : ""
},
{
    "uri": "/license/",
	"title": "License",
	"description": "",
	"content": "Orange is a comprehensive, component-based software suite for machine learning and data mining, developed at Bioinformatics Laboratory, Faculty of Computer and Information Science, University of Ljubljana, Slovenia, together with open source community.\nOrange is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option) any later version.\nOrange is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.\nOrange documentation, content on its website and other non-code content are all available under Creative Commons Attribution-ShareAlike license unless specified otherwise. Attribution should be made to Orange, Data Mining Fruitful \u0026 Fun, with a link to its website https://orange.biolab.si/.\nOrange Widgets and Canvas are based on Qt, which is distributed under GPL 3.0 (as well as LGPL 2.1, see https://www.qt.io/licensing/.\nOrange add-ons may have additional licensing requirements. Please, see their respective license files.\nIf you want to cite Orange or give it an attribution, you can find more information here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Orange is a comprehensive, component-based software suite for machine learning and data mining, developed at Bioinformatics Laboratory, Faculty of Computer and Information Science, University of Ljubljana, Slovenia, together with open source community.\nOrange is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option) any later version." ,
	"author" : "",
	"summary" : "Orange is a comprehensive, component-based software suite for machine learning and data mining, developed at Bioinformatics Laboratory, Faculty of Computer and Information Science, University of Ljubljana, Slovenia, together with open source community.\nOrange is free software; you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation; either version 3.0 of the License, or (at your option) any later version.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "license",
	"LinkTitle" : "License",
	"icon" : ""
},
{
    "uri": "/widget-catalog/evaluate/liftcurve/",
	"title": "Lift Curve",
	"description": "",
	"content": "Lift Curve Measures the performance of a chosen classifier against a random classifier.\nInputs\n Evaluation Results: results of testing classification algorithms  The Lift curve shows the curves for analysing the proportion of true positive data instances in relation to the classifier’s threshold or the number of instances that we classify as positive.\nCumulative gains chart shows the proportion of true positive instances (for example, the number of clients who accept the offer) as a function of the number of positive instances (the number of clients contacted), assuming the the instances are ordered according to the model’s probability of being positive (e.g. ranking of clients).\nLift curve shows the ratio between the proportion of true positive instances in the selection and the proportion of customers contacted. See a tutorial for more details.\n Choose the desired Target class. The default is chosen alphabetically. Choose whether to observe lift curve or cumulative gains. If test results contain more than one classifier, the user can choose which curves she or he wants to see plotted. Click on a classifier to select or deselect the curve. Show lift convex hull plots a convex hull over lift curves for all classifiers (yellow curve). The curve shows the optimal classifier (or combination thereof) for each desired lift or cumulative gain. Press Save Image to save the created image in a .svg or .png format. Produce a report. A plot with lift or cumulative gain vs. positive rate. The dashed line represents the behavior of a random classifier.  Example The widgets that provide the right type of the signal needed by the Lift Curve (evaluation data) are Test \u0026 Score and Predictions.\nIn the example below, we observe the lift curve and cumulative gain for the bank marketing data, where the classification goal is to predict whether the client will accept a term deposit offer based on his age, job, education, marital status and similar data. The data set is available in the Datasets widget. We run the learning algorithms in the Test and Score widget and send the results to Lift Curve to see their performance against a random model. Of the two algorithms tested, logistic regression outperforms the naive Bayesian classifier. The curve tells us that by picking the first 20 % of clients as ranked by the model, we are going to hit four times more positive instances than by selecting a random sample with 20 % of clients.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Lift Curve Measures the performance of a chosen classifier against a random classifier.\nInputs\n Evaluation Results: results of testing classification algorithms  The Lift curve shows the curves for analysing the proportion of true positive data instances in relation to the classifier\u0026rsquo;s threshold or the number of instances that we classify as positive.\nCumulative gains chart shows the proportion of true positive instances (for example, the number of clients who accept the offer) as a function of the number of positive instances (the number of clients contacted), assuming the the instances are ordered according to the model\u0026rsquo;s probability of being positive (e." ,
	"author" : "",
	"summary" : "Lift Curve Measures the performance of a chosen classifier against a random classifier.\nInputs\n Evaluation Results: results of testing classification algorithms  The Lift curve shows the curves for analysing the proportion of true positive data instances in relation to the classifier\u0026rsquo;s threshold or the number of instances that we classify as positive.\nCumulative gains chart shows the proportion of true positive instances (for example, the number of clients who accept the offer) as a function of the number of positive instances (the number of clients contacted), assuming the the instances are ordered according to the model\u0026rsquo;s probability of being positive (e.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Lift Curve",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/line_chart/",
	"title": "Line Chart",
	"description": "",
	"content": "Line Chart Visualize time series’ sequence and progression in the most basic time series visualization imaginable.\nInputs\n Time series: Time series as output by As Timeseries widget. Forecast: Time series forecast as output by one of the models (like VAR or ARIMA).  You can visualize the time series in this widget.\n Stack a new line chart below the current charts. Remove the associated stacked chart. Type of chart to draw. Options are: line, step line, column, area, spline. Switch between linear and logarithmic y axis. Select the time series to preview (select multiple series using the Ctrl key). See the selected series in this area.  Example Attach the model’s forecast to the Forecast input signal to preview it. The forecast is drawn with a dotted line and the confidence intervals as an ranged area.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Line Chart Visualize time series\u0026rsquo; sequence and progression in the most basic time series visualization imaginable.\nInputs\n Time series: Time series as output by As Timeseries widget. Forecast: Time series forecast as output by one of the models (like VAR or ARIMA).  You can visualize the time series in this widget.\n Stack a new line chart below the current charts. Remove the associated stacked chart. Type of chart to draw." ,
	"author" : "",
	"summary" : "Line Chart Visualize time series\u0026rsquo; sequence and progression in the most basic time series visualization imaginable.\nInputs\n Time series: Time series as output by As Timeseries widget. Forecast: Time series forecast as output by one of the models (like VAR or ARIMA).  You can visualize the time series in this widget.\n Stack a new line chart below the current charts. Remove the associated stacked chart. Type of chart to draw.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Line Chart",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/lineplot/",
	"title": "Line Plot",
	"description": "",
	"content": "Line Plot Visualization of data profiles (e.g., time series).\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  Line plot a type of plot which displays the data as a series of points, connected by straight line segments. It only works for numerical data, while categorical can be used for grouping of the data points.\n Information on the input data. Select what you wish to display:  Lines show individual data instances in a plot. Range shows the range of data points between 10th and 90th percentile. Mean adds the line for mean value. If group by is selected, means will be displayed per each group value. Error bars show the standard deviation of each attribute.   Select a categorical attribute to use for grouping of data instances. Use None to show ungrouped data. Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as a line selection, meaning the data under the selected line plots will be sent on the output. Scroll in or out for zoom. If Send Automatically is ticked, changes are communicated automatically. Alternatively, click Send.  Example Line Plot is a standard visualization widget, which displays data profiles, normally of ordered numerical data. In this simple example, we will display the iris data in a line plot, grouped by the iris attribute. The plot shows how petal length nicely separates between class values.\nIf we observe this in a Scatter Plot, we can confirm this is indeed so. Petal length is an interesting attribute for separation of classes, especially when enhanced with petal width, which is also nicely separated in the line plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Line Plot Visualization of data profiles (e.g., time series).\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  Line plot a type of plot which displays the data as a series of points, connected by straight line segments. It only works for numerical data, while categorical can be used for grouping of the data points." ,
	"author" : "",
	"summary" : "Line Plot Visualization of data profiles (e.g., time series).\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  Line plot a type of plot which displays the data as a series of points, connected by straight line segments. It only works for numerical data, while categorical can be used for grouping of the data points.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Line Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/linearprojection/",
	"title": "Linear Projection",
	"description": "",
	"content": "Linear Projection A linear projection method with explorative data analysis.\nInputs\n Data: input dataset Data Subset: subset of instances Projection: custom projection vectors  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: projection vectors  This widget displays linear projections of class-labeled data. It supports various types of projections such as circular, linear discriminant analysis, and principal component analysis.\nConsider, for a start, a projection of the Iris dataset shown below. Notice that it is the sepal width and sepal length that already separate Iris setosa from the other two, while the petal length is the attribute best separating Iris versicolor from Iris virginica.\n Axes in the projection that are displayed and other available axes. Optimize your projection by using Suggest Features. This feature scores attributes and returns the top scoring attributes with a simultaneous visualization update. Feature scoring computes the classification accuracy (for classification) or MSE (regression) of k-nearest neighbors classifier on the projected, two-dimensional data. The score reflects how well the classes in the projection are separated. Choose the type of projection:  Circular Placement Linear Discriminant Analysis Principal Component Analysis   Set the color of the displayed points. Set shape, size, and label to differentiate between points. Label only selected points labels only selected data instances. Adjust plot properties:  Symbol size: set the size of the points. Opacity: set the transparency of the points. Jittering: Randomly disperse points with jittering to prevent them from overlapping. Hide radius: Axes inside the radius are hidden. Drag the slider to change the radius.   Additional plot properties:  Show color regions colors the graph by class. Show legend displays a legend on the right. Click and drag the legend to move it.   Select, zoom, pan and zoom to fit are the options for exploring the graph. Manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom. If Send automatically is ticked, changes are communicated automatically. Alternatively, press Send.  Example The Linear Projection widget works just like other visualization widgets. Below, we connected it to the File widget to see the set projected on a 2-D plane. Then we selected the data for further analysis and connected it to the Data Table widget to see the details of the selected subset.\nReferences Koren Y., Carmel L. (2003). Visualization of labeled data using linear transformations. In Proceedings of IEEE Information Visualization 2003, (InfoVis'03). Available here.\nBoulesteix A.-L., Strimmer K. (2006). Partial least squares: a versatile tool for the analysis of high-dimensional genomic data. Briefings in Bioinformatics, 8(1), 32-44. Abstract here.\nLeban G., Zupan B., Vidmar G., Bratko I. (2006). VizRank: Data Visualization Guided by Machine Learning. Data Mining and Knowledge Discovery, 13, 119-136. Available here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Linear Projection A linear projection method with explorative data analysis.\nInputs\n Data: input dataset Data Subset: subset of instances Projection: custom projection vectors  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: projection vectors  This widget displays linear projections of class-labeled data. It supports various types of projections such as circular, linear discriminant analysis, and principal component analysis." ,
	"author" : "",
	"summary" : "Linear Projection A linear projection method with explorative data analysis.\nInputs\n Data: input dataset Data Subset: subset of instances Projection: custom projection vectors  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: projection vectors  This widget displays linear projections of class-labeled data. It supports various types of projections such as circular, linear discriminant analysis, and principal component analysis.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Linear Projection",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/linearregression/",
	"title": "Linear Regression",
	"description": "",
	"content": "Linear Regression A linear regression algorithm with optional L1 (LASSO), L2 (ridge) or L1L2 (elastic net) regularization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: linear regression learning algorithm Model: trained model Coefficients: linear regression coefficients  The Linear Regression widget constructs a learner/predictor that learns a linear function from its input data. The model can identify the relationship between a predictor xi and the response variable y. Additionally, Lasso and Ridge regularization parameters can be specified. Lasso regression minimizes a penalized version of the least squares loss function with L1-norm penalty and Ridge regularization with L2-norm penalty.\nLinear regression works only on regression tasks.\n The learner/predictor name Choose a model to train:  no regularization a Ridge regularization (L2-norm penalty) a Lasso bound (L1-norm penalty) an Elastic net regularization   Produce a report. Press Apply to commit changes. If Apply Automatically is ticked, changes are committed automatically.  Example Below, is a simple workflow with housing dataset. We trained Linear Regression and Random Forest and evaluated their performance in Test \u0026 Score.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Linear Regression A linear regression algorithm with optional L1 (LASSO), L2 (ridge) or L1L2 (elastic net) regularization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: linear regression learning algorithm Model: trained model Coefficients: linear regression coefficients  The Linear Regression widget constructs a learner/predictor that learns a linear function from its input data. The model can identify the relationship between a predictor xi and the response variable y." ,
	"author" : "",
	"summary" : "Linear Regression A linear regression algorithm with optional L1 (LASSO), L2 (ridge) or L1L2 (elastic net) regularization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: linear regression learning algorithm Model: trained model Coefficients: linear regression coefficients  The Linear Regression widget constructs a learner/predictor that learns a linear function from its input data. The model can identify the relationship between a predictor xi and the response variable y.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Linear Regression",
	"icon" : ""
},
{
    "uri": "/workflows/Literary-Analysis/",
	"title": "Literary Analysis",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Literary Analysis",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/load_data/",
	"title": "Load Data",
	"description": "",
	"content": "Load Data Load samples for multi-sample analysis.\nOutputs\n Data: Single cell dataset.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Load Data Load samples for multi-sample analysis.\nOutputs\n Data: Single cell dataset.  " ,
	"author" : "",
	"summary" : "Load Data Load samples for multi-sample analysis.\nOutputs\n Data: Single cell dataset.  ",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Load Data",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/loadmodel/",
	"title": "Load Model",
	"description": "",
	"content": "Load Model Load a model from an input file.\nOutputs\n Model: trained model   Choose from a list of previously used models. Browse for saved models. Reload the selected model.  Example When you want to use a custom-set model that you’ve saved before, open the Load Model widget and select the desired file with the Browse icon. This widget loads the existing model into Predictions widget. Datasets used with Load Model have to contain compatible attributes!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Load Model Load a model from an input file.\nOutputs\n Model: trained model   Choose from a list of previously used models. Browse for saved models. Reload the selected model.  Example When you want to use a custom-set model that you\u0026rsquo;ve saved before, open the Load Model widget and select the desired file with the Browse icon. This widget loads the existing model into Predictions widget. Datasets used with Load Model have to contain compatible attributes!" ,
	"author" : "",
	"summary" : "Load Model Load a model from an input file.\nOutputs\n Model: trained model   Choose from a list of previously used models. Browse for saved models. Reload the selected model.  Example When you want to use a custom-set model that you\u0026rsquo;ve saved before, open the Load Model widget and select the desired file with the Browse icon. This widget loads the existing model into Predictions widget. Datasets used with Load Model have to contain compatible attributes!",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Load Model",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/logisticregression/",
	"title": "Logistic Regression",
	"description": "",
	"content": "Logistic Regression The logistic regression classification algorithm with LASSO (L1) or ridge (L2) regularization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: logistic regression learning algorithm Model: trained model Coefficients: logistic regression coefficients  Logistic Regression learns a Logistic Regression model from the data. It only works for classification tasks.\n A name under which the learner appears in other widgets. The default name is “Logistic Regression”. Regularization type (either L1 or L2). Set the cost strength (default is C=1). Press Apply to commit changes. If Apply Automatically is ticked, changes will be communicated automatically.  Example The widget is used just as any other widget for inducing a classifier. This is an example demonstrating prediction results with logistic regression on the hayes-roth dataset. We first load hayes-roth_learn in the File widget and pass the data to Logistic Regression. Then we pass the trained model to Predictions.\nNow we want to predict class value on a new dataset. We load hayes-roth_test in the second File widget and connect it to Predictions. We can now observe class values predicted with Logistic Regression directly in Predictions.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Logistic Regression The logistic regression classification algorithm with LASSO (L1) or ridge (L2) regularization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: logistic regression learning algorithm Model: trained model Coefficients: logistic regression coefficients  Logistic Regression learns a Logistic Regression model from the data. It only works for classification tasks.\n A name under which the learner appears in other widgets. The default name is \u0026ldquo;Logistic Regression\u0026rdquo;." ,
	"author" : "",
	"summary" : "Logistic Regression The logistic regression classification algorithm with LASSO (L1) or ridge (L2) regularization.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: logistic regression learning algorithm Model: trained model Coefficients: logistic regression coefficients  Logistic Regression learns a Logistic Regression model from the data. It only works for classification tasks.\n A name under which the learner appears in other widgets. The default name is \u0026ldquo;Logistic Regression\u0026rdquo;.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Logistic Regression",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/louvainclustering/",
	"title": "Louvain Clustering",
	"description": "",
	"content": "Louvain Clustering Groups items using the Louvain clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute Graph (with the Network addon): the weighted k-nearest neighbor graph  The widget first converts the input data into a k-nearest neighbor graph. To preserve the notions of distance, the Jaccard index for the number of shared neighbors is used to weight the edges. Finally, a modularity optimization community detection algorithm is applied to the graph to retrieve clusters of highly interconnected nodes. The widget outputs a new dataset in which the cluster index is used as a meta attribute.\n PCA processing is typically applied to the original data to remove noise. The distance metric is used for finding specified number of nearest neighbors. The number of nearest neighbors to use to form the KNN graph. Resolution is a parameter for the Louvain community detection algorithm that affects the size of the recovered clusters. Smaller resolutions recover smaller, and therefore a larger number of clusters, and conversely, larger values recover clusters containing more data points. When Apply Automatically is ticked, the widget will automatically communicate all changes. Alternatively, click Apply.  Example Louvain Clustering converts the dataset into a graph, where it finds highly interconnected nodes. We can visualize the graph itself using the Network Explorer from the Network addon.\nReferences Blondel, Vincent D., et al. “Fast unfolding of communities in large networks.” Journal of statistical mechanics: theory and experiment 2008.10 (2008): P10008.\nLambiotte, Renaud, J-C. Delvenne, and Mauricio Barahona. “Laplacian dynamics and multiscale modular structure in networks.” arXiv preprint, arXiv:0812.1770 (2008).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Louvain Clustering Groups items using the Louvain clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute Graph (with the Network addon): the weighted k-nearest neighbor graph  The widget first converts the input data into a k-nearest neighbor graph. To preserve the notions of distance, the Jaccard index for the number of shared neighbors is used to weight the edges. Finally, a modularity optimization community detection algorithm is applied to the graph to retrieve clusters of highly interconnected nodes." ,
	"author" : "",
	"summary" : "Louvain Clustering Groups items using the Louvain clustering algorithm.\nInputs\n Data: input dataset  Outputs\n Data: dataset with cluster index as a class attribute Graph (with the Network addon): the weighted k-nearest neighbor graph  The widget first converts the input data into a k-nearest neighbor graph. To preserve the notions of distance, the Jaccard index for the number of shared neighbors is used to weight the edges. Finally, a modularity optimization community detection algorithm is applied to the graph to retrieve clusters of highly interconnected nodes.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Louvain Clustering",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/manifoldlearning/",
	"title": "Manifold Learning",
	"description": "",
	"content": "Manifold Learning Nonlinear dimensionality reduction.\nInputs\n Data: input dataset  Outputs\n Transformed Data: dataset with reduced coordinates  Manifold Learning is a technique which finds a non-linear manifold within the higher-dimensional space. The widget then outputs new coordinates which correspond to a two-dimensional space. Such data can be later visualized with Scatter Plot or other visualization widgets.\n Method for manifold learning:  t-SNE MDS, see also MDS widget Isomap Locally Linear Embedding Spectral Embedding   Set parameters for the method:  t-SNE (distance measures):  Euclidean distance Manhattan Chebyshev Jaccard Mahalanobis Cosine   MDS (iterations and initialization):  max iterations: maximum number of optimization interactions initialization: method for initialization of the algorithm (PCA or random)   Isomap:  number of neighbors   Locally Linear Embedding:  method:  standard modified hessian eigenmap local   number of neighbors max iterations   Spectral Embedding:  affinity:  nearest neighbors RFB kernel       Output: the number of reduced features (components). If Apply automatically is ticked, changes will be propagated automatically. Alternatively, click Apply. Produce a report.  Manifold Learning widget produces different embeddings for high-dimensional data.\nFrom left to right, top to bottom: t-SNE, MDS, Isomap, Locally Linear Embedding and Spectral Embedding.\nExample Manifold Learning widget transforms high-dimensional data into a lower dimensional approximation. This makes it great for visualizing datasets with many features. We used voting.tab to map 16-dimensional data onto a 2D graph. Then we used Scatter Plot to plot the embeddings.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Manifold Learning Nonlinear dimensionality reduction.\nInputs\n Data: input dataset  Outputs\n Transformed Data: dataset with reduced coordinates  Manifold Learning is a technique which finds a non-linear manifold within the higher-dimensional space. The widget then outputs new coordinates which correspond to a two-dimensional space. Such data can be later visualized with Scatter Plot or other visualization widgets.\n Method for manifold learning:  t-SNE MDS, see also MDS widget Isomap Locally Linear Embedding Spectral Embedding   Set parameters for the method:  t-SNE (distance measures):  Euclidean distance Manhattan Chebyshev Jaccard Mahalanobis Cosine   MDS (iterations and initialization):  max iterations: maximum number of optimization interactions initialization: method for initialization of the algorithm (PCA or random)   Isomap:  number of neighbors   Locally Linear Embedding:  method:  standard modified hessian eigenmap local   number of neighbors max iterations   Spectral Embedding:  affinity:  nearest neighbors RFB kernel       Output: the number of reduced features (components)." ,
	"author" : "",
	"summary" : "Manifold Learning Nonlinear dimensionality reduction.\nInputs\n Data: input dataset  Outputs\n Transformed Data: dataset with reduced coordinates  Manifold Learning is a technique which finds a non-linear manifold within the higher-dimensional space. The widget then outputs new coordinates which correspond to a two-dimensional space. Such data can be later visualized with Scatter Plot or other visualization widgets.\n Method for manifold learning:  t-SNE MDS, see also MDS widget Isomap Locally Linear Embedding Spectral Embedding   Set parameters for the method:  t-SNE (distance measures):  Euclidean distance Manhattan Chebyshev Jaccard Mahalanobis Cosine   MDS (iterations and initialization):  max iterations: maximum number of optimization interactions initialization: method for initialization of the algorithm (PCA or random)   Isomap:  number of neighbors   Locally Linear Embedding:  method:  standard modified hessian eigenmap local   number of neighbors max iterations   Spectral Embedding:  affinity:  nearest neighbors RFB kernel       Output: the number of reduced features (components).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Manifold Learning",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/marker_genes/",
	"title": "Marker Genes",
	"description": "",
	"content": "Marker Genes Widget provides access to a public database of marker genes.\nOutputs\n Genes  Database sources:\n  PanglaoDB\nOscar Franzén, Li-Ming Gan, Johan L M Björkegren, PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data, Database, Volume 2019, 2019, baz046, https://doi.org/10.1093/database/baz046\n     CellMarker\nCellMarker: a manually curated resource of cell markers in human and mouse. Nucleic Acids Research. 2018.\n  Data is preprocessed in Orange readable format and it is hosted here. One can use Databases update widget to download (or update) files locally.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Marker Genes Widget provides access to a public database of marker genes.\nOutputs\n Genes  Database sources:\n  PanglaoDB\nOscar Franzén, Li-Ming Gan, Johan L M Björkegren, PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data, Database, Volume 2019, 2019, baz046, https://doi.org/10.1093/database/baz046\n     CellMarker\nCellMarker: a manually curated resource of cell markers in human and mouse. Nucleic Acids Research. 2018." ,
	"author" : "",
	"summary" : "Marker Genes Widget provides access to a public database of marker genes.\nOutputs\n Genes  Database sources:\n  PanglaoDB\nOscar Franzén, Li-Ming Gan, Johan L M Björkegren, PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data, Database, Volume 2019, 2019, baz046, https://doi.org/10.1093/database/baz046\n     CellMarker\nCellMarker: a manually curated resource of cell markers in human and mouse. Nucleic Acids Research. 2018.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Marker Genes",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/mds/",
	"title": "MDS",
	"description": "",
	"content": "MDS Multidimensional scaling (MDS) projects items onto a plane fitted to given distances between points.\nInputs\n Data: input dataset Distances: distance matrix Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: dataset with MDS coordinates  Multidimensional scaling is a technique which finds a low-dimensional (in our case a two-dimensional) projection of points, where it tries to fit distances between points as well as possible. The perfect fit is typically impossible to obtain since the data is high-dimensional or the distances are not Euclidean.\nIn the input, the widget needs either a dataset or a matrix of distances. When visualizing distances between rows, you can also adjust the color of the points, change their shape, mark them, and output them upon selection.\nThe algorithm iteratively moves the points around in a kind of a simulation of a physical model: if two points are too close to each other (or too far away), there is a force pushing them apart (or together). The change of the point’s position at each time interval corresponds to the sum of forces acting on it.\n The widget redraws the projection during optimization. Optimization is run automatically in the beginning and later by pushing Start.  Max iterations: The optimization stops either when the projection changes only minimally at the last iteration or when a maximum number of iterations has been reached. Initialization: PCA (Torgerson) positions the initial points along principal coordinate axes. Random sets the initial points to a random position and then readjusts them. Refresh: Set how often you want to refresh the visualization. It can be at Every iteration, Every 5/10/25/50 steps or never (None). Setting a lower refresh interval makes the animation more visually appealing, but can be slow if the number of points is high.   Defines how the points are visualized. These options are available only when visualizing distances between rows (selected in the Distances widget).  Color: Color of points by attribute (gray for continuous, colored for discrete). Shape: Shape of points by attribute (only for discrete). Size: Set the size of points (Same size or select an attribute) or let the size depend on the value of the continuous attribute the point represents (Stress). Label: Discrete attributes can serve as a label. Symbol size: Adjust the size of the dots. Symbol opacity: Adjust the transparency level of the dots. Show similar pairs: Adjust the strength of network lines. Jitter: Set jittering to prevent the dots from overlapping.   Adjust the graph with Zoom/Select. The arrow enables you to select data instances. The magnifying glass enables zooming, which can be also done by scrolling in and out. The hand allows you to move the graph around. The rectangle readjusts the graph proportionally. Select the desired output:  Original features only (input dataset) Coordinates only (MDS coordinates) Coordinates as features (input dataset + MDS coordinates as regular attributes) Coordinates as meta attributes (input dataset + MDS coordinates as meta attributes)   Sending the instances can be automatic if Send selected automatically is ticked. Alternatively, click Send selected. Save Image allows you to save the created image either as .svg or .png file to your device. Produce a report.  The MDS graph performs many of the functions of the Visualizations widget. It is in many respects similar to the Scatter Plot \u003c../visualize/scatterplot\u003e widget, so we recommend reading that widget’s description as well.\nExample The above graphs were drawn using the following simple schema. We used the iris.tab dataset. Using the Distances \u003c../unsupervised/distances\u003e widget we input the distance matrix into the MDS widget, where we see the Iris data displayed in a 2-dimensional plane. We can see the appended coordinates in the Data Table \u003c../data/datatable\u003e widget.\nReferences Wickelmaier, F. (2003). An Introduction to MDS. Sound Quality Research Unit, Aalborg University. Available here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "MDS Multidimensional scaling (MDS) projects items onto a plane fitted to given distances between points.\nInputs\n Data: input dataset Distances: distance matrix Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: dataset with MDS coordinates  Multidimensional scaling is a technique which finds a low-dimensional (in our case a two-dimensional) projection of points, where it tries to fit distances between points as well as possible." ,
	"author" : "",
	"summary" : "MDS Multidimensional scaling (MDS) projects items onto a plane fitted to given distances between points.\nInputs\n Data: input dataset Distances: distance matrix Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: dataset with MDS coordinates  Multidimensional scaling is a technique which finds a low-dimensional (in our case a two-dimensional) projection of points, where it tries to fit distances between points as well as possible.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "MDS",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/mergedata/",
	"title": "Merge Data",
	"description": "",
	"content": "Merge Data Merges two datasets, based on values of selected attributes.\nInputs\n Data: input dataset Extra Data: additional dataset  Outputs\n Data: dataset with features added from extra data  The Merge Data widget is used to horizontally merge two datasets, based on the values of selected attributes (columns). In the input, two datasets are required, data and extra data. Rows from the two data sets are matched by the values of pairs of attributes, chosen by the user. The widget produces one output. It corresponds to the instances from the input data to which attributes (columns) from input extra data are appended.\nIf the selected attribute pair does not contain unique values (in other words, the attributes have duplicate values), the widget will give a warning. Instead, one can match by more than one attribute. Click on the plus icon to add the attribute to merge on. The final result has to be a unique combination for each individual row.\n Information on main data. Information on data to append. Merging type:  Append columns from Extra Data outputs all rows from the Data, augmented by the columns in the Extra Data. Rows without matches are retained, even where the data in the extra columns are missing. Find matching pairs of rows outputs rows from the Data, augmented by the columns in the Extra Data. Rows without matches are removed from the output. Concatenate tables treats both data sources symmetrically. The output is similar to the first option, except that non-matched values from Extra Data are appended at the end.   List of attributes from Data input. List of attributes from Extra Data input. Produce a report.  Merging Types #####Append Columns from Extra Data (left join)\nColumns from the Extra Data are added to the Data. Instances with no matching rows will have missing values added.\nFor example, the first table may contain city names and the second would be a list of cities and their coordinates. Columns with coordinates would then be appended to the data with city names. Where city names cannot be matched, missing values will appear.\nIn our example, the first Data input contained 6 cities, but the Extra Data did not provide Lat and Lon values for Bratislava, so the fields will be empty.\n#####Find matching pairs of rows (inner join)\nOnly those rows that are matched will be present on the output, with the Extra Data columns appended. Rows without matches are removed.\nIn our example, Bratislava from the Data input did not have Lat and Lon values, while Belgrade from the Extra Data could not be found in the City column we were merging on. Hence both instances are remove - only the intersection of instances is sent to the output.\n#####Concatenate tables (outer join)\nThe rows from both the Data and the Extra Data will be present on the output. Where rows cannot be matched, missing values will appear.\nIn our example, both Bratislava and Belgrade are now present. Bratislava will have missing Lat and Lon values, while Belgrade will have a missing Population value.\n#####Row index\nData will be merged in the same order as they appear in the table. Row number 1 from the Data input will be joined with row number 1 from the Extra Data input. Row numbers are assigned by Orange based on the original order of the data instances.\n#####Instance ID\nThis is a more complex option. Sometimes, data in transformed in the analysis and the domain is no longer the same. Nevertheless, the original row indices are still present in the background (Orange remembers them). In this case one can merge on instance ID. For example if you transformed the data with PCA, visualized it in the Scatter Plot, selected some data instances and now you wish to see the original information of the selected subset. Connect the output of Scatter Plot to Merge Data, add the original data set as Extra Data and merge by Instance ID.\n#####Merge by two or more attributes\nSometimes our data instances are unique with respect to a combination of columns, not a single column. To merge by more than a single column, add the Row matching condition by pressing plus next to the matching condition. To remove it, press the x.\nIn the below example, we are merging by student column and class column.\nSay we have two data sets with student names and the class they’re in. The first data set has students’ grades and the second on the elective course they have chosen. Unfortunately, there are two Jacks in our data, one from class A and the other from class B. Same for Jane.\nTo distinguish between the two, we can match rows on both, the student’s name and her class.\nExamples Merging two datasets results in appending new attributes to the original file, based on a selected common attribute. In the example below, we wanted to merge the zoo.tab file containing only factual data with zoo-with-images.tab containing images. Both files share a common string attribute names. Now, we create a workflow connecting the two files. The zoo.tab data is connected to Data input of the Merge Data widget, and the zoo-with-images.tab data to the Extra Data input. Outputs of the Merge Data widget is then connected to the Data Table widget. In the latter, the Merged Data channels are shown, where image attributes are added to the original data.\nThe case where we want to include all instances in the output, even those where no match by attribute names was found, is shown in the following workflow.\nThe third type of merging is shown in the next workflow. The output consists of both inputs, with unknown values assigned where no match was found.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Merge Data Merges two datasets, based on values of selected attributes.\nInputs\n Data: input dataset Extra Data: additional dataset  Outputs\n Data: dataset with features added from extra data  The Merge Data widget is used to horizontally merge two datasets, based on the values of selected attributes (columns). In the input, two datasets are required, data and extra data. Rows from the two data sets are matched by the values of pairs of attributes, chosen by the user." ,
	"author" : "",
	"summary" : "Merge Data Merges two datasets, based on values of selected attributes.\nInputs\n Data: input dataset Extra Data: additional dataset  Outputs\n Data: dataset with features added from extra data  The Merge Data widget is used to horizontally merge two datasets, based on the values of selected attributes (columns). In the input, two datasets are required, data and extra data. Rows from the two data sets are matched by the values of pairs of attributes, chosen by the user.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Merge Data",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/model_evaluation_w/",
	"title": "Model Evaluation",
	"description": "",
	"content": "Model Evaluation Evaluate different time series’ models.\nInputs\n Time series: Time series as output by As Timeseries widget. Time series model(s): The time series model(s) to evaluate (e.g. VAR or ARIMA).  Evaluate different time series’ models. by comparing the errors they make in terms of: root mean squared error (RMSE), median absolute error (MAE), mean absolute percent error (MAPE), prediction of change in direction (POCID), coefficient of determination (R²), Akaike information criterion (AIC), and Bayesian information criterion (BIC).\n Number of folds for time series cross-validation. Number of forecast steps to produce in each fold. Results for various error measures and information criteria on cross-validated and in-sample data.  This slide (source) shows how cross validation on time series is performed. In this case, the number of folds (1) is 10 and the number of forecast steps in each fold (2) is 1.\nIn-sample errors are the errors calculated on the training data itself. A stable model is one where in-sample errors and out-of-sample errors don’t differ significantly.\n####See also\nARIMA Model, VAR Model\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Model Evaluation Evaluate different time series\u0026rsquo; models.\nInputs\n Time series: Time series as output by As Timeseries widget. Time series model(s): The time series model(s) to evaluate (e.g. VAR or ARIMA).  Evaluate different time series\u0026rsquo; models. by comparing the errors they make in terms of: root mean squared error (RMSE), median absolute error (MAE), mean absolute percent error (MAPE), prediction of change in direction (POCID), coefficient of determination (R²), Akaike information criterion (AIC), and Bayesian information criterion (BIC)." ,
	"author" : "",
	"summary" : "Model Evaluation Evaluate different time series\u0026rsquo; models.\nInputs\n Time series: Time series as output by As Timeseries widget. Time series model(s): The time series model(s) to evaluate (e.g. VAR or ARIMA).  Evaluate different time series\u0026rsquo; models. by comparing the errors they make in terms of: root mean squared error (RMSE), median absolute error (MAE), mean absolute percent error (MAPE), prediction of change in direction (POCID), coefficient of determination (R²), Akaike information criterion (AIC), and Bayesian information criterion (BIC).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Model Evaluation",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/mosaicdisplay/",
	"title": "Mosaic Display",
	"description": "",
	"content": "Mosaic Display Display data in a mosaic plot.\nInputs\n Data: input dataset Data subset: subset of instances  Outputs\n Selected data: instances selected from the plot  The Mosaic plot is a graphical representation of a two-way frequency table or a contingency table. It is used for visualizing data from two or more qualitative variables and was introduced in 1981 by Hartigan and Kleiner and expanded and refined by Friendly in 1994. It provides the user with the means to more efficiently recognize relationships between different variables. If you wish to read up on the history of Mosaic Display, additional reading is available here.\n Select the variables you wish to see plotted. Select interior coloring. You can color the interior according to class or you can use the Pearson residual, which is the difference between observed and fitted values, divided by an estimate of the standard deviation of the observed value. If Compare to total is clicked, a comparison is made to all instances. Save image saves the created image to your computer in a .svg or .png format. Produce a report.  Example We loaded the titanic dataset and connected it to the Mosaic Display widget. We decided to focus on two variables, namely status, sex and survival. We colored the interiors according to Pearson residuals in order to demonstrate the difference between observed and fitted values.\nWe can see that the survival rates for men and women clearly deviate from the fitted value.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Mosaic Display Display data in a mosaic plot.\nInputs\n Data: input dataset Data subset: subset of instances  Outputs\n Selected data: instances selected from the plot  The Mosaic plot is a graphical representation of a two-way frequency table or a contingency table. It is used for visualizing data from two or more qualitative variables and was introduced in 1981 by Hartigan and Kleiner and expanded and refined by Friendly in 1994." ,
	"author" : "",
	"summary" : "Mosaic Display Display data in a mosaic plot.\nInputs\n Data: input dataset Data subset: subset of instances  Outputs\n Selected data: instances selected from the plot  The Mosaic plot is a graphical representation of a two-way frequency table or a contingency table. It is used for visualizing data from two or more qualitative variables and was introduced in 1981 by Hartigan and Kleiner and expanded and refined by Friendly in 1994.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Mosaic Display",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/moving_transform_w/",
	"title": "Moving Transform",
	"description": "",
	"content": "Moving Transform Apply rolling window functions to the time series. Use this widget to get a series’ mean.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: The input time series with the added series’ transformations.  In this widget, you define what aggregation functions to run over the time series and with what window sizes.\n Define a new transformation. Remove the selected transformation. Time series you want to run the transformation over. Desired window size. Aggregation function to aggregate the values in the window with. Options are: mean, sum, max, min, median, mode, standard deviation, variance, product, linearly-weighted moving average, exponential moving average, harmonic mean, geometric mean, non-zero count, cumulative sum, and cumulative product. Select Non-overlapping windows options if you don’t want the moving windows to overlap but instead be placed side-to-side with zero intersection. In the case of non-overlapping windows, define the fixed window width(overrides and widths set in (4).  Example To get a 5-day moving average, we can use a rolling window with mean aggregation.\nTo integrate time series’ differences from Difference widget, use Cumulative sum aggregation over a window wide enough to grasp the whole series.\nSee also Seasonal Adjustment\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Moving Transform Apply rolling window functions to the time series. Use this widget to get a series\u0026rsquo; mean.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: The input time series with the added series\u0026rsquo; transformations.  In this widget, you define what aggregation functions to run over the time series and with what window sizes.\n Define a new transformation. Remove the selected transformation." ,
	"author" : "",
	"summary" : "Moving Transform Apply rolling window functions to the time series. Use this widget to get a series\u0026rsquo; mean.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: The input time series with the added series\u0026rsquo; transformations.  In this widget, you define what aggregation functions to run over the time series and with what window sizes.\n Define a new transformation. Remove the selected transformation.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Moving Transform",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/multifile/",
	"title": "Multifile",
	"description": "",
	"content": "Multifile Read data from input files and send a data table to the output.\nOutputs\n Data: a data table of all the loaded files  The Multifile widget loads data from different sources and works like Concatenate widget for spectroscopy. The widget will output a union of attributes and features, with missing values for non-matching wavenumbers. To interpolate missing data, use the Interpolate widget.\n Loaded files. Load local files. Remove the selected file. Clear all files. Label the concatenated data. Reload the files. Domain editor. Features can be edited by double-clicking on them. The user can change the attribute names, select the type of variable per each attribute (Continuous, Nominal, String, Datetime), and choose how to further define the attributes (as Features, Targets or Meta). The user can also decide to ignore an attribute. Add Multifile to the report. Apply to commit the changes.  Example Here is a simple example on how to use the Multifile widget. We have loaded two data set that were stored on our local machine. We used the folder icon to access the files and load them. Now our files are displayed in the top box. We have labelled the files collagen, make it clear what it is about.\nWe can observe the concatenated data in the Spectra widget or in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Multifile Read data from input files and send a data table to the output.\nOutputs\n Data: a data table of all the loaded files  The Multifile widget loads data from different sources and works like Concatenate widget for spectroscopy. The widget will output a union of attributes and features, with missing values for non-matching wavenumbers. To interpolate missing data, use the Interpolate widget.\n Loaded files. Load local files." ,
	"author" : "",
	"summary" : "Multifile Read data from input files and send a data table to the output.\nOutputs\n Data: a data table of all the loaded files  The Multifile widget loads data from different sources and works like Concatenate widget for spectroscopy. The widget will output a union of attributes and features, with missing values for non-matching wavenumbers. To interpolate missing data, use the Interpolate widget.\n Loaded files. Load local files.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Multifile",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/naivebayes/",
	"title": "Naive Bayes",
	"description": "",
	"content": "Naive Bayes A fast and simple probabilistic classifier based on Bayes’ theorem with the assumption of feature independence.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: naive bayes learning algorithm Model: trained model  Naive Bayes learns a Naive Bayesian model from the data. It only works for classification tasks.\nThis widget has two options: the name under which it will appear in other widgets and producing a report. The default name is Naive Bayes. When you change it, you need to press Apply.\nExamples Here, we present two uses of this widget. First, we compare the results of the Naive Bayes with another model, the Random Forest. We connect iris data from File to Test \u0026 Score. We also connect Naive Bayes and Random Forest to Test \u0026 Score and observe their prediction scores.\nThe second schema shows the quality of predictions made with Naive Bayes. We feed the Test \u0026 Score widget a Naive Bayes learner and then send the data to the Confusion Matrix. We also connect Scatter Plot with File. Then we select the misclassified instances in the Confusion Matrix and show feed them to Scatter Plot. The bold dots in the scatterplot are the misclassified instances from Naive Bayes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Naive Bayes A fast and simple probabilistic classifier based on Bayes\u0026rsquo; theorem with the assumption of feature independence.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: naive bayes learning algorithm Model: trained model  Naive Bayes learns a Naive Bayesian model from the data. It only works for classification tasks.\nThis widget has two options: the name under which it will appear in other widgets and producing a report." ,
	"author" : "",
	"summary" : "Naive Bayes A fast and simple probabilistic classifier based on Bayes\u0026rsquo; theorem with the assumption of feature independence.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: naive bayes learning algorithm Model: trained model  Naive Bayes learns a Naive Bayesian model from the data. It only works for classification tasks.\nThis widget has two options: the name under which it will appear in other widgets and producing a report.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Naive Bayes",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/neighbors/",
	"title": "Neighbors",
	"description": "",
	"content": "Neighbors Compute nearest neighbors in data according to reference.\nInputs\n Data: An input data set. Reference: A reference data for neighbor computation.  Outputs\n Neighbors: A data table of nearest neighbors according to reference.  The Neighbors widget computes nearest neighbors for a given reference and for a given distance measure. The reference can be either one instance or more instances. In the case with one reference widget outputs closest n instances from data where n is set by the Number of neighbors option in the widget. When reference contains more instances widget computes the combined distance for each data instance as a minimum of distances to each reference. Widget outputs n data instances with lowest combined distance.\n Distance measure for computing neighbors. Supported measures are: Euclidean, Manhattan, Mahalanobis, Cosine, Jaccard, Spearman, absolute Spearman, Pearson, absolute Pearson. Number of neighbors on the output. If Exclude rows (equal to) references is ticked, data instances that are highly similar to the reference (distance \u003c 1e-5), will be excluded. Click Apply to commit the changes. To communicate changes automatically tick Apply Automatically. Status bar with access to widget help and information on the input and output data.  Examples In the first example, we used iris data and passed it to Neighbors and to Data Table. In Data Table, we selected an instance of iris, that will serve as our reference, meaning we wish to retrieve 10 closest examples to the select data instance. We connect Data Table to Neighbors as well.\nWe can observe the results of neighbor computation in Data Table (1), where we can see 10 closest images to our selected iris flower.\nNow change the selection Data Table to multiple examples. As a result, we get instances with closest combined distances to the references. The method computes the combined distance as a minimum of distances to each reference.\nAnother example requires the installation of Image Analytics add-on. We loaded 15 paintings from famous painters with Import Images widget and passed them to Image Embedding, where we selected Painters embedder.\nThen the procedure is the same as above. We passed embedded images to Image Viewer and selected a painting from Monet to serve as our reference image. We passed the image to Neighbors, where we set the distance measure to cosine, ticked off Exclude reference and set the neighbors to 2. This allows us to find the actual closest neighbor to a reference painting and observe them side by side in Image Viewer (1).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Neighbors Compute nearest neighbors in data according to reference.\nInputs\n Data: An input data set. Reference: A reference data for neighbor computation.  Outputs\n Neighbors: A data table of nearest neighbors according to reference.  The Neighbors widget computes nearest neighbors for a given reference and for a given distance measure. The reference can be either one instance or more instances. In the case with one reference widget outputs closest n instances from data where n is set by the Number of neighbors option in the widget." ,
	"author" : "",
	"summary" : "Neighbors Compute nearest neighbors in data according to reference.\nInputs\n Data: An input data set. Reference: A reference data for neighbor computation.  Outputs\n Neighbors: A data table of nearest neighbors according to reference.  The Neighbors widget computes nearest neighbors for a given reference and for a given distance measure. The reference can be either one instance or more instances. In the case with one reference widget outputs closest n instances from data where n is set by the Number of neighbors option in the widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Neighbors",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkanalysis/",
	"title": "Network Analysis",
	"description": "",
	"content": "Network Analysis Statistical analysis of network data.\nInputs\n Network: An instance of Network Graph. Items: Properties of a network file.  Outputs\n Network: An instance of Network Graph with appended information. Items: New properties of a network file.  Network Analysis widget computes node-level and graph-level summary statistics for the network. It outputs a network with the new computed statistics and an extended item data table (node-level indices only).\n####Graph level\n Number of nodes: number of vertices in a network. Number of edges: number of connections in a network. Average degree: average number of connections per node. Density: ratio between actual number of edges and maximum number of edges (fully connected graph). Diameter: maximum eccentricity of the graph. Radius: minimum eccentricity of the graph. Average shortest path length: expected distance between two nodes in the graph. Number of strongly connected components: parts of network where every vertex is reachable from every other vertex (for directed graphs only). Number of weakly connected components: parts of network where replacing all of its directed edges with undirected edges produces a connected (undirected) graph (for directed graphs only).  ####Node level\n Degree: number of edges per node. In-degree: number of incoming edges in a directed graph. Out-degree: number of outgoing edges in a directed graph. Average neighbor degree: average degree of neighboring nodes. Degree centrality: ratio of other nodes connected to the node. In-degree centrality: ratio of incoming edges to a node in a directed graph. Out-degree centrality: ratio of outgoing edges from a node in directed graph. Closeness centrality: distance to all other nodes.  Example This simple example shows how Network Analysis can enrich the workflow. We have used lastfm.net as our input network from Network File and sent it to Network Analysis. We’ve decided to compute degree, degree centrality and closeness centrality at node level.\nWe can visualize the network in Network Explorer. In the widget we color by best tag, as is the default for this data set. But now we can also set the size of the nodes to correspond to the computed Degree centrality. This is a great way to visualize the properties of the network.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network Analysis Statistical analysis of network data.\nInputs\n Network: An instance of Network Graph. Items: Properties of a network file.  Outputs\n Network: An instance of Network Graph with appended information. Items: New properties of a network file.  Network Analysis widget computes node-level and graph-level summary statistics for the network. It outputs a network with the new computed statistics and an extended item data table (node-level indices only)." ,
	"author" : "",
	"summary" : "Network Analysis Statistical analysis of network data.\nInputs\n Network: An instance of Network Graph. Items: Properties of a network file.  Outputs\n Network: An instance of Network Graph with appended information. Items: New properties of a network file.  Network Analysis widget computes node-level and graph-level summary statistics for the network. It outputs a network with the new computed statistics and an extended item data table (node-level indices only).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network Analysis",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkclustering/",
	"title": "Network Clustering",
	"description": "",
	"content": "Network Clustering Detect clusters in a network.\nInputs\n Network: An instance of Network Graph.  Outputs\n Network: An instance of Network Graph with clustering information appended.  Network Clustering widget finds clusters in a network. Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find appropriate clusters, and one from Leung et al. (2009), which builds upon the work from Raghavan and adds hop attenuation as a parameters for cluster formation.\n Max. iterations: maximum number of iteration allowed for the algorithm to run (can converge before reaching the maximum). Clustering method:  Label propagation clustering (Raghavan et al., 2007) Label propagation clustering (Leung et al., 2009) with hop attenuation.   Above appears the information on the number of clusters found. If Auto-commit is ticked, results will be automatically sent to the output. Alternatively, press Commit.  Example Network Clustering can help you uncover cliques and highly connected groups in a network. First, we will use Network File to load lastfm.net data set. Then we will pass the network through Network Clustering. The widget found 79 clusters in a network. To visualize the results, use Network Explorer and set Color attribute to Cluster. This will color network nodes with the corresponding cluster color - this is a great way to visualize highly connected groups in dense networks.\nKeep in mind that Network Explorer will color the 10 largest clusters and color the rest as ‘Other’.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network Clustering Detect clusters in a network.\nInputs\n Network: An instance of Network Graph.  Outputs\n Network: An instance of Network Graph with clustering information appended.  Network Clustering widget finds clusters in a network. Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find appropriate clusters, and one from Leung et al. (2009), which builds upon the work from Raghavan and adds hop attenuation as a parameters for cluster formation." ,
	"author" : "",
	"summary" : "Network Clustering Detect clusters in a network.\nInputs\n Network: An instance of Network Graph.  Outputs\n Network: An instance of Network Graph with clustering information appended.  Network Clustering widget finds clusters in a network. Clustering works with two algorithms, one from Raghavan et al. (2007), which uses label propagation to find appropriate clusters, and one from Leung et al. (2009), which builds upon the work from Raghavan and adds hop attenuation as a parameters for cluster formation.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network Clustering",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkexplorer/",
	"title": "Network Explorer",
	"description": "",
	"content": "Network Explorer Visually explore the network and its properties.\nInputs\n Network: An instance of Network Graph. Node Subset: A subset of vertices. Node Data: Information on vertices. Node Distances: Data on distances between nodes.  Outputs\n Selected sub-network: A network of selected nodes. Distance Matrix: Distance matrix. Selected Items: Information on selected vertices. Highlighted Items: Information on highlighted vertices. Remaining Items: Information on remaining items (not selected or highlighted).  Network Explorer is the primary widget for visualizing network graphs. It displays a graph with Fruchterman-Reingold layout optimization and enables setting the color, size and label of nodes. One can also highlight nodes of specific properties and output them.\nThe visualization in Network Explorer works just like the one for Scatter Plot. To select a subset of nodes, draw a rectangle around the subset. Shift will add a new group. Ctrl-Shift (Cmd-Shift) will add to the existing group. Alt (Option) will remove from the group. Pressing outside of the network will remove the selection.\n Information on the network. Reports on the number (and proportion) of nodes and edges. Press ‘Re-layout’ to re-compute nodes with Fruchterman-Reingold optimization. Select ‘Randomize positions’ starts from random position of nodes. Set the color, shape, size and label of the nodes by attribute. Color will display the 10 most frequent values and color the rest as ‘Other’. Shape is assigned to the 5 most frequent values and the rest is marked as ‘Other’. Label only selection and subset is handy for keeping the projection organized. Set the (relative) node size and edge width. By default, edge widths correspond to their weights. To see the weight value, select Show edge weights. By default, only the edges of selected nodes are labeled to keep the projection organized. Show color regions colors the projection according to the majority node value. Deselect Show legend to hide the legend. Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom.  ####Marking\nPressing Select will select and output the highlighted nodes. Add to Group adds to the existing selection, while Add New Group creates a new group.\nThe widget enables selection of nodes by the specified criterium:\n Mark nodes whose label starts with. Set the condition to highlight the nodes whose label starts with the specified text. Label must be set for the highlighting to work. Press Select to select the highlighted nodes. Mark nodes whose label contains. Set the condition to highlight the nodes whose label contains the specified text. Label must be set for the highlighting to work. Press Select to select the highlighted nodes. Mark nodes whose data contains. Set the condition to highlight the nodes whose attributes contain the specified text. Press Select to select the highlighted nodes. Mark nodes reachable from selected. Highlight the nodes that can be reached from the selected nodes. At least one node has to be selected for the highlighting to work. Mark nodes in vicinity of selection. Highlight the nodes that are a selected number of hops away (first degree neighbors, second degree neighbors, etc.). Mark nodes from subset signal. Highlight the nodes that are neighbors of the nodes from the Node Subset input. Mark nodes with few connections. Highlight the nodes that have equal or less connections than the set number. Mark nodes with many connections. Highlight the nodes that have equal or more connections than the set number. Mark nodes with most connections. Highlight the nodes that have the most connections. The number of marked specifies how many top connected nodes to highlight (list is ranked). Mark nodes with more connections than any neighbor. Highlight the most connected nodes. Mark nodes with more connections than average neighbor. Highlight nodes whose degree is above average.  ####Selecting a subset\nJust like Scatter Plot widget, the Network Explorer supports group selection. To create the first group, either select nodes from the plot or highlight them by setting the criterium and pressing Select.\nIn the example below, we selected a single node (blue). Then we used Mark nodes in vicinity of selection to highlight neighbors of the selected node. We used Add New Group to create a new (red) group. Pressing Add New Group again will create yet another (green) group.\nAdd to Group would add the highlighted (light blue) nodes to the last (green) group. This make this widget a nice tool for visualizing hops and network propagation.\nExample In this example we will use the lastfm data set that can be loaded in the Network File widget under Browse documentation networks. The nodes of the network are musicians, which are characterized by the genre they play, number of albums produced and so on. The edges are the number of listeners on LastFm.\nThe entire data set is visualized in Network Explorer. In the widget, we removed the coloring and set the size of the nodes to correspond to the album count. Then we selected some nodes from the network. We can observe the selection in Network Explorer (1).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network Explorer Visually explore the network and its properties.\nInputs\n Network: An instance of Network Graph. Node Subset: A subset of vertices. Node Data: Information on vertices. Node Distances: Data on distances between nodes.  Outputs\n Selected sub-network: A network of selected nodes. Distance Matrix: Distance matrix. Selected Items: Information on selected vertices. Highlighted Items: Information on highlighted vertices. Remaining Items: Information on remaining items (not selected or highlighted)." ,
	"author" : "",
	"summary" : "Network Explorer Visually explore the network and its properties.\nInputs\n Network: An instance of Network Graph. Node Subset: A subset of vertices. Node Data: Information on vertices. Node Distances: Data on distances between nodes.  Outputs\n Selected sub-network: A network of selected nodes. Distance Matrix: Distance matrix. Selected Items: Information on selected vertices. Highlighted Items: Information on highlighted vertices. Remaining Items: Information on remaining items (not selected or highlighted).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network Explorer",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkfile/",
	"title": "Network File",
	"description": "",
	"content": "Network File Read network graph file in Pajek format.\nOutputs\n Network: An instance of Network Graph. Items: Properties of a network file.  Network File widget reads network files and sends the input data to its output channel. History of the most recently opened files in maintained in the widget. The widget also includes a directory with sample data sets that come pre-installed with the add-on.\nThe widget reads data in .net and .pajek formats. A complimentary .tab, .tsv or .csv data set can be provided for node information. Orange by default matches a file with the same name as .net file.\n Load network file. The widget construct a data table from the data whose filename matches the graph filename (i.e. lastfm.net and lastfm.tab) or, if no match is found, from the graph. A dropdown menu provides access to documentation data sets with Browse documentation networks…. The folder icon provides access to local data files. Reload the data file from 1. Status bar reports on the number of nodes and edges and the type of the graph. Information, warnings and errors. Hover over a message to read it all.  Example We loaded the lastfm.net from documentation data set (dropdown → Browse documentation networks). The nicest way to observe network data is with the Network Explorer widget. Network File widget automatically matched the corresponding data file (lastfm.net with lastfm.tab), so node attributes are available in the widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network File Read network graph file in Pajek format.\nOutputs\n Network: An instance of Network Graph. Items: Properties of a network file.  Network File widget reads network files and sends the input data to its output channel. History of the most recently opened files in maintained in the widget. The widget also includes a directory with sample data sets that come pre-installed with the add-on.\nThe widget reads data in ." ,
	"author" : "",
	"summary" : "Network File Read network graph file in Pajek format.\nOutputs\n Network: An instance of Network Graph. Items: Properties of a network file.  Network File widget reads network files and sends the input data to its output channel. History of the most recently opened files in maintained in the widget. The widget also includes a directory with sample data sets that come pre-installed with the add-on.\nThe widget reads data in .",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network File",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkfromdistances/",
	"title": "Network From Distances",
	"description": "",
	"content": "Network From Distances Constructs a network from distances between instances.\nInputs\n Distances: A distance matrix.  Outputs\n Network: An instance of Network Graph. Data: Attribute-valued data set. Distances: A distance matrix.  Network from Distances constructs a network graph from a given distance matrix. Graph is constructed by connecting nodes from the matrix where the distance between nodes is below the given threshold. In other words, all instances with a distance lower than the selected threshold, will be connected.\n Edges:  Distance threshold: a closeness threshold for the formation of edges. Percentile: the percentile of data instances to be connected. Include also closest neighbors: include a number of closest neighbors to the selected instances.   Node selection:  Keep all nodes: entire network is on the output. Components with at least X nodes: filters out nodes with less than the set number of nodes. Largest connected component: keep only the largest cluster.   Edge weights:  Proportional to distance: weights are set to reflect the distance (closeness). Inverted distance: weights are set to reflect the inverted distance (difference).   Information on the constructed network:  Data items on input: number of instances on the input. Network nodes: number of nodes in the network (and the percentage of the original data). Network edges: number of constructed edges/connections (and the average number of connections per node).    Example Network from Distances creates networks from distance matrices. It can transform data sets from a data table via distance matrix into a network graph. This widget is great for visualizing instance similarity as a graph of connected instances.\nWe took iris.tab to visualize instance similarity in a graph. We sent the output of File widget to Distances, where we computed Euclidean distances between rows (instances). Then we sent the output of Distances to Network from Distances, where we set the distance threshold (how similar the instances have to be to draw an edge between them) to 0.222. We kept all nodes and set edge weights to proportional to distance.\nThen we observed the constructed network in a Network Explorer. We colored the nodes by iris attribute.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network From Distances Constructs a network from distances between instances.\nInputs\n Distances: A distance matrix.  Outputs\n Network: An instance of Network Graph. Data: Attribute-valued data set. Distances: A distance matrix.  Network from Distances constructs a network graph from a given distance matrix. Graph is constructed by connecting nodes from the matrix where the distance between nodes is below the given threshold. In other words, all instances with a distance lower than the selected threshold, will be connected." ,
	"author" : "",
	"summary" : "Network From Distances Constructs a network from distances between instances.\nInputs\n Distances: A distance matrix.  Outputs\n Network: An instance of Network Graph. Data: Attribute-valued data set. Distances: A distance matrix.  Network from Distances constructs a network graph from a given distance matrix. Graph is constructed by connecting nodes from the matrix where the distance between nodes is below the given threshold. In other words, all instances with a distance lower than the selected threshold, will be connected.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network From Distances",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkgenerator/",
	"title": "Network Generator",
	"description": "",
	"content": "Network Generator Construct example graphs.\nOutputs\n Generated Network: An instance of Network Graph.  Network Generator constructs exemplary networks. It is mostly intended for teaching/learning about networks.\nGraph options:\n Path: a graph that can be drawn so that all of its vertices and edges lie on a single straight line. Cycle: a graph that consists of a single cycle, i.e. some number of vertices (at least 3) are connected in a closed chain. Complete: simple undirected graph in which every pair of distinct vertices is connected by a unique edge. Complete bipartite: a graph whose vertices can be divided into two disjoint and independent sets. Barbell: two complete graphs connected by a path. Ladder: planar undirected graph with 2n vertices and 3n-2 edges. Circular ladder: Cartesian product of two path graphs. Grid: a graph whose drawing, embedded in some Euclidean space, forms a regular tiling. Hypercube: a graph formed from the vertices and edges of an n-dimensional hypercube. Star: Return the Star graph with n+1 nodes: one center node, connected to n outer nodes. Lollipop: a complete graph (clique) and a path graph, connected with a bridge. Geometric: an undirected graph constructed by randomly placing N nodes in some metric space.  Press Regenerate Network to output a new graph instance.\nExample Network Generator is a nice tool to explore typical graph structures.\nHere, we generated a Grid graph of height 4 and width 5 and sent it to Network Analysis. We computed node degrees and sent the data to Network Explorer. Finally, we observed the generated graph in the visualization and set the size and color of the nodes to Degree. This is a nice tool to observe and explain the properties of networks.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network Generator Construct example graphs.\nOutputs\n Generated Network: An instance of Network Graph.  Network Generator constructs exemplary networks. It is mostly intended for teaching/learning about networks.\nGraph options:\n Path: a graph that can be drawn so that all of its vertices and edges lie on a single straight line. Cycle: a graph that consists of a single cycle, i.e. some number of vertices (at least 3) are connected in a closed chain." ,
	"author" : "",
	"summary" : "Network Generator Construct example graphs.\nOutputs\n Generated Network: An instance of Network Graph.  Network Generator constructs exemplary networks. It is mostly intended for teaching/learning about networks.\nGraph options:\n Path: a graph that can be drawn so that all of its vertices and edges lie on a single straight line. Cycle: a graph that consists of a single cycle, i.e. some number of vertices (at least 3) are connected in a closed chain.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network Generator",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/networkofgroups/",
	"title": "Network Of Groups",
	"description": "",
	"content": "Network Of Groups Group instances by feature and connect related groups.\nInputs\n Network: An instance of network graph. Data: Properties of a network graph.  Outputs:\n Network: A grouped network graph. Data: Properties of the group network graph.  Network of Groups is the network version of the group-by operation. Nodes with the same values of the attribute, selected in the dropdown, will be represented as a single node.\n Information on the input and output network. Select the attribute to group by. Compute weights:  No weights: all weights are set to 1. Number of connections: weight edges by the number of connections between the groups. Sum of connection weights: weight edges by the sum of weights of connections between the groups. Normalize by geometric mean divides weights by the geometric mean of the number of connections between the two groups.    Example In this example we are using airtraffic data set, that we loaded in the Network File widget. We see the entire data set in Network Explorer (1).\nThen we use Network of Groups to group the network by the FAA Classifications attribute. All nodes with the same value of this attribute will be represented as a single node in the output. There is an edge between the two nodes, if they share connections in the original network.\nThe grouped network is shown in Network Explorer.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Network Of Groups Group instances by feature and connect related groups.\nInputs\n Network: An instance of network graph. Data: Properties of a network graph.  Outputs:\n Network: A grouped network graph. Data: Properties of the group network graph.  Network of Groups is the network version of the group-by operation. Nodes with the same values of the attribute, selected in the dropdown, will be represented as a single node." ,
	"author" : "",
	"summary" : "Network Of Groups Group instances by feature and connect related groups.\nInputs\n Network: An instance of network graph. Data: Properties of a network graph.  Outputs:\n Network: A grouped network graph. Data: Properties of the group network graph.  Network of Groups is the network version of the group-by operation. Nodes with the same values of the attribute, selected in the dropdown, will be represented as a single node.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Network Of Groups",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/neuralnetwork/",
	"title": "Neural Network",
	"description": "",
	"content": "Neural Network A multi-layer perceptron (MLP) algorithm with backpropagation.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: multi-layer perceptron learning algorithm Model: trained model  The Neural Network widget uses sklearn’s Multi-layer Perceptron algorithm that can learn non-linear models as well as linear.\n  A name under which it will appear in other widgets. The default name is “Neural Network”.\n  Set model parameters:\n Neurons per hidden layer: defined as the ith element represents the number of neurons in the ith hidden layer. E.g. a neural network with 3 layers can be defined as 2, 3, 2. Activation function for the hidden layer:  Identity: no-op activation, useful to implement linear bottleneck Logistic: the logistic sigmoid function tanh: the hyperbolic tan function ReLu: the rectified linear unit function   Solver for weight optimization:  L-BFGS-B: an optimizer in the family of quasi-Newton methods SGD: stochastic gradient descent Adam: stochastic gradient-based optimizer   Alpha: L2 penalty (regularization term) parameter Max iterations: maximum number of iterations  Other parameters are set to sklearn’s defaults.\n  Produce a report.\n  When the box is ticked (Apply Automatically), the widget will communicate changes automatically. Alternatively, click Apply.\n  Examples The first example is a classification task on iris dataset. We compare the results of Neural Network with the Logistic Regression.\nThe second example is a prediction task, still using the iris data. This workflow shows how to use the Learner output. We input the Neural Network prediction model into Predictions and observe the predicted values.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Neural Network A multi-layer perceptron (MLP) algorithm with backpropagation.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: multi-layer perceptron learning algorithm Model: trained model  The Neural Network widget uses sklearn\u0026rsquo;s Multi-layer Perceptron algorithm that can learn non-linear models as well as linear.\n  A name under which it will appear in other widgets. The default name is \u0026ldquo;Neural Network\u0026rdquo;.\n  Set model parameters:" ,
	"author" : "",
	"summary" : "Neural Network A multi-layer perceptron (MLP) algorithm with backpropagation.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: multi-layer perceptron learning algorithm Model: trained model  The Neural Network widget uses sklearn\u0026rsquo;s Multi-layer Perceptron algorithm that can learn non-linear models as well as linear.\n  A name under which it will appear in other widgets. The default name is \u0026ldquo;Neural Network\u0026rdquo;.\n  Set model parameters:",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Neural Network",
	"icon" : ""
},
{
    "uri": "/workflows/Nomogram/",
	"title": "Nomogram",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Nomogram",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/nomogram/",
	"title": "Nomogram",
	"description": "",
	"content": "Nomogram Nomograms for visualization of Naive Bayes and Logistic Regression classifiers.\nInputs\n Classifier: trained classifier Data: input dataset  Outputs\n Features: selected variables, 10 by default  The Nomogram enables some classifier’s (more precisely Naive Bayes classifier and Logistic Regression classifier) visual representation. It offers an insight into the structure of the training data and effects of the attributes on the class probabilities. Besides visualization of the classifier, the widget offers interactive support for prediction of class probabilities. A snapshot below shows the nomogram of the Titanic dataset, that models the probability for a passenger not to survive the disaster of the Titanic.\nWhen there are too many attributes in the plotted dataset, only best ranked ones can be selected for display. It is possible to choose from ‘No sorting’, ‘Name’, ‘Absolute importance’, ‘Positive influence’ and ‘Negative influence’ for Naive Bayes representation and from ‘No sorting’, ‘Name’ and ‘Absolute importance’ for Logistic Regression representation.\nThe probability for the chosen target class is computed by ‘1-vs-all’ principle, which should be taken in consideration when dealing with multiclass data (alternating probabilities do not sum to 1). To avoid this inconvenience, you can choose to normalize probabilities.\n Select the target class you want to model the probability for. Select, whether you want to normalize the probabilities or not. By default Scale is set to Log odds ration. For easier understanding and interpretation option Point scale can be used. The unit is obtained by re-scaling the log odds so that the maximal absolute log odds ratio in the nomogram represents 100 points. Display all attributes or only the best ranked ones. Sort them and set the projection type.  Continuous attributes can be plotted in 2D (only for Logistic Regression).\nExamples The Nomogram widget should be used immediately after trained classifier widget (e.g. Naive Bayes or Logistics Regression). It can also be passed a data instance using any widget that enables selection (e.g. Data Table) as shown in the workflow below.\nReferring to the Titanic dataset once again, 1490 (68%) passengers on Titanic out of 2201 died. To make a prediction, the contribution of each attribute is measured as a point score and the individual point scores are summed to determine the probability. When the value of the attribute is unknown, its contribution is 0 points. Therefore, not knowing anything about the passenger, the total point score is 0 and the corresponding probability equals the unconditional prior. The nomogram in the example shows the case when we know that the passenger is a male adult from the first class. The points sum to -0.36, with a corresponding probability of not surviving of about 53%.\nFeatures output The second example shows how to use the Features output. Let us use heart_disease data for this exercise and load it in the File widget. Now connect File to Naive Bayes (or Logistic Regression) and add Nomogram to Naive Bayes. Finally, connect File to Select Columns.\nSelect Columns selects a subset of variables, while Nomogram shows the top scoring variables for the trained classifier. To filter the data by the variables selected in the Nomogram, connect Nomogram to Select Columns as shown below. Nomogram will pass a list of selected variables to Select Columns, which will retain only the variables from the list. For this to work, you have to press Use input features in Select Columns (or tick it to always apply it).\nWe have selected the top 5 variables in Nomogram and used Select Columns to retain only those variables.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Nomogram Nomograms for visualization of Naive Bayes and Logistic Regression classifiers.\nInputs\n Classifier: trained classifier Data: input dataset  Outputs\n Features: selected variables, 10 by default  The Nomogram enables some classifier\u0026rsquo;s (more precisely Naive Bayes classifier and Logistic Regression classifier) visual representation. It offers an insight into the structure of the training data and effects of the attributes on the class probabilities. Besides visualization of the classifier, the widget offers interactive support for prediction of class probabilities." ,
	"author" : "",
	"summary" : "Nomogram Nomograms for visualization of Naive Bayes and Logistic Regression classifiers.\nInputs\n Classifier: trained classifier Data: input dataset  Outputs\n Features: selected variables, 10 by default  The Nomogram enables some classifier\u0026rsquo;s (more precisely Naive Bayes classifier and Logistic Regression classifier) visual representation. It offers an insight into the structure of the training data and effects of the attributes on the class probabilities. Besides visualization of the classifier, the widget offers interactive support for prediction of class probabilities.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Nomogram",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/nytimes/",
	"title": "NY Times",
	"description": "",
	"content": "NY Times Loads data from the New York Times’ Article Search API.\nInputs\n None  Outputs\n Corpus: A collection of documents from the New York Times newspaper.  NYTimes widget loads data from New York Times’ Article Search API. You can query NYTimes articles from September 18, 1851 to today, but the API limit is set to allow retrieving only a 1000 documents per query. Define which features to use for text mining, Headline and Abstract being selected by default.\nTo use the widget, you must enter your own API key.\n  To begin your query, insert NY Times’ Article Search API key. The key is securely saved in your system keyring service (like Credential Vault, Keychain, KWallet, etc.) and won’t be deleted when clearing widget settings.\n  Set query parameters:\n Query Query time frame. The widget allows querying articles from September 18, 1851 onwards. Default is set to 1 year back from the current date.    Define which features to include as text features.\n  Information on the output.\n  Produce report.\n  Run or stop the query.\n  Example NYTimes is a data retrieving widget, similar to Twitter and Wikipedia. As it can retrieve geolocations, that is geographical locations the article mentions, it is great in combination with Document Map widget.\nFirst, let’s query NYTimes for all articles on Slovenia. We can retrieve the articles found and view the results in Corpus Viewer. The widget displays all the retrieved features, but includes on selected features as text mining features.\nNow, let’s inspect the distribution of geolocations from the articles mentioning Slovenia. We can do this with Document Map. Unsurprisingly, Croatia and Hungary appear the most often in articles on Slovenia (discounting Slovenia itself), with the rest of Europe being mentioned very often as well.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "NY Times Loads data from the New York Times\u0026rsquo; Article Search API.\nInputs\n None  Outputs\n Corpus: A collection of documents from the New York Times newspaper.  NYTimes widget loads data from New York Times\u0026rsquo; Article Search API. You can query NYTimes articles from September 18, 1851 to today, but the API limit is set to allow retrieving only a 1000 documents per query. Define which features to use for text mining, Headline and Abstract being selected by default." ,
	"author" : "",
	"summary" : "NY Times Loads data from the New York Times\u0026rsquo; Article Search API.\nInputs\n None  Outputs\n Corpus: A collection of documents from the New York Times newspaper.  NYTimes widget loads data from New York Times\u0026rsquo; Article Search API. You can query NYTimes articles from September 18, 1851 to today, but the API limit is set to allow retrieving only a 1000 documents per query. Define which features to use for text mining, Headline and Abstract being selected by default.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "NY Times",
	"icon" : ""
},
{
    "uri": "/workflows/Outliers/",
	"title": "Outliers",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Outliers",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/outliers/",
	"title": "Outliers",
	"description": "",
	"content": "Outliers Outlier detection widget.\nInputs\n Data: input dataset  Outputs\n Outliers: instances scored as outliers Inliers: instances not scored as outliers Data: input dataset appended Outlier variable  The Outliers widget applies one of the four methods for outlier detection. All methods apply classification to the dataset. One-class SVM with non-linear kernels (RBF) performs well with non-Gaussian distributions, while Covariance estimator works only for data with Gaussian distribution. One efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor algorithm. The algorithm computes a score reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. Another efficient way of performing outlier detection in high-dimensional datasets is to use random forests (Isolation Forest).\n Method for outlier detection:  One Class SVM Covariance Estimator Local Outlier Factor Isolation Forest   Set parameters for the method:  One class SVM with non-linear kernel (RBF): classifies data as similar or different from the core class:  Nu is a parameter for the upper bound on the fraction of training errors and a lower bound of the fraction of support vectors Kernel coefficient is a gamma parameter, which specifies how much influence a single data instance has   Covariance estimator: fits ellipsis to central points with Mahalanobis distance metric:  Contamination is the proportion of outliers in the dataset Support fraction specifies the proportion of points included in the estimate   Local Outlier Factor: obtains local density from the k-nearest neighbors:  Contamination is the proportion of outliers in the dataset Neighbors represents number of neighbors Metric is the distance measure   Isolation Forest: isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature:  Contamination is the proportion of outliers in the dataset Replicabe training fixes random seed     If Apply automatically is ticked, changes will be propagated automatically. Alternatively, click Apply. Produce a report. Number of instances on the input, followed by number of instances scored as inliers.  Example Below is an example of how to use this widget. We used subset (versicolor and virginica instances) of the Iris dataset to detect the outliers. We chose the Local Outlier Factor method, with Euclidean distance. Then we observed the annotated instances in the Scatter Plot widget. In the next step we used the setosa instances to demonstrate novelty detection using Apply Domain widget. After concatenating both outputs we examined the outliers in the Scatter Plot (1).\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Outliers Outlier detection widget.\nInputs\n Data: input dataset  Outputs\n Outliers: instances scored as outliers Inliers: instances not scored as outliers Data: input dataset appended Outlier variable  The Outliers widget applies one of the four methods for outlier detection. All methods apply classification to the dataset. One-class SVM with non-linear kernels (RBF) performs well with non-Gaussian distributions, while Covariance estimator works only for data with Gaussian distribution. One efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor algorithm." ,
	"author" : "",
	"summary" : "Outliers Outlier detection widget.\nInputs\n Data: input dataset  Outputs\n Outliers: instances scored as outliers Inliers: instances not scored as outliers Data: input dataset appended Outlier variable  The Outliers widget applies one of the four methods for outlier detection. All methods apply classification to the dataset. One-class SVM with non-linear kernels (RBF) performs well with non-Gaussian distributions, while Covariance estimator works only for data with Gaussian distribution. One efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor algorithm.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Outliers",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/paintdata/",
	"title": "Paint Data",
	"description": "",
	"content": "Paint Data Paints data on a 2D plane. You can place individual data points or use a brush to paint larger datasets.\nOutputs\n Data: dataset as painted in the plot  The widget supports the creation of a new dataset by visually placing data points on a two-dimension plane. Data points can be placed on the plane individually (Put) or in a larger number by brushing (Brush). Data points can belong to classes if the data is intended to be used in supervised learning.\n Name the axes and select a class to paint data instances. You can add or remove classes. Use only one class to create classless, unsupervised datasets. Drawing tools. Paint data points with Brush (multiple data instances) or Put (individual data instance). Select data points with Select and remove them with the Delete/Backspace key. Reposition data points with Jitter (spread) and Magnet (focus). Use Zoom and scroll to zoom in or out. Below, set the radius and intensity for Brush, Put, Jitter and Magnet tools. Reset to Input Data. Save Image saves the image to your computer in a .svg or .png format. Produce a report. Tick the box on the left to automatically commit changes to other widgets. Alternatively, press Send to apply them.  Example In the example below, we have painted a dataset with 4 classes. Such dataset is great for demonstrating k-means and hierarchical clustering methods. In the screenshot, we see that k-Means, overall, recognizes clusters better than Hierarchical Clustering. It returns a score rank, where the best score (the one with the highest value) means the most likely number of clusters. Hierarchical clustering, however, doesn’t group the right classes together. This is a great tool for learning and exploring statistical concepts.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Paint Data Paints data on a 2D plane. You can place individual data points or use a brush to paint larger datasets.\nOutputs\n Data: dataset as painted in the plot  The widget supports the creation of a new dataset by visually placing data points on a two-dimension plane. Data points can be placed on the plane individually (Put) or in a larger number by brushing (Brush). Data points can belong to classes if the data is intended to be used in supervised learning." ,
	"author" : "",
	"summary" : "Paint Data Paints data on a 2D plane. You can place individual data points or use a brush to paint larger datasets.\nOutputs\n Data: dataset as painted in the plot  The widget supports the creation of a new dataset by visually placing data points on a two-dimension plane. Data points can be placed on the plane individually (Put) or in a larger number by brushing (Brush). Data points can belong to classes if the data is intended to be used in supervised learning.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Paint Data",
	"icon" : ""
},
{
    "uri": "/workflows/PCA/",
	"title": "PCA",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "PCA",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/PCA/",
	"title": "PCA",
	"description": "",
	"content": "PCA PCA linear transformation of input data.\nInputs\n Data: input dataset  Outputs\n Transformed Data: PCA transformed data Components: Eigenvectors.  Principal Component Analysis (PCA) computes the PCA linear transformation of the input data. It outputs either a transformed dataset with weights of individual instances or weights of principal components.\n Select how many principal components you wish in your output. It is best to choose as few as possible with variance covered as high as possible. You can also set how much variance you wish to cover with your principal components. You can normalize data to adjust the values to common scale. When Apply Automatically is ticked, the widget will automatically communicate all changes. Alternatively, click Apply. Press Save Image if you want to save the created image to your computer. Produce a report. Principal components graph, where the red (lower) line is the variance covered per component and the green (upper) line is cumulative variance covered by components.  The number of components of the transformation can be selected either in the Components Selection input box or by dragging the vertical cutoff line in the graph.\nExamples PCA can be used to simplify visualizations of large datasets. Below, we used the Iris dataset to show how we can improve the visualization of the dataset with PCA. The transformed data in the Scatter Plot show a much clearer distinction between classes than the default settings.\nThe widget provides two outputs: transformed data and principal components. Transformed data are weights for individual instances in the new coordinate system, while components are the system descriptors (weights for principal components). When fed into the Data Table, we can see both outputs in numerical form. We used two data tables in order to provide a more clean visualization of the workflow, but you can also choose to edit the links in such a way that you display the data in just one data table. You only need to create two links and connect the Transformed data and Components inputs to the Data output.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "PCA PCA linear transformation of input data.\nInputs\n Data: input dataset  Outputs\n Transformed Data: PCA transformed data Components: Eigenvectors.  Principal Component Analysis (PCA) computes the PCA linear transformation of the input data. It outputs either a transformed dataset with weights of individual instances or weights of principal components.\n Select how many principal components you wish in your output. It is best to choose as few as possible with variance covered as high as possible." ,
	"author" : "",
	"summary" : "PCA PCA linear transformation of input data.\nInputs\n Data: input dataset  Outputs\n Transformed Data: PCA transformed data Components: Eigenvectors.  Principal Component Analysis (PCA) computes the PCA linear transformation of the input data. It outputs either a transformed dataset with weights of individual instances or weights of principal components.\n Select how many principal components you wish in your output. It is best to choose as few as possible with variance covered as high as possible.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "PCA",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/periodogram_w/",
	"title": "Periodogram",
	"description": "",
	"content": "Periodogram Visualize time series’ cycles, seasonality, periodicity, and most significant periods.\nInputs\n Time series: Time series as output by As Timeseries widget.  In this widget, you can visualize the most significant periods of the time series.\n Select the series to calculate the periodogram for. See the periods and their respective relative power spectral density estimates.  Periodogram for non-equispaced series is calculated using Lomb-Scargle method.\nSee also Correlogram\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Periodogram Visualize time series\u0026rsquo; cycles, seasonality, periodicity, and most significant periods.\nInputs\n Time series: Time series as output by As Timeseries widget.  In this widget, you can visualize the most significant periods of the time series.\n Select the series to calculate the periodogram for. See the periods and their respective relative power spectral density estimates.  Periodogram for non-equispaced series is calculated using Lomb-Scargle method.\nSee also Correlogram" ,
	"author" : "",
	"summary" : "Periodogram Visualize time series\u0026rsquo; cycles, seasonality, periodicity, and most significant periods.\nInputs\n Time series: Time series as output by As Timeseries widget.  In this widget, you can visualize the most significant periods of the time series.\n Select the series to calculate the periodogram for. See the periods and their respective relative power spectral density estimates.  Periodogram for non-equispaced series is calculated using Lomb-Scargle method.\nSee also Correlogram",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Periodogram",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/pie-chart/",
	"title": "Pie Chart",
	"description": "",
	"content": "Pie Chart The widget for visualizing discrete attributes in the pie chart.\nInputs\n Data: input data set  The aim of this widget is to demonstrate that pie charts are a terrible visualization. Please don’t use it for any other purpose.\n Select the attribute you want to visualize. Select the attribute which is used to split data in more charts. Check if you want pies to be exploded (parts of the pie will have space in between). You will see your data visualized here. With those buttons, you can either get help, save the plot, or include plots in the report.  Example We load the Titanic dataset in File widget and connected the data to Pie Chart. Here we show the distribution of gender data and split pies by survived attributes. We notice that in the group of passengers that did not survive there are mainly male while there is a higher proportion of women in the group of people that survived. While the pie chart can shed some light of data we still suggest using more informative visualizations, e.g. Box Plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Pie Chart The widget for visualizing discrete attributes in the pie chart.\nInputs\n Data: input data set  The aim of this widget is to demonstrate that pie charts are a terrible visualization. Please don\u0026rsquo;t use it for any other purpose.\n Select the attribute you want to visualize. Select the attribute which is used to split data in more charts. Check if you want pies to be exploded (parts of the pie will have space in between)." ,
	"author" : "",
	"summary" : "Pie Chart The widget for visualizing discrete attributes in the pie chart.\nInputs\n Data: input data set  The aim of this widget is to demonstrate that pie charts are a terrible visualization. Please don\u0026rsquo;t use it for any other purpose.\n Select the attribute you want to visualize. Select the attribute which is used to split data in more charts. Check if you want pies to be exploded (parts of the pie will have space in between).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Pie Chart",
	"icon" : ""
},
{
    "uri": "/workflows/Pivot-Table/",
	"title": "Pivot Table",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Pivot Table",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/pivot/",
	"title": "Pivot Table",
	"description": "",
	"content": "Pivot Table Reshape data table based on column values.\nInputs\n Data: input data set  Outputs\n Pivot Table: contingency matrix as shown in the widget Filtered Data: subset selected from the plot Grouped Data: aggregates over groups defined by row values  Pivot Table summarizes the data of a more extensive table into a table of statistics. The statistics can include sums, averages, counts, etc. The widget also allows selecting a subset from the table and grouping by row values, which have to be a discrete variable. Data with only numeric variables cannot be displayed in the table.\n Discrete or numeric variable used for row values. Numeric variables are considered as integers. Discrete variable used for column values. Variable values will appear as columns in the table. Values used for aggregation. Aggregated values will appear as cells in the table. Aggregation methods:  For any variable type:  Count: number of instances with the given row and column value. Count defined: number of instances where the aggregation value is defined.   For numeric variables:  Sum: sum of values. Mean: average of values. Mode: most frequent value of the subset. Min: smallest value. Max: highest value. Median: middle value. Var: variance of the subset.   For discrete variables:  Majority: most frequent value of the subset.     Tick the box on the left to automatically output any changes. Alternatively, press Apply .  Discrete variables Example of a pivot table with only discrete variables selected. We are using heart-disease data set for this example. Rows correspond to values of diameter narrowing variable. Our columns are values of gender, namely female and male. We are using thal as values in our cells.\nWe have selected Count and Majority as aggregation methods. In the pivot table, we can see the number of instances that do not have diameter narrowing and are female. There are 72 such patients. Concurrently, there are 92 male patients that don’t have diameter narrowing. Thal values don’t have any effect here, we are just counting occurrences in the data.\nThe second row shows majority. This means most female patients that don’t have diameter narrowing have normal thal results. Conversely, female patients that have diameter narrowing most often have reversable defect.\nNumeric variables Example of a pivot table with numeric variables. We are using heart-disease data set for this example. Rows correspond to values of diameter narrowing variable. Our columns are values of gender, namely female and male. We are using rest SBP as values in our cells.\nWe have selected Count, Sum and Median as aggregation methods. Under Count, we see there are 72 female patients that don’t have diameter narrowing, same as before for discrete values. What is different are the sum and median aggregations. We see that the sum of resting systolic blood pressure for female patients that don’t have diameter narrowing is 9269 and the median value is 130.\nExample We are using Forest Fires for this example. The data is loaded in the Datasets widget and passed to Pivot Table. Forest Fires datasets reports forest fires by the month and day they happened. We can aggregate all occurrences of forest fires by selecting Count as aggregation method and using month as row and day as column values. Since we are using Count, Values variable will have no effect.\nWe can plot the counts in Line Plot. But first, let us organize our data a bit. With Edit Domain, we will reorder rows values so that months will appear in the correct order, namely from January to December. To do the same for columns, we will use Select Columns and reorder day to go from Monday to Sunday.\nFinally, our data is ready. Let us pass it to Line Plot. We can see that forest fires are most common in August and September, while their frequency is higher during the weekend than during weekdays.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Pivot Table Reshape data table based on column values.\nInputs\n Data: input data set  Outputs\n Pivot Table: contingency matrix as shown in the widget Filtered Data: subset selected from the plot Grouped Data: aggregates over groups defined by row values  Pivot Table summarizes the data of a more extensive table into a table of statistics. The statistics can include sums, averages, counts, etc. The widget also allows selecting a subset from the table and grouping by row values, which have to be a discrete variable." ,
	"author" : "",
	"summary" : "Pivot Table Reshape data table based on column values.\nInputs\n Data: input data set  Outputs\n Pivot Table: contingency matrix as shown in the widget Filtered Data: subset selected from the plot Grouped Data: aggregates over groups defined by row values  Pivot Table summarizes the data of a more extensive table into a table of statistics. The statistics can include sums, averages, counts, etc. The widget also allows selecting a subset from the table and grouping by row values, which have to be a discrete variable.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Pivot Table",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/polynomial-classification/",
	"title": "Polynomial Classification",
	"description": "",
	"content": "Polynomial Classification Educational widget that visually demonstrates classification in two classes for any classifier.\nInputs\n Data: input data set Preprocessor: data preprocessors Learner: classification algorithm used in the widget. Default set to Logistic Regression Learner.  Outputs\n Learner: classification algorithm used in the widget Classifier: trained classifier Coefficients: classifier coefficients if it has them  Description This widget interactively shows classification probabilities for classification in two classes using color gradient and contour lines for any classifiers from the Model section. In the widget, polynomial expansion can be set. Polynomial expansion is a regulation of the degree of polynom that is used to transform the input data and has an effect on the classification. If polynomial expansion is set to 1 it means that untransformed data are used in the regression. If polynomial expansion is set to 2 we get following additional attributes:\n first attribute on power 2 first attribute * second attribute second attribute on power 2   Classifier name. X: attribute on axis x. Y: attribute on axis y. Target class: Class in input data that is classified apart from others classes because widget support only two class classification. Polynomial expansion: Degree of polynom that is used to transform the input data. Show contours: Enable contour lines in the graph. Contour step: Density of contour lines. Save Image saves the image to the computer in a .svg or .png format. Report includes widget parameters and visualization in the report.  Example We loaded the iris data set with the File widget and connected it to the Polynomial Classification widget. To demonstrate output connections, we connected Coefficients to the Data Table widget where we can inspect their values. Learner output can be connected to Test \u0026 Score widget and Classifier to Predictions widget.\nIn the widget we selected sepal length as our X variable and sepal width as our Y variable. We set the Polynomial expansion to 1. That performs classification on non transformed data. Result is shown in the figure below. Color gradient represents the probability of the area to belong to a particular class value. Blue color represents classification to the target class and red color classification to the class with all other examples.\nIn the next example we changed the File widget to the Paint data widget and plotted some custom data. Because the center of the data is of one class and the surrounding of another, Polynomial expansion degree 1 does not perform good classification. We set Polynomial expansion to 2 and get the classification in the figure below. We also selected to use contour lines.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Polynomial Classification Educational widget that visually demonstrates classification in two classes for any classifier.\nInputs\n Data: input data set Preprocessor: data preprocessors Learner: classification algorithm used in the widget. Default set to Logistic Regression Learner.  Outputs\n Learner: classification algorithm used in the widget Classifier: trained classifier Coefficients: classifier coefficients if it has them  Description This widget interactively shows classification probabilities for classification in two classes using color gradient and contour lines for any classifiers from the Model section." ,
	"author" : "",
	"summary" : "Polynomial Classification Educational widget that visually demonstrates classification in two classes for any classifier.\nInputs\n Data: input data set Preprocessor: data preprocessors Learner: classification algorithm used in the widget. Default set to Logistic Regression Learner.  Outputs\n Learner: classification algorithm used in the widget Classifier: trained classifier Coefficients: classifier coefficients if it has them  Description This widget interactively shows classification probabilities for classification in two classes using color gradient and contour lines for any classifiers from the Model section.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Polynomial Classification",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/polynomial-regression/",
	"title": "Polynomial Regression",
	"description": "",
	"content": "Polynomial Regression Educational widget that interactively shows regression line for different regressors.\nInputs\n Data: input data set. It needs at least two continuous attributes. Preprocessor: data preprocessors Learner: regression algorithm used in the widget. Default set to Linear Regression.  Outputs\n Learner: regression algorithm used in the widget Predictor: trained regressor Coefficients: regressor coefficients if any  Description This widget interactively shows the regression line using any of the regressors from the Model module. In the widget, polynomial expansion can be set. Polynomial expansion is a regulation of the degree of the polynom that is used to transform the input data and has an effect on the shape of a curve. If polynomial expansion is set to 1 it means that untransformed data are used in the regression.\n Regressor name. Input: independent variable on axis x. Polynomial expansion: degree of polynomial expansion. Target: dependent variable on axis y. Save Image saves the image to the computer in a .svg or .png format. Report includes widget parameters and visualization in the report.  Example We loaded iris data set with the File widget. Then we connected Linear Regression learner to the Polynomial Regression widget. In the widget we selected petal length as our Input variable and petal width as our Target variable. We set Polynomial expansion to 1 which gives us a linear regression line. The result is shown in the figure below.\nThe line can fit better if we increase the Polynomial expansion parameter. Say, we set it to 3.\nTo observe different results, change Linear Regression to any other regression learner from Orange. Example below is done with the Tree learner.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Polynomial Regression Educational widget that interactively shows regression line for different regressors.\nInputs\n Data: input data set. It needs at least two continuous attributes. Preprocessor: data preprocessors Learner: regression algorithm used in the widget. Default set to Linear Regression.  Outputs\n Learner: regression algorithm used in the widget Predictor: trained regressor Coefficients: regressor coefficients if any  Description This widget interactively shows the regression line using any of the regressors from the Model module." ,
	"author" : "",
	"summary" : "Polynomial Regression Educational widget that interactively shows regression line for different regressors.\nInputs\n Data: input data set. It needs at least two continuous attributes. Preprocessor: data preprocessors Learner: regression algorithm used in the widget. Default set to Linear Regression.  Outputs\n Learner: regression algorithm used in the widget Predictor: trained regressor Coefficients: regressor coefficients if any  Description This widget interactively shows the regression line using any of the regressors from the Model module.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Polynomial Regression",
	"icon" : ""
},
{
    "uri": "/widget-catalog/evaluate/predictions/",
	"title": "Predictions",
	"description": "",
	"content": "Predictions Shows models’ predictions on the data.\nInputs\n Data: input dataset Predictors: predictors to be used on the data  Outputs\n Predictions: data with added predictions Evaluation Results: results of testing classification algorithms  The widget receives a dataset and one or more predictors (predictive models, not learning algorithms - see the example below). It outputs the data and the predictions.\n Information on the input, namely the number of instances to predict, the number of predictors and the task (classification or regression). If you have sorted the data table by attribute and you wish to see the original view, press Restore Original Order. You can select the options for classification. If Predicted class is ticked, the view provides information on predicted class. If Predicted probabilities for is ticked, the view provides information on probabilities predicted by the classifier(s). You can also select the predicted class displayed in the view. The option Draw distribution bars provides a visualization of probabilities. By ticking the Show full dataset, you can view the entire data table (otherwise only class variable will be shown). Select the desired output. Predictions.  The widget show the probabilities and final decisions of predictive models. The output of the widget is another dataset, where predictions are appended as new meta attributes. You can select which features you wish to output (original data, predictions, probabilities). The result can be observed in a Data Table. If the predicted data includes true class values, the result of prediction can also be observed in a Confusion Matrix.\nExamples In the first example, we will use Attrition - Train data from the Datasets widget. This is a data on attrition of employees. In other words, we wish to know whether a certain employee will resign from the job or not. We will construct a predictive model with the Tree widget and observe probabilities in Predictions.\nFor predictions we need both the training data, which we have loaded in the first Datasets widget and the data to predict, which we will load in another Datasets widget. We will use Attrition - Predict data this time. Connect the second data set to Predictions. Now we can see predictions for the three data instances from the second data set.\nThe Tree model predicts none of the employees will leave the company. You can try other model and see if predictions change. Or test the predictive scores first in the Test \u0026 Score widget.\nIn the second example, we will see how to properly use Preprocess with Predictions or Test \u0026 Score.\nThis time we are using the heart disease.tab data from the File widget. You can access the data through the dropdown menu. This is a dataset with 303 patients that came to the doctor suffering from a chest pain. After the tests were done, some patients were found to have diameter narrowing and others did not (this is our class variable).\nThe heart disease data have some missing values and we wish to account for that. First, we will split the data set into train and test data with Data Sampler.\nThen we will send the Data Sample into Preprocess. We will use Impute Missing Values, but you can try any combination of preprocessors on your data. We will send preprocessed data to Logistic Regression and the constructed model to Predictions.\nFinally, Predictions also needs the data to predict on. We will use the output of Data Sampler for prediction, but this time not the Data Sample, but the Remaining Data, this is the data that wasn’t used for training the model.\nNotice how we send the remaining data directly to Predictions without applying any preprocessing. This is because Orange handles preprocessing on new data internally to prevent any errors in the model construction. The exact same preprocessor that was used on the training data will be used for predictions. The same process applies to Test \u0026 Score.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Predictions Shows models\u0026rsquo; predictions on the data.\nInputs\n Data: input dataset Predictors: predictors to be used on the data  Outputs\n Predictions: data with added predictions Evaluation Results: results of testing classification algorithms  The widget receives a dataset and one or more predictors (predictive models, not learning algorithms - see the example below). It outputs the data and the predictions.\n Information on the input, namely the number of instances to predict, the number of predictors and the task (classification or regression)." ,
	"author" : "",
	"summary" : "Predictions Shows models\u0026rsquo; predictions on the data.\nInputs\n Data: input dataset Predictors: predictors to be used on the data  Outputs\n Predictions: data with added predictions Evaluation Results: results of testing classification algorithms  The widget receives a dataset and one or more predictors (predictive models, not learning algorithms - see the example below). It outputs the data and the predictions.\n Information on the input, namely the number of instances to predict, the number of predictors and the task (classification or regression).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Predictions",
	"icon" : ""
},
{
    "uri": "/workflows/Predictive-models/",
	"title": "Predictive models",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Predictive models",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/preprocess/",
	"title": "Preprocess",
	"description": "",
	"content": "Preprocess Preprocesses data with selected methods.\nInputs\n Data: input dataset  Outputs\n Preprocessor: preprocessing method Preprocessed Data: data preprocessed with selected methods  Preprocessing is crucial for achieving better-quality analysis results. The Preprocess widget offers several preprocessing methods that can be combined in a single preprocessing pipeline. Some methods are available as separate widgets, which offer advanced techniques and greater parameter tuning.\n List of preprocessors. Double click the preprocessors you wish to use and shuffle their order by dragging them up or down. You can also add preprocessors by dragging them from the left menu to the right. Preprocessing pipeline. When the box is ticked (Send Automatically), the widget will communicate changes automatically. Alternatively, click Send.  Preprocessors  Discretization of continuous values:  Entropy-MDL discretization by Fayyad and Irani that uses expected information to determine bins. Equal frequency discretization splits by frequency (same number of instances in each bin. Equal width discretization creates bins of equal width (span of each bin is the same). Remove numeric features altogether.   Continuization of discrete values:  Most frequent as base treats the most frequent discrete value as 0 and others as 1. The discrete attributes with more than 2 values, the most frequent will be considered as a base and contrasted with remaining values in corresponding columns. One feature per value creates columns for each value, place 1 where an instance has that value and 0 where it doesn’t. Essentially One Hot Encoding. Remove non-binary features retains only categorical features that have values of either 0 or 1 and transforms them into continuous. Remove categorical features removes categorical features altogether. Treat as ordinal takes discrete values and treats them as numbers. If discrete values are categories, each category will be assigned a number as they appear in the data. Divide by number of values is similar to treat as ordinal, but the final values will be divided by the total number of values and hence the range of the new continuous variable will be [0, 1].   Impute missing values:  Average/Most frequent replaces missing values (NaN) with the average (for continuous) or most frequent (for discrete) value. Replace with random value replaces missing values with random ones within the range of each variable. Remove rows with missing values.   Select relevant features:  Similar to Rank, this preprocessor outputs only the most informative features. Score can be determined by information gain, gain ratio, gini index, ReliefF, fast correlation based filter, ANOVA, Chi2, RReliefF, and Univariate Linear Regression. Strategy refers to how many variables should be on the output. Fixed returns a fixed number of top scored variables, while Percentile return the selected top percent of the features.    Select random features outputs either a fixed number of features from the original data or a percentage. This is mainly used for advanced testing and educational purposes. Normalize adjusts values to a common scale. Center values by mean or median or omit centering altogether. Similar for scaling, one can scale by SD (standard deviation), by span or not at all. Randomize instances. Randomize classes shuffles class values and destroys connection between instances and class. Similarly, one can randomize features or meta data. If replicable shuffling is on, randomization results can be shared and repeated with a saved workflow. This is mainly used for advanced testing and educational purposes. Remove sparse features retains features that have more than user-defined threshold percentage of non-zero values. The rest are discarded. Principal component analysis outputs results of a PCA transformation. Similar to the PCA widget. CUR matrix decomposition is a dimensionality reduction method, similar to SVD.  Examples In the first example, we have used the heart_disease.tab dataset available in the dropdown menu of the File widget. then we used Preprocess to impute missing values and normalize features. We can observe the changes in the Data Table and compare it to the non-processed data.\nIn the second example, we show how to use Preprocess for predictive modeling.\nThis time we are using the heart_disease.tab data from the File widget. You can access the data through the dropdown menu. This is a dataset with 303 patients that came to the doctor suffering from a chest pain. After the tests were done, some patients were found to have diameter narrowing and others did not (this is our class variable).\nThe heart disease data have some missing values and we wish to account for that. First, we will split the data set into train and test data with Data Sampler.\nThen we will send the Data Sample into Preprocess. We will use Impute Missing Values, but you can try any combination of preprocessors on your data. We will send preprocessed data to Logistic Regression and the constructed model to Predictions.\nFinally, Predictions also needs the data to predict on. We will use the output of Data Sampler for prediction, but this time not the Data Sample, but the Remaining Data, this is the data that wasn’t used for training the model.\nNotice how we send the remaining data directly to Predictions without applying any preprocessing. This is because Orange handles preprocessing on new data internally to prevent any errors in the model construction. The exact same preprocessor that was used on the training data will be used for predictions. The same process applies to Test \u0026 Score.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Preprocess Preprocesses data with selected methods.\nInputs\n Data: input dataset  Outputs\n Preprocessor: preprocessing method Preprocessed Data: data preprocessed with selected methods  Preprocessing is crucial for achieving better-quality analysis results. The Preprocess widget offers several preprocessing methods that can be combined in a single preprocessing pipeline. Some methods are available as separate widgets, which offer advanced techniques and greater parameter tuning.\n List of preprocessors. Double click the preprocessors you wish to use and shuffle their order by dragging them up or down." ,
	"author" : "",
	"summary" : "Preprocess Preprocesses data with selected methods.\nInputs\n Data: input dataset  Outputs\n Preprocessor: preprocessing method Preprocessed Data: data preprocessed with selected methods  Preprocessing is crucial for achieving better-quality analysis results. The Preprocess widget offers several preprocessing methods that can be combined in a single preprocessing pipeline. Some methods are available as separate widgets, which offer advanced techniques and greater parameter tuning.\n List of preprocessors. Double click the preprocessors you wish to use and shuffle their order by dragging them up or down.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Preprocess",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/preprocess-spectra/",
	"title": "Preprocess Spectra",
	"description": "",
	"content": "Preprocess Spectra Construct a data preprocessing pipeline.\nInputs\n Data: required input data set Reference: optional reference data set used in some preprocessing methods  Outputs\n Preprocessed Data: transformed data set Preprocessor: preprocessing methods  The Preprocess Spectra widget applies a series of preprocessing methods to spectral data. You can select the preprocessing method from the list and press the triangle button on the right to visualize the result. The order of the preprocessing matters, so to change the order of the preprocessing, just drag and drop the method to its proper place.\nThe input data for the selected method is displayed in the top plot, while the preprocessed data is shown in the bottom plot.\nYou can observe each preprocessing step by pressing the triangle button on the right. To apply all of then and observe the final result plot, press Final preview. To output the data, press Commit.\nThe reference data set is processed along the input data: only the first preprocessor uses the reference as on the input. If the reference needs to stay fixed, split your preprocessing methods among multiple Preprocess Spectra widgets and connect references accordingly.\nBelow is an example of the Preprocess Spectra widget in action with some explanation of the main features.  Add a preprocessor from the dropdown menu. Preview plot with its editor menu like in the Spectra widget. The top plot shows the data before and the bottom after preprocessing. Preview a single preprocessor (the upper plot will show its input, the plot below its output). Observe the final result of preprocessing by clicking the Final Preview button. Change the number of spectra shown in the plot. Press Commit to calculate and output the preprocessed data.  Preprocessing Methods  Cut (keep): Select the cutoff value of the spectral area you wish to keep. Cut (remove): Select the cutoff value of the spectral area you wish to discard. Gaussian smoothing: apply Gaussian smoothing. Savitzky-Golay Filter: apply Savitzky-Golay filter. Baseline Correction: correct the baseline Normalize Spectra: apply normalization.  Vector normalization: calculates the L2 norm Min-Max normalization: divides each spectra with its Ymax - Ymin range Area normalization: provides several methods, also allows the selection of a specific range for the calculation Attribute normalization: normalize each spectrum with one of the available pre-calculated attributes Standard Normal Variate (SNV): \\(\\tilde{X}^{SNV}_i = (X_i - \\tilde{X}_i) / \\sigma_i\\) Normalize by Reference: divides each spectrum with the reference spectrum on the input   Integrate: compute integrals of selected area. Similar to the Integrate Spectra widget. PCA denoising: denoise the data with PCA. Transmittance to Absorbance: convert from transmittance to absorbance spectra. Absorbance to Transmittance: convert absorbance spectra to transmittance. Shift spectra: shift things around. EMSC: special Norweigan method. Spike Removal: Removes spikes in spectra through a modified z-score. More…. Asymmetric Least Squares Smoothing: Three ALS methods which can be used for baseline subtraction. More…  Example Normally, we would use Preprocess Spectra at the beginning of the analysis. We will use the liver spectroscopy data from the Datasets widget.\nIn Preprocess Spectra we will select a couple of preprocessing methods and observe their output. First, let us use the Baseline Correction which removes the baseline from the spectra.\nThen we will cut an area of interest with the Cut (keep) method. To set the area we wish to keep, drag the red lines left or right in the plot. You will see how the bottom changes with a change in selection.\nTo see the end result of preprocessing, press Final preview and once you are satisfied with the results, press Commit. We can observe the end result in a Spectra widget or use the preprocessed data in the downstream analysis.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Preprocess Spectra Construct a data preprocessing pipeline.\nInputs\n Data: required input data set Reference: optional reference data set used in some preprocessing methods  Outputs\n Preprocessed Data: transformed data set Preprocessor: preprocessing methods  The Preprocess Spectra widget applies a series of preprocessing methods to spectral data. You can select the preprocessing method from the list and press the triangle button on the right to visualize the result. The order of the preprocessing matters, so to change the order of the preprocessing, just drag and drop the method to its proper place." ,
	"author" : "",
	"summary" : "Preprocess Spectra Construct a data preprocessing pipeline.\nInputs\n Data: required input data set Reference: optional reference data set used in some preprocessing methods  Outputs\n Preprocessed Data: transformed data set Preprocessor: preprocessing methods  The Preprocess Spectra widget applies a series of preprocessing methods to spectral data. You can select the preprocessing method from the list and press the triangle button on the right to visualize the result. The order of the preprocessing matters, so to change the order of the preprocessing, just drag and drop the method to its proper place.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Preprocess Spectra",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/preprocesstext/",
	"title": "Preprocess Text",
	"description": "",
	"content": "Preprocess Text Preprocesses corpus with selected methods.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Preprocessed corpus.  Preprocess Text splits your text into smaller units (tokens), filters them, runs normalization (stemming, lemmatization), creates n-grams and tags tokens with part-of-speech labels. Steps in the analysis are applied sequentially and can be reordered. Click and drag the preprocessor to change the order.\n Available preprocessors. Transformation transforms input data. It applies lowercase transformation by default.  Lowercase will turn all text to lowercase. Remove accents will remove all diacritics/accents in text. naïve → naive Parse html will detect html tags and parse out text only. \u003ca href…\u003eSome text\u003c/a\u003e → Some text Remove urls will remove urls from text. This is a http://orange.biolab.si/ url. → This is a url.   Tokenization  is the method of breaking the text into smaller components (words, sentences, bigrams).  Word \u0026 Punctuation will split the text by words and keep punctuation symbols. This example. → (This), (example), (.) Whitespace will split the text by whitespace only. This example. → (This), (example.) Sentence will split the text by full stop, retaining only full sentences. This example. Another example. → (This example.), (Another example.) Regexp will split the text by provided regex. It splits by words only by default (omits punctuation). Tweet will split the text by pre-trained Twitter model, which keeps hashtags, emoticons and other special symbols. This example. :-) #simple → (This), (example), (.), (:-)), (#simple)   Normalization applies stemming and lemmatization to words. (I’ve always loved cats. → I have alway love cat.) For languages other than English use Snowball Stemmer (offers languages available in its NLTK implementation) or UDPipe.  Porter Stemmer applies the original Porter stemmer. Snowball Stemmer applies an improved version of Porter stemmer (Porter2). Set the language for normalization, default is English. WordNet Lemmatizer applies a networks of cognitive synonyms to tokens based on a large lexical database of English. UDPipe applies a pre-trained model for normalizing data.   Filtering removes or keeps a selection of words.  Stopwords removes stopwords from text (e.g. removes ‘and’, ‘or’, ‘in’…). Select the language to filter by, English is set as default. You can also load your own list of stopwords provided in a simple *.txt file with one stopword per line. Click ‘browse’ icon to select the file containing stopwords. If the file was properly loaded, its name will be displayed next to pre-loaded stopwords. Change ‘English’ to ‘None’ if you wish to filter out only the provided stopwords. Click ‘reload’ icon to reload the list of stopwords. Lexicon keeps only words provided in the file. Load a *.txt file with one word per line to use as lexicon. Click ‘reload’ icon to reload the lexicon. Regexp removes words that match the regular expression. Default is set to remove punctuation. Document frequency keeps tokens that appear in not less than and not more than the specified number / percentage of documents. Absolute keeps only tokens that appear in the specified number of documents. E.g. DF = (3, 5) keeps only tokens that appear in 3 or more and 5 or less documents. Relative keeps only tokens that appear in the specified percentage of documents. E.g. DF = (0.3, 0.5) keeps only tokens that appear in 30% to 50% of documents. Most frequent tokens keeps only the specified number of most frequent tokens. Default is a 100 most frequent tokens.   N-grams Range creates n-grams from tokens. Numbers specify the range of n-grams. Default returns one-grams and two-grams. POS Tagger runs part-of-speech tagging on tokens.  Averaged Perceptron Tagger runs POS tagging with Matthew Honnibal’s averaged perceptron tagger. Treebank POS Tagger (MaxEnt) runs POS tagging with a trained Penn Treebank model.   Preview of preprocessed data. If Commit Automatically is on, changes are communicated automatically. Alternatively press Commit.  Note! Preprocess Text applies preprocessing steps in the order they are listed. A good order is to first transform the text, then apply tokenization, POS tags, normalization, filtering and finally constructs n-grams based on given tokens. This is especially important for WordNet Lemmatizer since it requires POS tags for proper normalization.\nUseful Regular Expressions Here are some useful regular expressions for quick filtering:\n\\bword\\b: matches exact word \\w+: matches only words, no punctuation \\b(B|b)\\w+\\b: matches words beginning with the letter b \\w{4,}: matches words that are longer than 4 characters\n\\b\\w+(Y|y)\\b: matches words ending with the letter y\nExamples In the first example we will observe the effects of preprocessing on our text. We are working with book-excerpts.tab that we’ve loaded with Corpus widget. We have connected Preprocess Text to Corpus and retained default preprocessing methods (lowercase, per-word tokenization and stopword removal). The only additional parameter we’ve added as outputting only the first 100 most frequent tokens. Then we connected Preprocess Text with Word Cloud to observe words that are the most frequent in our text. Play around with different parameters, to see how they transform the output.\nThe second example is slightly more complex. We first acquired our data with Twitter widget. We quired the internet for tweets from users @HillaryClinton and @realDonaldTrump and got their tweets from the past two weeks, 242 in total.\nIn Preprocess Text there’s Tweet tokenization available, which retains hashtags, emojis, mentions and so on. However, this tokenizer doesn’t get rid of punctuation, thus we expanded the Regexp filtering with symbols that we wanted to get rid of. We ended up with word-only tokens, which we displayed in Word Cloud. Then we created a schema for predicting author based on tweet content, which is explained in more details in the documentation for Twitter widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Preprocess Text Preprocesses corpus with selected methods.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Preprocessed corpus.  Preprocess Text splits your text into smaller units (tokens), filters them, runs normalization (stemming, lemmatization), creates n-grams and tags tokens with part-of-speech labels. Steps in the analysis are applied sequentially and can be reordered. Click and drag the preprocessor to change the order.\n Available preprocessors. Transformation transforms input data." ,
	"author" : "",
	"summary" : "Preprocess Text Preprocesses corpus with selected methods.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Preprocessed corpus.  Preprocess Text splits your text into smaller units (tokens), filters them, runs normalization (stemming, lemmatization), creates n-grams and tags tokens with part-of-speech labels. Steps in the analysis are applied sequentially and can be reordered. Click and drag the preprocessor to change the order.\n Available preprocessors. Transformation transforms input data.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Preprocess Text",
	"icon" : ""
},
{
    "uri": "/workflows/Preprocessing/",
	"title": "Preprocessing",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Preprocessing",
	"icon" : ""
},
{
    "uri": "/privacy/",
	"title": "Privacy",
	"description": "",
	"content": "Last updated: 14.1.2016 Laboratory of Bioinformatics, Faculty of Computer and Information Science, University of Ljubljana (‘‘us’’, ‘‘we’’, or ‘‘ours’') operates https://orange.biolab.si (the ‘‘Site’'). This page informs you of our policies regarding the collection, use and disclosure of Personal Information we receive from users of the Site. We use your Personal Information only for providing and improving the Site. By using the Site, you agree to the collection and use of information in accordance with this policy. Information Collection and Use While using our Site, we may ask you to provide us with certain personally identifiable information that can be used to contact or identify you. Personally identifiable information may include, but is not limited to your name (‘‘Personal Information’').\nLog Data Like many site operators, we collect information that your browser sends whenever you visit our Site (‘‘Log Data’'). This Log Data may include information such as your computer’s Internet Protocol (‘‘IP’') address, browser type, browser version, the pages of our Site that you visit, the time and date of your visit, the time spent on those pages and other statistics. In addition, we are using third party services, i.e. Google Analytics, that collect, monitor and analyze these data. If you wish to opt-out of data collection on our site, we recommend you to use Google Analytics Opt-Out Add On.\nCommunications We may use your Personal Information to contact you when replying to your contact request or conducting a survey of activities.\nSecurity The security of your Personal Information is important to us, but remember that no method of transmission over the Internet, or method of electronic storage, is 100% secure. While we strive to use commercially acceptable means to protect your Personal Information, we cannot guarantee its absolute security.\nChanges to this Privacy Policy This Privacy Policy is effective as of 14.1.2016 and will remain in effect except with respect to any changes in its provisions in the future, which will be in effect immediately after being posted on this page.\nWe reserve the right to update or change our Privacy Policy at any time and you should check this Privacy Policy periodically. Your continued use of the Service after we post any modifications to the Privacy Policy on this page will constitute your acknowledgement of the modifications and your consent to abide and be bound by the modified Privacy Policy.\nContact Us If you have any questions about this Privacy Policy, please contact us through the contact form on our website.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Last updated: 14.1.2016 Laboratory of Bioinformatics, Faculty of Computer and Information Science, University of Ljubljana (\u0026lsquo;\u0026lsquo;us\u0026rsquo;\u0026rsquo;, \u0026lsquo;\u0026lsquo;we\u0026rsquo;\u0026rsquo;, or \u0026lsquo;\u0026lsquo;ours\u0026rsquo;') operates https://orange.biolab.si (the \u0026lsquo;\u0026lsquo;Site\u0026rsquo;'). This page informs you of our policies regarding the collection, use and disclosure of Personal Information we receive from users of the Site. We use your Personal Information only for providing and improving the Site. By using the Site, you agree to the collection and use of information in accordance with this policy." ,
	"author" : "",
	"summary" : "Last updated: 14.1.2016 Laboratory of Bioinformatics, Faculty of Computer and Information Science, University of Ljubljana (\u0026lsquo;\u0026lsquo;us\u0026rsquo;\u0026rsquo;, \u0026lsquo;\u0026lsquo;we\u0026rsquo;\u0026rsquo;, or \u0026lsquo;\u0026lsquo;ours\u0026rsquo;') operates https://orange.biolab.si (the \u0026lsquo;\u0026lsquo;Site\u0026rsquo;'). This page informs you of our policies regarding the collection, use and disclosure of Personal Information we receive from users of the Site. We use your Personal Information only for providing and improving the Site. By using the Site, you agree to the collection and use of information in accordance with this policy.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "privacy",
	"LinkTitle" : "Privacy",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/pubmed/",
	"title": "Pubmed",
	"description": "",
	"content": "Pubmed Fetch data from PubMed journals.\nInputs\n None  Outputs\n Corpus: A collection of documents from the PubMed online service.  PubMed comprises more than 26 million citations for biomedical literature from MEDLINE, life science journals, and online books. The widget allows you to query and retrieve these entries. You can use regular search or construct advanced queries.\n Enter a valid e-mail to retrieve queries. Regular search:  Author: queries entries from a specific author. Leave empty to query by all authors. From: define the time frame of publication. Query: enter the query. Advanced search: enables you to construct complex queries. See PubMed’s website to learn how to construct such queries. You can also copy-paste constructed queries from the website.   Find records finds available data from PubMed matching the query. Number of records found will be displayed above the button. Define the output. All checked features will be on the output of the widget. Set the number of record you wish to retrieve. Press Retrieve records to get results of your query on the output. Below the button is an information on the number of records on the output.  Example PubMed can be used just like any other data widget. In this example we’ve queried the database for records on orchids. We retrieved 1000 records and kept only ‘abstract’ in our meta features to limit the construction of tokens only to this feature.\nWe used Preprocess Text to remove stopword and words shorter than 3 characters (regexp \\b\\w{1,2}\\b). This will perhaps get rid of some important words denoting chemicals, so we need to be careful with what we filter out. For the sake of quick inspection we only retained longer words, which are displayed by frequency in Word Cloud.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Pubmed Fetch data from PubMed journals.\nInputs\n None  Outputs\n Corpus: A collection of documents from the PubMed online service.  PubMed comprises more than 26 million citations for biomedical literature from MEDLINE, life science journals, and online books. The widget allows you to query and retrieve these entries. You can use regular search or construct advanced queries.\n Enter a valid e-mail to retrieve queries. Regular search:  Author: queries entries from a specific author." ,
	"author" : "",
	"summary" : "Pubmed Fetch data from PubMed journals.\nInputs\n None  Outputs\n Corpus: A collection of documents from the PubMed online service.  PubMed comprises more than 26 million citations for biomedical literature from MEDLINE, life science journals, and online books. The widget allows you to query and retrieve these entries. You can use regular search or construct advanced queries.\n Enter a valid e-mail to retrieve queries. Regular search:  Author: queries entries from a specific author.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Pubmed",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/purgedomain/",
	"title": "Purge Domain",
	"description": "",
	"content": "Purge Domain Removes unused attribute values and useless attributes, sorts the remaining values.\nInputs\n Data: input dataset  Outputs\n Data: filtered dataset  Definitions of nominal attributes sometimes contain values which don’t appear in the data. Even if this does not happen in the original data, filtering the data, selecting exemplary subsets and alike can remove all examples for which the attribute has some particular value. Such values clutter data presentation, especially various visualizations, and should be removed.\nAfter purging an attribute, it may become single-valued or, in extreme case, have no values at all (if the value of this attribute was undefined for all examples). In such cases, the attribute can be removed.\nA different issue is the order of attribute values: if the data is read from a file in a format in which values are not declared in advance, they are sorted “in order of appearance”. Sometimes we would prefer to have them sorted alphabetically.\n Purge attributes. Purge classes. Purge meta attributes. Information on the filtering process. Produce a report. If Apply automatically is ticked, the widget will output data at each change of widget settings.  Such purification is done by the widget Purge Domain. Ordinary attributes and class attributes are treated separately. For each, we can decide if we want the values sorted or not. Next, we may allow the widget to remove attributes with less than two values or remove the class attribute if there are less than two classes. Finally, we can instruct the widget to check which values of attributes actually appear in the data and remove the unused values. The widget cannot remove values if it is not allowed to remove the attributes, since having attributes without values makes no sense.\nThe new, reduced attributes get the prefix “R”, which distinguishes them from the original ones. The values of new attributes can be computed from the old ones, but not the other way around. This means that if you construct a classifier from the new attributes, you can use it to classify the examples described by the original attributes. But not the opposite: constructing a classifier from the old attributes and using it on examples described by the reduced ones won’t work. Fortunately, the latter is seldom the case. In a typical setup, one would explore the data, visualize it, filter it, purify it… and then test the final model on the original data.\nExample The Purge Domain widget would typically appear after data filtering, for instance when selecting a subset of visualized examples.\nIn the above schema, we play with the adult.tab dataset: we visualize it and select a portion of the data, which contains only four out of the five original classes. To get rid of the empty class, we put the data through Purge Domain before going on to the Box Plot widget. The latter shows only the four classes which are in the Purge Data output. To see the effect of data purification, uncheck Remove unused class variable values and observe the effect this has on Box Plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Purge Domain Removes unused attribute values and useless attributes, sorts the remaining values.\nInputs\n Data: input dataset  Outputs\n Data: filtered dataset  Definitions of nominal attributes sometimes contain values which don’t appear in the data. Even if this does not happen in the original data, filtering the data, selecting exemplary subsets and alike can remove all examples for which the attribute has some particular value. Such values clutter data presentation, especially various visualizations, and should be removed." ,
	"author" : "",
	"summary" : "Purge Domain Removes unused attribute values and useless attributes, sorts the remaining values.\nInputs\n Data: input dataset  Outputs\n Data: filtered dataset  Definitions of nominal attributes sometimes contain values which don’t appear in the data. Even if this does not happen in the original data, filtering the data, selecting exemplary subsets and alike can remove all examples for which the attribute has some particular value. Such values clutter data presentation, especially various visualizations, and should be removed.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Purge Domain",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/pythagoreanforest/",
	"title": "Pythagorean Forest",
	"description": "",
	"content": "Pythagorean Forest Pythagorean forest for visualizing random forests.\nInputs\n Random Forest: tree models from random forest  Outputs\n Tree: selected tree model  Pythagorean Forest shows all learned decision tree models from Random Forest widget. It displays them as Pythagorean trees, each visualization pertaining to one randomly constructed tree. In the visualization, you can select a tree and display it in Pythagorean Tree widget. The best tree is the one with the shortest and most strongly colored branches. This means few attributes split the branches well.\nWidget displays both classification and regression results. Classification requires discrete target variable in the dataset, while regression requires a continuous target variable. Still, they both should be fed a Tree on the input.\n Information on the input random forest model. Display parameters:  Depth: set the depth to which the trees are grown. Target class: set the target class for coloring the trees. If None is selected, the tree will be white. If the input is a classification tree, you can color the nodes by their respective class. If the input is a regression tree, the options are Class mean, which will color tree nodes by the class mean value and Standard deviation, which will color them by the standard deviation value of the node. Size: set the size of the nodes. Normal will keep the nodes the size of the subset in the node. Square root and Logarithmic are the respective transformations of the node size. Zoom: allows you to see the size of the tree visualizations.   Save Image: save the visualization to your computer as a .svg or .png file. Report: produce a report.  Example Pythagorean Forest is great for visualizing several built trees at once. In the example below, we’ve used housing dataset and plotted all 10 trees we’ve grown with Random Forest. When changing the parameters in Random Forest, visualization in Pythagorean Forest will change as well.\nThen we’ve selected a tree in the visualization and inspected it further with Pythagorean Tree widget.\nReferences Beck, F., Burch, M., Munz, T., Di Silvestro, L. and Weiskopf, D. (2014). Generalized Pythagoras Trees for Visualizing Hierarchies. In IVAPP ‘14 Proceedings of the 5th International Conference on Information Visualization Theory and Applications, 17-28.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Pythagorean Forest Pythagorean forest for visualizing random forests.\nInputs\n Random Forest: tree models from random forest  Outputs\n Tree: selected tree model  Pythagorean Forest shows all learned decision tree models from Random Forest widget. It displays them as Pythagorean trees, each visualization pertaining to one randomly constructed tree. In the visualization, you can select a tree and display it in Pythagorean Tree widget. The best tree is the one with the shortest and most strongly colored branches." ,
	"author" : "",
	"summary" : "Pythagorean Forest Pythagorean forest for visualizing random forests.\nInputs\n Random Forest: tree models from random forest  Outputs\n Tree: selected tree model  Pythagorean Forest shows all learned decision tree models from Random Forest widget. It displays them as Pythagorean trees, each visualization pertaining to one randomly constructed tree. In the visualization, you can select a tree and display it in Pythagorean Tree widget. The best tree is the one with the shortest and most strongly colored branches.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Pythagorean Forest",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/pythagoreantree/",
	"title": "Pythagorean Tree",
	"description": "",
	"content": "Pythagorean Tree Pythagorean tree visualization for classification or regression trees.\nInputs\n Tree: tree model Selected Data: instances selected from the tree  Pythagorean Trees are plane fractals that can be used to depict general tree hierarchies as presented in an article by Fabian Beck and co-authors. In our case, they are used for visualizing and exploring tree models, such as Tree.\n Information on the input tree model. Visualization parameters:  Depth: set the depth of displayed trees. Target class (for classification trees): the intensity of the color for nodes of the tree will correspond to the probability of the target class. If None is selected, the color of the node will denote the most probable class. Node color (for regression trees): node colors can correspond to mean or standard deviation of class value of the training data instances in the node. Size: define a method to compute the size of the square representing the node. Normal will keep node sizes correspond to the size of training data subset in the node. Square root and Logarithmic are the respective transformations of the node size. Log scale factor is only enabled when logarithmic transformation is selected. You can set the log factor between 1 and 10.   Plot properties:  Enable tooltips: display node information upon hovering. Show legend: shows color legend for the plot.   Reporting:  Save Image: save the visualization to a SVG or PNG file. Report: add visualization to the report.    Pythagorean Tree can visualize both classification and regression trees. Below is an example for regression tree. The only difference between the two is that regression tree doesn’t enable coloring by class, but can color by class mean or standard deviation.\nExample The workflow from the screenshot below demonstrates the difference between Tree Viewer and Pythagorean Tree. They can both visualize Tree, but Pythagorean visualization takes less space and is more compact, even for a small Iris flower dataset. For both visualization widgets, we have hidden the control area on the left by clicking on the splitter between control and visualization area.\nPythagorean Tree is interactive: click on any of the nodes (squares) to select training data instances that were associated with that node. The following workflow explores these feature.\nThe selected data instances are shown as a subset in the Scatter Plot, sent to the Data Table and examined in the Box Plot. We have used brown-selected dataset in this example. The tree and scatter plot are shown below; the selected node in the tree has a black outline.\nReferences Beck, F., Burch, M., Munz, T., Di Silvestro, L. and Weiskopf, D. (2014). Generalized Pythagoras Trees for Visualizing Hierarchies. In IVAPP ‘14 Proceedings of the 5th International Conference on Information Visualization Theory and Applications, 17-28.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Pythagorean Tree Pythagorean tree visualization for classification or regression trees.\nInputs\n Tree: tree model Selected Data: instances selected from the tree  Pythagorean Trees are plane fractals that can be used to depict general tree hierarchies as presented in an article by Fabian Beck and co-authors. In our case, they are used for visualizing and exploring tree models, such as Tree.\n Information on the input tree model. Visualization parameters:  Depth: set the depth of displayed trees." ,
	"author" : "",
	"summary" : "Pythagorean Tree Pythagorean tree visualization for classification or regression trees.\nInputs\n Tree: tree model Selected Data: instances selected from the tree  Pythagorean Trees are plane fractals that can be used to depict general tree hierarchies as presented in an article by Fabian Beck and co-authors. In our case, they are used for visualizing and exploring tree models, such as Tree.\n Information on the input tree model. Visualization parameters:  Depth: set the depth of displayed trees.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Pythagorean Tree",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/pythonscript/",
	"title": "Python Script",
	"description": "",
	"content": "Python Script Extends functionalities through Python scripting.\nInputs\n Data (Orange.data.Table): input dataset bound to in_data variable Learner (Orange.classification.Learner): input learner bound to in_learner variable Classifier (Orange.classification.Learner): input classifier bound to in_classifier variable Object: input Python object bound to in_object variable  Outputs\n Data (Orange.data.Table): dataset retrieved from out_data variable Learner (Orange.classification.Learner): learner retrieved from out_learner variable Classifier (Orange.classification.Learner): classifier retrieved from out_classifier variable Object: Python object retrieved from out_object variable  Python Script widget can be used to run a python script in the input, when a suitable functionality is not implemented in an existing widget. The script has in_data, in_distance, in_learner, in_classifier and in_object variables (from input signals) in its local namespace. If a signal is not connected or it did not yet receive any data, those variables contain None.\nAfter the script is executed variables from the script’s local namespace are extracted and used as outputs of the widget. The widget can be further connected to other widgets for visualizing the output.\nFor instance the following script would simply pass on all signals it receives:\nout_data = in_data out_distance = in_distance out_learner = in_learner out_classifier = in_classifier out_object = in_object  Note: You should not modify the input objects in place.\n Info box contains names of basic operators for Orange Python script. The Library control can be used to manage multiple scripts. Pressing “+” will add a new entry and open it in the Python script editor. When the script is modified, its entry in the Library will change to indicate it has unsaved changes. Pressing Update will save the script (keyboard shortcut “Ctrl+S”). A script can be removed by selecting it and pressing the “-” button. Pressing Execute in the Run box executes the script (keyboard shortcut “Ctrl+R”). Any script output (from print) is captured and displayed in the Console below the script. The Python script editor on the left can be used to edit a script (it supports some rudimentary syntax highlighting). Console displays the output of the script.  Examples Python Script widget is intended to extend functionalities for advanced users. Classes from Orange library are described in the documentation. To find further information about orange Table class see Table, Domain, and Variable documentation.\nOne can, for example, do batch filtering by attributes. We used zoo.tab for the example and we filtered out all the attributes that have more than 5 discrete values. This in our case removed only ‘leg’ attribute, but imagine an example where one would have many such attributes.\nfrom Orange.data import Domain, Table domain = Domain([attr for attr in in_data.domain.attributes if attr.is_continuous or len(attr.values) \u003c= 5], in_data.domain.class_vars) out_data = Table(domain, in_data)  The second example shows how to round all the values in a few lines of code. This time we used wine.tab and rounded all the values to whole numbers.\nimport numpy as np out_data = in_data.copy() #copy, otherwise input data will be overwritten np.round(out_data.X, 0, out_data.X)  The third example introduces some Gaussian noise to the data. Again we make a copy of the input data, then walk through all the values with a double for loop and add random noise.\nimport random from Orange.data import Domain, Table new_data = in_data.copy() for inst in new_data: for f in inst.domain.attributes: inst[f] += random.gauss(0, 0.02) out_data = new_data  The final example uses Orange3-Text add-on. Python Script is very useful for custom preprocessing in text mining, extracting new features from strings, or utilizing advanced nltk or gensim functions. Below, we simply tokenized our input data from deerwester.tab by splitting them by whitespace.\nprint('Running Preprocessing ...') tokens = [doc.split(' ') for doc in in_data.documents] print('Tokens:', tokens) out_object = in_data out_object.store_tokens(tokens)  You can add a lot of other preprocessing steps to further adjust the output. The output of Python Script can be used with any widget that accepts the type of output your script produces. In this case, connection is green, which signalizes the right type of input for Word Cloud widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Python Script Extends functionalities through Python scripting.\nInputs\n Data (Orange.data.Table): input dataset bound to in_data variable Learner (Orange.classification.Learner): input learner bound to in_learner variable Classifier (Orange.classification.Learner): input classifier bound to in_classifier variable Object: input Python object bound to in_object variable  Outputs\n Data (Orange.data.Table): dataset retrieved from out_data variable Learner (Orange.classification.Learner): learner retrieved from out_learner variable Classifier (Orange.classification.Learner): classifier retrieved from out_classifier variable Object: Python object retrieved from out_object variable  Python Script widget can be used to run a python script in the input, when a suitable functionality is not implemented in an existing widget." ,
	"author" : "",
	"summary" : "Python Script Extends functionalities through Python scripting.\nInputs\n Data (Orange.data.Table): input dataset bound to in_data variable Learner (Orange.classification.Learner): input learner bound to in_learner variable Classifier (Orange.classification.Learner): input classifier bound to in_classifier variable Object: input Python object bound to in_object variable  Outputs\n Data (Orange.data.Table): dataset retrieved from out_data variable Learner (Orange.classification.Learner): learner retrieved from out_learner variable Classifier (Orange.classification.Learner): classifier retrieved from out_classifier variable Object: Python object retrieved from out_object variable  Python Script widget can be used to run a python script in the input, when a suitable functionality is not implemented in an existing widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Python Script",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/radviz/",
	"title": "Radviz",
	"description": "",
	"content": "Radviz Radviz vizualization with explorative data analysis and intelligent data visualization enhancements.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: Radviz vectors  Radviz (Hoffman et al. 1997) is a non-linear multi-dimensional visualization technique that can display data defined by three or more variables in a 2-dimensional projection. The visualized variables are presented as anchor points equally spaced around the perimeter of a unit circle. Data instances are shown as points inside the circle, with their positions determined by a metaphor from physics: each point is held in place with springs that are attached at the other end to the variable anchors. The stiffness of each spring is proportional to the value of the corresponding variable and the point ends up at the position where the spring forces are in equilibrium. Prior to visualization, variable values are scaled to lie between 0 and 1. Data instances that are close to a set of variable anchors have higher values for these variables than for the others.\nThe snapshot shown below shows a Radviz widget with a visualization of the dataset from functional genomics (Brown et al. 2000). In this particular visualization the data instances are colored according to the corresponding class, and the visualization space is colored according to the computed class probability. Notice that the particular visualization very nicely separates data instances of different class, making the visualization interesting and potentially informative.\nJust like all point-based visualizations, this widget includes tools for intelligent data visualization (VizRank, see Leban et al. 2006) and an interface for explorative data analysis - selection of data points in visualization. Just like the Scatter Plot widget, it can be used to find a set of variables that would result in an interesting visualization. The Radviz graph above is according to this definition an example of a very good visualization, while the one below - where we show an VizRank’s interface (Suggest features button) with a list of 3-attribute visualizations and their scores - is not.\nReferences Hoffman, P. E. et al. (1997) DNA visual and analytic data mining. In the Proceedings of the IEEE Visualization. Phoenix, AZ, pp. 437-441.\nBrown, M. P., W. N. Grundy et al. (2000). “Knowledge-based analysis of microarray gene expression data by using support vector machines.” Proc Natl Acad Sci U S A 97(1): 262-7.\nLeban, G., B. Zupan et al. (2006). “VizRank: Data Visualization Guided by Machine Learning.” Data Mining and Knowledge Discovery 13(2): 119-136.\nMramor, M., G. Leban, J. Demsar, and B. Zupan. Visualization-based cancer microarray data classification analysis. Bioinformatics 23(16): 2147-2154, 2007.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Radviz Radviz vizualization with explorative data analysis and intelligent data visualization enhancements.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: Radviz vectors  Radviz (Hoffman et al. 1997) is a non-linear multi-dimensional visualization technique that can display data defined by three or more variables in a 2-dimensional projection." ,
	"author" : "",
	"summary" : "Radviz Radviz vizualization with explorative data analysis and intelligent data visualization enhancements.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected Components: Radviz vectors  Radviz (Hoffman et al. 1997) is a non-linear multi-dimensional visualization technique that can display data defined by three or more variables in a 2-dimensional projection.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Radviz",
	"icon" : ""
},
{
    "uri": "/widget-catalog/educational/random-data/",
	"title": "Random Data",
	"description": "",
	"content": "Random Data Generate random data sample.\nInputs\n None  Outputs\n Data: randomly generated data  Random Data allows creating random data sets, where variables correspond to the selected distributions. The user can specify the number of rows (samples) and the number of variables for each distribution. Distributions from the Scipy’s stats module are used.\n Normal: A normal continuous random variable. Set the number of variables, the mean and the variance. Bernoulli: A Bernoulli discrete random variable. Set the number of variables and the probability mass function. Binomial: A binomial discrete random variable. Set the number of variables, the number of trials and probability of success. Uniform: A uniform continuous random variable. Set the number of variables and the lower and upper bound of the distribution. Discrete uniform: A uniform discrete random variable. Set the number of variables and the number of values per variable. Multinomial: A multinomial random variable. Set the probabilities and the number of trials. The probabilities should sum to one. The number of probabilities corresponds to the final number of variables generated. Add more variables… enables selecting new distributions from the list and with that adding additional variables. Distributions can be removed by pressing an X in the top left corner of each distribution. Define the sample size (i.e. number of rows, default 1000) and press Generate to output the data set.   Hypergeometric: A hypergeometric discrete random variable. Set the number of variables, number of objects, positives and trials. Negative binomial: A negative binomial discrete random variable. Set the number of variables, number of successes and the probability of a success. Poisson: A Poisson discrete random variable. Set the number of variables and the event rate (expected number of occurrences). Exponential: An exponential continuous random variable. Set the number of variables. Gamma: A gamma continuous random variable. Set the number of variables, the shape and scale. The larger the scale parameter, the more spread out the distribution. Student’s t: A Student’s t continuous random variable. Set the number of variables and the degrees of freedom. Bivariate normal: A multivariate normal random variable where the number of variables is fixed to 2. The number of variables is set to two and cannot be changed. Set the mean and variance of each variable and the covariance matrix of the distribution.  Example We normaly wouldn’t create a data set with so many different distributions but rather, for instance, a set of normally distributed variables and perhaps a binary variable, which we will use as the target variable. In this example, we use the default settings, which generate 10 normally distributed variables and a single binomial variable.\nWe observe the generated data in a Data Table and in Distributions.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Random Data Generate random data sample.\nInputs\n None  Outputs\n Data: randomly generated data  Random Data allows creating random data sets, where variables correspond to the selected distributions. The user can specify the number of rows (samples) and the number of variables for each distribution. Distributions from the Scipy\u0026rsquo;s stats module are used.\n Normal: A normal continuous random variable. Set the number of variables, the mean and the variance." ,
	"author" : "",
	"summary" : "Random Data Generate random data sample.\nInputs\n None  Outputs\n Data: randomly generated data  Random Data allows creating random data sets, where variables correspond to the selected distributions. The user can specify the number of rows (samples) and the number of variables for each distribution. Distributions from the Scipy\u0026rsquo;s stats module are used.\n Normal: A normal continuous random variable. Set the number of variables, the mean and the variance.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Random Data",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/randomforest/",
	"title": "Random Forest",
	"description": "",
	"content": "Random Forest Predict using an ensemble of decision trees.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: random forest learning algorithm Model: trained model  Random forest is an ensemble learning method used for classification, regression and other tasks. It was first proposed by Tin Kam Ho and further developed by Leo Breiman (Breiman, 2001) and Adele Cutler.\nRandom Forest builds a set of decision trees. Each tree is developed from a bootstrap sample from the training data. When developing individual trees, an arbitrary subset of attributes is drawn (hence the term “Random”), from which the best attribute for the split is selected. The final model is based on the majority vote from individually developed trees in the forest.\nRandom Forest works for both classification and regression tasks.\n Specify the name of the model. The default name is “Random Forest”. Basic properties:  Number of trees: Specify how many decision trees will be included in the forest. Number of trees considered at each split: Specify how many attributes will be arbitrarily drawn for consideration at each node. If the latter is not specified (option Number of attributes… left unchecked), this number is equal to the square root of the number of attributes in the data. Replicable training: Fix the seed for tree generation, which enables replicability of the results. Balance class distribution: Weigh classes inversely proportional to their frequencies.   Growth control:  Limit depth of individual trees: Original Breiman’s proposal is to grow the trees without any pre-pruning, but since pre-pruning often works quite well and is faster, the user can set the depth to which the trees will be grown. Do not split subsets smaller than: Select the smallest subset that can be split.   Click Apply to communicate the changes to other widgets. Alternatively, tick the box on the left side of the Apply button and changes will be communicated automatically.  Examples For classification tasks, we use iris dataset. Connect it to Predictions. Then, connect File to Random Forest and Tree and connect them further to Predictions. Finally, observe the predictions for the two models.\nFor regressions tasks, we will use housing data. Here, we will compare different models, namely Random Forest, Linear Regression and Constant, in the Test \u0026 Score widget.\nReferences Breiman, L. (2001). Random Forests. In Machine Learning, 45(1), 5-32. Available here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Random Forest Predict using an ensemble of decision trees.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: random forest learning algorithm Model: trained model  Random forest is an ensemble learning method used for classification, regression and other tasks. It was first proposed by Tin Kam Ho and further developed by Leo Breiman (Breiman, 2001) and Adele Cutler.\nRandom Forest builds a set of decision trees. Each tree is developed from a bootstrap sample from the training data." ,
	"author" : "",
	"summary" : "Random Forest Predict using an ensemble of decision trees.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: random forest learning algorithm Model: trained model  Random forest is an ensemble learning method used for classification, regression and other tasks. It was first proposed by Tin Kam Ho and further developed by Leo Breiman (Breiman, 2001) and Adele Cutler.\nRandom Forest builds a set of decision trees. Each tree is developed from a bootstrap sample from the training data.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Random Forest",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/randomize/",
	"title": "Randomize",
	"description": "",
	"content": "Randomize Shuffles classes, attributes and/or metas of an input dataset.\nInputs\n Data: input dataset  Outputs\n Data: randomized dataset  The Randomize widget receives a dataset in the input and outputs the same dataset in which the classes, attributes or/and metas are shuffled.\n Select group of columns of the dataset you want to shuffle. Select proportion of the dataset you want to shuffle. Produce replicable output. If Apply automatically is ticked, changes are committed automatically. Otherwise, you have to press Apply after each change. Produce a report.  Example The Randomize widget is usually placed right after (e.g. File widget. The basic usage is shown in the following workflow, where values of class variable of Iris dataset are randomly shuffled.\nIn the next example we show how shuffling class values influences model performance on the same dataset as above.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Randomize Shuffles classes, attributes and/or metas of an input dataset.\nInputs\n Data: input dataset  Outputs\n Data: randomized dataset  The Randomize widget receives a dataset in the input and outputs the same dataset in which the classes, attributes or/and metas are shuffled.\n Select group of columns of the dataset you want to shuffle. Select proportion of the dataset you want to shuffle. Produce replicable output. If Apply automatically is ticked, changes are committed automatically." ,
	"author" : "",
	"summary" : "Randomize Shuffles classes, attributes and/or metas of an input dataset.\nInputs\n Data: input dataset  Outputs\n Data: randomized dataset  The Randomize widget receives a dataset in the input and outputs the same dataset in which the classes, attributes or/and metas are shuffled.\n Select group of columns of the dataset you want to shuffle. Select proportion of the dataset you want to shuffle. Produce replicable output. If Apply automatically is ticked, changes are committed automatically.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Randomize",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/rank/",
	"title": "Rank",
	"description": "",
	"content": "Rank Ranking of attributes in classification or regression datasets.\nInputs\n Data: input dataset Scorer: models for feature scoring  Outputs\n Reduced Data: dataset with selected attributes Scores: data table with feature scores Features: list of attributes  The Rank widget scores variables according to their correlation with discrete or numeric target variable, based on applicable internal scorers (like information gain, chi-square and linear regression) and any connected external models that supports scoring, such as linear regression, logistic regression, random forest, SGD, etc. The widget can also handle unsupervised data, but only by external scorers, such as PCA.\n Select scoring methods. See the options for classification, regression and unsupervised data in the Scoring methods section. Select attributes to output. None won’t output any attributes, while All will output all of them. With manual selection, select the attributes from the table on the right. Best ranked will output n best ranked attributes. If Send Automatically is ticked, the widget automatically communicates changes to other widgets. Status bar. Produce a report by clicking on the file icon. Observe input and output of the widget. On the right, warnings and errors are shown.  Scoring methods (classification)  Information Gain: the expected amount of information (reduction of entropy) Gain Ratio: a ratio of the information gain and the attribute’s intrinsic information, which reduces the bias towards multivalued features that occurs in information gain Gini: the inequality among values of a frequency distribution ANOVA: the difference between average vaules of the feature in different classes Chi2: dependence between the feature and the class as measure by the chi-square statistic ReliefF: the ability of an attribute to distinguish between classes on similar data instances FCBF (Fast Correlation Based Filter): entropy-based measure, which also identifies redundancy due to pairwise correlations between features  Additionally, you can connect certain learners that enable scoring the features according to how important they are in models that the learners build (e.g. Logistic Regression, Random Forest, SGD). Please note that the data is normalized before ranking.\nScoring methods (regression)  Univariate Regression: linear regression for a single variable RReliefF: relative distance between the predicted (class) values of the two instances.  Additionally, you can connect regression learners (e.g. Linear Regression, Random Forest, SGD). Please note that the data is normalized before ranking.\nScoring method (unsupervised) Currently, only PCA is supported for unsupervised data. Connect PCA to Rank to obtain the scores. The scores correspond to the correlation of a variable with the individual principal component.\nExample: Attribute Ranking and Selection Below, we have used the Rank widget immediately after the File widget to reduce the set of data attributes and include only the most informative ones:\nNotice how the widget outputs a dataset that includes only the best-scored attributes:\nExample: Feature Subset Selection for Machine Learning What follows is a bit more complicated example. In the workflow below, we first split the data into a training set and a test set. In the upper branch, the training data passes through the Rank widget to select the most informative attributes, while in the lower branch there is no feature selection. Both feature selected and original datasets are passed to their own Test \u0026 Score widgets, which develop a Naive Bayes classifier and score it on a test set.\nFor datasets with many features, a naive Bayesian classifier feature selection, as shown above, would often yield a better predictive accuracy.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Rank Ranking of attributes in classification or regression datasets.\nInputs\n Data: input dataset Scorer: models for feature scoring  Outputs\n Reduced Data: dataset with selected attributes Scores: data table with feature scores Features: list of attributes  The Rank widget scores variables according to their correlation with discrete or numeric target variable, based on applicable internal scorers (like information gain, chi-square and linear regression) and any connected external models that supports scoring, such as linear regression, logistic regression, random forest, SGD, etc." ,
	"author" : "",
	"summary" : "Rank Ranking of attributes in classification or regression datasets.\nInputs\n Data: input dataset Scorer: models for feature scoring  Outputs\n Reduced Data: dataset with selected attributes Scores: data table with feature scores Features: list of attributes  The Rank widget scores variables according to their correlation with discrete or numeric target variable, based on applicable internal scorers (like information gain, chi-square and linear regression) and any connected external models that supports scoring, such as linear regression, logistic regression, random forest, SGD, etc.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Rank",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/reshape-map/",
	"title": "Reshape Map",
	"description": "",
	"content": "Reshape Map Builds or modifies the shape of the input dataset to create 2D maps from series data or change the dimensions of existing 2D datasets.\nInputs\n Data: input dataset  Outputs\n Map Data: data as a map  The Reshape Map widget transforms the input data to a map.\n Map shape:  The X dimension. The Y dimension.   Send data automatically or press Send.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Reshape Map Builds or modifies the shape of the input dataset to create 2D maps from series data or change the dimensions of existing 2D datasets.\nInputs\n Data: input dataset  Outputs\n Map Data: data as a map  The Reshape Map widget transforms the input data to a map.\n Map shape:  The X dimension. The Y dimension.   Send data automatically or press Send.  " ,
	"author" : "",
	"summary" : "Reshape Map Builds or modifies the shape of the input dataset to create 2D maps from series data or change the dimensions of existing 2D datasets.\nInputs\n Data: input dataset  Outputs\n Map Data: data as a map  The Reshape Map widget transforms the input data to a map.\n Map shape:  The X dimension. The Y dimension.   Send data automatically or press Send.  ",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Reshape Map",
	"icon" : ""
},
{
    "uri": "/widget-catalog/evaluate/rocanalysis/",
	"title": "ROC Analysis",
	"description": "",
	"content": "ROC Analysis Plots a true positive rate against a false positive rate of a test.\nInputs\n Evaluation Results: results of testing classification algorithms  The widget shows ROC curves for the tested models and the corresponding convex hull. It serves as a mean of comparison between classification models. The curve plots a false positive rate on an x-axis (1-specificity; probability that target=1 when true value=0) against a true positive rate on a y-axis (sensitivity; probability that target=1 when true value=1). The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the classifier. Given the costs of false positives and false negatives, the widget can also determine the optimal classifier and threshold.\n  Choose the desired Target Class. The default class is chosen alphabetically.\n  If test results contain more than one classifier, the user can choose which curves she or he wants to see plotted. Click on a classifier to select or deselect it.\n  When the data comes from multiple iterations of training and testing, such as k-fold cross validation, the results can be (and usually are) averaged. The averaging options are:\n Merge predictions from folds (top left), which treats all the test data as if they came from a single iteration Mean TP rate (top right) averages the curves vertically, showing the corresponding confidence intervals Mean TP and FP at threshold (bottom left) traverses over threshold, averages the positions of curves and shows horizontal and vertical confidence intervals Show individual curves (bottom right) does not average but prints all the curves instead    Option Show convex ROC curves refers to convex curves over each individual classifier (the thin lines positioned over curves). Show ROC convex hull plots a convex hull combining all classifiers (the gray area below the curves). Plotting both types of convex curves makes sense since selecting a threshold in a concave part of the curve cannot yield optimal results, disregarding the cost matrix. Besides, it is possible to reach any point on the convex curve by combining the classifiers represented by the points on the border of the concave region. The diagonal dotted line represents the behavior of a random classifier. The full diagonal line represents iso-performance. A black “A” symbol at the bottom of the graph proportionally readjusts the graph.\n  The final box is dedicated to the analysis of the curve. The user can specify the cost of false positives (FP) and false negatives (FN), and the prior target class probability.\n Default threshold (0.5) point shows the point on the ROC curve achieved by the classifier if it predicts the target class if its probability equals or exceeds 0.5. Show performance line shows iso-performance in the ROC space so that all the points on the line give the same profit/loss. The line further to the upper left is better than the one down and right. The direction of the line depends upon costs and probabilities. This gives a recipe for depicting the optimal threshold for the given costs: this is the point where the tangent with the given inclination touches the curve and it is marked in the plot. If we push the iso-performance higher or more to the left, the points on the iso-performance line cannot be reached by the learner. Going down or to the right, decreases the performance. The widget allows setting the costs from 1 to 1000. Units are not important, as are not the magnitudes. What matters is the relation between the two costs, so setting them to 100 and 200 will give the same result as 400 and 800. Defaults: both costs equal (500), Prior target class probability 50%(from the data). False positive cost: 830, False negative cost 650, Prior target class probability 73%.     Press Save Image if you want to save the created image to your computer in a .svg or .png format.\n  Produce a report.\n  Example At the moment, the only widget which gives the right type of signal needed by the ROC Analysis is Test \u0026 Score. Below, we compare two classifiers, namely Tree and Naive Bayes, in Test\u0026Score and then compare their performance in ROC Analysis, Life Curve and Calibration Plot.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "ROC Analysis Plots a true positive rate against a false positive rate of a test.\nInputs\n Evaluation Results: results of testing classification algorithms  The widget shows ROC curves for the tested models and the corresponding convex hull. It serves as a mean of comparison between classification models. The curve plots a false positive rate on an x-axis (1-specificity; probability that target=1 when true value=0) against a true positive rate on a y-axis (sensitivity; probability that target=1 when true value=1)." ,
	"author" : "",
	"summary" : "ROC Analysis Plots a true positive rate against a false positive rate of a test.\nInputs\n Evaluation Results: results of testing classification algorithms  The widget shows ROC curves for the tested models and the corresponding convex hull. It serves as a mean of comparison between classification models. The curve plots a false positive rate on an x-axis (1-specificity; probability that target=1 when true value=0) against a true positive rate on a y-axis (sensitivity; probability that target=1 when true value=1).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "ROC Analysis",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/save/",
	"title": "Save Data",
	"description": "",
	"content": "Save Data Saves data to a file.\nInputs\n Data: input dataset  The Save Data widget considers a dataset provided in the input channel and saves it to a data file with a specified name. It can save the data as:\n a tab-delimited file (.tab) comma-separated file (.csv) pickle (.pkl), used for storing preprocessing of Corpus objects Excel spreadsheets (.xlsx) spectra ASCII (.dat) hyperspectral map ASCII (.xyz) compressed formats (.tab.gz, .csv.gz, .pkl.gz)  The widget does not save the data every time it receives a new signal in the input as this would constantly (and, mostly, inadvertently) overwrite the file. Instead, the data is saved only after a new file name is set or the user pushes the Save button.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise, it will store an absolute path but disable auto save for security reasons.\n Add type annotations to header: Include Orange’s three-row header in the output file. Autosave when receiving new data: Always save new data. Be careful! This will overwrite existing data on your system. Save by overwriting the existing file. Save as to create a new file.  Example In the workflow below, we used the Zoo dataset. We loaded the data into the Scatter Plot widget, with which we selected a subset of data instances and pushed them to the Save Data widget to store them in a file.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Save Data Saves data to a file.\nInputs\n Data: input dataset  The Save Data widget considers a dataset provided in the input channel and saves it to a data file with a specified name. It can save the data as:\n a tab-delimited file (.tab) comma-separated file (.csv) pickle (.pkl), used for storing preprocessing of Corpus objects Excel spreadsheets (.xlsx) spectra ASCII (.dat) hyperspectral map ASCII (.xyz) compressed formats (." ,
	"author" : "",
	"summary" : "Save Data Saves data to a file.\nInputs\n Data: input dataset  The Save Data widget considers a dataset provided in the input channel and saves it to a data file with a specified name. It can save the data as:\n a tab-delimited file (.tab) comma-separated file (.csv) pickle (.pkl), used for storing preprocessing of Corpus objects Excel spreadsheets (.xlsx) spectra ASCII (.dat) hyperspectral map ASCII (.xyz) compressed formats (.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Save Data",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/savedistancematrix/",
	"title": "Save Distance Matrix",
	"description": "",
	"content": "Save Distance Matrix Saves a distance matrix.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise it will store an absolute path, but disable auto save for security reasons.\nInputs\n Distances: distance matrix   By clicking Save, you choose from previously saved distance matrices. Alternatively, tick the box on the left side of the Save button and changes will be communicated automatically. By clicking Save as, you save the distance matrix to your computer, you only need to enter the name of the file and click Save. The distance matrix will be saved as type .dst.  Example In the snapshot below, we used the Distance Transformation widget to transform the distances in the Iris dataset. We then chose to save the transformed version to our computer, so we could use it later on. We decided to output all data instances. You can choose to output just a minor subset of the data matrix. Pairs are marked automatically. If you wish to know what happened to our changed file, see Distance File.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Save Distance Matrix Saves a distance matrix.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise it will store an absolute path, but disable auto save for security reasons.\nInputs\n Distances: distance matrix   By clicking Save, you choose from previously saved distance matrices. Alternatively, tick the box on the left side of the Save button and changes will be communicated automatically." ,
	"author" : "",
	"summary" : "Save Distance Matrix Saves a distance matrix.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise it will store an absolute path, but disable auto save for security reasons.\nInputs\n Distances: distance matrix   By clicking Save, you choose from previously saved distance matrices. Alternatively, tick the box on the left side of the Save button and changes will be communicated automatically.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Save Distance Matrix",
	"icon" : ""
},
{
    "uri": "/widget-catalog/image-analytics/saveimages/",
	"title": "Save Images",
	"description": "",
	"content": "Save Images Save images in the directory structure.\nInputs\n Data: images to save.  Save Images is a simple widget that saves images sent to its input. Images will be saved as separate files in their own directory. When a class is present in the data, images will be saved in subdirectories based on the class variable.\n Attribute containing the path to the image. If Scale images to is ticked, images will be resized to the size used in the selected embedder:  Inception v3: 299x299 SqueezeNet: 227x227 VGG-16: 224x224 VGG-19: 224x224 Painters: 256x256 DeepLoc: 64x64 openface: 256x256   File format to save images in. See the next section for information on supported formats. If Autosave when receiving new data or settings change is on, images will be saved upon every change. Save will save images, while Save as… enables the user to set the name and the folder where to save the images.  Supported Formats Save Images can save images in the following formats:\n .png .jpeg .gif .tiff .pdf .bmp .eps .ico  Example Here is a simple example how to use Save Images. We loaded 14 paintings from Picasso, sent them to Image Embedding using Painters embedder, then to Distances using cosine distance and finally to Hierarchical Clustering to construct a dendrogram. Then we selected a cluster from the plot and saved the images belonging to the selected cluster with Save Images.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Save Images Save images in the directory structure.\nInputs\n Data: images to save.  Save Images is a simple widget that saves images sent to its input. Images will be saved as separate files in their own directory. When a class is present in the data, images will be saved in subdirectories based on the class variable.\n Attribute containing the path to the image. If Scale images to is ticked, images will be resized to the size used in the selected embedder:  Inception v3: 299x299 SqueezeNet: 227x227 VGG-16: 224x224 VGG-19: 224x224 Painters: 256x256 DeepLoc: 64x64 openface: 256x256   File format to save images in." ,
	"author" : "",
	"summary" : "Save Images Save images in the directory structure.\nInputs\n Data: images to save.  Save Images is a simple widget that saves images sent to its input. Images will be saved as separate files in their own directory. When a class is present in the data, images will be saved in subdirectories based on the class variable.\n Attribute containing the path to the image. If Scale images to is ticked, images will be resized to the size used in the selected embedder:  Inception v3: 299x299 SqueezeNet: 227x227 VGG-16: 224x224 VGG-19: 224x224 Painters: 256x256 DeepLoc: 64x64 openface: 256x256   File format to save images in.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Save Images",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/savemodel/",
	"title": "Save Model",
	"description": "",
	"content": "Save Model Save a trained model to an output file.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise it will store an absolute path, but disable auto save for security reasons.\nInputs\n Model: trained model   Choose from previously saved models. Save the created model with the Browse icon. Click on the icon and enter the name of the file. The model will be saved to a pickled file.  Save the model.  Example When you want to save a custom-set model, feed the data to the model (e.g. Logistic Regression) and connect it to Save Model. Name the model; load it later into workflows with Load Model. Datasets used with Load Model have to contain compatible attributes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Save Model Save a trained model to an output file.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise it will store an absolute path, but disable auto save for security reasons.\nInputs\n Model: trained model   Choose from previously saved models. Save the created model with the Browse icon. Click on the icon and enter the name of the file." ,
	"author" : "",
	"summary" : "Save Model Save a trained model to an output file.\nIf the file is saved to the same directory as the workflow or in the subtree of that directory, the widget remembers the relative path. Otherwise it will store an absolute path, but disable auto save for security reasons.\nInputs\n Model: trained model   Choose from previously saved models. Save the created model with the Browse icon. Click on the icon and enter the name of the file.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Save Model",
	"icon" : ""
},
{
    "uri": "/workflows/Scatter-Plot/",
	"title": "Scatter Plot",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Scatter Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/scatterplot/",
	"title": "Scatter Plot",
	"description": "",
	"content": "Scatter Plot Scatter plot visualization with explorative analysis and intelligent data visualization enhancements.\nInputs\n Data: input dataset Data Subset: subset of instances Features: list of attributes  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Scatter Plot widget provides a 2-dimensional scatter plot visualization for continuous attributes. The data is displayed as a collection of points, each having the value of the x-axis attribute determining the position on the horizontal axis and the value of the y-axis attribute determining the position on the vertical axis. Various properties of the graph, like color, size and shape of the points, axis titles, maximum point size and jittering can be adjusted on the left side of the widget. A snapshot below shows the scatter plot of the Iris dataset with the coloring matching of the class attribute.\n Select the x and y attribute. Optimize your projection by using Rank Projections. This feature scores attribute pairs by average classification accuracy and returns the top scoring pair with a simultaneous visualization update. Set jittering to prevent the dots overlapping. If Jitter continuous values is ticked, continuous instances will be dispersed. Set the color of the displayed points (you will get colors for discrete values and grey-scale points for continuous). Set label, shape and size to differentiate between points. Set symbol size and opacity for all data points. Set the desired colors scale. Adjust plot properties:  Show legend displays a legend on the right. Click and drag the legend to move it. Show gridlines displays the grid behind the plot. Show all data on mouse hover enables information bubbles if the cursor is placed on a dot. Show class density colors the graph by class (see the screenshot below). Show regression line draws the regression line for pair of continuous attributes. Label only selected points allows you to select individual data instances and label them.   Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom. If Send automatically is ticked, changes are communicated automatically. Alternatively, press Send. Save Image saves the created image to your computer in a .svg or .png format. Produce a report.  Here is an example of the Scatter Plot widget if the Show class density and Show regression line boxes are ticked.\nIntelligent Data Visualization If a dataset has many attributes, it is impossible to manually scan through all the pairs to find interesting or useful scatter plots. Orange implements intelligent data visualization with the Find Informative Projections option in the widget.\nIf a categorical variable is selected in the Color section, the score is computed as follows. For each data instance, the method finds 10 nearest neighbors in the projected 2D space, that is, on the combination of attribute pairs. It then checks how many of them have the same color. The total score of the projection is then the average number of same-colored neighbors.\nComputation for continuous colors is similar, except that the coefficient of determination is used for measuring the local homogeneity of the projection.\nTo use this method, go to the Find Informative Projections option in the widget, open the subwindow and press Start Evaluation. The feature will return a list of attribute pairs by average classification accuracy score.\nBelow, there is an example demonstrating the utility of ranking. The first scatter plot projection was set as the default sepal width to sepal length plot (we used the Iris dataset for simplicity). Upon running Find Informative Projections optimization, the scatter plot converted to a much better projection of petal width to petal length plot.\nSelection Selection can be used to manually defined subgroups in the data. Use Shift modifier when selecting data instances to put them into a new group. Shift + Ctrl (or Shift + Cmd on macOs) appends instances to the last group.\nSignal data outputs a data table with an additional column that contains group indices.\nExplorative Data Analysis The Scatter Plot, as the rest of Orange widgets, supports zooming-in and out of part of the plot and a manual selection of data instances. These functions are available in the lower left corner of the widget.\nThe default tool is Select, which selects data instances within the chosen rectangular area. Pan enables you to move the scatter plot around the pane. With Zoom you can zoom in and out of the pane with a mouse scroll, while Reset zoom resets the visualization to its optimal size. An example of a simple schema, where we selected data instances from a rectangular region and sent them to the Data Table widget, is shown below. Notice that the scatter plot doesn’t show all 52 data instances, because some data instances overlap (they have the same values for both attributes used).\nExample The Scatter Plot can be combined with any widget that outputs a list of selected data instances. In the example below, we combine Tree and Scatter Plot to display instances taken from a chosen decision tree node (clicking on any node of the tree will send a set of selected data instances to the scatter plot and mark selected instances with filled symbols).\nReferences Gregor Leban and Blaz Zupan and Gaj Vidmar and Ivan Bratko (2006) VizRank: Data Visualization Guided by Machine Learning. Data Mining and Knowledge Discovery, 13 (2). pp. 119-136. Available here.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Scatter Plot Scatter plot visualization with explorative analysis and intelligent data visualization enhancements.\nInputs\n Data: input dataset Data Subset: subset of instances Features: list of attributes  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Scatter Plot widget provides a 2-dimensional scatter plot visualization for continuous attributes. The data is displayed as a collection of points, each having the value of the x-axis attribute determining the position on the horizontal axis and the value of the y-axis attribute determining the position on the vertical axis." ,
	"author" : "",
	"summary" : "Scatter Plot Scatter plot visualization with explorative analysis and intelligent data visualization enhancements.\nInputs\n Data: input dataset Data Subset: subset of instances Features: list of attributes  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Scatter Plot widget provides a 2-dimensional scatter plot visualization for continuous attributes. The data is displayed as a collection of points, each having the value of the x-axis attribute determining the position on the horizontal axis and the value of the y-axis attribute determining the position on the vertical axis.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Scatter Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/score_cells/",
	"title": "Score Cells",
	"description": "",
	"content": "Score Cells Add a cell score based on the given set of genes.\nInputs\n Data: input dataset Genes: dataset with gene names  Outputs\n Data: dataset with a meta column reporting on cell scores  Score Cells scores the cells (rows) in the input data based on expression of input marker genes. Input data has to include gene names in column headers (as attribute names). The score is computed independently for each cell and is equal to the maximum expression of the marker genes. If expressions for all the marker genes are missing, the cell obtains a score of zero. If Genes input is provided, one of the columns must include names of genes that match those from expression data.\n A column name from table on genes that provides names of genes that are included in the input gene expression table. Tick to automatically process input data and send the result of scoring to the output. If left unchecked, processing must be triggered manually.  Example We will use Single Cell Datasets widget to load Bone marrow mononuclear cells with AML (sample) data. Then we will pass it through k-Means and select 2 clusters from Silhouette Scores. Ok, it looks like there might be two distinct clusters here.\nBut can we find subpopulations in these cells? Let us load Bone marrow mononuclear cells with AML (markers) with Single Cell Datasets. Now, pass the marker genes to Data Table and select, for example, natural killer cells from the list (NKG7).\nPass the markers and k-Means results to Score Cells widget and select geneName to match markers with genes. Finally, add t-SNE to visualize the results.\nIn t-SNE, use Scores attribute to color the points and set their size. We see that killer cells are nicely clustered together and that t-SNE indeed found subpopulations.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Score Cells Add a cell score based on the given set of genes.\nInputs\n Data: input dataset Genes: dataset with gene names  Outputs\n Data: dataset with a meta column reporting on cell scores  Score Cells scores the cells (rows) in the input data based on expression of input marker genes. Input data has to include gene names in column headers (as attribute names). The score is computed independently for each cell and is equal to the maximum expression of the marker genes." ,
	"author" : "",
	"summary" : "Score Cells Add a cell score based on the given set of genes.\nInputs\n Data: input dataset Genes: dataset with gene names  Outputs\n Data: dataset with a meta column reporting on cell scores  Score Cells scores the cells (rows) in the input data based on expression of input marker genes. Input data has to include gene names in column headers (as attribute names). The score is computed independently for each cell and is equal to the maximum expression of the marker genes.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Score Cells",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/score_genes/",
	"title": "Score Genes",
	"description": "",
	"content": "Score Genes Gene scoring based on statistics of their expression profiles or information content about cell types.\nInputs\n Data: input dataset Scorer: (multiple) prediction model(s)  Outputs\n Reduced Data: expression data containing selected genes Scores: gene scores  The Score Genes widget considers gene expression data with genes in columns and single cell expression profiles in rows. It scores the genes based on statistics of gene expression. For examples, it can sort the genes by their mean expression. The widget supports selection of best scored genes and performs gene filtering.\n Scoring method. Those selected will be shown in the scoring table. Table with gene scores. Click on the column header to sort by specific score. Gene scoring method. If set to Manual than gene selection includes those whose rows in gene scored table are selected (click on rows and use modifer keys). Best ranked will include top-ranked genes under selected gene sorting. If Send Automatically is ticked, the widget automatically outputs the data according to chosen selection method. This option should be used to automatically update the output of the widget upon any change of the input data set.  Scoring methods  Mean: arithmetic mean gene expression, computed across all the cells in the input data. Variance: squared deviation from gene expression mean Dispersion: the deviation of gene expression variance from the mean, see also negative binomial distribution Coefficient of variation: relative standard deviation of a gene expression. Measures relative variability of gene expression, and is the ratio of the standard deviation to the mean.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Score Genes Gene scoring based on statistics of their expression profiles or information content about cell types.\nInputs\n Data: input dataset Scorer: (multiple) prediction model(s)  Outputs\n Reduced Data: expression data containing selected genes Scores: gene scores  The Score Genes widget considers gene expression data with genes in columns and single cell expression profiles in rows. It scores the genes based on statistics of gene expression. For examples, it can sort the genes by their mean expression." ,
	"author" : "",
	"summary" : "Score Genes Gene scoring based on statistics of their expression profiles or information content about cell types.\nInputs\n Data: input dataset Scorer: (multiple) prediction model(s)  Outputs\n Reduced Data: expression data containing selected genes Scores: gene scores  The Score Genes widget considers gene expression data with genes in columns and single cell expression profiles in rows. It scores the genes based on statistics of gene expression. For examples, it can sort the genes by their mean expression.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Score Genes",
	"icon" : ""
},
{
    "uri": "/screenshots/",
	"title": "Screenshots",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "screenshots",
	"LinkTitle" : "Screenshots",
	"icon" : ""
},
{
    "uri": "/search/",
	"title": "Search",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "search",
	"LinkTitle" : "Search",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/seasonal_adjustment/",
	"title": "Seasonal Adjustment",
	"description": "",
	"content": "Seasonal Adjustment Decompose the time series into seasonal, trend, and residual components.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Original time series with some additional columns: seasonal component, trend component, residual component, and seasonally adjusted time series.   Length of the season in periods (e.g. 12 for monthly data). Time series decomposition model, additive or multiplicative. The series to seasonally adjust.  Example See also Moving Transform\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Seasonal Adjustment Decompose the time series into seasonal, trend, and residual components.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Original time series with some additional columns: seasonal component, trend component, residual component, and seasonally adjusted time series.   Length of the season in periods (e.g. 12 for monthly data). Time series decomposition model, additive or multiplicative. The series to seasonally adjust." ,
	"author" : "",
	"summary" : "Seasonal Adjustment Decompose the time series into seasonal, trend, and residual components.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series: Original time series with some additional columns: seasonal component, trend component, residual component, and seasonally adjusted time series.   Length of the season in periods (e.g. 12 for monthly data). Time series decomposition model, additive or multiplicative. The series to seasonally adjust.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Seasonal Adjustment",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/select-by-data-index/",
	"title": "Select by Data Index",
	"description": "",
	"content": "Select by Data Index Match instances by index from data subset.\nInputs\n Data: reference data set Data Subset: subset to match  Outputs\n Matching data: subset from reference data set that matches indices from subset data Unmatched data: subset from reference data set that does not match indices from subset data Annotated data: reference data set with an additional column defining matches  Select by Data Index enables matching the data by indices. Each row in a data set has an index and given a subset, this widget can match these indices to indices from the reference data. Most often it is used to retrieve the original data from the transformed data (say, from PCA space).\n Information on the reference data set. This data is used as index reference. Information on the data subset. The indices of this data set are used to find matching data in the reference data set. Matching data are on the output by default.  Example A typical use of Select by Data Index is to retrieve the original data after a transformation. We will load iris.tab data in the File widget. Then we will transform this data with PCA. We can project the transformed data in a Scatter Plot, where we can only see PCA components and not the original features.\nNow we will select an interesting subset (we could also select the entire data set). If we observe it in a Data Table, we can see that the data is transformed. If we would like to see this data with the original features, we will have to retrieve them with Select by Data Index.\nConnect the original data and the subset from Scatter Plot to Select by Data Index. The widget will match the indices of the subset with the indices of the reference (original) data and output the matching reference data. A final inspection in another Data Table confirms the data on the output is from the original data space.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Select by Data Index Match instances by index from data subset.\nInputs\n Data: reference data set Data Subset: subset to match  Outputs\n Matching data: subset from reference data set that matches indices from subset data Unmatched data: subset from reference data set that does not match indices from subset data Annotated data: reference data set with an additional column defining matches  Select by Data Index enables matching the data by indices." ,
	"author" : "",
	"summary" : "Select by Data Index Match instances by index from data subset.\nInputs\n Data: reference data set Data Subset: subset to match  Outputs\n Matching data: subset from reference data set that matches indices from subset data Unmatched data: subset from reference data set that does not match indices from subset data Annotated data: reference data set with an additional column defining matches  Select by Data Index enables matching the data by indices.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Select by Data Index",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/selectcolumns/",
	"title": "Select Columns",
	"description": "",
	"content": "Select Columns Manual selection of data attributes and composition of data domain.\nInputs\n Data: input dataset  Outputs\n Data: dataset with columns as set in the widget  The Select Columns widget is used to manually compose your data domain. The user can decide which attributes will be used and how. Orange distinguishes between ordinary attributes, (optional) class attributes and meta attributes. For instance, for building a classification model, the domain would be composed of a set of attributes and a discrete class attribute. Meta attributes are not used in modeling, but several widgets can use them as instance labels.\nOrange attributes have a type and are either discrete, continuous or a character string. The attribute type is marked with a symbol appearing before the name of the attribute (D, C, S, respectively).\n Left-out data attributes that will not be in the output data file Data attributes in the new data file Target variable. If none, the new dataset will be without a target variable. Meta attributes of the new data file. These attributes are included in the dataset but are, for most methods, not considered in the analysis. Produce a report. Reset the domain composition to that of the input data file. Tick if you wish to auto-apply changes of the data domain. Apply changes of the data domain and send the new data file to the output channel of the widget.  Examples In the workflow below, the Iris data from the File widget is fed into the Select Columns widget, where we select to output only two attributes (namely petal width and petal length). We view both the original dataset and the dataset with selected columns in the Data Table widget.\nFor a more complex use of the widget, we composed a workflow to redefine the classification problem in the heart-disease dataset. Originally, the task was to predict if the patient has a coronary artery diameter narrowing. We changed the problem to that of gender classification, based on age, chest pain and cholesterol level, and informatively kept the diameter narrowing as a meta attribute.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Select Columns Manual selection of data attributes and composition of data domain.\nInputs\n Data: input dataset  Outputs\n Data: dataset with columns as set in the widget  The Select Columns widget is used to manually compose your data domain. The user can decide which attributes will be used and how. Orange distinguishes between ordinary attributes, (optional) class attributes and meta attributes. For instance, for building a classification model, the domain would be composed of a set of attributes and a discrete class attribute." ,
	"author" : "",
	"summary" : "Select Columns Manual selection of data attributes and composition of data domain.\nInputs\n Data: input dataset  Outputs\n Data: dataset with columns as set in the widget  The Select Columns widget is used to manually compose your data domain. The user can decide which attributes will be used and how. Orange distinguishes between ordinary attributes, (optional) class attributes and meta attributes. For instance, for building a classification model, the domain would be composed of a set of attributes and a discrete class attribute.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Select Columns",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/selectrows/",
	"title": "Select Rows",
	"description": "",
	"content": "Select Rows Selects data instances based on conditions over data features.\nInputs\n Data: input dataset  Outputs\n Matching Data: instances that match the conditions Non-Matching Data: instances that do not match the conditions Data: data with an additional column showing whether a instance is selected  This widget selects a subset from an input dataset, based on user-defined conditions. Instances that match the selection rule are placed in the output Matching Data channel.\nCriteria for data selection are presented as a collection of conjunct terms (i.e. selected items are those matching all the terms in ‘Conditions').\nCondition terms are defined through selecting an attribute, selecting an operator from a list of operators, and, if needed, defining the value to be used in the condition term. Operators are different for discrete, continuous and string attributes.\n Conditions you want to apply, their operators and related values Add a new condition to the list of conditions. Add all the possible variables at once. Remove all the listed variables at once. Information on the input dataset and information on instances that match the condition(s) Purge the output data. When the Send automatically box is ticked, all changes will be automatically communicated to other widgets. Produce a report.  Any change in the composition of the condition will update the information pane (Data Out).\nIf Send automatically is selected, then the output is updated on any change in the composition of the condition or any of its terms.\nExample In the workflow below, we used the Zoo data from the File widget and fed it into the Select Rows widget. In the widget, we chose to output only two animal types, namely fish and reptiles. We can inspect both the original dataset and the dataset with selected rows in the Data Table widget.\nIn the next example, we used the data from the Titanic dataset and similarly fed it into the Box Plot widget. We first observed the entire dataset based on survival. Then we selected only first class passengers in the Select Rows widget and fed it again into the Box Plot. There we could see all the first class passengers listed by their survival rate and grouped by gender.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Select Rows Selects data instances based on conditions over data features.\nInputs\n Data: input dataset  Outputs\n Matching Data: instances that match the conditions Non-Matching Data: instances that do not match the conditions Data: data with an additional column showing whether a instance is selected  This widget selects a subset from an input dataset, based on user-defined conditions. Instances that match the selection rule are placed in the output Matching Data channel." ,
	"author" : "",
	"summary" : "Select Rows Selects data instances based on conditions over data features.\nInputs\n Data: input dataset  Outputs\n Matching Data: instances that match the conditions Non-Matching Data: instances that do not match the conditions Data: data with an additional column showing whether a instance is selected  This widget selects a subset from an input dataset, based on user-defined conditions. Instances that match the selection rule are placed in the output Matching Data channel.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Select Rows",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/selforganizingmap/",
	"title": "Self-Organizing Map",
	"description": "",
	"content": "Self-Organizing Map Computation of a self-organizing map.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  A self-organizing map (SOM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a two-dimensional, discretized representation of the data. It is a method to do dimensionality reduction. Self-organizing maps use a neighborhood function to preserve the topological properties of the input space.\nThe points in the grid represent data instances. By default, the size of the point corresponds to the number of instances represented by the point. The points are colored by majority class (if available), while the intensity of interior color shows the proportion of majority class. To see the class distribution, select Show pie charts option.\nJust like other visualization widgets, Self-Organizing Maps also supports interactive selection of groups. Use Shift key to select a new group and Ctr+Shift to add to the existing group.\n SOM properties:  Set the grid type. Options are hexagonal or square grid. If Set dimensions automatically is checked, the size of the plot will be set automatically. Alternatively, set the size manually. Set the initialization type for the SOM projection. Options are PCA initialization, random initialization and replicable random (random_seed = 0). Once the parameters are set, press Start to re-run the optimization.   Set the color of the instances in the plot. The widget colors by class by default (if available).  Show pie charts turns points into pie-charts that show the distributions of the values used for coloring. Size by number of instances scales the points according to the number of instances represented by the point.    Example Self-organizing maps are low-dimensional projections of the input data. We will use the brown-selected data and display the data instance in a 2-D projection. Seems like the three gene types are well-separated. We can select a subset from the grid and display it in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Self-Organizing Map Computation of a self-organizing map.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  A self-organizing map (SOM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a two-dimensional, discretized representation of the data. It is a method to do dimensionality reduction. Self-organizing maps use a neighborhood function to preserve the topological properties of the input space." ,
	"author" : "",
	"summary" : "Self-Organizing Map Computation of a self-organizing map.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  A self-organizing map (SOM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a two-dimensional, discretized representation of the data. It is a method to do dimensionality reduction. Self-organizing maps use a neighborhood function to preserve the topological properties of the input space.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Self-Organizing Map",
	"icon" : ""
},
{
    "uri": "/workflows/Sentiment-Analysis/",
	"title": "Sentiment Analysis",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Sentiment Analysis",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/sentimentanalysis/",
	"title": "Sentiment Analysis",
	"description": "",
	"content": "Sentiment Analysis Predict sentiment from text.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: A corpus with information on the sentiment of each document.  Sentiment Analysis predicts sentiment for each document in a corpus. It uses Liu \u0026 Hu and Vader sentiment modules from NLTK and multilingual sentiment lexicons from the Data Science Lab. All of them are lexicon-based. For Liu \u0026 Hu, you can choose English or Slovenian version. Vader works only on English. Multilingual sentiment supports several languages, which are listed at the bottom of this page.\n Liu Hu: lexicon-based sentiment analysis (supports English and Slovenian). The final score is the difference between the sum of positive and sum of negative words, normalized by the length of the document and multiplied by a 100. The final score reflects the percentage of sentiment difference in the document. Vader: lexicon- and rule-based sentiment analysis Multilingual sentiment: lexicon-based sentiment analysis for several languages Custom dictionary: add you own positive and negative sentiment dictionaries. Accepted source type is .txt file with each word in its own line. The final score is computed in the same way as Liu Hu. If Auto commit is on, sentiment-tagged corpus is communicated automatically. Alternatively press Commit.  Example Sentiment Analysis can be used for constructing additional features with sentiment prediction from corpus. First, we load Election-2016-tweets.tab in Corpus. Then we connect Corpus to Sentiment Analysis. The widget will append 4 new features for Vader method: positive score, negative score, neutral score and compound (combined score).\nWe can observe new features in a Data Table, where we sorted the compound by score. Compound represents the total sentiment of a tweet, where -1 is the most negative and 1 the most positive.\nNow let us visualize the data. We have some features we are currently not interested in, so we will remove them with Select Columns.\nThen we will make our corpus a little smaller, so it will be easier to visualize. Pass the data to Data Sampler and retain a random 10% of the tweets.\nNow pass the filtered corpus to Heat Map. Use Merge by k-means to merge tweets with the same polarity into one line. Then use Cluster by rows to create a clustered visualization where similar tweets are grouped together. Click on a cluster to select a group of tweets - we selected the negative cluster.\nTo observe the selected subset, pass the tweets to Corpus Viewer.\nReferences Hutto, C.J. and E. E. Gilbert (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\nHu, Minqing and Bing Liu (2004). Mining opinion features in customer reviews. In Proceedings of AAAI Conference on Artificial Intelligence, vol. 4, pp. 755–760. Available online.\nKadunc, Klemen and Marko Robnik-Šikonja (2016). Analiza mnenj s pomočjo strojnega učenja in slovenskega leksikona sentimenta. Conference on Language Technologies \u0026 Digital Humanities, Ljubljana (in Slovene). Available online.\nMultilingual Sentiment Languages  Afrikaans Arabic Azerbaijani Belarusian Bosnian Bulgarian Catalan Chinese Chinese Characters Croatian Czech Danish Dutch English Estonian Farsi Finnish French Gaelic German Greek Hebrew Hindi Hungarian Indonesian Italian Japanese Korean Latin Latvian Lithuanian Macedonian Norwegian Norwegian Nynorsk Polish Portuguese Romanian Russian Serbian Slovak Slovene Spanish Swedish Turkish Ukrainian  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Sentiment Analysis Predict sentiment from text.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: A corpus with information on the sentiment of each document.  Sentiment Analysis predicts sentiment for each document in a corpus. It uses Liu \u0026amp; Hu and Vader sentiment modules from NLTK and multilingual sentiment lexicons from the Data Science Lab. All of them are lexicon-based. For Liu \u0026amp; Hu, you can choose English or Slovenian version." ,
	"author" : "",
	"summary" : "Sentiment Analysis Predict sentiment from text.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: A corpus with information on the sentiment of each document.  Sentiment Analysis predicts sentiment for each document in a corpus. It uses Liu \u0026amp; Hu and Vader sentiment modules from NLTK and multilingual sentiment lexicons from the Data Science Lab. All of them are lexicon-based. For Liu \u0026amp; Hu, you can choose English or Slovenian version.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Sentiment Analysis",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/sievediagram/",
	"title": "Sieve Diagram",
	"description": "",
	"content": "Sieve Diagram Plots a sieve diagram for a pair of attributes.\nInputs\n Data: input dataset  A Sieve Diagram is a graphical method for visualizing frequencies in a two-way contingency table and comparing them to expected frequencies under assumption of independence. It was proposed by Riedwyl and Schüpbach in a technical report in 1983 and later called a parquet diagram (Riedwyl and Schüpbach 1994). In this display, the area of each rectangle is proportional to the expected frequency, while the observed frequency is shown by the number of squares in each rectangle. The difference between observed and expected frequency (proportional to the standard Pearson residual) appears as the density of shading, using color to indicate whether the deviation from independence is positive (blue) or negative (red).\n Select the attributes you want to display in the sieve plot. Score combinations enables you to fin the best possible combination of attributes. Save Image saves the created image to your computer in a .svg or .png format. Produce a report.  The snapshot below shows a sieve diagram for the Titanic dataset and has the attributes sex and survived (the latter is a class attribute in this dataset). The plot shows that the two variables are highly associated, as there are substantial differences between observed and expected frequencies in all of the four quadrants. For example, and as highlighted in the balloon, the chance for surviving the accident was much higher for female passengers than expected (0.06 vs. 0.15).\nPairs of attributes with interesting associations have a strong shading, such as the diagram shown in the above snapshot. For contrast, a sieve diagram of the least interesting pair (age vs. survival) is shown below.\nExample Below, we see a simple schema using the Titanic dataset, where we use the Rank widget to select the best attributes (the ones with the highest information gain, gain ratio or Gini index) and feed them into the Sieve Diagram. This displays the sieve plot for the two best attributes, which in our case are sex and status. We see that the survival rate on the Titanic was very high for women of the first class and very low for female crew members.\nThe Sieve Diagram also features the Score Combinations option, which makes the ranking of attributes even easier.\nReferences Riedwyl, H., and Schüpbach, M. (1994). Parquet diagram to plot contingency tables. In Softstat ‘93: Advances in Statistical Software, F. Faulbaum (Ed.). New York: Gustav Fischer, 293-299.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Sieve Diagram Plots a sieve diagram for a pair of attributes.\nInputs\n Data: input dataset  A Sieve Diagram is a graphical method for visualizing frequencies in a two-way contingency table and comparing them to expected frequencies under assumption of independence. It was proposed by Riedwyl and Schüpbach in a technical report in 1983 and later called a parquet diagram (Riedwyl and Schüpbach 1994). In this display, the area of each rectangle is proportional to the expected frequency, while the observed frequency is shown by the number of squares in each rectangle." ,
	"author" : "",
	"summary" : "Sieve Diagram Plots a sieve diagram for a pair of attributes.\nInputs\n Data: input dataset  A Sieve Diagram is a graphical method for visualizing frequencies in a two-way contingency table and comparing them to expected frequencies under assumption of independence. It was proposed by Riedwyl and Schüpbach in a technical report in 1983 and later called a parquet diagram (Riedwyl and Schüpbach 1994). In this display, the area of each rectangle is proportional to the expected frequency, while the observed frequency is shown by the number of squares in each rectangle.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Sieve Diagram",
	"icon" : ""
},
{
    "uri": "/workflows/Silhouette/",
	"title": "Silhouette",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Silhouette",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/silhouetteplot/",
	"title": "Silhouette Plot",
	"description": "",
	"content": "Silhouette Plot A graphical representation of consistency within clusters of data.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Silhouette Plot widget offers a graphical representation of consistency within clusters of data and provides the user with the means to visually assess cluster quality. The silhouette score is a measure of how similar an object is to its own cluster in comparison to other clusters and is crucial in the creation of a silhouette plot. The silhouette score close to 1 indicates that the data instance is close to the center of the cluster and instances possessing the silhouette scores close to 0 are on the border between two clusters.\n Choose the distance metric. You can choose between:  Euclidean (“straight line” distance between two points) Manhattan (the sum of absolute differences for all attributes) Cosine (1 - cosine of the angle between two vectors)   Select the cluster label. You can decide whether to group the instances by cluster or not. Display options:  Choose bar width. Annotations: annotate the silhouette plot.   Save Image saves the created silhouette plot to your computer in a .png or .svg format. Produce a report. Output:  Add silhouette scores (good clusters have higher silhouette scores) By clicking Commit, changes are communicated to the output of the widget. Alternatively, tick the box on the left and changes will be communicated automatically.   The created silhouette plot.  Example In the snapshot below, we have decided to use the Silhouette Plot on the iris dataset. We selected data instances with low silhouette scores and passed them on as a subset to the Scatter Plot widget. This visualization only confirms the accuracy of the Silhouette Plot widget, as you can clearly see that the subset lies in the border between two clusters.\nIf you are interested in other uses of the Silhouette Plot widget, feel free to explore our blog post.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Silhouette Plot A graphical representation of consistency within clusters of data.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Silhouette Plot widget offers a graphical representation of consistency within clusters of data and provides the user with the means to visually assess cluster quality. The silhouette score is a measure of how similar an object is to its own cluster in comparison to other clusters and is crucial in the creation of a silhouette plot." ,
	"author" : "",
	"summary" : "Silhouette Plot A graphical representation of consistency within clusters of data.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Silhouette Plot widget offers a graphical representation of consistency within clusters of data and provides the user with the means to visually assess cluster quality. The silhouette score is a measure of how similar an object is to its own cluster in comparison to other clusters and is crucial in the creation of a silhouette plot.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Silhouette Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/similarityhashing/",
	"title": "Similarity Hashing",
	"description": "",
	"content": "Similarity Hashing Computes documents hashes.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with simhash value as attributes.  Similarity Hashing is a widget that transforms documents into similarity vectors. The widget uses SimHash method from from Moses Charikar.\n Set Simhash size (how many attributes will be on the output, corresponds to bits of information) and shingle length (how many tokens are used in a shingle). Commit Automatically output the data automatically. Alternatively, press Commit.  Example We will use deerwester.tab to find similar documents in this small corpus. Load the data with Corpus and pass it to Similarity Hashing. We will keep the default hash size and shingle length. We can observe what the widget outputs in a Data Table. There are 64 new attributes available, corresponding to the Simhash size parameter.\nReferences Charikar, M. (2002) Similarity estimation techniques from rounding algorithms. STOC ‘02 Proceedings of the thirty-fourth annual ACM symposium on Theory of computing, p. 380-388.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Similarity Hashing Computes documents hashes.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with simhash value as attributes.  Similarity Hashing is a widget that transforms documents into similarity vectors. The widget uses SimHash method from from Moses Charikar.\n Set Simhash size (how many attributes will be on the output, corresponds to bits of information) and shingle length (how many tokens are used in a shingle)." ,
	"author" : "",
	"summary" : "Similarity Hashing Computes documents hashes.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with simhash value as attributes.  Similarity Hashing is a widget that transforms documents into similarity vectors. The widget uses SimHash method from from Moses Charikar.\n Set Simhash size (how many attributes will be on the output, corresponds to bits of information) and shingle length (how many tokens are used in a shingle).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Similarity Hashing",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/single_cell_datasets/",
	"title": "Single Cell Datasets",
	"description": "",
	"content": "Single Cell Datasets Load a single cell data from an online repository.\nOutputs\n Data: A single cell dataset containing cells and their gene expression or gene markers.  Single Cell Datasets retrieves a selected data set from the data base server and sends the data to the output. Gene expression data sets include cells in rows and genes in columns. The data set file is downloaded to the local memory, and for subsequent requests instantly available even without the internet connection.\n Information on the number of data sets available and the number of them downloaded to the local memory. List of available data sets with information on the number of cells (instances) and genes (variables). Textual description of the selected data set and its source. If Send Data Automatically is ticked, selected data set automatically loaded and pushed to the output of the widget. Notice that some data sets are big and downloading them may take time. Alternatively, uncheck the Send Data Automatically, browse through the data set list and press Send Data upon finding a suitable one for analysis.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Single Cell Datasets Load a single cell data from an online repository.\nOutputs\n Data: A single cell dataset containing cells and their gene expression or gene markers.  Single Cell Datasets retrieves a selected data set from the data base server and sends the data to the output. Gene expression data sets include cells in rows and genes in columns. The data set file is downloaded to the local memory, and for subsequent requests instantly available even without the internet connection." ,
	"author" : "",
	"summary" : "Single Cell Datasets Load a single cell data from an online repository.\nOutputs\n Data: A single cell dataset containing cells and their gene expression or gene markers.  Single Cell Datasets retrieves a selected data set from the data base server and sends the data to the output. Gene expression data sets include cells in rows and genes in columns. The data set file is downloaded to the local memory, and for subsequent requests instantly available even without the internet connection.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Single Cell Datasets",
	"icon" : ""
},
{
    "uri": "/widget-catalog/single-cell/single_cell_preprocess/",
	"title": "Single Cell Preprocess",
	"description": "",
	"content": "Single Cell Preprocess Preprocess Single Cell data set.\nInputs\n Data: Single cell dataset.  Outputs\n Preprocessed Data: Preprocessed dataset.  ",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Single Cell Preprocess Preprocess Single Cell data set.\nInputs\n Data: Single cell dataset.  Outputs\n Preprocessed Data: Preprocessed dataset.  " ,
	"author" : "",
	"summary" : "Single Cell Preprocess Preprocess Single Cell data set.\nInputs\n Data: Single cell dataset.  Outputs\n Preprocessed Data: Preprocessed dataset.  ",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Single Cell Preprocess",
	"icon" : ""
},
{
    "uri": "/widget-catalog/networks/singlemode/",
	"title": "Single Mode",
	"description": "",
	"content": "Single Mode Convert multimodal graphs to single modal.\nInputs\n Network: An instance of a bipartite network graph.  Outputs\n Network: An instance of single network graph.  Single Mode works with bipartite (or multipartite) networks, where different parts are distinguished by values of the chosen discrete variable. A typical example would be a network that connects persons with events that they attended. The widget creates a new network, which contains the nodes from the chosen group of original network’s nodes (e.g. persons). Two nodes in the resulting network are connected if they share a common neighbor from the second chosen group (e.g. events).\n Mode indicator:  Feature: discrete feature labeling network subsets. Connect: value used as nodes. by: value used as edges.   Edge weights: compute weights for the output network. Information on the output network.  ####Edge weights\n No weights: all weights are set to 1. Number of connections: weights correspond to the number of common connections (e.g. events the two people participated in). Weighted number of connections: weights correspond to the sum of the product of original edge weights that connect each person with the event.  For details and for description of other options, see Vlado Batagelj’s Introduction to Network Science using Pajek, section 7, page 38.\nExample For this example we have used the famous davis data set that describes ladies from the south of the United States and the events they have participated in. The network thus consists of nodes that are either persons or events. Node role is described in the attribute role. We load the network with Network File.\nWe see the original file in Network Explorer. The blue nodes are events and the red ones are persons. Events are attended by persons. A node connects a person with the event, if the person attended the event. Now we can observe which people attended the same event or, conversely, which events were attended by the same people.\nIn Single Mode we set the feature describing the role of the nodes. It so happens that our attribute is named role. We connect persons (nodes) by the events they attended (edges). Edge weight will be the number of connections in common. In translation, edge weights will be the number of events both people attended.\nNetwork Explorer (1) shows the final network.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Single Mode Convert multimodal graphs to single modal.\nInputs\n Network: An instance of a bipartite network graph.  Outputs\n Network: An instance of single network graph.  Single Mode works with bipartite (or multipartite) networks, where different parts are distinguished by values of the chosen discrete variable. A typical example would be a network that connects persons with events that they attended. The widget creates a new network, which contains the nodes from the chosen group of original network\u0026rsquo;s nodes (e." ,
	"author" : "",
	"summary" : "Single Mode Convert multimodal graphs to single modal.\nInputs\n Network: An instance of a bipartite network graph.  Outputs\n Network: An instance of single network graph.  Single Mode works with bipartite (or multipartite) networks, where different parts are distinguished by values of the chosen discrete variable. A typical example would be a network that connects persons with events that they attended. The widget creates a new network, which contains the nodes from the chosen group of original network\u0026rsquo;s nodes (e.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Single Mode",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/spectra/",
	"title": "Spectra",
	"description": "",
	"content": "Spectra Visually explore series of spectra with no spatial information.\nInputs\n Data: input dataset Data Subset:subset of the data  Outputs\n Selection: selected spectra  The Spectra widget allows visual exploration of multiple spectra. To output some spectra, select them by clicking. For multiple selection, hold the modifier key (Ctrl or Cmd) or use line selection (see the plot options menu). Selected spectra will appear dashed.\n Open the plot options menu A spectrum The X and Y position of the cursor The legend (appears only is spectra are colored)  Navigation\n Click + drag: move the plot Right-click: zoom to fit Right-click + drag: zoom with mouse movement Scroll: zoom X axis Scroll + modifier: zoom Y axis  \nPlot options\n Resample curves (R): resample the displayed a subset (only a subset is displayed for performance) Resampling reset (Mod + R): resample to the default view Zoom in (Z): zoom to a region (selected afterwards) Zoom to fit (Backspace): return to the original plot Rescale Y to fit (D): rescale the Y axis to fit the screen (useful if zoomed-in) Show averages (A): show the average and standard deviation (per group) Show grid (G): show the grid for a better inspection of the plot Invert X (X): invert the order of the X axis Select (line) (S): select the spectra touching a line (draw a line with a mouse) Save graph (Mod + S): export the visualization to an imags Define view range: define a specific range to display Color by: a categorical feature for coloring Title, X-axis, Y-axis: annotate the plot  Example The Spectra widget is used to visualize spectral data. X axis normally shows wavenumbers, while the Y axis shows the absorbance. We will plot the Liver spectroscopy data from the Datasets widget as an example.\nWe have used Color by option to display the type of each spectrum. Or you can also press ‘C’ and the plot will show colors. Colors are defined with the data; to change colors, use the Color widget.\nNow, let’s say I am interested in those spectra, that are quite separated from the rest at wavenumber around 1027. I will press ‘S’ and drag a line. This will select the spectra under the line I have dragged.\nI can observe the selection in another Spectra widget or use it for further analysis.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Spectra Visually explore series of spectra with no spatial information.\nInputs\n Data: input dataset Data Subset:subset of the data  Outputs\n Selection: selected spectra  The Spectra widget allows visual exploration of multiple spectra. To output some spectra, select them by clicking. For multiple selection, hold the modifier key (Ctrl or Cmd) or use line selection (see the plot options menu). Selected spectra will appear dashed.\n Open the plot options menu A spectrum The X and Y position of the cursor The legend (appears only is spectra are colored)  Navigation" ,
	"author" : "",
	"summary" : "Spectra Visually explore series of spectra with no spatial information.\nInputs\n Data: input dataset Data Subset:subset of the data  Outputs\n Selection: selected spectra  The Spectra widget allows visual exploration of multiple spectra. To output some spectra, select them by clicking. For multiple selection, hold the modifier key (Ctrl or Cmd) or use line selection (see the plot options menu). Selected spectra will appear dashed.\n Open the plot options menu A spectrum The X and Y position of the cursor The legend (appears only is spectra are colored)  Navigation",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Spectra",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/spiralogram/",
	"title": "Spiralogram",
	"description": "",
	"content": "Spiralogram Visualize variables’ auto-correlation.\nInputs\n Time series: Time series as output by As Timeseries widget.  Visualize time series’ periodicity in a spiral heatmap.\n Unit of the vertical axis. Options are: years, months, or days (as present in the series), months of year, days of week, days of month, days of year, weeks of year, weeks of month, hours of day, minutes of hour. Unit of the radial axis (options are the same as for (1)). Aggregation function. The series is aggregated on intervals selected in (1) and (2). Select the series to include.  Example The image above shows traffic for select French highways. We see a strong seasonal pattern (high summer) and somewhat of an anomaly on July 1992. In this month, there was an important trucker strike in protest of the new road laws.\nSee also Aggregate\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Spiralogram Visualize variables\u0026rsquo; auto-correlation.\nInputs\n Time series: Time series as output by As Timeseries widget.  Visualize time series\u0026rsquo; periodicity in a spiral heatmap.\n Unit of the vertical axis. Options are: years, months, or days (as present in the series), months of year, days of week, days of month, days of year, weeks of year, weeks of month, hours of day, minutes of hour. Unit of the radial axis (options are the same as for (1))." ,
	"author" : "",
	"summary" : "Spiralogram Visualize variables\u0026rsquo; auto-correlation.\nInputs\n Time series: Time series as output by As Timeseries widget.  Visualize time series\u0026rsquo; periodicity in a spiral heatmap.\n Unit of the vertical axis. Options are: years, months, or days (as present in the series), months of year, days of week, days of month, days of year, weeks of year, weeks of month, hours of day, minutes of hour. Unit of the radial axis (options are the same as for (1)).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Spiralogram",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/sqltable/",
	"title": "SQL Table",
	"description": "",
	"content": "SQL Table Reads data from an SQL database.\nOutputs\n Data: dataset from the database  The SQL widget accesses data stored in an SQL database. It can connect to PostgreSQL (requires psycopg2 module) or SQL Server (requires pymssql module).\nTo handle large databases, Orange attempts to execute a part of the computation in the database itself without downloading the data. This only works with PostgreSQL database and requires quantile and tsm_system_time extensions installed on server. If these extensions are not installed, the data will be downloaded locally.\n Database type (can be either PostgreSQL or MSSQL). Host name. Database name. Username. Password. Press the blue button to connect to the database. Then select the table in the dropdown. Auto-discover categorical variables will cast INT and CHAR columns with less than 20 distinct values as categorical variables (finding all distinct values can be slow on large tables). When not selected, INT will be treated as numeric and CHAR as text. Download to local memory downloads the selected table to your local machine.  ##Installation Instructions\n###PostgreSQL\nInstall the backend.\npip install psycopg2  Alternatively, you can follow these instructions for installing the backend.\nIf the installation of psycopg2 fails, follow to instructions in the error message you get (it explains how to solve the error) or install an already compiled version of psycopg2-binary package:\npip install psycopg2-binary  Note: psycopg2-binary comes with own versions of a few C libraries, among which libpq and libssl, which will be used regardless of other libraries available on the client: upgrading the system libraries will not upgrade the libraries used by psycopg2. Please build psycopg2 from source if you want to maintain binary upgradeability.\nInstall the extensions. [optional]\n###MSSQL\nInstall the backend.\npip install pymssql  If you are encountering issues, follow these instructions.\n##Example\nHere is a simple example on how to use the SQL Table widget. Place the widget on the canvas, enter your database credentials and connect to your database. Then select the table you wish to analyse.\nConnect SQL Table to Data Table widget to inspect the output. If the table is populated, your data has transferred correctly. Now, you can use the SQL Table widget in the same way as the File widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "SQL Table Reads data from an SQL database.\nOutputs\n Data: dataset from the database  The SQL widget accesses data stored in an SQL database. It can connect to PostgreSQL (requires psycopg2 module) or SQL Server (requires pymssql module).\nTo handle large databases, Orange attempts to execute a part of the computation in the database itself without downloading the data. This only works with PostgreSQL database and requires quantile and tsm_system_time extensions installed on server." ,
	"author" : "",
	"summary" : "SQL Table Reads data from an SQL database.\nOutputs\n Data: dataset from the database  The SQL widget accesses data stored in an SQL database. It can connect to PostgreSQL (requires psycopg2 module) or SQL Server (requires pymssql module).\nTo handle large databases, Orange attempts to execute a part of the computation in the database itself without downloading the data. This only works with PostgreSQL database and requires quantile and tsm_system_time extensions installed on server.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "SQL Table",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/stacking/",
	"title": "Stacking",
	"description": "",
	"content": "Stacking Stack multiple models.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Learners: learning algorithm Aggregate: model aggregation method  Outputs\n Learner: aggregated (stacked) learning algorithm Model: trained model  Stacking is an ensemble method that computes a meta model from several base models. The Stacking widget has the Aggregate input, which provides a method for aggregating the input models. If no aggregation input is given the default methods are used. Those are Logistic Regression for classification and Ridge Regression for regression problems.\n The meta learner can be given a name under which it will appear in other widgets. The default name is “Stack”. Click Apply to commit the aggregated model. That will put the new learner in the output and, if the training examples are given, construct a new model and output it as well. To communicate changes automatically tick Apply Automatically. Access help and produce a report.  Example We will use Paint Data to demonstrate how the widget is used. We painted a complex dataset with 4 class labels and sent it to Test \u0026 Score. We also provided three kNN learners, each with a different parameters (number of neighbors is 5, 10 or 15). Evaluation results are good, but can we do better?\nLet’s use Stacking. Stacking requires several learners on the input and an aggregation method. In our case, this is Logistic Regression. A constructed meta learner is then sent to Test \u0026 Score. Results have improved, even if only marginally. Stacking normally works well on complex data sets.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Stacking Stack multiple models.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Learners: learning algorithm Aggregate: model aggregation method  Outputs\n Learner: aggregated (stacked) learning algorithm Model: trained model  Stacking is an ensemble method that computes a meta model from several base models. The Stacking widget has the Aggregate input, which provides a method for aggregating the input models. If no aggregation input is given the default methods are used." ,
	"author" : "",
	"summary" : "Stacking Stack multiple models.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s) Learners: learning algorithm Aggregate: model aggregation method  Outputs\n Learner: aggregated (stacked) learning algorithm Model: trained model  Stacking is an ensemble method that computes a meta model from several base models. The Stacking widget has the Aggregate input, which provides a method for aggregating the input models. If no aggregation input is given the default methods are used.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Stacking",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/statistics/",
	"title": "Statistics",
	"description": "",
	"content": "Statistics Create new statistic variables for documents.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with additional attributes.  Statistics is a feature constructor widget that adds simple document statistics to a corpus. It supports both standard statistical measures and user-defined variables.\n  Add or remove features. Features can be added with the + sign below. They can be removed with the x sign on the left side. Feature options are:\n Words count: number of words in the document. Characters count: number of characters in the document. N-grams count: number of n-grams. Define n-grams in [Preprocess Text], otherwise only unigrams will be reported. Average word length: ratio between character count and the number of words Punctuations count: number of punctuations Capitals count: number of capital letters Vowels count: number of vowels. The default is ‘a, e, i, o, u’, but the user can add her own. Consonants count: number of consonants. Default is given, but the user can adjust it. Per cent unique words: ratio of unique words to all the words (types/tokens). Starts with: number of times a token begins with the specified sequence. Ends with: number of times a token ends with the specified sequence. Contains: number of times a specified sequence is in the token. Regex: number of times the provided regular expression matches the token. POS tag: count specified POS tags. Requires POS tagged tokens from Preprocess Text. List of Tree POS tags for English can be found here.    Press Apply to output corpus with new features.\n  Status line with help on the left and input and output on the right.\n  Example Here is a simple example how Statistics widget works. As it is a basic feature construction widget, it can be used directly after Corpus. We have added a couple of features, namely word count, character count, percent unique words and number of words containing ‘oran’. We can observe the table with additional columns in a Data Table.\nWe can also use the output of Statistics for predictive modeling with Test and Score. Normally, however, we would use Statistics only to enhance features from the Bag of Words widget. Some features require POS tagged tokens, which can be created with Preprocess Text widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Statistics Create new statistic variables for documents.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with additional attributes.  Statistics is a feature constructor widget that adds simple document statistics to a corpus. It supports both standard statistical measures and user-defined variables.\n  Add or remove features. Features can be added with the + sign below. They can be removed with the x sign on the left side." ,
	"author" : "",
	"summary" : "Statistics Create new statistic variables for documents.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with additional attributes.  Statistics is a feature constructor widget that adds simple document statistics to a corpus. It supports both standard statistical measures and user-defined variables.\n  Add or remove features. Features can be added with the + sign below. They can be removed with the x sign on the left side.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Statistics",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/stochasticgradient/",
	"title": "Stochastic Gradient Descent",
	"description": "",
	"content": "Stochastic Gradient Descent Minimize an objective function using a stochastic approximation of gradient descent.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: stochastic gradient descent learning algorithm Model: trained model  The Stochastic Gradient Descent widget uses stochastic gradient descent that minimizes a chosen loss function with a linear function. The algorithm approximates a true gradient by considering one sample at a time, and simultaneously updates the model based on the gradient of the loss function. For regression, it returns predictors as minimizers of the sum, i.e. M-estimators, and is especially useful for large-scale and sparse datasets.\n  Specify the name of the model. The default name is “SGD”.\n  Algorithm parameters:\n Classification loss function:  Hinge (linear SVM) Logistic Regression (logistic regression SGD) Modified Huber (smooth loss that brings tolerance to outliers as well as probability estimates) Squared Hinge (quadratically penalized hinge) Perceptron (linear loss used by the perceptron algorithm) Squared Loss (fitted to ordinary least-squares) Huber (switches to linear loss beyond ε) Epsilon insensitive (ignores errors within ε, linear beyond it) Squared epsilon insensitive (loss is squared beyond ε-region).   Regression loss function:  Squared Loss (fitted to ordinary least-squares) Huber (switches to linear loss beyond ε) Epsilon insensitive (ignores errors within ε, linear beyond it) Squared epsilon insensitive (loss is squared beyond ε-region).      Regularization norms to prevent overfitting:\n None. Lasso (L1) (L1 leading to sparse solutions) Ridge (L2) (L2, standard regularizer) Elastic net (mixing both penalty norms).  Regularization strength defines how much regularization will be applied (the less we regularize, the more we allow the model to fit the data) and the mixing parameter what the ratio between L1 and L2 loss will be (if set to 0 then the loss is L2, if set to 1 then it is L1).\n  Learning parameters.\n Learning rate:  Constant: learning rate stays the same through all epochs (passes) Optimal: a heuristic proposed by Leon Bottou Inverse scaling: earning rate is inversely related to the number of iterations   Initial learning rate. Inverse scaling exponent: learning rate decay. Number of iterations: the number of passes through the training data. If Shuffle data after each iteration is on, the order of data instances is mixed after each pass. If Fixed seed for random shuffling is on, the algorithm will use a fixed random seed and enable replicating the results.    Produce a report.\n  Press Apply to commit changes. Alternatively, tick the box on the left side of the Apply button and changes will be communicated automatically.\n  Examples For the classification task, we will use iris dataset and test two models on it. We connected Stochastic Gradient Descent and Tree to Test \u0026 Score. We also connected File to Test \u0026 Score and observed model performance in the widget.\nFor the regression task, we will compare three different models to see which predict what kind of results. For the purpose of this example, the housing dataset is used. We connect the File widget to Stochastic Gradient Descent, Linear Regression and kNN widget and all four to the Predictions widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Stochastic Gradient Descent Minimize an objective function using a stochastic approximation of gradient descent.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: stochastic gradient descent learning algorithm Model: trained model  The Stochastic Gradient Descent widget uses stochastic gradient descent that minimizes a chosen loss function with a linear function. The algorithm approximates a true gradient by considering one sample at a time, and simultaneously updates the model based on the gradient of the loss function." ,
	"author" : "",
	"summary" : "Stochastic Gradient Descent Minimize an objective function using a stochastic approximation of gradient descent.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: stochastic gradient descent learning algorithm Model: trained model  The Stochastic Gradient Descent widget uses stochastic gradient descent that minimizes a chosen loss function with a linear function. The algorithm approximates a true gradient by considering one sample at a time, and simultaneously updates the model based on the gradient of the loss function.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Stochastic Gradient Descent",
	"icon" : ""
},
{
    "uri": "/workflows/Story-Arcs/",
	"title": "Story Arcs",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Story Arcs",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/svm/",
	"title": "SVM",
	"description": "",
	"content": "SVM Support Vector Machines map inputs to higher-dimensional feature spaces.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: linear regression learning algorithm Model: trained model Support Vectors: instances used as support vectors  Support vector machine (SVM) is a machine learning technique that separates the attribute space with a hyperplane, thus maximizing the margin between the instances of different classes or class values. The technique often yields supreme predictive performance results. Orange embeds a popular implementation of SVM from the LIBSVM package. This widget is its graphical user interface.\nFor regression tasks, SVM performs linear regression in a high dimension feature space using an ε-insensitive loss. Its estimation accuracy depends on a good setting of C, ε and kernel parameters. The widget outputs class predictions based on a SVM Regression.\nThe widget works for both classification and regression tasks.\n The learner can be given a name under which it will appear in other widgets. The default name is “SVM”. SVM type with test error settings. SVM and ν-SVM are based on different minimization of the error function. On the right side, you can set test error bounds:  SVM:  Cost: penalty term for loss and applies for classification and regression tasks. ε: a parameter to the epsilon-SVR model, applies to regression tasks. Defines the distance from true values within which no penalty is associated with predicted values.   ν-SVM:  Cost: penalty term for loss and applies only to regression tasks ν: a parameter to the ν-SVR model, applies to classification and regression tasks. An upper bound on the fraction of training errors and a lower bound of the fraction of support vectors.     Kernel is a function that transforms attribute space to a new feature space to fit the maximum-margin hyperplane, thus allowing the algorithm to create the model with Linear, Polynomial, RBF and Sigmoid kernels. Functions that specify the kernel are presented upon selecting them, and the constants involved are:  g for the gamma constant in kernel function (the recommended value is 1/k, where k is the number of the attributes, but since there may be no training set given to the widget the default is 0 and the user has to set this option manually), c for the constant c0 in the kernel function (default 0), and d for the degree of the kernel (default 3).   Set permitted deviation from the expected value in Numerical Tolerance. Tick the box next to Iteration Limit to set the maximum number of iterations permitted. Produce a report. Click Apply to commit changes. If you tick the box on the left side of the Apply button, changes will be communicated automatically.  Examples In the first (regression) example, we have used housing dataset and split the data into two data subsets (Data Sample and Remaining Data) with Data Sampler. The sample was sent to SVM which produced a Model, which was then used in Predictions to predict the values in Remaining Data. A similar schema can be used if the data is already in two separate files; in this case, two File widgets would be used instead of the File - Data Sampler combination.\nThe second example shows how to use SVM in combination with Scatter Plot. The following workflow trains a SVM model on iris data and outputs support vectors, which are those data instances that were used as support vectors in the learning phase. We can observe which are these data instances in a scatter plot visualization. Note that for the workflow to work correctly, you must set the links between widgets as demonstrated in the screenshot below.\nReferences Introduction to SVM on StatSoft.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "SVM Support Vector Machines map inputs to higher-dimensional feature spaces.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: linear regression learning algorithm Model: trained model Support Vectors: instances used as support vectors  Support vector machine (SVM) is a machine learning technique that separates the attribute space with a hyperplane, thus maximizing the margin between the instances of different classes or class values. The technique often yields supreme predictive performance results." ,
	"author" : "",
	"summary" : "SVM Support Vector Machines map inputs to higher-dimensional feature spaces.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: linear regression learning algorithm Model: trained model Support Vectors: instances used as support vectors  Support vector machine (SVM) is a machine learning technique that separates the attribute space with a hyperplane, thus maximizing the margin between the instances of different classes or class values. The technique often yields supreme predictive performance results.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "SVM",
	"icon" : ""
},
{
    "uri": "/workflows/t-SNE/",
	"title": "t-SNE",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "t-SNE",
	"icon" : ""
},
{
    "uri": "/widget-catalog/unsupervised/tsne/",
	"title": "t-SNE",
	"description": "",
	"content": "t-SNE Two-dimensional data projection with t-SNE.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The t-SNE widget plots the data with a t-distributed stochastic neighbor embedding method. t-SNE is a dimensionality reduction technique, similar to MDS, where points are mapped to 2-D space by their probability distribution.\n Parameters for plot optimization:  measure of perplexity. Roughly speaking, it can be interpreted as the number of nearest neighbors to distances will be preserved from each point. Using smaller values can reveal small, local clusters, while using large values tends to reveal the broader, global relationships between data points. Preserve global structure: this option will combine two different perplexity values (50 and 500) to try preserve both the local and global structure. Exaggeration: this parameter increases the attractive forces between points, and can directly be used to control the compactness of clusters. Increasing exaggeration may also better highlight the global structure of the data. t-SNE with exaggeration set to 4 is roughly equal to UMAP. PCA components: in Orange, we always run t-SNE on the principal components of the input data. This parameter controls the number of principal components to use when calculating distances between data points. Normalize data: We can apply standardization before running PCA. Standardization normalizes each column by subtracting the column mean and dividing by the standard deviation. Press Start to (re-)run the optimization.   Set the color of the displayed points. Set shape, size and label to differentiate between points. If Label only selection and subset is ticked, only selected and/or highlighted points will be labelled. Set symbol size and opacity for all data points. Set jittering to randomly disperse data points. Show color regions colors the graph by class, while Show legend displays a legend on the right. Click and drag the legend to move it. Select, zoom, pan and zoom to fit are the options for exploring the graph. The manual selection of data instances works as an angular/square selection tool. Double click to move the projection. Scroll in or out for zoom. If Send selected automatically is ticked, changes are communicated automatically. Alternatively, press Send Selected.  Examples The first example is a simple t-SNE plot of brown-selected data set. Load brown-selected with the File widget. Then connect t-SNE to it. The widget will show a 2D map of yeast samples, where samples with similar gene expression profiles will be close together. Select the region, where the gene function is mixed and inspect it in a Data Table.\nFor the second example, use Single Cell Datasets widget from the Single Cell add-on to load Bone marrow mononuclear cells with AML (sample) data. Then pass it through k-Means and select 2 clusters from Silhouette Scores. Ok, it looks like there might be two distinct clusters here.\nBut can we find subpopulations in these cells? Select a few marker genes with the Marker Genes widget, for example natural killer cells (NK cells). Pass the marker genes and k-Means results to Score Cells widget. Finally, add t-SNE to visualize the results.\nIn t-SNE, use Cluster attribute to color the points and Score attribute to set their size. We see that killer cells are nicely clustered together and that t-SNE indeed found subpopulations.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "t-SNE Two-dimensional data projection with t-SNE.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The t-SNE widget plots the data with a t-distributed stochastic neighbor embedding method. t-SNE is a dimensionality reduction technique, similar to MDS, where points are mapped to 2-D space by their probability distribution." ,
	"author" : "",
	"summary" : "t-SNE Two-dimensional data projection with t-SNE.\nInputs\n Data: input dataset Data Subset: subset of instances  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The t-SNE widget plots the data with a t-distributed stochastic neighbor embedding method. t-SNE is a dimensionality reduction technique, similar to MDS, where points are mapped to 2-D space by their probability distribution.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "t-SNE",
	"icon" : ""
},
{
    "uri": "/widget-catalog/evaluate/testandscore/",
	"title": "Test and Score",
	"description": "",
	"content": "Test and Score Tests learning algorithms on data.\nInputs\n Data: input dataset Test Data: separate data for testing Learner: learning algorithm(s)  Outputs\n Evaluation Results: results of testing classification algorithms  The widget tests learning algorithms. Different sampling schemes are available, including using separate test data. The widget does two things. First, it shows a table with different classifier performance measures, such as classification accuracy and area under the curve. Second, it outputs evaluation results, which can be used by other widgets for analyzing the performance of classifiers, such as ROC Analysis or Confusion Matrix.\nThe Learner signal has an uncommon property: it can be connected to more than one widget to test multiple learners with the same procedures.\n The widget supports various sampling methods.  Cross-validation splits the data into a given number of folds (usually 5 or 10). The algorithm is tested by holding out examples from one fold at a time; the model is induced from other folds and examples from the held out fold are classified. This is repeated for all the folds. Cross validation by feature performs cross-validation but folds are defined by the selected categorical feature from meta-features. Random sampling randomly splits the data into the training and testing set in the given proportion (e.g. 70:30); the whole procedure is repeated for a specified number of times. Leave-one-out is similar, but it holds out one instance at a time, inducing the model from all others and then classifying the held out instances. This method is obviously very stable, reliable… and very slow. Test on train data uses the whole dataset for training and then for testing. This method practically always gives wrong results. Test on test data: the above methods use the data from Data signal only. To input another dataset with testing examples (for instance from another file or some data selected in another widget), we select Separate Test Data signal in the communication channel and select Test on test data.   For classification, Target class can be selected at the bottom of the widget. When Target class is (Average over classes), methods return scores that are weighted averages over all classes. For example, in case of the classifier with 3 classes, scores are computed for class 1 as a target class, class 2 as a target class, and class 3 as a target class. Those scores are averaged with weights based on the class size to retrieve the final score. The widget will compute a number of performance statistics. A few are shown by default. To see others, right-click on the header and select the desired statistic.  Classification  Area under ROC is the area under the receiver-operating curve. Classification accuracy is the proportion of correctly classified examples. F-1 is a weighted harmonic mean of precision and recall (see below). Precision is the proportion of true positives among instances classified as positive, e.g. the proportion of Iris virginica correctly identified as Iris virginica. Recall is the proportion of true positives among all positive instances in the data, e.g. the number of sick among all diagnosed as sick. Specificity is the proportion of true negatives among all negative instances, e.g. the number of non-sick among all diagnosed as non-sick. LogLoss or cross-entropy loss takes into account the uncertainty of your prediction based on how much it varies from the actual label. Train time - cumulative time in seconds used for training models. Test time - cumulative time in seconds used for testing models.   Regression  MSE measures the average of the squares of the errors or deviations (the difference between the estimator and what is estimated). RMSE is the square root of the arithmetic mean of the squares of a set of numbers (a measure of imperfection of the fit of the estimator to the data) MAE is used to measure how close forecasts or predictions are to eventual outcomes. R2 is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable. CVRMSE is RMSE normalized by the mean value of actual values. Train time - cumulative time in seconds used for training models. Test time - cumulative time in seconds used for testing models.     Choose the score for pairwise comparison of models and the region of practical equivalence (ROPE), in which differences are considered negligible. Pairwise comparison of models using the selected score (available only for cross-validation). The number in the table gives the probability that the model corresponding to the row is better than the model corresponding to the column. If negligible difference is enabled, the smaller number below shows the probability that the difference between the pair is negligible. The test is based on the Bayesian interpretation of the t-test (shorter introduction). Get help and produce a report.  Example In a typical use of the widget, we give it a dataset and a few learning algorithms and we observe their performance in the table inside the Test \u0026 Score widget and in the ROC. The data is often preprocessed before testing; in this case we did some manual feature selection (Select Columns widget) on Titanic dataset, where we want to know only the sex and status of the survived and omit the age.\nIn the bottom table, we have a pairwise comparison of models. We selected that comparison is based on the area under ROC curve statistic. The number in the table gives the probability that the model corresponding to the row is better than the model corresponding to the column. We can, for example, see that probability for the tree to be better than SVM is almost one, and the probability that tree is better than Naive Bayes is 0.001. Smaller numbers in the table are probabilities that the difference between the pair is negligible based on the negligible threshold 0.1.\nAnother example of using this widget is presented in the documentation for the Confusion Matrix widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Test and Score Tests learning algorithms on data.\nInputs\n Data: input dataset Test Data: separate data for testing Learner: learning algorithm(s)  Outputs\n Evaluation Results: results of testing classification algorithms  The widget tests learning algorithms. Different sampling schemes are available, including using separate test data. The widget does two things. First, it shows a table with different classifier performance measures, such as classification accuracy and area under the curve." ,
	"author" : "",
	"summary" : "Test and Score Tests learning algorithms on data.\nInputs\n Data: input dataset Test Data: separate data for testing Learner: learning algorithm(s)  Outputs\n Evaluation Results: results of testing classification algorithms  The widget tests learning algorithms. Different sampling schemes are available, including using separate test data. The widget does two things. First, it shows a table with different classifier performance measures, such as classification accuracy and area under the curve.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Test and Score",
	"icon" : ""
},
{
    "uri": "/workflows/Text-Mining/",
	"title": "Text Mining",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Text Mining",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/guardian-widget/",
	"title": "The Guardian",
	"description": "",
	"content": "The Guardian Fetching data from The Guardian Open Platform.\nInputs\n None  Outputs\n Corpus: A collection of documents from the Guardian newspaper.  Guardian retrieves articles from the Guardian newspaper via their API. For the widget to work, you need to provide the API key, which you can get at their access platform.\n  Insert the API key for the widget to work.\n  Provide the query and set the time frame from which to retrieve the articles.\n  Define which features to retrieve from the Guardian platform.\n  Information on the output.\n  Press Search to start retrieving the articles or Stop to stop the retrieval.\n  Example Guardian can be used just like any other data retrieval widget in Orange, namely NY Times, Wikipedia, Twitter or PubMed.\nWe will retrieve 240 articles mentioning slovenia between september 2017 and september 2018. The text will include article headline and content. Upon pressing Search, the articles will be retrieved.\nWe can observe the results in the Corpus Viewer widget.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "The Guardian Fetching data from The Guardian Open Platform.\nInputs\n None  Outputs\n Corpus: A collection of documents from the Guardian newspaper.  Guardian retrieves articles from the Guardian newspaper via their API. For the widget to work, you need to provide the API key, which you can get at their access platform.\n  Insert the API key for the widget to work.\n  Provide the query and set the time frame from which to retrieve the articles." ,
	"author" : "",
	"summary" : "The Guardian Fetching data from The Guardian Open Platform.\nInputs\n None  Outputs\n Corpus: A collection of documents from the Guardian newspaper.  Guardian retrieves articles from the Guardian newspaper via their API. For the widget to work, you need to provide the API key, which you can get at their access platform.\n  Insert the API key for the widget to work.\n  Provide the query and set the time frame from which to retrieve the articles.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "The Guardian",
	"icon" : ""
},
{
    "uri": "/widget-catalog/spectroscopy/tilefile/",
	"title": "Tile File",
	"description": "",
	"content": "Tile File Read data tile-by-tile from input file(s), preprocess the spectra, and send a data table to the output.\nInputs\n Preprocessor: A preprocessor list from the Preprocess Spectra widget  Outputs\n Data: preprocessed dataset read from the input file(s)  The Tilefile widgets loads data from compatible mosaic spectral images and applies the supplied preprocessor(s) to the data. The preprocessing is applied one mosaic tile at a time, and the resulting processed dataset is combined into a single Data Table.\nAt least one of the preprocessors should reduce the dataset size (such as Cut, Integrate) to take advantage of this file loader and reduce total memory usage.\nBy default, the widget will not load the dataset automatically. This prevents loading a large dataset into memory before the desired preprocessor chain is configured. Press the “Reload” button to load the data.\n Browse through previously opened data files, or load any of the sample ones. Browse for a data file. (Re)loads currently selected data file. Insert data from URL addresses. Information on the preprocessed dataset: dataset size, number and types of data features. Additional information on the features in the preprocessed dataset. Features can be edited by double-clicking on them. The user can change the attribute names, select the type of variable per each attribute (Continuous, Nominal, String, Datetime), and choose how to further define the attributes (as Features, Targets or Meta). The user can also decide to ignore an attribute. Browse documentation datasets. Information on the applied preprocessor list. Produce a report.  Example Here is a simple example on how to use the Tilefile widget. We configured a preprocessor list in Preprocess Spectra and connected the Preprocessor output to the input on the Tilefile widget. We have loaded a mosaic data set that was stored on our local machine. We used the folder icon to access the file and load them. We check the preprocessor that will be applied and press “Reload” to load the data. Now information about the preprocessed dataset is displayed in the info box and domain editor.\nWe can observe the preprocessed data in the HyperSpectra widget or in a Data Table.\nThis example workflow can be found in Help/Example Workflows.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Tile File Read data tile-by-tile from input file(s), preprocess the spectra, and send a data table to the output.\nInputs\n Preprocessor: A preprocessor list from the Preprocess Spectra widget  Outputs\n Data: preprocessed dataset read from the input file(s)  The Tilefile widgets loads data from compatible mosaic spectral images and applies the supplied preprocessor(s) to the data. The preprocessing is applied one mosaic tile at a time, and the resulting processed dataset is combined into a single Data Table." ,
	"author" : "",
	"summary" : "Tile File Read data tile-by-tile from input file(s), preprocess the spectra, and send a data table to the output.\nInputs\n Preprocessor: A preprocessor list from the Preprocess Spectra widget  Outputs\n Data: preprocessed dataset read from the input file(s)  The Tilefile widgets loads data from compatible mosaic spectral images and applies the supplied preprocessor(s) to the data. The preprocessing is applied one mosaic tile at a time, and the resulting processed dataset is combined into a single Data Table.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Tile File",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/time_slice/",
	"title": "Time Slice",
	"description": "",
	"content": "Time Slice Select a slice of measurements on a time interval.\nInputs\n Data: Time series as output by As Timeseries widget.  Outputs\n Subset: Selected time slice from the time series.  Time slice is a subset selection widget designed specifically for time series and for interactive visualizations. It enables selecting a subset of the data by date and/or hour. Moreover, it can output data from a sliding window with options for step size and speed of the output change.\n Visual representation of time series with the selected time slice. Click and drag the red lines to adjust the time window, or click and drag the yellow frame to move it around. Alternatively, set the to and from dates below to output the desired subset. If Loop playback is selected the data will ‘replay’ continuously. Custom step size defines how the time slice move. If it is set to, say, 1 day, the window will output n + 1 day once it moves. Without the custom step size defined, the slice will move to the next frame of the same size without any overlap. Press play to being the sliding window and stop to stop it. Backwards and forwards buttons move the slice by the specified step size. Set the speed of the sliding window.  Example This simple example uses Yahoo Finance widget to retrieve financial data from Yahoo, namely the AMNZ stock index from 2015 to 2020. Next, we will use Time Slice to observe how the data changed through time. We can observe the output of Time Slice in Line Chart. Press Play in Time Slice and see how Line Chart changes interactively.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Time Slice Select a slice of measurements on a time interval.\nInputs\n Data: Time series as output by As Timeseries widget.  Outputs\n Subset: Selected time slice from the time series.  Time slice is a subset selection widget designed specifically for time series and for interactive visualizations. It enables selecting a subset of the data by date and/or hour. Moreover, it can output data from a sliding window with options for step size and speed of the output change." ,
	"author" : "",
	"summary" : "Time Slice Select a slice of measurements on a time interval.\nInputs\n Data: Time series as output by As Timeseries widget.  Outputs\n Subset: Selected time slice from the time series.  Time slice is a subset selection widget designed specifically for time series and for interactive visualizations. It enables selecting a subset of the data by date and/or hour. Moreover, it can output data from a sliding window with options for step size and speed of the output change.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Time Slice",
	"icon" : ""
},
{
    "uri": "/workflows/Timeseries/",
	"title": "Timeseries",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Timeseries",
	"icon" : ""
},
{
    "uri": "/workflows/Tokenization/",
	"title": "Tokenization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Tokenization",
	"icon" : ""
},
{
    "uri": "/workflows/Topic-Modeling/",
	"title": "Topic Modeling",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Topic Modeling",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/topicmodelling-widget/",
	"title": "Topic Modelling",
	"description": "",
	"content": "Topic Modelling Topic modelling with Latent Dirichlet Allocation, Latent Semantic Indexing or Hierarchical Dirichlet Process.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with topic weights appended. Topics: Selected topics with word weights. All Topics: Token weights per topic.  Topic Modelling discovers abstract topics in a corpus based on clusters of words found in each document and their respective frequency. A document typically contains multiple topics in different proportions, thus the widget also reports on the topic weight per document.\nThe widget wraps gensim’s topic models (LSI, LDA, HDP).\nThe first, LSI, can return both positive and negative words (words that are in a topic and those that aren’t) and concurrently topic weights, that can be positive or negative. As stated by the main gensim’s developer, Radim Řehůřek: “LSI topics are not supposed to make sense; since LSI allows negative numbers, it boils down to delicate cancellations between topics and there’s no straightforward way to interpret a topic.\"\nLDA can be more easily interpreted, but is slower than LSI. HDP has many parameters - the parameter that corresponds to the number of topics is Top level truncation level (T). The smallest number of topics that one can retrieve is 10.\n Topic modelling algorithm:  Latent Semantic Indexing. Returns both negative and positive words and topic weights. Latent Dirichlet Allocation Hierarchical Dirichlet Process   Parameters for the algorithm. LSI and LDA accept only the number of topics modelled, with the default set to 10. HDP, however, has more parameters. As this algorithm is computationally very demanding, we recommend you to try it on a subset or set all the required parameters in advance and only then run the algorithm (connect the input to the widget).  First level concentration (γ): distribution at the first (corpus) level of Dirichlet Process Second level concentration (α): distribution at the second (document) level of Dirichlet Process The topic Dirichlet (α): concentration parameter used for the topic draws Top level truncation (Τ): corpus-level truncation (no of topics) Second level truncation (Κ): document-level truncation (no of topics) Learning rate (κ): step size Slow down parameter (τ)   Produce a report. If Commit Automatically is on, changes are communicated automatically. Alternatively press Commit.  Examples Exploring Individual Topics In the first example, we present a simple use of the Topic Modelling widget. First we load grimm-tales-selected.tab data set and use Preprocess Text to tokenize by words only and remove stopwords. Then we connect Preprocess Text to Topic Modelling, where we use a simple Latent Semantic Indexing to find 10 topics in the text.\nLSI provides both positive and negative weights per topic. A positive weight means the word is highly representative of a topic, while a negative weight means the word is highly unrepresentative of a topic (the less it occurs in a text, the more likely the topic). Positive words are colored green and negative words are colored red.\nWe then select the first topic and display the most frequent words in the topic in Word Cloud. We also connected Preprocess Text to Word Cloud in order to be able to output selected documents. Now we can select a specific word in the word cloud, say little. It will be colored red and also highlighted in the word list on the left.\nNow we can observe all the documents containing the word little in Corpus Viewer.\nTopic Visualization In the second example, we will look at the correlation between topics and words/documents. We are still using the grimm-tales-selected.tab corpus. In Preprocess Text we are using the default preprocessing, with an additional filter by document frequency (0.1 - 0.9). In Topic Modelling we are using LDA model with 5 topics.\nConnect Topic Modelling to MDS. Ensure the link is set to All Topics - Data. Topic Modelling will output a matrix of word weights by topic.\nIn MDS, the points are now topics. We have set the size of the points to Marginal topic probability, which is an additional columns of All Topics - it reports on the marginal probability of the topic in the corpus (how strongly represented is the topic in the corpus).\nWe can now explore which words are representative for the topic. Select, say, Topic 5 from the plot and connect MDS to Box Plot. Make sure the output is set to Data - Data (not Selected Data - Data).\nIn Box Plot, set the subgroup to Selected and check the Order by relevance to subgroups box. This option will sort the variables by how well they separate between the selected subgroup values. In our case, this means which words are the most representative for the topic we have selected in the plot (subgroup Yes means selected).\nWe can see that little, children and kings are the most representative words for Topic 5, with good separation between the word frequency for this topic and all the others. Select other topics in MDS and see how the Box Plot changes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Topic Modelling Topic modelling with Latent Dirichlet Allocation, Latent Semantic Indexing or Hierarchical Dirichlet Process.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with topic weights appended. Topics: Selected topics with word weights. All Topics: Token weights per topic.  Topic Modelling discovers abstract topics in a corpus based on clusters of words found in each document and their respective frequency. A document typically contains multiple topics in different proportions, thus the widget also reports on the topic weight per document." ,
	"author" : "",
	"summary" : "Topic Modelling Topic modelling with Latent Dirichlet Allocation, Latent Semantic Indexing or Hierarchical Dirichlet Process.\nInputs\n Corpus: A collection of documents.  Outputs\n Corpus: Corpus with topic weights appended. Topics: Selected topics with word weights. All Topics: Token weights per topic.  Topic Modelling discovers abstract topics in a corpus based on clusters of words found in each document and their respective frequency. A document typically contains multiple topics in different proportions, thus the widget also reports on the topic weight per document.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Topic Modelling",
	"icon" : ""
},
{
    "uri": "/training/",
	"title": "Training",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "training",
	"LinkTitle" : "Training",
	"icon" : ""
},
{
    "uri": "/training-inquiry/",
	"title": "Training inquiry",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "training-inquiry",
	"LinkTitle" : "Training inquiry",
	"icon" : ""
},
{
    "uri": "/widget-catalog/data/transpose/",
	"title": "Transpose",
	"description": "",
	"content": "Transpose Transposes a data table.\nInputs\n Data: input dataset  Outputs\n Data: transposed dataset  Transpose widget transposes data table.\nExample This is a simple workflow showing how to use Transpose. Connect the widget to File widget. The output of Transpose is a transposed data table with rows as columns and columns as rows. You can observe the result in a Data Table.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Transpose Transposes a data table.\nInputs\n Data: input dataset  Outputs\n Data: transposed dataset  Transpose widget transposes data table.\nExample This is a simple workflow showing how to use Transpose. Connect the widget to File widget. The output of Transpose is a transposed data table with rows as columns and columns as rows. You can observe the result in a Data Table." ,
	"author" : "",
	"summary" : "Transpose Transposes a data table.\nInputs\n Data: input dataset  Outputs\n Data: transposed dataset  Transpose widget transposes data table.\nExample This is a simple workflow showing how to use Transpose. Connect the widget to File widget. The output of Transpose is a transposed data table with rows as columns and columns as rows. You can observe the result in a Data Table.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Transpose",
	"icon" : ""
},
{
    "uri": "/widget-catalog/model/tree/",
	"title": "Tree",
	"description": "",
	"content": "Tree A tree algorithm with forward pruning.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: decision tree learning algorithm Model: trained model  Tree is a simple algorithm that splits the data into nodes by class purity. It is a precursor to Random Forest. Tree in Orange is designed in-house and can handle both discrete and continuous datasets.\nIt can also be used for both classification and regression tasks.\n The learner can be given a name under which it will appear in other widgets. The default name is “Tree”. Tree parameters:  Induce binary tree: build a binary tree (split into two child nodes) Min. number of instances in leaves: if checked, the algorithm will never construct a split which would put less than the specified number of training examples into any of the branches. Do not split subsets smaller than: forbids the algorithm to split the nodes with less than the given number of instances. Limit the maximal tree depth: limits the depth of the classification tree to the specified number of node levels.   Stop when majority reaches [%]: stop splitting the nodes after a specified majority threshold is reached Produce a report. After changing the settings, you need to click Apply, which will put the new learner on the output and, if the training examples are given, construct a new classifier and output it as well. Alternatively, tick the box on the left and changes will be communicated automatically.  Examples There are two typical uses for this widget. First, you may want to induce a model and check what it looks like in Tree Viewer.\nThe second schema trains a model and evaluates its performance against Logistic Regression.\nWe used the iris dataset in both examples. However, Tree works for regression tasks as well. Use housing dataset and pass it to Tree. The selected tree node from Tree Viewer is presented in the Scatter Plot and we can see that the selected examples exhibit the same features.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Tree A tree algorithm with forward pruning.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: decision tree learning algorithm Model: trained model  Tree is a simple algorithm that splits the data into nodes by class purity. It is a precursor to Random Forest. Tree in Orange is designed in-house and can handle both discrete and continuous datasets.\nIt can also be used for both classification and regression tasks." ,
	"author" : "",
	"summary" : "Tree A tree algorithm with forward pruning.\nInputs\n Data: input dataset Preprocessor: preprocessing method(s)  Outputs\n Learner: decision tree learning algorithm Model: trained model  Tree is a simple algorithm that splits the data into nodes by class purity. It is a precursor to Random Forest. Tree in Orange is designed in-house and can handle both discrete and continuous datasets.\nIt can also be used for both classification and regression tasks.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Tree",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/treeviewer/",
	"title": "Tree Viewer",
	"description": "",
	"content": "Tree Viewer A visualization of classification and regression trees.\nInputs\n Tree: decision tree  Outputs\n Selected Data: instances selected from the tree node Data: data with an additional column showing whether a point is selected  This is a versatile widget with 2-D visualization of classification and regression trees. The user can select a node, instructing the widget to output the data associated with the node, thus enabling explorative data analysis.\n Information on the input. Display options:  Zoom in and zoom out Select the tree width. The nodes display information bubbles when hovering over them. Select the depth of your tree. Select edge width. The edges between the nodes in the tree graph are drawn based on the selected edge width.  All the edges will be of equal width if Fixed is chosen. When Relative to root is selected, the width of the edge will correspond to the proportion of instances in the corresponding node with respect to all the instances in the training data. Under this selection, the edge will get thinner and thinner when traversing toward the bottom of the tree. Relative to parent makes the edge width correspond to the proportion of instances in the nodes with respect to the instances in their parent node.   Define the target class, which you can change based on classes in the data.   Press Save image to save the created tree graph to your computer as a .svg or .png file. Produce a report.  Examples Below, is a simple classification schema, where we have read the data, constructed the decision tree and viewed it in our Tree Viewer. If both the viewer and Tree are open, any re-run of the tree induction algorithm will immediately affect the visualization. You can thus use this combination to explore how the parameters of the induction algorithm influence the structure of the resulting tree.\nClicking on any node will output the related data instances. This is explored in the schema below that shows the subset in the data table and in the Scatter Plot. Make sure that the tree data is passed as a data subset; this can be done by connecting the Scatter Plot to the File widget first, and connecting it to the Tree Viewer widget next. Selected data will be displayed as bold dots.\nTree Viewer can also export labeled data. Connect Data Table to Tree Viewer and set the link between widgets to Data instead of Selected Data. This will send the entire data to Data Table with an additional meta column labeling selected data instances (Yes for selected and No for the remaining).\nFinally, Tree Viewer can be used also for visualizing regression trees. Connect Random Forest to File widget using housing.tab dataset. Then connect Pythagorean Forest to Random Forest. In Pythagorean Forest select a regression tree you wish to further analyze and pass it to the Tree Viewer. The widget will display the constructed tree. For visualizing larger trees, especially for regression, Pythagorean Tree could be a better option.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Tree Viewer A visualization of classification and regression trees.\nInputs\n Tree: decision tree  Outputs\n Selected Data: instances selected from the tree node Data: data with an additional column showing whether a point is selected  This is a versatile widget with 2-D visualization of classification and regression trees. The user can select a node, instructing the widget to output the data associated with the node, thus enabling explorative data analysis." ,
	"author" : "",
	"summary" : "Tree Viewer A visualization of classification and regression trees.\nInputs\n Tree: decision tree  Outputs\n Selected Data: instances selected from the tree node Data: data with an additional column showing whether a point is selected  This is a versatile widget with 2-D visualization of classification and regression trees. The user can select a node, instructing the widget to output the data associated with the node, thus enabling explorative data analysis.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Tree Viewer",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/tweetprofiler/",
	"title": "Tweet Profiler",
	"description": "",
	"content": "Tweet Profiler Detect Ekman’s, Plutchik’s or Profile of Mood States’ emotions in tweets.\nInputs\n Corpus: A collection of tweets (or other documents).  Outputs\n Corpus: A corpus with information on the sentiment of each document.  Tweet Profiler retrieves information on sentiment from the server for each given tweet (or document). The widget sends data to the server, where a model computes emotion probabilities and/or scores. The widget support three classifications of emotion, namely Ekman’s, Plutchik’s and Profile of Mood States (POMS).\n Options:  Attribute to use as content. Emotion classification, either Ekman’s, Plutchik’s or Profile of Mood States. Multi-class will output one most probable emotion per document, while multi-label will output values in columns per each emotion. The widget can output classes of emotion (categorical), probabilities (numeric), or embeddings (an emotional vector of the document).   Commit Automatically automatically outputs the result. Alternatively, press Commit.  Example We will use election-tweets-2016.tab for this example. Load the data with Corpus and connect it to Tweet Profiler. We will use Content attribute for the analysis, Ekman’s classification of emotion with multi-class option and we will output the result as class. We will observe the results in a Box Plot. In the widget, we have selected to observe the Emotion variable, grouped by Author. This way we can see which emotion prevails by which author.\nReferences Colnerič, Niko and Janez Demšar (2018). Emotion Recognition on Twitter: Comparative Study and Training a Unison Model. In IEEE Transactions on Affective Computing. Available online.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Tweet Profiler Detect Ekman\u0026rsquo;s, Plutchik\u0026rsquo;s or Profile of Mood States\u0026rsquo; emotions in tweets.\nInputs\n Corpus: A collection of tweets (or other documents).  Outputs\n Corpus: A corpus with information on the sentiment of each document.  Tweet Profiler retrieves information on sentiment from the server for each given tweet (or document). The widget sends data to the server, where a model computes emotion probabilities and/or scores. The widget support three classifications of emotion, namely Ekman\u0026rsquo;s, Plutchik\u0026rsquo;s and Profile of Mood States (POMS)." ,
	"author" : "",
	"summary" : "Tweet Profiler Detect Ekman\u0026rsquo;s, Plutchik\u0026rsquo;s or Profile of Mood States\u0026rsquo; emotions in tweets.\nInputs\n Corpus: A collection of tweets (or other documents).  Outputs\n Corpus: A corpus with information on the sentiment of each document.  Tweet Profiler retrieves information on sentiment from the server for each given tweet (or document). The widget sends data to the server, where a model computes emotion probabilities and/or scores. The widget support three classifications of emotion, namely Ekman\u0026rsquo;s, Plutchik\u0026rsquo;s and Profile of Mood States (POMS).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Tweet Profiler",
	"icon" : ""
},
{
    "uri": "/workflows/Twitter/",
	"title": "Twitter",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Twitter",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/twitter-widget/",
	"title": "Twitter",
	"description": "",
	"content": "Twitter Fetching data from The Twitter Search API.\nInputs\n None  Outputs\n Corpus: A collection of tweets from the Twitter API.  Twitter widget enables querying tweets through Twitter API. You can query by content, author or both and accumulate results should you wish to create a larger data set. The widget only supports REST API and allows queries for up to two weeks back.\n  To begin your queries, insert Twitter key and secret. They are securely saved in your system keyring service (like Credential Vault, Keychain, KWallet, etc.) and won’t be deleted when clearing widget settings. You must first create a Twitter app to get API keys.\n  Set query parameters:\n Query word list: list desired queries, one per line. Queries are automatically joined by OR. Search by: specify whether you want to search by content, author or both. If searching by author, you must enter proper Twitter handle (without @) in the query list. Language: set the language of retrieved tweets. Any will retrieve tweets in any language. Max tweets: set the top limit of retrieved tweets. If box is not ticked, no upper bound will be set - widget will retrieve all available tweets. Allow retweets: if ‘Allow retweets’ is checked, retweeted tweets will also appear on the output. This might duplicate some results. Collect results: if ‘Collect results’ is ticked, widget will append new queries to the previous ones. Enter new queries, run Search and new results will be appended to the previous ones.    Define which features to include as text features.\n  Information on the number of tweets on the output.\n  Run query.\n  Examples First, let’s try a simple query. We will search for tweets containing either ‘data mining’ or ‘machine learning’ in the content and allow retweets. We will further limit our search to only a 100 tweets in English.\nFirst, we’re checking the output in Corpus Viewer to get the initial idea about our results. Then we’re preprocessing the tweets with lowercase, url removal, tweet tokenizer and removal of stopword and punctuation. The best way to see the results is with Word Cloud. This will display the most popular words in field of data mining and machine learning in the past two weeks.\nOur next example is a bit more complex. We’re querying tweets from Hillary Clinton and Donald Trump from the presidential campaign 2016.\nThen we’ve used Preprocess Text to get suitable tokens on our output. We’ve connected Preprocess Text to Bag of Words in order to create a table with words as features and their counts as values. A quick check in Word Cloud gives us an idea about the results.\nNow we would like to predict the author of the tweet. With Select Columns we’re setting ‘Author’ as our target variable. Then we connect Select Columns to Test \u0026 Score. We’ll be using Logistic Regression as our learner, which we also connect to Test \u0026 Score.\nWe can observe the results of our author predictions directly in the widget. AUC score is quite ok. Seems like we can to some extent predict who is the author of the tweet based on the tweet content.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Twitter Fetching data from The Twitter Search API.\nInputs\n None  Outputs\n Corpus: A collection of tweets from the Twitter API.  Twitter widget enables querying tweets through Twitter API. You can query by content, author or both and accumulate results should you wish to create a larger data set. The widget only supports REST API and allows queries for up to two weeks back.\n  To begin your queries, insert Twitter key and secret." ,
	"author" : "",
	"summary" : "Twitter Fetching data from The Twitter Search API.\nInputs\n None  Outputs\n Corpus: A collection of tweets from the Twitter API.  Twitter widget enables querying tweets through Twitter API. You can query by content, author or both and accumulate results should you wish to create a larger data set. The widget only supports REST API and allows queries for up to two weeks back.\n  To begin your queries, insert Twitter key and secret.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Twitter",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/var/",
	"title": "VAR Model",
	"description": "",
	"content": "VAR Model Model the time series using vector autoregression (VAR) model.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series model: The VAR model fitted to input time series. Forecast: The forecast time series. Fitted values: The values that the model was actually fitted to, equals to original values - residuals. Residuals: The errors the model made at each step.  Using this widget, you can model the time series using VAR model.\n Model’s name. By default, the name is derived from the model and its parameters. Desired model order (number of parameters). If other than None, optimize the number of model parameters (up to the value selected in (2)) with the selected information criterion (one of: AIC, BIC, HQIC, FPE, or a mix thereof). Choose this option to add additional “trend” columns to the data:  Constant: a single column of ones is added Constant and linear: a column of ones and a column of linearly increasing numbers are added Constant, linear and quadratic: an additional column of quadratics is added   Number of forecast steps the model should output, along with the desired confidence intervals values at each step.  Example See also ARIMA Model, Model Evaluation\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "VAR Model Model the time series using vector autoregression (VAR) model.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series model: The VAR model fitted to input time series. Forecast: The forecast time series. Fitted values: The values that the model was actually fitted to, equals to original values - residuals. Residuals: The errors the model made at each step.  Using this widget, you can model the time series using VAR model." ,
	"author" : "",
	"summary" : "VAR Model Model the time series using vector autoregression (VAR) model.\nInputs\n Time series: Time series as output by As Timeseries widget.  Outputs\n Time series model: The VAR model fitted to input time series. Forecast: The forecast time series. Fitted values: The values that the model was actually fitted to, equals to original values - residuals. Residuals: The errors the model made at each step.  Using this widget, you can model the time series using VAR model.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "VAR Model",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/venndiagram/",
	"title": "Venn Diagram",
	"description": "",
	"content": "Venn Diagram Plots a Venn diagram for two or more data subsets.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: entire data with a column indicating whether an instance was selected or not  The Venn Diagram widget displays logical relations between datasets by showing the number of common data instances (rows) or the number of shared features (columns). Selecting a part of the visualization outputs the corresponding instances or features.\n Select whether to count common features or instances. Select whether to include duplicates or to output only unique rows (applicable only when matching by instances). If Auto commit is on, changes are automatically communicated to other widgets.  Rows can be matched by their identity, e.g. rows from different data sets match if they came from the same row in a file. Instead of using identities, we can choose a string variable to match the rows by. A warning is shown if data sets have no common string variable.\nExamples The easiest way to use the Venn Diagram is to select data subsets and find matching instances in the visualization. We use the breast-cancer dataset to select two subsets with Select Rows widget - the first subset is that of breast cancer patients aged between 40 and 49 and the second is that of patients with a tumor size between 20 and 29. The Venn Diagram helps us find instances that correspond to both criteria, which can be found in the intersection of the two circles.\nThe Venn Diagram widget can be also used for exploring different prediction models. In the following example, we analysed 3 prediction methods, namely Naive Bayes, SVM and Random Forest, according to their misclassified instances.\nBy selecting misclassifications in the three Confusion Matrix widgets and sending them to Venn diagram, we can see all the misclassification instances visualized per method used. Then we open Venn Diagram and select, for example, the misclassified instances that were identified by all three methods. This is represented as an intersection of all three circles. Click on the intersection to see this two instances marked in the Scatter Plot widget. Try selecting different diagram sections to see how the scatter plot visualization changes.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Venn Diagram Plots a Venn diagram for two or more data subsets.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: entire data with a column indicating whether an instance was selected or not  The Venn Diagram widget displays logical relations between datasets by showing the number of common data instances (rows) or the number of shared features (columns). Selecting a part of the visualization outputs the corresponding instances or features." ,
	"author" : "",
	"summary" : "Venn Diagram Plots a Venn diagram for two or more data subsets.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: entire data with a column indicating whether an instance was selected or not  The Venn Diagram widget displays logical relations between datasets by showing the number of common data instances (rows) or the number of shared features (columns). Selecting a part of the visualization outputs the corresponding instances or features.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Venn Diagram",
	"icon" : ""
},
{
    "uri": "/widget-catalog/visualize/violinplot/",
	"title": "Violin Plot",
	"description": "",
	"content": "Violin Plot Visualize the distribution of feature values in a violin plot.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Violin Plot widget plays a similar role as a Box Plot. It shows the distribution of quantitative data across several levels of a categorical variable such that those distributions can be compared. Unlike the Box Plot, in which all of the plot components correspond to actual data points, the Violin Plot features a kernel density estimation of the underlying distribution.\n  Select the variable you want to plot. Tick Order by relevance to subgroups to order variables by Chi2 or ANOVA over the selected subgroup.\n  Choose Subgroups to see violin plots displayed by a discrete subgroup. Tick Order by relevance to variable to order subgroups by Chi2 or ANOVA over the selected variable.\n  Box plot: Tick to show the underlying box plot.\nStrip plot: Tick to show the underlying data represented by points.\nRug plot: Tick to show the underlying data represented by lines.\nOrder subgroups: Tick to order violins by median (ascending).\nOrientation: Determine violin orientation.\n  Kernel: Select the kernel used to estimate the density. Possible kernels are: Normal, Epanechnikov and Linear.\nScale: Select the method used to scale the width of each violin. If area is selected, each violin will have the same area. If count is selected, the width of the violins will be scaled by the number of observations in that bin. If width is selected, each violin will have the same width.\n  Examples The Violin Plot widget is most commonly used immediately after the File widget to observe the statistical properties of a dataset. In the first example, we have used heart-disease data to inspect our variables.\nThe Violin Plot could also be used for outlier detection. In the next example we eliminate the outliers by selecting only instances that fall inside the Q1 − 1.5 and Q3 + 1.5 IQR.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Violin Plot Visualize the distribution of feature values in a violin plot.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Violin Plot widget plays a similar role as a Box Plot. It shows the distribution of quantitative data across several levels of a categorical variable such that those distributions can be compared." ,
	"author" : "",
	"summary" : "Violin Plot Visualize the distribution of feature values in a violin plot.\nInputs\n Data: input dataset  Outputs\n Selected Data: instances selected from the plot Data: data with an additional column showing whether a point is selected  The Violin Plot widget plays a similar role as a Box Plot. It shows the distribution of quantitative data across several levels of a categorical variable such that those distributions can be compared.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Violin Plot",
	"icon" : ""
},
{
    "uri": "/workflows/Visualization/",
	"title": "Visualization",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "term",
	"type" : "workflows",
	"LinkTitle" : "Visualization",
	"icon" : ""
},
{
    "uri": "/widget-catalog/bioinformatics/volcano_plot/",
	"title": "Volcano Plot",
	"description": "",
	"content": "Volcano Plot Plots significance versus fold-change for gene expression rates.\nInputs\n Data: Input data set.  Outputs\n Selected Data: Data subset.  Volcano plot is a graphical method for visualizing changes in replicate data. The widget plots a binary logarithm of fold-change on the x-axis versus statistical significance (negative base 10 logarithm of p-value) on the y-axis.\nVolcano Plot is useful for a quick visual identification of statistically significant data (genes). Genes that are highly dysregulated are farther to the left and right, while highly significant fold changes appear higher on the plot. A combination of the two are those genes that are statistically significant.\nExample TODO!\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Volcano Plot Plots significance versus fold-change for gene expression rates.\nInputs\n Data: Input data set.  Outputs\n Selected Data: Data subset.  Volcano plot is a graphical method for visualizing changes in replicate data. The widget plots a binary logarithm of fold-change on the x-axis versus statistical significance (negative base 10 logarithm of p-value) on the y-axis.\nVolcano Plot is useful for a quick visual identification of statistically significant data (genes)." ,
	"author" : "",
	"summary" : "Volcano Plot Plots significance versus fold-change for gene expression rates.\nInputs\n Data: Input data set.  Outputs\n Selected Data: Data subset.  Volcano plot is a graphical method for visualizing changes in replicate data. The widget plots a binary logarithm of fold-change on the x-axis versus statistical significance (negative base 10 logarithm of p-value) on the y-axis.\nVolcano Plot is useful for a quick visual identification of statistically significant data (genes).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Volcano Plot",
	"icon" : ""
},
{
    "uri": "/widget-catalog/",
	"title": "Widget catalog",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "widget-catalog",
	"LinkTitle" : "Widget catalog",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/wikipedia-widget/",
	"title": "Wikipedia",
	"description": "",
	"content": "Wikipedia Fetching data from MediaWiki RESTful web service API.\nInputs\n None  Outputs\n Corpus: A collection of documents from the Wikipedia.  Wikipedia widget is used to retrieve texts from Wikipedia API and it is useful mostly for teaching and demonstration.\n Query parameters:  Query word list, where each query is listed in a new line. Language of the query. English is set by default. Number of articles to retrieve per query (range 1-25). Please note that querying is done recursively and that disambiguations are also retrieved, sometimes resulting in a larger number of queries than set on the slider.   Select which features to include as text features. Information on the output. Produce a report. Run query.  Example This is a simple example, where we use Wikipedia and retrieve the articles on ‘Slovenia’ and ‘Germany’. Then we simply apply default preprocessing with Preprocess Text and observe the most frequent words in those articles with Word Cloud.\nWikipedia works just like any other corpus widget (NY Times, Twitter) and can be used accordingly.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Wikipedia Fetching data from MediaWiki RESTful web service API.\nInputs\n None  Outputs\n Corpus: A collection of documents from the Wikipedia.  Wikipedia widget is used to retrieve texts from Wikipedia API and it is useful mostly for teaching and demonstration.\n Query parameters:  Query word list, where each query is listed in a new line. Language of the query. English is set by default. Number of articles to retrieve per query (range 1-25)." ,
	"author" : "",
	"summary" : "Wikipedia Fetching data from MediaWiki RESTful web service API.\nInputs\n None  Outputs\n Corpus: A collection of documents from the Wikipedia.  Wikipedia widget is used to retrieve texts from Wikipedia API and it is useful mostly for teaching and demonstration.\n Query parameters:  Query word list, where each query is listed in a new line. Language of the query. English is set by default. Number of articles to retrieve per query (range 1-25).",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Wikipedia",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/wordcloud/",
	"title": "Word Cloud",
	"description": "",
	"content": "Word Cloud Generates a word cloud from corpus.\nInputs\n Topic: Selected topic. Corpus: A collection of documents.  Outputs\n Corpus: Documents that match the selection. Selected Word: Selected word that can be used as query in Concordance. Word Counts: Words and their weights.  Word Cloud displays tokens in the corpus, their size denoting the frequency of the word in corpus or an average bag of words count, when bag of words features are at the input of the widget. Words are listed by their frequency (weight) in the widget. The widget outputs documents, containing selected tokens from the word cloud.\n Information on the input.  number of words (tokens) in a topic number of documents and tokens in the corpus   Adjust the plot.  If Color words is ticked, words will be assigned a random color. If unchecked, the words will be black. Word tilt adjust the tilt of words. The current state of tilt is displayed next to the slider (‘no’ is the default). Regenerate word cloud plot the cloud anew.   Words \u0026 weights displays a sorted list of words (tokens) by their frequency in the corpus or topic. Clicking on a word will select that same word in the cloud and output matching documents. Use Ctrl to select more than one word. Documents matching ANY of the selected words will be on the output (logical OR). Save Image saves the image to your computer in a .svg or .png format.  Example Word Cloud is an excellent widget for displaying the current state of the corpus and for monitoring the effects of preprocessing.\nUse Corpus to load the data. Connect Preprocess Text to it and set your parameters. We’ve used defaults here, just to see the difference between the default preprocessing in the Word Cloud widget and the Preprocess Text widget.\nWe can see from the two widgets, that Preprocess Text displays only words, while default preprocessing in the Word Cloud tokenizes by word and punctuation.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Word Cloud Generates a word cloud from corpus.\nInputs\n Topic: Selected topic. Corpus: A collection of documents.  Outputs\n Corpus: Documents that match the selection. Selected Word: Selected word that can be used as query in Concordance. Word Counts: Words and their weights.  Word Cloud displays tokens in the corpus, their size denoting the frequency of the word in corpus or an average bag of words count, when bag of words features are at the input of the widget." ,
	"author" : "",
	"summary" : "Word Cloud Generates a word cloud from corpus.\nInputs\n Topic: Selected topic. Corpus: A collection of documents.  Outputs\n Corpus: Documents that match the selection. Selected Word: Selected word that can be used as query in Concordance. Word Counts: Words and their weights.  Word Cloud displays tokens in the corpus, their size denoting the frequency of the word in corpus or an average bag of words count, when bag of words features are at the input of the widget.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Word Cloud",
	"icon" : ""
},
{
    "uri": "/widget-catalog/text-mining/wordenrichment/",
	"title": "Word Enrichment",
	"description": "",
	"content": "Word Enrichment Word enrichment analysis for selected documents.\nInputs\n Corpus: A collection of documents. Selected Data: Selected instances from corpus.  Outputs\n None  Word Enrichment displays a list of words with lower p-values (higher significance) for a selected subset compared to the entire corpus. Lower p-value indicates a higher likelihood that the word is significant for the selected subset (not randomly occurring in a text). FDR (False Discovery Rate) is linked to p-value and reports on the expected percent of false predictions in the set of predictions, meaning it account for false positives in list of low p-values.\n Information on the input.  Cluster words are all the tokens from the corpus. Selected words are all the tokens from the selected subset. After filtering reports on the enriched words found in the subset.   Filter enables you to filter by:  p-value false discovery rate (FDR)    Example In the example below, we’re retrieved recent tweets from the 2016 presidential candidates, Donald Trump and Hillary Clinton. Then we’ve preprocessed the tweets to get only words as tokens and to remove the stopwords. We’ve connected the preprocessed corpus to Bag of Words to get a table with word counts for our corpus.\nThen we’ve connected Corpus Viewer to Bag of Words and selected only those tweets that were published by Donald Trump. See how we marked only the Author as our Search feature to retrieve those tweets.\nWord Enrichment accepts two inputs - the entire corpus to serve as a reference and a selected subset from the corpus to do the enrichment on. First connect Corpus Viewer to Word Enrichment (input Matching Docs → Selected Data) and then connect Bag of Words to it (input Corpus → Data). In the Word Enrichment widget we can see the list of words that are more significant for Donald Trump than they are for Hillary Clinton.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Word Enrichment Word enrichment analysis for selected documents.\nInputs\n Corpus: A collection of documents. Selected Data: Selected instances from corpus.  Outputs\n None  Word Enrichment displays a list of words with lower p-values (higher significance) for a selected subset compared to the entire corpus. Lower p-value indicates a higher likelihood that the word is significant for the selected subset (not randomly occurring in a text). FDR (False Discovery Rate) is linked to p-value and reports on the expected percent of false predictions in the set of predictions, meaning it account for false positives in list of low p-values." ,
	"author" : "",
	"summary" : "Word Enrichment Word enrichment analysis for selected documents.\nInputs\n Corpus: A collection of documents. Selected Data: Selected instances from corpus.  Outputs\n None  Word Enrichment displays a list of words with lower p-values (higher significance) for a selected subset compared to the entire corpus. Lower p-value indicates a higher likelihood that the word is significant for the selected subset (not randomly occurring in a text). FDR (False Discovery Rate) is linked to p-value and reports on the expected percent of false predictions in the set of predictions, meaning it account for false positives in list of low p-values.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Word Enrichment",
	"icon" : ""
},
{
    "uri": "/workflows/",
	"title": "Workflows",
	"description": "",
	"content": "",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "" ,
	"author" : "",
	"summary" : "",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "section",
	"type" : "workflows",
	"LinkTitle" : "Workflows",
	"icon" : ""
},
{
    "uri": "/widget-catalog/time-series/yahoo_finance/",
	"title": "Yahoo Finance",
	"description": "",
	"content": "Yahoo Finance Generate time series from Yahoo Finance stock market data.\nOutputs\n Time series: Time series table of open, high, low, close (OHLC) prices, volume and adjusted close price.  This widget fetches historical stock market data from Yahoo Finance and outputs it as a time series data table.\n Stock (e.g. GOOG) or index (e.g. DJI) symbol you are interested in. Date range you are interested in. Desired resolution of the time series. Can be one of: daily, weekly, monthly, or dividends. The last option outputs a table of dates when dividends were issued, along with their respective dividend amounts.  Example Since the output data type is inherently a Table, you can connect it to wherever a data table is expected. Naturally, you can use it to test some functions in the Timeseries add-on.\n",
	"image" : "",
	"thumbImage" : "",
	"shortExcerpt" : "",
	"longExcerpt" :  "Yahoo Finance Generate time series from Yahoo Finance stock market data.\nOutputs\n Time series: Time series table of open, high, low, close (OHLC) prices, volume and adjusted close price.  This widget fetches historical stock market data from Yahoo Finance and outputs it as a time series data table.\n Stock (e.g. GOOG) or index (e.g. DJI) symbol you are interested in. Date range you are interested in. Desired resolution of the time series." ,
	"author" : "",
	"summary" : "Yahoo Finance Generate time series from Yahoo Finance stock market data.\nOutputs\n Time series: Time series table of open, high, low, close (OHLC) prices, volume and adjusted close price.  This widget fetches historical stock market data from Yahoo Finance and outputs it as a time series data table.\n Stock (e.g. GOOG) or index (e.g. DJI) symbol you are interested in. Date range you are interested in. Desired resolution of the time series.",
	"date" : "Jan 1, 0001",
	"frontPageImage" :"",
	"images" : null,
	"image" : "",
	"kind" : "page",
	"type" : "widget-catalog",
	"LinkTitle" : "Yahoo Finance",
	"icon" : ""
},
 {} ]
