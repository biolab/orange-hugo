<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Classification on Orange</title>
    <link>/workflows/Classification/</link>
    <description>Recent content in Classification on Orange</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/workflows/Classification/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Classification Tree</title>
      <link>/workflows/tree-scatterplot/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/tree-scatterplot/</guid>
      <description>This workflow combines the interface and visualization of classification trees with scatter plot. When both the tree viewer and the scatter plot are open, selection of any node of the tree sends the related data instances to scatter plot. In the workflow, the selected data is treated as a subset of the entire dataset and is highlighted in the scatter plot. With simple combination of widgets we have constructed an interactive classification tree browser.</description>
    </item>
    
    <item>
      <title>Train and Test Data</title>
      <link>/workflows/data-sampler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/data-sampler/</guid>
      <description>In building predictive models it is important to have a separate train and test data sets in order to avoid overfitting and to properly score the models. Here we use Data Sampler to split the data into training and test data, use training data for building a model and, finally, test on test data. Try several other classifiers to see how the scores change.</description>
    </item>
    
    <item>
      <title>Cross Validation</title>
      <link>/workflows/cross-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/cross-validation/</guid>
      <description>How good are supervised data mining methods on your classification dataset? Here&amp;rsquo;s a workflow that scores various classification techniques on a dataset from medicine. The central widget here is the one for testing and scoring, which is given the data and a set of learners, does cross-validation and scores predictive accuracy, and outputs the scores for further examination.</description>
    </item>
    
    <item>
      <title>Where Are Misclassifications</title>
      <link>/workflows/where-are-misclassifications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/where-are-misclassifications/</guid>
      <description>Cross-validation of, say, logistic regression can expose the data instances which were misclassified. There are six such instances for iris dataset and ridge-regularized logistic regression. We can select different types of misclassification in Confusion Matrix and highlight them in the Scatter Plot. No surprise: the misclassified instances are close to the class-bordering regions in the scatter plot projection.</description>
    </item>
    
    <item>
      <title>Text Classification</title>
      <link>/workflows/text-classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-classification/</guid>
      <description>We can use predictive models to classify documents by authorship, their type, sentiment and so on. In this workflow we classify documents by their Aarne-Thompshon-Uther index, that is the defining topic of the tale. We use two simple learners, Logistic Regression and Naive Bayes, both of which can be inspected in the Nomogram.</description>
    </item>
    
  </channel>
</rss>