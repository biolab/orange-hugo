<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fairness on Orange</title><link>/workflows/Fairness/</link><description>Recent content in Fairness on Orange</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/workflows/Fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Dataset Bias Examination</title><link>/workflows/fairness-dataset-bias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-dataset-bias/</guid><description>Understanding the potential biases within datasets is crucial for fair machine-learning outcomes. This workflow detects dataset bias using a straightforward algorithm. After loading the dataset, we add specific fairness attributes to it, which are essential for our calculations. We then compute the fairness metrics via the Dataset Bias widget and explain the results in a Box Plot.</description></item><item><title>Reweighing a Dataset</title><link>/workflows/fairness-reweighing-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-reweighing-dataset/</guid><description>Detecting bias is only the first step in ensuring fair machine learning. The next step is to mitigate the bias. This workflow illustrates removing bias at the dataset level using the Reweighing widget. Initially, split the data into training and validation subsets. We then check for bias in the validation set before reweighing. Using the training set, we train the reweighing algorithm and apply it to the validation set. Finally, we check for bias in the reweighed validation set.</description></item></channel></rss>