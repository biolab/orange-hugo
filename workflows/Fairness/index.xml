<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fairness on Orange</title><link>/workflows/Fairness/</link><description>Recent content in Fairness on Orange</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/workflows/Fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Dataset Bias Examination</title><link>/workflows/fairness-dataset-bias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-dataset-bias/</guid><description>Understanding the potential biases within datasets is crucial for fair machine-learning outcomes. This workflow detects dataset bias using a straightforward algorithm. After loading the dataset, we add specific fairness attributes to it, which are essential for our calculations. We then compute the fairness metrics via the Dataset Bias widget and explain the results in a Box Plot.</description></item><item><title>Reweighing a Dataset</title><link>/workflows/fairness-reweighing-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-reweighing-dataset/</guid><description>Detecting bias is only the first step in ensuring fair machine learning. The next step is to mitigate the bias. This workflow illustrates removing bias at the dataset level using the Reweighing widget. Initially, split the data into training and validation subsets. We then check for bias in the validation set before reweighing. Using the training set, we train the reweighing algorithm and apply it to the validation set. Finally, we check for bias in the reweighed validation set.</description></item><item><title>Reweighing as a preprocessor</title><link>/workflows/fairness-reweighing-preprocessor/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-reweighing-preprocessor/</guid><description>We can use the reweighing fairness algorithm for more than just adding weights to a dataset. It can also be used as a preprocessor for a specific model. This workflow illustrates how to use the Reweighing widget as a preprocessor for the Logistic Regression model. Initially, we load the dataset and input it into the Test and Score widget, which we will use to evaluate our model. Now, we need to connect a Logistic Regression model, which has the Reweighing widget as a preprocessor, to the Test and Score widget.</description></item><item><title>Adversarial Debiasing</title><link>/workflows/fairness-adversarial-debiasing/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-adversarial-debiasing/</guid><description>The easiest method to address bias in machine learning is to use a bias-aware model. This approach eliminates the need for fairness preprocessing or postprocessing. In this workflow, we will employ a bias-aware model named Adversarial Debasing for classification. We will train two versions of this model: one with and one without debiasing. Finally, we will compare and display the fairness metrics using a box plot widget.</description></item></channel></rss>