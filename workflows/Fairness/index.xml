<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Fairness on Orange</title><link>/workflows/Fairness/</link><description>Recent content in Fairness on Orange</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><atom:link href="/workflows/Fairness/index.xml" rel="self" type="application/rss+xml"/><item><title>Dataset Bias Examination</title><link>/workflows/fairness-dataset-bias/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/workflows/fairness-dataset-bias/</guid><description>Understanding the potential biases within datasets is crucial for fair machine-learning outcomes. This workflow detects dataset bias using a straightforward algorithm. After loading the dataset, we add specific fairness attributes to it, which are essential for our calculations. We then compute the fairness metrics via the Dataset Bias widget and explain the results in a Box Plot.</description></item></channel></rss>