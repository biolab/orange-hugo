<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Predictive models on Orange</title>
    <link>/workflows/Predictive-models/</link>
    <description>Recent content in Predictive models on Orange</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    
	<atom:link href="/workflows/Predictive-models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Train and Test Data</title>
      <link>/workflows/data-sampler/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/data-sampler/</guid>
      <description>In building predictive models it is important to have a separate train and test data sets in order to avoid overfitting and to properly score the models. Here we use Data Sampler to split the data into training and test data, use training data for building a model and, finally, test on test data. Try several other classifiers to see how the scores change.</description>
    </item>
    
    <item>
      <title>Cross Validation</title>
      <link>/workflows/cross-validation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/cross-validation/</guid>
      <description>How good are supervised data mining methods on your classification dataset? Here&amp;rsquo;s a workflow that scores various classification techniques on a dataset from medicine. The central widget here is the one for testing and scoring, which is given the data and a set of learners, does cross-validation and scores predictive accuracy, and outputs the scores for further examination.</description>
    </item>
    
  </channel>
</rss>