<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Text Mining on Orange</title>
    <link>/workflows/Text-Mining/</link>
    <description>Recent content in Text Mining on Orange</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language><atom:link href="/workflows/Text-Mining/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Text Preprocessing</title>
      <link>/workflows/text-preprocessing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-preprocessing/</guid>
      <description>Text mining requires careful preprocessing. Here&amp;rsquo;s a workflow that uses simple preprocessing for creating tokens from documents. First, it applies lowercase, then splits text into words, and finally, it removes frequent stopwords. Preprocessing is language specific, so change the language to the language of texts where required. Results of preprocessing can be observe in a Word Cloud.</description>
    </item>
    
    <item>
      <title>Text Clustering</title>
      <link>/workflows/text-clustering/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-clustering/</guid>
      <description>The workflow clusters Grimm&amp;rsquo;s tales corpus. We start by preprocessing the data and constructing the bag of words matrix. Then we compute cosine distances between documents and use Hierarchical Clustering, which displays the dendrogram. We observe how well the type of the tale corresponds to the cluster in the MDS.</description>
    </item>
    
    <item>
      <title>Text Classification</title>
      <link>/workflows/text-classification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-classification/</guid>
      <description>We can use predictive models to classify documents by authorship, their type, sentiment and so on. In this workflow we classify documents by their Aarne-Thompshon-Uther index, that is the defining topic of the tale. We use two simple learners, Logistic Regression and Naive Bayes, both of which can be inspected in the Nomogram.</description>
    </item>
    
    <item>
      <title>Twitter Data Analysis</title>
      <link>/workflows/twitter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/twitter/</guid>
      <description>Tweets are a valuable source of information, for social scientists, marketing managers, linguists, economists, and so on. In this workflow we retrieve data from Twitter, preprocess it, and uncover latent topics with topic modeling. We observe the topics in a Heat Map.</description>
    </item>
    
    <item>
      <title>Story Arcs</title>
      <link>/workflows/story-arcs/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/story-arcs/</guid>
      <description>In this workflow we explore story arcs in the Little Match Seller story. First we select the story from the corpus of Andersen tales. Then we create a table, where each sentence of the tale is a separate row. We use sentiment analysis to compute the sentiment of each sentence, then observe the emotional arcs through the story. We also inspect sentences with similar scores in the Heat Map and Corpus Viewer.</description>
    </item>
    
    <item>
      <title>Load Text Corpus from the Server Repository</title>
      <link>/workflows/text-loading-from-repository/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-loading-from-repository/</guid>
      <description>The workflow loads the corpus from the text repository on the server. The repository contains documents with raw text and associated YAML files with meta-features. We here use some pre-processing and then display the most frequent words in a word cloud. This workflow could work on your repository: just change the URL in the Import Documents widget.</description>
    </item>
    
    <item>
      <title>Semantic Document Map</title>
      <link>/workflows/text-semantic-document-map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-semantic-document-map/</guid>
      <description>Document maps may reveal clusters of documents with semantically similar content. Here we show a workflow that loads the corpus, performs some text preprocessing and embeds the documents in the vector space using the fastText deep model. The t-SNE widget reveals the document map, where we can select a set of documents and then explore them in Corpus Viewer or characterize them in the display of the most frequent words in the Word Cloud.</description>
    </item>
    
    <item>
      <title>Semantic Word Map</title>
      <link>/workflows/text-semantic-word-map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-semantic-word-map/</guid>
      <description>We can find clusters of semantically related words either by hierarchical clustering or t-SNE visualizations. Here, we show a workflow that loads the documents, extracts frequent words, embeds them in a vector space, and explores word clusters.</description>
    </item>
    
    <item>
      <title>Keyword Extraction from a Set of Text Documents</title>
      <link>/workflows/text-keyword-extraction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-keyword-extraction/</guid>
      <description>The Extract Keywords widget can characterize a set of textual documents. In this workflow, we load the documents from the server, preprocess them and embed them in the vector space, and display a semantic document map in the t-SNE widget. In this widget, we can select a set of similar documents and then characterize them through keyword extraction. Extract keywords support different inference techniques, including TF-IDF and deep network-based characterization.</description>
    </item>
    
    <item>
      <title>Keyword-Based Text Document Scoring</title>
      <link>/workflows/text-keyword-based-scoring/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-keyword-based-scoring/</guid>
      <description>We can score the text documents based on a list of keywords, say, to find the documents which include the keywords or are semantically related to the list of keywords. This workflow shows the Score Documents widget for scoring and the Word List widget to compose a list of keywords. The scores are visualized in the t-SNE document map.</description>
    </item>
    
    <item>
      <title>Corpus and Word Maps</title>
      <link>/workflows/text-corpus-and-word-map/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-corpus-and-word-map/</guid>
      <description>This workflow shows how to extract the most common words from the documents and observe clusters of semantically similar words with Hierarchical Clustering. We select a group of words (connected to the traffic and roads) and use them to score documents according to selection with the Score Documents widget. The scores are visualized in the document map by the Self-Organizing Maps widget.</description>
    </item>
    
    <item>
      <title>Document Map Annotation</title>
      <link>/workflows/text-annotator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-annotator/</guid>
      <description>Documents maps can be enhanced with the keywords annotations. This workflow embeds documents in vector space, computes a t-SNE document map and annotates it. The Annotator widget identifies clusters on the map and annotates them with keywords representing a cluster.</description>
    </item>
    
    <item>
      <title>Ontology Generation from Keywords</title>
      <link>/workflows/text-ontology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>/workflows/text-ontology/</guid>
      <description>We can automatically build the otology from the set of words. In the workflow, we select a group of documents with similar content. From the selected documents, we extract keywords and generate a new ontology from the subset of keywords with the Ontology widget.</description>
    </item>
    
  </channel>
</rss>
